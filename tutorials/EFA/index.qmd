---
title: "Exploratory Factor Analysis with R"
author: "W. Joel Schneider"
date: 2024-03-12
knitr: 
  opts_chunk: 
    dev: "ragg_png"
    out-width: "100%"
format: 
  html: 
    # css: "tutorial.css"
    smooth-scroll: true
    fig-height: 8
    fig-cap-location: margin
    reference-location: margin
    citation-location: margin
    code-annotations: hover
    html-math-method: katex
    fig-width: 8
    toc: true
    toc-float: true
    toc-location: left
bibliography: ../../bibliography.bib
csl: apa.csl
---

```{r setup}
#| include: false
library(tidyverse)
library(simstandard)
library(GPArotation)
library(psych)
library(easystats)
read_csv <- function(...) {
  readr::read_csv(..., show_col_types = FALSE)
}
options(knitr.kable.NA = '', digits = 4)
bg_color <- "#ECE5D3"
fore_color <- "#2D2718"
line_color <- "#7C4728"
area_color <- "#624E3E"
myfont <- "Roboto Condensed"
myfontsize <- 16

CurrentQuestion <- 0

inc_num <- function() {
  CurrentQuestion <<- CurrentQuestion + 1
  CurrentQuestion
  }

show_answer <- FALSE

```

# Load Packages

The following code will install the tidyverse, GPArotation, and psych packages if they are not installed already.

```{r installpackages}
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("GPArotation")) install.packages("GPArotation")
if (!require("psych")) install.packages("psych")
if (!require("psych")) install.packages("easystats")
```



```{r loadpackages}

library(tidyverse)
library(GPArotation)
library(psych)
library(easystats)
```

# Data Import

```{r makedata}
#| eval: false
#| echo: false
d_bfi <- bfi %>% select(A1:O5) %>% select(-O2) %>% as_tibble() 
colnames(d_bfi) <- bfi.dictionary$Item[1:25][-22]
fa.parallel(d_bfi, fm = "pa")
fa(d_bfi, nfactors = 6, fm = "pa") %>% fa.sort()

write_csv(d_bfi, path = "HW4_BFI.csv")

```

Let's import a version of a data set from the psych package altered to make it suitable for an assignment like this. It consists of personality test items from the International Personality Item Pool [@goldbergBroadbandwidthPublicDomain1999]. The data were collected as a part of the SAPA project [@revellePsychProceduresPsychological2015].

```{r importdata}
#| messsage: false
d_bfi <- read_csv("https://github.com/wjschne/EDUC5529/raw/master/HW4_BFI.csv")
```

Here are the personality items:

```{r}
#| echo: false
#| tab-cap: Big Five Inventory Personality Items
#| label: tbl-bfi
tibble(Items = colnames(d_bfi)) %>% 
  knitr::kable(format = "html") %>% kableExtra::kable_styling(bootstrap_options = "striped")

```

# How many factors to extract?

In one line of code, we can get information we need to apply the Kaiser-Guttman rule, the scree plot rule, and parallel analysis. For the personality data:

```{r showparallel}
fa.parallel(d_bfi)
```

-   There are 6 eigenvalues above 1 (from the PC series with the Xs), so the Kaiser-Guttman rule says that we should select 6 factors.
-   There are 6 eigenvalues above the scree in the PC series. So the scree plot rule says to extract 6 factors.
-   The principal components version of parallel analysis says to extract 5 factors. This method works best if the factors have weak correlations. Thus, if the factors are weakly correlated (\< 0.3 or so), we should extract 5 factors.
-   The factor analysis version of parallel analysis says to extract 6 factors (the FA series with the triangles). If the factors are strongly correlated (\> 0.3 or so), we should extract 6 factors.

Here is the same plot, but rendered to be more easily interpreted:


```{r polishedparallel}
#| echo: false
quiet <- function(x) { 
  sink(tempfile()) 
  on.exit(sink()) 
  invisible(force(x)) 
} 

pa <- quiet(fa.parallel(d_bfi,plot = F))

tibble(
  Number = 1:length(pa$pc.values),
  `Observed_Principal Components` = pa$pc.values,
  `Observed_Factor Analysis` = pa$fa.values,
  `Simulated_Principal Components` = pa$pc.sim,
  `Simulated_Factor Analysis` = pa$fa.sim
) %>%
  pivot_longer(-Number,
               names_pattern = "(.*)_(.*)",
               names_to = c(".value", "Series")) %>%
  arrange(Series, Number) %>%
  mutate(isgreater = Observed > Simulated) %>%
  mutate(countgreater = sum(1 * isgreater), .by = Series) %>% 
  mutate(
    lastgreater = ifelse(
      isgreater &
        !lead(isgreater),
      paste0(countgreater, 
             " observed eigenvalue", 
             ifelse(countgreater > 1, "s", ""), 
             "<br>above simulated data"),
      NA
    ),
    .by = Series
  ) %>%
  select(-countgreater) %>%
  ggplot(aes(Number, Observed, color = Series)) +
  geom_line(alpha = .3) +
  geom_point() +
  geomtextpath::geom_labelpath(
    aes(y = Simulated, label = "Simulated"),
    linetype = "dashed",
    hjust = .9,
    straight = T,
    boxcolour = NA,
    family = "Roboto Condensed", 
    size = 5
  ) +
  ggtext::geom_richtext(
    data = . %>% filter(!is.na(lastgreater)),
    aes(label = lastgreater),
    label.color = NA,
    vjust = 0,
    hjust = 0,
    label.margin = margin(b = 2, l = 2),
    show.legend = F,
    size = 5,
    family = "Roboto Condensed"
  ) +
  facet_wrap( ~ Series) +
  theme_light(base_family = "Roboto Condensed", base_size = 16) +
  theme(legend.position = "none") +
  scale_color_discrete(type = c("#461f6b", "#1e6119"))

```



# Conducting Exploratory Factor Analysis

There are many variants of factor analysis and each has its advantages and disadvantages. Remember, EFA is called "exploratory" for a reason. With EFA, there are no firm answers that we can stand behind forever and always and in every situation.

Let's extract 5 factors and then 6 factors. We are going to use the most traditional method of EFA by setting the factor method `fm` to "pa" (principal-axis factor analysis). By default, the most common method of rotation is used: oblimin rotation.

```{r showloadings}
fa(d_bfi, nfactors = 5, fm = "pa") %>% 
  parameters() 
```

The amount of output can be overwhelming. Let's simplify it by sorting the output by the strongest loadings in each factor and removing all loadings smaller than .2. 

```{r showsortedloadings}
fa(d_bfi, nfactors = 5, fm = "pa") %>% 
  # WJSmisc::plot_loading()
  parameters(sort = TRUE, threshold = .2) 
```


Also, we can see the patterns more easily in the data if we color table so that small loadings have faint colors. 


```{r displayloadings}
#| code-fold: true
fa(d_bfi, nfactors = 5, fm = "pa") %>%
  parameters(sort = TRUE) %>%
  mutate(Variable = fct_inorder(Variable) %>% 
           fct_rev()) %>%
  select(-Complexity, -Uniqueness) %>%
  pivot_longer(-Variable) %>%
  mutate(maxrow = max(abs(value)), 
         .by = Variable) %>%
  mutate(sortfactor = if_else(abs(value) == maxrow, 
                             true = name, 
                             false = NA)) %>%
  arrange(sortfactor, desc(maxrow)) %>%
  mutate(Variable = fct_inorder(Variable) %>% 
           fct_rev()) %>%
  ggplot(aes(name, Variable)) +
  geom_tile(aes(fill = value)) +
  geom_text(aes(label = WJSmisc::prob_label(value)),
            hjust = 1,
            nudge_x = .06) +
  theme_minimal(base_size = 14, 
                base_family = "Roboto Condensed") +
  theme(legend.position = "bottom", legend.key.width = unit(2,"cm")) +
  scale_fill_gradient2(NULL,
                       limits = c(-1, 1),
                       labels = WJSmisc::prob_label) +
  scale_x_discrete(NULL, expand = expansion(), 
                   position = c("top")) +
  scale_y_discrete(NULL, expand = expansion())


```





## Junk Factors

We want to make sure that each factor makes sense theoretically and that there are no "junk" factors. Junk factors group unrelated items that have no underlying theoretical construct that unites them. EFA is "exploratory" and sometimes it groups items together because of chance fluctuations in the correlations. Junk factors typically have low loadings on all items (\< 0.4 or so).

If you find a junk factor, you can leave it in and not interpret it or you can extract 1 factor fewer.

In this case, all the factors are interpretable:

* PA1 = Extraversion 
* PA2 = Neuroticism 
* PA3 = Conscientiousness 
* PA4 = Openness 
* PA5 = Agreeableness

## Singleton Factors

We also worry about "singleton" factors. These have a high loading (\>0.5 or so) on a single item and all other loadings are small (\<0.3 or so). EFA is for finding factors that explain variability across items. Singleton factors do not do this. When your solution has a singleton factor, it is generally best to extract 1 factor fewer. However, doing so does not always get rid of the singleton factor. It might collapse two legitimate factors into one factor. In this case, you probably want to return to the previous solution with the singleton (or exclude the problematic item from the analysis).

## Factor correlations

The correlation matrrix of the latent factors in a an EFA is sometimes referred to as the &Phi; (Phi) matrix. To extract the factor correlations from the `fa` function's output, we can do like so:

```{r extractphi}
fa(d_bfi, nfactors = 5, fm = "pa")$Phi
```

Plotted, the &Phi; matrix look like this:

```{r showphi}
#| code-fold: true

factor_labels <- c("Extraversion", "Neuroticism", "Conscientiousness", "Openness", "Agreeableness")

fa(d_bfi, nfactors = 5, fm = "pa")$Phi %>% 
  corrr::as_cordf(diagonal = 1) %>% 
  corrr::stretch() %>% 
  mutate(x = factor(x, labels = factor_labels),
         y = factor(y, labels = factor_labels)) %>% 
  ggplot(aes(x,y)) + 
  geom_tile(aes(fill = r)) + 
  geom_text(aes(label = WJSmisc::prob_label(r)), family  = "Roboto Condensed", hjust = 1, nudge_x = .1, size.unit = "pt", size = 12) + 
  scale_fill_gradient2(limits = c(-1,1), labels = \(x) WJSmisc::prob_label(x, .1), breaks = seq(-1,1,.1)) + 
  theme_minimal(base_family = "Roboto Condensed", base_size = 12) + 
  theme(legend.key.height = unit(1, 
                                 units = "null"), 
        legend.text = element_text(hjust = 1)) +
  scale_x_discrete(NULL, expand = expansion()) + 
  scale_y_discrete(NULL, expand = expansion()) +
  coord_equal()

```

The correlations among these factors are generally low (\< 0.3 or so). Thus, the principal components method of parallel analysis is likely more accurate than the factor analysis version of parallel analysis. Thus, we should probably stick with 5 factors.

However, we should probably extract 6 factors and see what we get. After all, this is exploratory work.

```{r show6factors}
fa(d_bfi, nfactors = 6, fm = "pa") %>% 
  fa.sort() %>% 
  print(cut = 0.2)
```

The output does not all fit so the loadings for PA6 are below the loadings of the other factors.

Notice that all the PA6 loadings are small. Also notice that they do not really make sense to group together. For example, what construct causes people to be "indifferent to the feelings of others" and at the same time "Make people feel at ease"?

Two hypotheses:

-   Read in the right manner, all of the items are consistent with a person who uses charm and guile to manipulate others.
-   Four of the six items have words or phrases that require a higher reading level (indifferent, probe, captivate, half-way manner). Perhaps the factor emerged because of differences in literacy.

Both these hypotheses might be true. They are not mutually exclusive.

However, given that PA6 has no strong loadings, it is a weak factor and possibly a junk factor. Furthermore, our parallel analyses suggested we retain 5 factors, not 6.

# Exercise questions

The second data file is a questionnaire generated to capture the kinds of responses that beginning elementary school teachers give when asked what they do when the class's attention starts to wander and the teacher needs to redirect the class to get back on task or pay attention to the teacher.

```{r datacreation}
#| echo: false
#| eval: false
#| include: false
set.seed(9)
d_items <- data.frame(stringsAsFactors = FALSE,
   Description = c("I make my lessons extremely organized.",
                   "I look at the students sternly.",
                   "I point out the children who are disruptive.",
                   "I remind the class about the rules on the wall.", "I raise my voice.",
                   "I express my disappointment in the children.",
                   "I change the subject to something more interesting.", "I act more enthusiastic.",
                   "I engage distracted children with questions.", "I insist on silence.",
                   "I wait until the children are quiet.",
                   "I use a ritual the children know to get the children's attention.",
                   "I let the class get out of hand.",
                   "I talk about why the activity is more interesting than they think.",
                   "I become visibly distressed.",
                   "I chat with students about things they are interested instead."),
          Item = c("Item_1", "Item_2", "Item_3", "Item_4", "Item_5", "Item_6",
                   "Item_7", "Item_8", "Item_9", "Item_10", "Item_11",
                   "Item_12", "Item_13", "Item_14","Item_15","Item_16"),
             N = c(0, 0, 0, 0, 0.4, 0.7, 0, 0, 0, 0, 0, 0, 0.4, 0,.6,0),
             E = c(0, 0, -0.2, 0, 0, 0, 0, 0.8, 0.4, 0, 0, 0, 0, 0.1,0,.7),
             O = c(0, 0, 0, 0, 0, 0, 0.7, 0, 0.6, 0, 0, 0, 0, 0.7,0,0),
             A = c(0, -0.5, -0.4, -0.1, -0.6, 0, 0.4, 0.4, 0.3, -0.4, 0.8, 0,
                   0.4, 0.3,0,0),
             C = c(0.7, 0.1, 0.3, 0.7, 0, 0, 0.4, 0, 0, 0.4, 0, 0.7, 0, 0,0,0)
)
S <- diag(5)
rownames(S) <- colnames(S) <-colnames(d_items)[c(-1,-2)]
S["E","A"] <- 0.3
S["A","E"] <- 0.3
S["E","O"] <- 0.2
S["O","E"] <- 0.2


A <- d_items %>% select(N:C) %>% as.matrix
rownames(A) <- d_items$Item
d <- simstandard::matrix2lavaan(measurement_model = A, covariances = S) %>% sim_standardized(latent = F, errors = F) %>% select(Item_1, Item_2, Item_3, Item_4,Item_5, Item_6,Item_7, Item_8,Item_9, Item_10,Item_11, Item_12,Item_13, Item_14, Item_15, Item_16)

colnames(d) <- d_items$Description

fa.parallel(d)
fa(d, 4, fm = "pa") %>% fa.sort() %>% print(cut = 0.2)
fa(d, 5, fm = "pa") %>% fa.sort() %>% print(cut = 0.2)
readr::write_csv(x = d, path = "HW4_redirect.csv")
```

```{r redirectimport}
d_redirect <- read_csv("https://github.com/wjschne/EDUC5529/raw/master/HW4_Redirect.csv")
```

Here are the questionnaire items for teacher redirection strategies:

```{r showredirectitems}
tibble(Items = colnames(d_redirect)) %>% 
  knitr::kable(caption = "Table 2. Teacher redirection strategies",format = "html") %>% 
  kableExtra::kable_styling(kable_input = ., bootstrap_options = "striped")
```

:::{.instruction}
Run a parallel analysis on the `d_redirect` data with the `fa.parallel` function and answer the next 4 questions.
:::

:::{.question}
**Question `r inc_num()`**: How many factors should be retained according to the Kaiser-Guttman rule (i.e., the number of eigenvalues from the PC Actual Data greater than 1)?
:::

:::{.question}
**Question `r inc_num()`**: According to the scree plot rule, how many factors should be retained? (This one is a little subjective, and there are several candidates that people might see as above the scree. Thus, I'll offer 3 wildly unlikely answers and one choice with all the places I perceive might be places above the scree.)
:::

:::{.question}
**Question `r inc_num()`**: According to the factor analysis version of parallel analysis, how many factors should be retained? (Go by what the text output says. Sometimes the plot can appear a bit off.)
:::

:::{.question}
**Question `r inc_num()`**: According to the principal components analysis version of parallel analysis, how many factors should be retained? (Go by what the text output says. Sometimes the plot can appear a bit off.)
:::



:::{.instruction}
Run an EFA on the `d_redirect` data extracting 4 factors using the `fm`= "pa" method.
:::

```{r peekatanswers}
#| eval: false
#| echo: false
fa.parallel(d_redirect)

fa(d_redirect, 4, fm = "pa") %>% 
  fa.sort() %>% 
  print(cut = 0.2)

fa(d_redirect, 5, fm = "pa") %>% 
  fa.sort() %>% 
  print(cut = 0.2)
```

:::{.question}
**Question `r inc_num()`**: When you extract 4 factors, are there any singleton factors (i.e,. factors with only one loading with an absolute value higher than 0.5 or so and all other loadings are low (\< .3 or so)?
:::

:::{.question}
**Question `r inc_num()`**: When you extract 4 factors, are there any of the factors strongly correlated (\>0.3 or so)?
:::

:::{.instruction}
Run an EFA on the `d_redirect` data extracting 5 factors using the `fm`= "pa" method.
:::

:::{.question}
**Question `r inc_num()`**: When you extract 5 factors, are there any singleton factors (i.e,. factors with only one loading with an absolute value higher than 0.5 or so and all other loadings are low (\< .3 or so)?
:::

:::{.question}
**Question `r inc_num()`**: When you extract 5 factors, are there any of the factors strongly correlated (\>0.3 or so)?
:::

:::{.question}
**Question `r inc_num()`**: According to the guidelines given in this tutorial, should you extract 4 or 5 factors?
:::

:::{.question}
**Question `r inc_num()`**: What do you think each factor might be measuring? Feel free to speculate.
:::
