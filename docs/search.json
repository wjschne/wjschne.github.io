[
  {
    "objectID": "vita.html#w.-joel-schneider",
    "href": "vita.html#w.-joel-schneider",
    "title": "Curriculum Vitae",
    "section": "W. Joel Schneider",
    "text": "W. Joel Schneider\n\n\nTemple University\nCollege of Education & Human Development\nPsychological Studies in Education\n493 Ritter Hall\nPhiladelphia, PA 19122-6091\n\nOffice: (215) 204-8093\nEmail: schneider@temple.edu\nWeb: Faculty Profile\nBlog: AssessingPsyche"
  },
  {
    "objectID": "vita.html#publication-indices",
    "href": "vita.html#publication-indices",
    "title": "Curriculum Vitae",
    "section": "Publication Indices",
    "text": "Publication Indices\n\nGoogle Scholar Citations: 4083\nh-index: 25\ni10-index: 39\n\n\n\n\n\n\nGoogle Scholar Citations of My Work"
  },
  {
    "objectID": "vita.html#books",
    "href": "vita.html#books",
    "title": "Curriculum Vitae",
    "section": "Books",
    "text": "Books\n\nCohen, R. J., Schneider, W. J., & Tobin, R. M. (2022). Psychological testing and assessment: An introduction to tests and measurement (10th ed.). McGraw Hill LLC.\n\n\nSchneider, W. J., Mather, N., Lichtenberger, E. O., & Kaufman, N. L. (2018). Essentials of assessment report writing (2nd ed.). Wiley."
  },
  {
    "objectID": "vita.html#journal-articles",
    "href": "vita.html#journal-articles",
    "title": "Curriculum Vitae",
    "section": "Journal Articles",
    "text": "Journal Articles\n\nSchneider, W. J., Flanagan, D. P., Niileksela, C. R., & Engler, J. R. (2024). The effect of measurement error on the positive predictive value of PSW methods for SLD identification: How buffer zones dispel the illusion of inaccuracy. Journal of School Psychology, 103, . https://doi.org/10.1016/j.jsp.2023.101280\n\n\nMcGrew, K. S., Schneider, W. J., Decker, S. L., & Bulut, O. (2023). A psychometric network analysis of CHC intelligence measures: Implications for research, theory, and interpretation of broad CHC scores “beyond g”. Journal of Intelligence, 11(1), 19. https://doi.org/10.3390/jintelligence11010019 \n\n\nSchneider, W. J., & Ji, F. (2023). Detecting unusual score patterns in the context of relevant predictors. Journal of Pediatric Neuropsychology, 9, 1–17. https://doi.org/10.1007/s40817-022-00137-x \n\n\nDowdy, A., Peltier, C., Tincani, M., Schneider, W. J., Hantula, D. A., & Travers, J. C. (2021). Meta‐analyses and effect sizes in applied behavior analysis: A review and discussion. Journal of Applied Behavior Analysis, 54(4), 1317–1340. https://doi.org/10.1002/jaba.862 \n\n\nDowdy, A., Tincani, M., & Schneider, W. J. (2020). Evaluation of publication bias in response interruption and redirection: A meta‐analysis. Journal of Applied Behavior Analysis, 53(4), 2151–2171. https://doi.org/10.1002/jaba.724 \n\n\nHajovsky, D. B., Villeneuve, E. F., Schneider, W. J., & Caemmerer, J. M. (2020). An alternative approach to cognitive and achievement relations research: An introduction to quantile regression. Journal of Pediatric Neuropsychology, 6, 83–95. https://doi.org/10.1007/s40817-020-00086-3 \n\n\nDombrowski, S. C., Beaujean, A. A., McGill, R. J., Benson, N. F., & Schneider, W. J. (2019). Using exploratory bifactor analysis to understand the latent structure of multidimensional psychological measures: An example featuring the WISC-V. Structural Equation Modeling: A Multidisciplinary Journal, 26(6), 847–860. https://doi.org/10.1080/10705511.2019.1622421 \n\n\nSchneider, W. J., & McGrew, K. S. (2019). Process Overlap Theory is a milestone achievement among intelligence theories. Journal of Applied Research in Memory and Cognition, 8(3), 273–276. https://doi.org/https://doi.org/10.1016/j.jarmac.2019.06.006 \n\n\nSchneider, W. J., & Roman, Z. (2018). Fine-tuning Cross-Battery Assessment procedures: After follow-up testing, use all valid scores, cohesive or not. Journal of Psychoeducational Assessment, 36(1), 34–54. https://doi.org/10.1177/0734282917722861 \n\n\nMagoon, M. A., Critchfield, T. S., Merrill, D., Newland, M. C., & Schneider, W. J. (2017). Are positive and negative reinforcement “different”? Insights from a free-operant differential outcomes effect. Journal of the Experimental Analysis of Behavior, 107(1), 39–64. https://doi.org/10.1002/jeab.243 \n\n\nSchneider, W. J., & Kaufman, A. S. (2017). Let’s not do away with comprehensive cognitive assessments just yet. Archives of Clinical Neuropsychology, 32(1), 8–20. https://doi.org/10.1093/arclin/acw104 \n\n\nFlanagan, D. P., & Schneider, W. J. (2016). Cross-Battery Assessment? XBA PSW? A case of mistaken identity: A commentary on Kranzler and colleagues’ “Classification agreement analysis of Cross-Battery Assessment in the identification of specific learning disorders in children and youth”. International Journal of School & Educational Psychology, 4(3), 137–145. https://doi.org/10.1080/21683603.2016.1192852 \n\n\nGadke, D. L., Tobin, R. M., & Schneider, W. J. (2016). Agreeableness, conflict resolution tactics, and school behavior in second graders. Journal of Individual Differences, 37, 145–151. https://doi.org/10.1027/1614-0001/a000199 \n\n\nSchneider, W. J., & Kaufman, A. S. (2016). Commentary on current practices and future directions for the assessment of child and adolescent intelligence in schools around the world. International Journal of School & Educational Psychology, 4(4), 283–288. https://doi.org/10.1080/21683603.2016.1206383 \n\n\nSchneider, W. J., Mayer, J. D., & Newman, D. A. (2016). Integrating hot and cool intelligences: Thinking broadly about broad abilities. Journal of Intelligence, 4(1), 1:1–25. https://doi.org/10.3390/jintelligence4010001 \n\n\nSchneider, W. J., & Newman, D. A. (2015). Intelligence is multidimensional: Theoretical review and implications of specific cognitive abilities. Human Resource Management Review, 25(1), 12–27. https://doi.org/10.1016/j.hrmr.2014.09.004 \n\n\nAbney, D. H., Wagman, J. B., & Schneider, W. J. (2014). Changing grasp position on a wielded object provides self-training for the perception of length. Attention, Perception, & Psychophysics, 76(1), 247–254. https://doi.org/10.3758/s13414-013-0550-x \n\n\nPornprasertmanit, S., & Schneider, W. J. (2014). Accuracy in parameter estimation in cluster randomized designs. Psychological Methods, 19(3), 356–379. https://doi.org/10.1037/a0037036 \n\n\nHerbstrith, J. C., Tobin, R. M., Hesson-McInnis, M. S., & Schneider, W. J. (2013). Preservice teacher attitudes toward gay and lesbian parents. School Psychology Quarterly, 28(3), 183–194. https://doi.org/10.1037/spq0000022 \n\n\nKahn, J. H., & Schneider, W. J. (2013). It’s the destination and it’s the journey: Using multilevel modeling to assess patterns of change in psychotherapy. Journal of Clinical Psychology, 69(6), 543–570. https://doi.org/10.1002/jclp.21964 \n\n\nSchneider, W. J. (2013). What if we took our models seriously? Estimating latent scores in individuals. Journal of Psychoeducational Assessment, 31(2), 186–201. https://doi.org/10.1177/0734282913478046 \n\n\nDecker, S. L., Schneider, W. J., & Hale, J. B. (2012). Estimating base rates of impairment in neuropsychological test batteries: A comparison of quantitative models. Archives of Clinical Neuropsychology, 7(1), 69–84. https://doi.org/10.1093/arclin/acr088 \n\n\nJones, G., & Schneider, W. J. (2010). IQ in the production function: Evidence from immigrant earnings. Economic Inquiry, 48(3), 743–755. https://doi.org/10.1111/j.1465-7295.2008.00206.x \n\n\nGuidry, J. A., Babin, B. J., Graziano, W. G., & Schneider, W. J. (2009). Pride and prejudice in the evaluation of wine? International Journal of Wine Business Research, 21(4), 298–311. https://doi.org/10.1108/17511060911004888 \n\n\nHoff, K. E., Reese-Weber, M., Schneider, W. J., & Stagg, J. W. (2009). The association between high status positions and aggressive behavior in early adolescence. Journal of School Psychology, 47(6), 395–426. https://doi.org/10.1016/j.jsp.2009.07.003 \n\n\nBarr, L. K., Kahn, J. H., & Schneider, W. J. (2008). Individual differences in emotion expression: Hierarchical structure and relations with psychological distress. Journal of Social and Clinical Psychology, 27(10), 1045–1077. https://doi.org/10.1521/jscp.2008.27.10.1045 \n\n\nKahn, J. H., Vogel, D. L., Schneider, W. J., Barr, L. K., & Herrell, K. (2008). The emotional content of client disclosures and session impact: An analogue study. Psychotherapy: Theory, Research, Practice, Training, 45(4), 539–545. https://doi.org/10.1037/a0014337 \n\n\nSchneider, W. J. (2008). Playing statistical Ouija board with commonality analysis: good questions, wrong assumptions. Applied Neuropsychology, 15(1), 44–53. https://doi.org/10.1080/09084280801917566 \n\n\nJones, G., & Schneider, W. J. (2006). Intelligence, human capital, and economic growth: A Bayesian Averaging of Classical Estimates (BACE) approach. Journal of Economic Growth, 11(1), 71–93. https://doi.org/10.2139/ssrn.552481 \n\n\nSchneider, W. J., Timothy Cavell, A., & Hughes, J. N. (2003). A sense of containment: Potential moderator of the relation between parenting practices and children’s externalizing behaviors. Development and Psychopathology, 15(1), 95–117. https://doi.org/10.1017/S0954579403000063"
  },
  {
    "objectID": "vita.html#chapters",
    "href": "vita.html#chapters",
    "title": "Curriculum Vitae",
    "section": "Chapters",
    "text": "Chapters\n\nSchneider, W. J. (2022). Statistical and clinical interpretation guidelines for school neuropsychological assessment. In D. Miller, D. Maricle, C. Bedford, & J. Gettman (Eds.) Best Practices in School Neuropsychology: Guidelines for Effective Practice, Assessment, and Evidence‐Based Intervention (1st ed., pp. 163–184). Wiley. https://doi.org/10.1002/9781119790563\n\n\nFloyd, R. G., Farmer, R. L., Schneider, W. J., & McGrew, K. S. (2021). Theories and measurement of intelligence. In L. Glidden, L. Abbeduto, L. L. McIntyre, & M. J. Tassé (Eds.) APA handbook of intellectual and developmental disabilities (Vol. 1, pp. 386–424). American Psychological Association. https://doi.org/10.1037/0000194-015 \n\n\nKaufman, A. S., Schneider, W. J., & Kaufman, J. C. (2019). Psychometric approaches to intelligence. In R. J. Sternberg (Ed.) Human intelligence: An introduction (pp. 67–103). Cambridge University Press. \n\n\nSchneider, W. J., & McGrew, K. S. (2018). The Cattell-Horn-Carroll theory of cognitive abilities. In D. P. Flanagan, & E. M. McDonough (Eds.) Contemporary intellectual assessment: Theories, tests, and issues (4th ed., pp. 73–130). Guilford Press. \n\n\nSchneider, W. J. (2016). Case 1—Liam, age 9: Emotionally intelligent testing with the WISC-V and CHC theory. In A. S. Kaufman, S. Raiford, & D. Coalson (Eds.) Intelligent testing with the WISC-V (pp. 265–282). Wiley.\n\n\nSchneider, W. J. (2016). Strengths and weaknesses of the Woodcock-Johnson IV Tests of Cognitive Abilities: Best practice from a scientist-practitioner perspective. In D. P. Flanagan, & V. C. Alfonso (Eds.) WJ IV Clinical Use and Interpretation (pp. 191–210). Academic Press. https://doi.org/10.1016/B978-0-12-802076-0.00007-4 \n\n\nSchneider, W. J., & Flanagan, D. P. (2015). The relationship between theories of intelligence and intelligence tests. In S. Goldstein, D. Princiotta, & J. A. Naglieri (Eds.) Handbook of intelligence: Evolutionary theory, historical perspective, and current concepts (pp. 317–340). Springer. \n\n\nTobin, R. M., Schneider, W. J., & Landau, S. (2014). Best practices in the assessment of youth with attention deficit hyperactivity disorder within a multitiered services framework. In A. Thomas, & P. Harrison (Eds.) Best practices in school psychology: Data-based and collaborative decision making (pp. 391–404). National Association of School Psychologists. \n\n\nSchneider, W. J. (2013). Principles of assessment of aptitude and achievement. In D. Saklofske, C. Reynolds, & V. Schwean (Eds.) The Oxford Handbook of Child Psychological Assessment (pp. 286–330). Oxford University Press. \n\n\nSchneider, W. J., & McGrew, K. S. (2013). Cognitive performance models: Individual differences in the ability to process information. In B. Irby, G. Brown, R. Laro-Alecio, & S. Jackson (Eds.) Handbook of educational theories (pp. 767–782). Information Age Publishing. \n\n\nSchneider, W. J., & McGrew, K. S. (2012). The Cattell-Horn-Carroll model of intelligence. In D. P. Flanagan, & P. L. Harrison (Eds.) Contemporary intellectual assessment: Theories, tests, and issues (3rd ed., pp. 99–144). Guilford Press. \n\n\nTobin, R. M., Schneider, W. J., Reck, S. G., & Landau, S. (2008). Best practices in the assessment of children with attention deficit hyperactivity disorder: Linking assessment to response to intervention. In P. Harrison, & A. Thomas (Eds.) Best Practices in School Psychology V (Vol. 2, pp. 617–631). National Association of School Psychologists. \n\n\nSnyder, D. K., Schneider, W. J., & Castellani, A. M. (2003). Tailoring couple therapy to individual differences: A conceptual approach. In D. K. Snyder (Ed.) Treating difficult couples: Helping clients with coexisting mental and relationship disorders (pp. 27–51). Guilford Press. \n\n\nSnyder, D. K., & Schneider, W. J. (2002). Affective reconstruction: A pluralistic, developmental approach. In N. S. Jacobson (Ed.) Clinical handbook of couple therapy (3rd ed., pp. 151–179). Guilford Press."
  },
  {
    "objectID": "vita.html#test-reviews",
    "href": "vita.html#test-reviews",
    "title": "Curriculum Vitae",
    "section": "Test Reviews",
    "text": "Test Reviews\n\nSwerdlik, M. E., & Schneider, W. J. (2010). Review of the PsychProfiler. In R. A. Spies, J. F. Carlson, B. S. Plake, & K. T. Geisinger (Eds.) The eighteenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J. (2007). Review of the Dean-Woodcock Neuropsychological Battery. In K. T. Geisinger, R. A. Spies, J. F. Carlson, & B. S. Plake (Eds.) The seventeenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J. (2007). Review of the Multiple Intelligence Developmental Assessment Scales (MIDAS). In K. T. Geisinger, R. A. Spies, J. F. Carlson, & B. S. Plake (Eds.) The seventeenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J., & Swerdlik, M. E. (2007). Review of the Youth Outcome Questionnaire 30.1. In K. T. Geisinger, R. A. Spies, J. F. Carlson, & B. S. Plake (Eds.) The seventeenth mental measurements yearbook. Buros Institute.\n\n\nSwerdlik, M. E., & Schneider, W. J. (2007). Review of the Behavior Evaluation Scale, Third Edition. In K. T. Geisinger, R. A. Spies, J. F. Carlson, & B. S. Plake (Eds.) The seventeenth mental measurements yearbook. Buros Institute.\n\n\nHoff, K. E., & Schneider, W. J. (2005). Review of the Child Symptom Inventory-4. In B. S. Plake, J. C. Impara, & R. A. Spies (Eds.) The sixteenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J., & Hoff, K. E. (2005). Review of the Word Identification and Spelling Test. In B. S. Plake, J. C. Impara, & R. A. Spies (Eds.) The sixteenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J., & Swerdlik, M. E. (2005). Review of the Memory Test for Older Adults. In B. S. Plake, J. C. Impara, & R. A. Spies (Eds.) The sixteenth mental measurements yearbook. Buros Institute.\n\n\nSwerdlik, M. E., & Schneider, W. J. (2005). Review of the Behavioral and Emotional Rating Scale-Second Edition. In B. S. Plake, J. C. Impara, & R. A. Spies (Eds.) The sixteenth mental measurements yearbook. Buros Institute."
  },
  {
    "objectID": "vita.html#scholarly-reports",
    "href": "vita.html#scholarly-reports",
    "title": "Curriculum Vitae",
    "section": "Scholarly Reports",
    "text": "Scholarly Reports\n\nGoldrick-Rab, S., Richardson, J., Schneider, W. J., Hernandez, A., & Cady, C. (2018). Still hungry and homeless in college. The Wisconsin HOPE Lab. \n\n\nSchneider, W. J. (2016). The RESCA-E subtests are thoughtfully designed and highly refined measures of CHC constructs: A review of the Receptive, Expressive & Social Communication Assessment–Elementary. Assessing Psyche, Engaging Gauss, Seeking Sophia.\n\n\nSchneider, W. J. (2016). Why are WJ IV cluster scores more extreme than the average of their parts? A gentle explanation of the composite score extremity effect. Houghton Mifflin HarcourtWoodcock-Johnson IV Assessment Service Bulletin.\n\n\nSchneider, W. J., & McGrew, K. (2011). “Just say no” to averaging IQ subtest scores. IAP Applied Psychometrics 101 Report."
  },
  {
    "objectID": "vita.html#software",
    "href": "vita.html#software",
    "title": "Curriculum Vitae",
    "section": "Software",
    "text": "Software\n\nSchneider, W. J. (2023). apaquarto: A Quarto extension for creating APA 7 style documents. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/apaquarto\n\n\nSchneider, W. J. (2023). arrowheadr: Create custom arrowheads for ggplot2 via ggarrow.. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/arrowheadr\n\n\nSchneider, W. J. (2022). condppv: Conditional positive predictive value in the accuracy of specific learning disability identification. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/conditionalppv\n\n\nSchneider, W. J. (2022). UnusualProfile: A user-friendly web application to impliment functions from the unusualprofile package.. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/unusualprofile_app\n\n\nSchneider, W. J. (2021). Area under the normal curve. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/AreaUnderNormalCurve\n\n\nSchneider, W. J. (2021). psycheval: A psychological evaluation toolkit. [Software] AssessingPsyche.\n\n\nSchneider, W. J. (2021). Simple regression with standard scores. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/simple_regression\n\n\nSchneider, W. J. (2021). ztestvis: A Jamovi module for conducting a one-sample z-test. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/ztest\n\n\nSchneider, W. J., & Ji, F. (2021). unusualprofile: Calculate conditional Mahalanobis distances. [Software] AssessingPsyche. Retrieved from https://cran.r-project.org/package=unusualprofile\n\n\nSchneider, W. J. (2018). ggnormalviolin. [Software] AssessingPsyche. Retrieved from https://cran.r-project.org/package=ggnormalviolin\n\n\nSchneider, W. J. (2018). simstandard. [Software] AssessingPsyche. Retrieved from https://cran.r-project.org/package=simstandard\n\n\nSchneider, W. J. (2012). TableMaker. [Software] AssessingPsyche.\n\n\nSchneider, W. J. (2010). The Compositator 1.0.. [Software] WMF Press."
  },
  {
    "objectID": "vita.html#lectures",
    "href": "vita.html#lectures",
    "title": "Curriculum Vitae",
    "section": "Lectures",
    "text": "Lectures\n\nSchneider, W. J. (2024, February 26). Making survey tests psychometrically sound. [Lecture]. Leadership Education in Neurodevelopmental Disabilities Program, Children’s Hospital of Philadelphia.\n\n\nEngler, J. R., Schneider, W. J., Flanagan, D. P., & Niileksela, C. R. (2024, February 16). Improve {PSW} case conceptualization accuracy by taking measurement error weriously. [NA]. National Association of School Psychologists Annual Convention, New Orleans, LA. https://apps.nasponline.org/professional-development/convention/session-detail.aspx?id=28123\n\n\nSchneider, W. J. (2024, January 25). Confidently correct specific learning disability identification. [Webinar]. Comprehensive Assessment for Intervention, NA. https://caipsychs.com/conferences/\n\n\nSchneider, W. J. (2023, November 4). Using arrowheadr with ggarrow and ggplot2. [Webinar]. ggplot2 Extenders, NA.\n\n\nSchneider, W. J. (2023, September 27). Intelligence, critical thinking, and creativity. [Symposium]. In G. Alves (Moderator), Para Além dos Testes Tradicionais: Integrando Inovações à Avaliação de Criatividade e Pensamento Crítico (Beyond Traditional Tests: Integrating Innovations into the Assessment of Creativity and Critical Thinking), Instituto Ayrton Senna, São Paulo, Brazil. https://institutoayrtonsenna.org.br/nossos-materiais/noticias/catedra-instituto-ayrton-senna-no-iea-usp-promove-live-integrando-inovacao-avaliacao-criatividade-pensamento-critico/\n\n\nSchneider, W. J. (2023, July 29). How recent theories of intelligence can improve the practice of cognitive ability assessment. [Panel]. International Society for Intelligence Research 2023 Annual Conference, Berkelety, CA. https://isironline.org/2023/01/isir-2023-berkeley-california-july-26-29/\n\n\nSchneider, W. J. (2023, March 24). Assessments that restore hope, promote understanding and inspire change. [Invited Lecture]. 42nd Annual School Psychology, Counseling Psychology and Applied Behavior Analysis Conference, Philadelphia, PA. https://education.temple.edu/node/50031\n\n\nSchneider, W. J. (2022, November 15). Using imaginary data to conserve real resources: Study design planning with model-based simulations. [Lecture]. Pearson 4th Developers Conversation, NA.\n\n\nFlanagan, D. P., Koponen, T., Wagner, R. K., Colvin, M. K., & Schneider, W. J. D. (2022, November 4). How do we diagnose learning disabilities? [Virtual Summit]. Learning Disabilities Summit: A Three-Part Virtual Event Focused on Crticial Learning Disability Issues, Learning Disabilities Association of America. https://ldaamerica.org/lda-to-hold-learning-disabilities-summit/\n\n\nSchneider, W. J. (2022, July 27). What is the current state of affairs of IQ testing? [Keynote Address]. 2022 Knowledge, Innovation & Enterprise (KIE) Conference: Unpacking Creativity: Culture, Innovation, and Motivation in Global Contexts, NA.\n\n\nSchneider, W. J. (2022, March 29). How measurement error affects learning disability identification accuracy. [Lecture]. Pearson Scientific Advisory Council, NA.\n\n\nSchneider, W. J. (2022, March 28). Practical psychometrics: Robust interpretation for individuals. [Lecture]. Temple University School Psychology Owl Hour, Philadelphia, PA.\n\n\nSchneider, W. J. (2022, January 27). Life-and-death psychometrics: IQ and the death penalty. [Webinar]. Owl Hour, Temple University. https://events.temple.edu/owl-hour-life-and-death-psychometrics-iq-and-the-death-penalty\n\n\nHajovsky, D. B., & Schneider, W. J. (2021, September 24). Cognitive assessment in the evaluation of specific learning disabilities. [Webinar]. Missouri Association of School Psychologists 2021 Fall Conference, Weldon Spring, MO. https://maosp.wildapricot.org/resources/Documents/Fall%20Conference.pdf\n\n\nSchneider, W. J. (2021, July 29). Advances in test interpretation with the Cattell-Horn-Carroll Theory of Cognitive Abilities. [Webinar]. Training for Region 19 Evaluation Staff, Education Service Center, Region 19, El Paso, TX. https://wjschne.github.io/media/CHCTheoryWebinar.pdf\n\n\nSchneider, W. J. (2021, June 14). Using CHC theory to better understand student needs. [Webinar]. Hill Country Summer Institute, NA. https://esc13.net/events/hill-country-summer-institute-2021\n\n\nSchneider, W. J. (2021, February 5). The accuracy of PSW methods. In J. R. Engler (Chair), PSW for SLD identification: Does cognition matter? [Symposium]. National Association of School Psychologists Annual Convention, Salt Lake City, UT. https://apps.nasponline.org/professional-development/convention/session-detail.aspx?id=20263\n\n\nSchneider, W. J. (2021, January 22). Improving LD identification with fewer myths and more science. [Invited Lecture]. Science to Practice Virtual Conference, Learning Disability Association of America.\n\n\nSchneider, W. J. (2020, July 29). Curious about CHC Theory? Take your evaluations to the next level! [Webinar]. Riverside Insights, Rolling Meadows, IL. https://info.riversideinsights.com/chc-theory-webinar\n\n\nSchneider, W. J. (2019, April 3). The evolution of PSW methods of SLD identification: What I have learned from PSW proponents. [Keynote Address]. Annual School Neuropsychology Conference, Long Beach, CA.\n\n\nSchneider, W. J. (2019, April 1). The evolution of PSW methods of SLD identification: What I have learned from PSW critics. [Keynote Address]. Annual School Neuropsychology Conference, Long Beach, CA.\n\n\nFlanagan, D. P., Schneider, W. J., & Alfonso, V. C. (2019, February 26). PSW methods: Comparisons, research, and how to use them responsibly. [Symposium]. National Association of School Psychologists Annual Convention, Atlanta, GA.\n\n\nSchneider, W. J. (2019, February 26). Consumer-oriented and legally defensible psychoeducational reports–Advanced workshop. [Invited Workshop]. National Association of School Psychologists Conference, Atlanta, GA.\n\n\nSchneider, W. J. (2019, February 26). Consumer-oriented and legally defensible psychoeducational reports–Introductory workshop. [Invited Workshop]. National Association of School Psychologists Conference, Atlanta, GA.\n\n\nSchneider, W. J. (2018, February 16). Re-evaluating the accuracy of the Dual Discrepancy/Consistency Model for SLD Identification. [Lecture]. National Association of School Psychologists Annual Convention, Chicago, IL.\n\n\nMcGrew, K. S., & Schneider, W. J. (2018, February 15). Revisions to the Cattell-Horn-Carroll (CHC) Theory of Intelligence. [Lecture]. National Association of School Psychologists Annual Convention, Chicago, IL.\n\n\nSchneider, W. J. (2016, October 24). How to write psychoeducational reports that people will want to read all the way through. [Webinar]. KIPP Austin Public Schools and Texas State University School Psychology Program, NA.\n\n\nMcGrew, K. S., & Schneider, W. J. (2016, October 19). Porque as habilidades cognitivas importam na educação [Why cognitive abilities matter in education]. [Invited Lecture]. Instituto Ayrton Senna, São Paulo, Brazil.\n\n\nSchneider, W. J., & McGrew, K. S. (2016, October 18). CHC theory, complex problem solving, critical thinking, and creativity. [Invited Lecture]. Instituto Ayrton Senna, São Paulo, Brazil.\n\n\nMcGrew, K. S., & Schneider, W. J. (2016, October 17). CHC theory and education in Brazil. [Invited Lecture]. Instituto Ayrton Senna, São Paulo, Brazil.\n\n\nSchneider, W. J. (2016, August 4). Writing reports worth reading all the way through. In R. Flanagan (Chair), Sophisticated simplicity: The art of writing reader-friendly assessment reports [Symposium]. Annual American Psychological Association Convention, Denver, CO.\n\n\nSchneider, W. J. (2016, July 29). How to write psychoeducational reports that people will want to read all the way through. [Webinar]. New Brunswick Department of Education and Early Childhood Development, Fredericton, NB.\n\n\nSchneider, W. J. (2016, April 12). How to write psychoeducational reports that people will want to read. [Webinar]. Region 10 Education Service Center, Richardson, TX.\n\n\nCormier, D. C., Bulut, O., Niileksela, C. R., Funamoto, A., & Schneider, W. J. D. (2016, February 12). Revisiting the relationship between CHC abilities and academic achievement. [Symposium]. National Association of School Psychologists Annual Convention, New Orleans, LA.\n\n\nSchneider, W. J. (2015, July 15). I didn’t know the WJ IV could do that! Answering practical assessment questions you didn’t know you could ask. [Invited Lecture]. School Neuropsychology Summer Institute, Grapevine, TX.\n\n\nSchneider, W. J. (2014, February 15). What if we took our models seriously? Estimating latent scores in individuals. [Lecture]. National Association of School Psychologists Convention, Washington, D.C..\n\n\nSchneider, W. J. (2013, July 15). Advances in CHC Theory and its application to individuals. [Invited Lecture]. School Neuropsychology Summer Institute, Grapevine, TX.\n\n\nSchneider, W. J. (2013, February 13). Holes in CHC Theory. [Lecture]. National Association of School Psychologists Conference, Seattle, WA.\n\n\nSchneider, W. J., & Taylor, S. J. (2008, October 1). Can the Implicit Association Test be used as a lie detector? Morality, implicit attitudes, and Big Brother’s help measuring illegal file-sharing behavior. [Invited Lecture]. Center for Study of Public Choice at George Mason University, Fairfax, VA.\n\n\nSchneider, W. J. (2006, November 15). We need to change (but I won’t do anything because I’m not the problem): Couples therapy outcomes and the transtheoretical model of change. [Invited Lecture]. Colloquium at Purdue Unversity, West Lafayette, IN.\n\n\nSchneider, W. J., & Huber, B. (2004, March 15). The interactive effects of fluid intelligence and executive functions on behavior disorders in children. [Lecture]. Annual Meeting of the Illinois School Psychology Association, Springfield, IL."
  },
  {
    "objectID": "vita.html#posters-and-papers",
    "href": "vita.html#posters-and-papers",
    "title": "Curriculum Vitae",
    "section": "Posters and Papers",
    "text": "Posters and Papers\n\nHajovsky, D. B., Niileksela, C. R., Flanagan, D. P., Alfonso, V. C., Schneider, W. J., & Zinkiewicz, C. J. (2022, August 4). Toward a Consensus Model of Cognitive-Achievement Relations Using Meta-SEM [Poster]. American Psychological Association Convention, Minneapolis, MN.\n\n\nSchneider, W. J., Flanagan, D. P., Niileksela, C. R., & Engler, J. R. (2020, February 20). Reevaluating the accuracy of PSW methods of SLD identification [Poster]. National Association of School Psychologists Convention, Baltimore, MD.\n\n\nSchneider, W. J., & Fiorello, C. A. (2019, August 8). Broad abilities are not doomed to be measured unreliably: A Monte Carlo study of omega statistics [Poster]. American Psychological Association Convention, Chicago, IL. https://github.com/wjschne/APA2019\n\n\nMulderink, T. D., Tobin, R. M., & Schneider, W. J. (2018, August 10). Personality, interactive history, and situational factors during children’s games [Poster]. Annual American Psychological Association Convention, San Francisco, CA.\n\n\nHowe, A. R., Curnock, A. D., Koppenhoefer, S. E., Schneider, W. J., & Tobin, R. M. (2016, August 4). Do vocabulary skills and instructor influence social-emotional learning? [Poster]. Annual American Psychological Association Convention, Denver, CO.\n\n\nNicholls, C. J., Ross, E., & Schneider, W. J. (2016, April 15). The neuropsychological profiles of twins discordant for Sotos Syndrome: A case study [NA]. American Academy of Pediatric Neuropsychology Annual Conference, Las Vegas, NV.\n\n\nCurnock, A. D., Pajor, K. E., Tobin, R. M., & Schneider, W. J. (2016, February 12). Preschoolers’ engagement during Second Step and their social-emotional knowledge [Poster]. National Association of School Psychologists Annual Convention, New Orleans, LA.\n\n\nEngelland, J. L., Tobin, R. M., Meyers, A., Huber, B., Schneider, W. J., Austen, J. H., & Corbin, A. L. (2014, August 12). Longitudinal effects of school climate on middle school students’ development [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nAffrunti, C. L., Schneider, W. J., Tobin, R. M., & Collins, K. D. (2014, August 7). Predictors of academic outcomes in college students suspected of having learning disorders [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nSondalle, A. A., Tobin, R. M., & Schneider, W. J. (2014, August 7). Behavioral engagement, Second Step edition, and children’s social-emotional functioning [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nSondalle, A. A., Probst, K. G., Carreno, C., Tobin, R. M., Schneider, W. J., Moore, N. A., & Mulderink, T. D. (2014, February 15). Engagement during Second Step and kindergarteners’ social-emotional and academic outcomes [Poster]. National Association of School Psychologists Convention, Washington, D.C..\n\n\nSondalle, A. A., Probst, K. G., Tobin, R. M., Schneider, W. J., & Kestian, J. M. (2014, February 15). Second Step and teacher ratings of children’s social-emotional functioning [Poster]. National Association of School Psychologists Conference, Washington, D.C..\n\n\nTobin, R. M., Schneider, W. J., Moore, N. A., Sondalle, A. A., & Willis, M. (2013, August 15). Effortful control and knowledge gains after the Second Step intervention [Poster]. Annual American Psychological Association Convention, Honolulu, HI.\n\n\nMoore, N. A., Sondalle, A. A., Mulderink, T. D., Tobin, R. M., Schneider, W. J., Willis, M., & Probst, K. G. (2013, February 15). Effortful control, Second Step, and behavior in kindergartners [Poster]. National Association of School Psychologists Convention, Seattle, WA.\n\n\nSondalle, A. A., Mulderink, T. D., Moore, N. A., Tobin, R. M., & Schneider, W. J. (2012, August 15). Engagement, dosage, and effectiveness of the kindergarten Second Step curriculum [Poster]. Annual American Psychological Association Convention, Orlando FL.\n\n\nMulderink, T. D., Sondalle, A. A., Moore, N. A., Tobin, R. M., Schneider, W. J., & Sheese, B. E. (2012, February 15). Influences of executive functioning and Second Step on behavior [Poster]. National Association of School Psychologists Conference, Philadelphia, PA.\n\n\nMulderink, T. D., Moore, N. A., Sondalle, A. A., Tobin, R. M., & Schneider, W. J. (2011, August 15). Executive functioning and responsiveness to Second Step [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nTobin, R. M., Mackin, J. M., Babcock, E. A., Mulderink, T. D., Moore, N. A., Carlson, L. A., Gioia, K. A., & Schneider, W. J. (2011, February 15). Intervention frequency and behavior in response to Second Step [Poster]. National Association of School Psychologists Convention, San Francisco, CA.\n\n\nTobin, R. M., Gioia, K. A., Mulderink, T. D., Carlson, L. A., Moore, N. A., & Schneider, W. J. (2010, August 15). Personality, Second Step dosage, and kindergartners’ social skills improvements [Poster]. Annual American Psychological Association Convention, San Diego CA.\n\n\nMoore, N. A., Gioia, K. A., Tobin, R. M., & Schneider, W. J. (2010, March 1). Effortful control and emotion regulation in kindergartners [Poster]. National Association of School Psychologists Convention, Chicago, IL.\n\n\nObilade, M. H., Gioia, K. A., Tobin, R. M., & Schneider, W. J. (2010, March 1). Verbal ability and responsiveness to the Second Step curriculum [Poster]. National Association of School Psychologists Convention, Chicago, IL.\n\n\nSchneider, W. J., & Tobin, R. M. (2010, March 1). Previously impossible feats of interpretation and explanation with cognitive and achievement data [Poster]. National Association of School Psychologists Conference, Chicago, IL.\n\n\nGioia, K. A., Tobin, R. M., & Schneider, W. J. (2010, January 15). Children’s knowledge gains in response to a social skills intervention [Poster]. Annual Illinois School Psychologist Association Convention, East Peoria, IL.\n\n\nTaylor, S. A., & Schneider, W. J. (2009, October 15). Implicit attitudes in folk explanations of digital piracy [Paper]. Frontiers in Service Conference, Honolulu, HI.\n\n\nBooth, B., Lederer, S., Dick, S., Linnell, J., Meyers, A., Schneider, W. J., Swerdlik, M. E., & Anweiler, J. (2009, August 15). Reintegration experiences of returning National Guard war veterans and implications for reintegration programming: Results from two states [Poster]. Annual American Psychological Association Convention, Toronto, ON.\n\n\nGadke, D. L., Tobin, R. M., & Schneider, W. J. (2009, August 15). Agreeableness and prejudice towards overweight men and women [Poster]. Annual American Psychological Association Convention, Toronto, ON.\n\n\nGioia, K. A., Tobin, R. M., & Schneider, W. J. (2009, August 15). Individual differences in children’s responsiveness to a social skills intervention [Poster]. Annual American Psychological Association Convention, Toronto, ON.\n\n\nGioia, K. A., Miller, K. A., Tobin, R. M., & Schneider, W. J. (2009, February 15). Children’s responsiveness to a social skills intervention [Poster]. National Association of School Psychologists Convention, Boston, MA.\n\n\nBaird, S. A., Schneider, W. J., & Tobin, R. M. (2008, August 15). Implicit agreeableness predicts laboratory measures of aggression [Poster]. Annual American Psychological Association Convention, Boston, MA.\n\n\nBarr, L. K., Kahn, J. H., & Schneider, W. J. (2008, March 15). Emotion expression tendencies and mood/anxiety symptoms [Poster]. International Counseling Psychology Conference, Chicago, IL.\n\n\nTobin, R. M., Schneider, W. J., & Landau, S. E. (2008, February 15). Assessing attention-deficit/hyperactivity disorder using an RTI approach [Poster]. National Association of School Psychologists Convention, New Orleans, LA.\n\n\nStagg, J. W., & Schneider, W. J. (2007, November 15). So you flunked the Stroop Test, so what? The utility of single vs. multiple indicators in predicting behavioral outcomes related to executive functioning among non-referred individuals [Poster]. Annual National Academy of Neuropsychology Conference, Scottsdale, AZ.\n\n\nSchneider, W. J. (2007, August 15). Is the TAT helpful in assessing complex aspects of attention and executive functions? An extremely labor-intensive celebration of the null hypothesis [Poster]. Annual American Psychological Association Convention, San Francisco, CA.\n\n\nSchneider, W. J., & Kasson, D. M. (2007, March 1). Do narrow abilities matter more for people with higher IQ? Implications of Spearman’s Law of Diminishing Returns for the discrepancy model of LD [Poster]. National Association of School Psychologists Convention, New York, NY.\n\n\nSchneider, W. J., & McKenna, T. L. (2006, August 15). Using the Implicit Association Test to measure early maladaptive schemas [Poster]. Annual American Psychological Association Convention, New Orleans, LA.\n\n\nKahn, J. H., Vogel, D. L., Schneider, W. J., Barr, L. K., & Henning, K. (2006, April 15). Emotional self-disclosures of college-student clients and counseling session outcome [Poster]. Great Lakes Conference, West Lafayette, IN.\n\n\nJones, G. B., & Schneider, W. J. (2005, August 15). Intelligence, human capital, and economic growth [Paper]. Econometric Society World Congress, London, UK.\n\n\nTobin, R. M., Bodner, A. C., & Schneider, W. J. (2005, August 15). Executive function and emotion regulation in children [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nMarch, A., Funk, K., Schneider, W. J., & Landau, S. E. (2005, March 1). Student and teacher attributions of school shootings [Poster]. National Association of School Psychologists Convention, Atlanta, GA.\n\n\nTobin, R. M., Schneider, W. J., Graziano, W. G., & Pizzitola, K. M. (2002, February 15). Nice kids in competitive situations [Poster]. Annual Society for Personality and Social Psychology Meeting, Savannah, GA.\n\n\nSnyder, D. K., Schneider, W. J., Oxford, M. C., & Quinn, T. (2001, July 15). Secondary prevention group intervention for couples [Paper]. World Congress of Behavioral and Cognitive Therapies, Vancouver, BC.\n\n\nTobin, R. M., Pizzitola, K. M., Schneider, W. J., & Graziano, W. G. (2001, April 15). Goal structures and competitiveness in children’s games [Poster]. Biennial Meeting of the Society for Research in Child Development, Minneapolis, MN.\n\n\nSchneider, W. J., Loss, R. M., Cavell, T. A., & Oxford, M. C. (2000, November 15). Parenting and children with callous-unemotional traits: Relationship quality as a potential socialization mechanism [Poster]. Annual Convention of the Association for the Advancement Behavior Therapy, New Orleans, LA.\n\n\nSnyder, D. K., Schneider, W. J., Oxford, M. C., & Quinn, T. (2000, November 15). SUCCESS: The efficacy of a relationship enhancement group [Poster]. Annual Convention of the Association for the Advancement Behavior Therapy, New Orleans, LA.\n\n\nSchneider, W. J., Cavell, T. A., Hughes, J. N., & Oxford, M. C. (1999, August 15). The development of the Perceived Containment Questionnaire [Poster]. Annual American Psychological Association Convention, Boston, MA."
  },
  {
    "objectID": "vita.html#courses",
    "href": "vita.html#courses",
    "title": "Curriculum Vitae",
    "section": "Courses",
    "text": "Courses\n\nTemple University\n\nCPSY 5519—Group Counseling (Spring 2018–2019)\nCPSY 5694—Introduction to Assessment (Fall 2017–2019)\nEDUC 5101—Critical Understanding of Social Science Research (Fall 2018)\nEDUC 5325—Introduction to Statistics and Research (Fall 2017, 2021)\nEPSY 5529—Tests and Measurements (Spring 2019–2020, 2022)\nEPSY 8825—Advanced Data Analysis (Spring 2020, 2022)\nSPSY 9687/8—Seminar in School Psychology/Psychoeducational Clinic (Fall 2019, Spring 2020–2021)\n\n\n\nIllinois State University\n\nPSY 110—Explaining Human Behavior/Fundamentals of Psychology (Fall 2002–2006, Spring 2003–2004, 2006–2007)\nPSY 138—Social Science Reasoning Using Statistics/Reasoning in Psychology Using Statistics (Spring 2004, 2008–2012, 2014; Fall 2007–2014, 2016)\nPSY 340—Statistics for the Social Sciences (Spring 2005)\nPSY 432—Psychodiagnostics I: Cognitive Assessment/ Theory and Practice of Cognitive Assessment (Fall 2004–2016)\nPSY 436—Clinical/Counseling Practicum (Spring 2004)\nPSY 442—Test Theory (Spring 2015, 2017)\nPSY 443—Regression Analysis (Spring 2016)\nPSY 444—Multivariate Analysis (Fall 2015){.smalldate}\nPSY 464—Theories and Techniques of Counseling: Adults (Spring 2005–2012, 2014–2016)\nPSY 480—Advanced Practicum in Dialectical Behavior Therapy Skills (Fall 2015–2017)\nPSY 480.31—Practicum in Dialectical Behavior Therapy Skills Training (Fall 2014, Spring 2105–2017)\nPSY 480.33—Seminar in Psychology: Supervision of a Dialectical Behavior Therapy Group (Fall 2014, Spring 2105, 2017)"
  },
  {
    "objectID": "vita.html#mentoring",
    "href": "vita.html#mentoring",
    "title": "Curriculum Vitae",
    "section": "Mentoring",
    "text": "Mentoring\n\nDoctoral Dissertation Chair\n\nTemple University\n\nMegan Barone\nRandy Taylor\nBernard Dillard\nStephanie Iaccarino (co-chair)\nJustin Harper (2022)\n\n\n\nIllinois State University\n\nC. Lee Affrunti (2013)\nDaniel L. Gadke (co-chair, 2012)\nJonathan W. Stagg (2008)\n\n\n\n\nDoctoral Dissertation Committee Member\n\nTemple University\n\nValerie Woxholt\nEmunah Mager-Garfield\nDenae Sisco\nOctavia Blount\nCodie Kane (2023)\nPatrick Clancy (2023)\nShana Levi-Nielsen (2022)\nKaiyla Darmer (2022)\nKathryn DeVries (2022)\nTera Gibbs (2022)\nMariah Davis (2022)\nLinda Ruan (2020)\n\n\n\nIllinois State University\n\nJennifer Engelland-Schultz (2015)\nMandi Martinez-Dick (2015)\nThomas Mulderink (2015)\nAlyssa A. Sondalle (2015)\nRachelle Cantin (2013)\nJennifer Wallace (2013)\nSilas Dick (2013)\nTrisha Mann (2012)\nKatherine A. Gioia (2009)\nSarah Reck (2008)\nAnna C. Bodner (2005)\n\n\n\nExternal Reviewer\n\nBrianna Paul (2022) Texas Women’s University\nJake Blair Kraska (2021) Monash University\nPaul Jewsbury (2014) University of Melbourne\n\n\n\n\nMaster’s Thesis Chair\n\nIllinois State University\n\nFeng Ji (2018)\nKatrin Klieme (2016)\nZachary Roman (2016)\nKiera Dymit (2015)\nMeera Afzal (2012)\nRachelle Bauer (2010)\nSunthud Pornprasertmanit (2010)\nDavid Kasson (2006)\nKristina Taylor (2006)\nRobin Van Herrmann (2010)\nMelissa Zygmun (2006)\n\n\n\n\nMaster’s Thesis Committee Member\n\nIllinois State University\n\nRyan Willard (2017)\nRachel Workman (2017)\nHayley Love (2016)\nAnges Strojewska (2016)\nDanielle Freund (2015)\nAmanda Fisher (2015)\nDaniel Nuccio (2014)\nKevin Wallpe (2014)\nNicole Moore (2013)\nDrew Abney (2012)\nThomas Mulderink (2012)\nJames Clinton (2011)\nJamie Hansen (2010)\nYin Ying Ong (2010)\nMelanie Hewett (2010)\nKaty Adler (2008)\nPoonam Joshi (2008)\nRebecca Hoerr (2008)\nArusha Sethi (2008)\nLeah Barr (2008)\nSara Byczek (2007)"
  },
  {
    "objectID": "vita.html#university-service",
    "href": "vita.html#university-service",
    "title": "Curriculum Vitae",
    "section": "University Service",
    "text": "University Service\n\nIllinois State University\n\n\n\n\n\n\n2007–2017\n\n\nCoordinator and Supervisor, College Learning Assessment Service (CLAS), Psychological Services Center"
  },
  {
    "objectID": "vita.html#college-service",
    "href": "vita.html#college-service",
    "title": "Curriculum Vitae",
    "section": "College Service",
    "text": "College Service\n\nTemple University\n\n\n\n\n\n\n2022–2023\n\n\nMember, CEHD Transition Committee\n\n\n\n\n2022–\n\n\nMember, CEHD Tenure and Promotion Committee\n\n\n\n\n2020\n\n\nMember, Higher Education Non-Tenure-Track Faculty Search Committee\n\n\n\n\n2019–2022\n\n\nChair, Faculty Merit Committee\n\n\n\n\n2019\n\n\nCo-Chair, Data and Assessment Working Group\n\n\n\n\n2019\n\n\nMember, Higher Education Tenure-Track Faculty Search Committee\n\n\n\n\n2019\n\n\nTraining presenter: Make Your Data POP! How to interpret and present data visually to maximize its impact\n\n\n\n\n2018–2019\n\n\nMember, Faculty Resource and Development Committee\n\n\n\n\n2018\n\n\nMember, Special Education Faculty Search committee\n\n\n\n\n2018\n\n\nTraining presenter: Introduction to Statistical Analysis with R Workshop"
  },
  {
    "objectID": "vita.html#departmental-service",
    "href": "vita.html#departmental-service",
    "title": "Curriculum Vitae",
    "section": "Departmental Service",
    "text": "Departmental Service\n\nTemple University\n\n\n\n\n\n\n2018–\n\n\nMember, School Psychology Program Committee\n\n\n\n\n2017–2020, 2023–\n\n\nMember, Counseling Psychology Program Committee\n\n\n\n\n2020–2023\n\n\nMember, Educational Leadership Program Committee\n\n\n\n\n2018–2020\n\n\nMember, Departmental Promotion and Tenure Committee\n\n\n\n\n\n\nIllinois State University\n\n\n\n\n\n\n2004–2017\n\n\nMember, Clinical/Counseling Psychology Coordinating committee\n\n\n\n\n2004–2017\n\n\nMember, IRB Committee\n\n\n\n\n2004–2017\n\n\nMember, Psychological Services Center Administrative Team\n\n\n\n\n2004–2017\n\n\nMember, Quantitative Psychology Sequence committee\n\n\n\n\n2004–2017\n\n\nMember, Research Committee\n\n\n\n\n2015–2017\n\n\nMember, Department Faculty Status Committee (Evaluates annual merit, tenure, and promotion)\n\n\n\n\n2017\n\n\nChair, Salary Compression Review Team\n\n\n\n\n2015\n\n\nProgram Coordinator, Quantitative Sequence (Fall only)\n\n\n\n\n2014\n\n\nMember, Clinical/Counseling Faculty Search committee\n\n\n\n\n2004–2005\n\n\nMember, Clinical/Counseling Faculty Search committee"
  },
  {
    "objectID": "vita.html#consulting",
    "href": "vita.html#consulting",
    "title": "Curriculum Vitae",
    "section": "Consulting",
    "text": "Consulting\n\n\n\n\n\n\n2019–\n\n\nAcademic Consultant, Empass Learning, developing an online test of cognitive abilities based on CHC Theory, Gurgaon, India\n\n\n\n\n2014–\n\n\nExpert Consultant, providing expert evaluations of death penalty and medical cases for various legal firms, US\n\n\n\n\n2017–2020\n\n\nAcademic Consultant, PT Melintas Cakrawala, developing AJT CogTest?a comprehensive test of cognitive abilities based on CHC Theory, Jakarta, Indonesia\n\n\n\n\n2015–2017\n\n\nAcademic Consultant, Ayrton Senna Institute, developing a comprehensive assessment of 21st century skills, Sao Paulo, Brazil\n\n\n\n\n2016\n\n\nAcademic Consultant, Academic Therapy Publications, developing and reviewing cognitive and oral language assessment batteries, Novato, CA\n\n\n\n\n2007–2010\n\n\nResearch Consultant, Illinois Army National Guard, conducting research examining reintegration of service personnel"
  },
  {
    "objectID": "vita.html#media-appearances",
    "href": "vita.html#media-appearances",
    "title": "Curriculum Vitae",
    "section": "Media Appearances",
    "text": "Media Appearances\n\n\n\n\n\n\n2/3/2014\n\n\nInterviewed in Scientific American Blog Beautiful Minds by Scott Barry Kaufman\n\n\n\n\n8/5/2016\n\n\nQuoted in the Wall Street Journal about IQ tests\n\n\n\n\n10/13/2016\n\n\nQuoted in ScienceNews for Students about IQ tests\n\n\n\n\n10/11/2017\n\n\nOpined in the Guardian about our president?s IQ\n\n\n\n\n4/9/2018\n\n\nAppeared as a guest of Knowledge@Wharton Radio to discuss hunger and homelessness among college students.\n\n\n\n\n4/3/2018\n\n\nAppeared on CNN Headline News (HLN) with MichaeLA to discuss hunger and homelessness among college students.\n\n\n\n\n10/20/2019\n\n\nPresented on Writing Assessment Reports People Will Read, Understand, and Remember on the School Psyched Podcast\n\n\n\n\n3/1/2020\n\n\nQuoted in APA Monitor on writing assessment reports\n\n\n\n\n3/29/2021\n\n\nPresented on the evolution of cognitive assessment on the Testing Psychologist Podcasts"
  },
  {
    "objectID": "vita.html#editorial-positions",
    "href": "vita.html#editorial-positions",
    "title": "Curriculum Vitae",
    "section": "Editorial Positions",
    "text": "Editorial Positions\n\n\n\n\n\n\n\n\n\n\n\n2019⁠–⁠2020\n*Journal of Psychoeducational Assessment*\nAssociate Editor\n\n\n2015\n*Journal of School Psychology*\nGuest Action Editor"
  },
  {
    "objectID": "vita.html#editorial-boards",
    "href": "vita.html#editorial-boards",
    "title": "Curriculum Vitae",
    "section": "Editorial Boards",
    "text": "Editorial Boards\n\n\n\n\n\n\n\n\n\n\n\n2018⁠–⁠\n*Journal of Intelligence*\nEditorial Board\n\n\n2011⁠–⁠\n*Journal of School Psychology*\nEditorial Board\n\n\n2010⁠–⁠\n*Journal of Psychoeducational Assessment*\nEditorial Board\n\n\n2017⁠–⁠\n*Archives of Scientific Psychology*\nConsulting Editor\n\n\n2009⁠–⁠2018\n*Psychological Assessment*\nConsulting Editor\n\n\n1997⁠–⁠2000\n*Clinician's Research Digest*\nEditorial Associate"
  },
  {
    "objectID": "vita.html#ad-hoc-reviewing",
    "href": "vita.html#ad-hoc-reviewing",
    "title": "Curriculum Vitae",
    "section": "Ad Hoc Reviewing",
    "text": "Ad Hoc Reviewing\n\nApplied Neuropsychology\nEuropean Journal of Psychological Assessment\nFrontiers in Human Neuroscience\nJournal of Family Psychology\nJournal of Psychoeducational Assessment\nNASP Communiqué\nPsychological Assessment\nPsychology in the Schools"
  },
  {
    "objectID": "vita.html#professional-memberships",
    "href": "vita.html#professional-memberships",
    "title": "Curriculum Vitae",
    "section": "Professional Memberships",
    "text": "Professional Memberships\n\nMember, American Psychological Association\nMember, National Association of School Psychologists"
  },
  {
    "objectID": "tutorials/Regression/regression.html",
    "href": "tutorials/Regression/regression.html",
    "title": "Regression in R",
    "section": "",
    "text": "Here we are going to use a small data set to predict people’s height using the height of their parents."
  },
  {
    "objectID": "tutorials/Regression/regression.html#install-tidyverse",
    "href": "tutorials/Regression/regression.html#install-tidyverse",
    "title": "Regression in R",
    "section": "Install tidyverse",
    "text": "Install tidyverse\nSome packages are designed to work with several other packages as a system. The tidyverse package is a “meta-package” that installs and loads a coherent set of packages designed to help you import, manipulate, visualize, and interpret data. It also installs the haven package, which we will use to import some data. If you do not have a recent version of tidyverse already installed, you can install it with this code:\n\ninstall.packages(\"tidyverse\")"
  },
  {
    "objectID": "tutorials/Regression/regression.html#install-easystats",
    "href": "tutorials/Regression/regression.html#install-easystats",
    "title": "Regression in R",
    "section": "Install easystats",
    "text": "Install easystats\nThe easystats package is another “meta-package” that installs a set of packages designed to work together to make data analysis easier.\nIf you do not have a recent version of easystats already installed, you can install it with this code:\n\ninstall.packages(\"easystats\")"
  },
  {
    "objectID": "tutorials/Regression/regression.html#save-the-plot",
    "href": "tutorials/Regression/regression.html#save-the-plot",
    "title": "Regression in R",
    "section": "Save the plot!",
    "text": "Save the plot!\nYou can save to your hard drive a high-quality plot with the ggsave function. It will save whatever the last plot you created with ggplot. It will guess the file format from the file extension.\nHere I save a .pdf file of the plot to the working directory:\n\nggsave(\"my_plot.pdf\")\n\nWhat is the working directory? Is the folder on your machine that R thinks it should look first if it needs to find files or save files. If you are every curious about which directory is the working directory, you can use the getwd function:\n\ngetwd()\n\nIf you saved the plot above as \"my_plot.pdf\", you will find the file in the working directory returned by getwd."
  },
  {
    "objectID": "tutorials/Regression/regression.html#vector-based-images",
    "href": "tutorials/Regression/regression.html#vector-based-images",
    "title": "Regression in R",
    "section": "Vector-based images",
    "text": "Vector-based images\nThe .pdf format gives the best image quality but can only be viewed in a .pdf reader. The .svg format is almost as good and can be incorporated into webpages and Office documents. One downside of their near-perfect image quality is that .pdf and .svg image file sizes can become quite large."
  },
  {
    "objectID": "tutorials/Regression/regression.html#raster-images",
    "href": "tutorials/Regression/regression.html#raster-images",
    "title": "Regression in R",
    "section": "Raster images",
    "text": "Raster images\nThe .png format gives good image quality and renders small file sizes.\nThe primary use of the .gif format is to create animated plots. Otherwise stick with .png.\nAlthough the .jpg format is good for photos, it is terrible for plots—it often renders text and sharp corners with pixelated smudges."
  },
  {
    "objectID": "tutorials/Regression/regression.html#model-level-statistics",
    "href": "tutorials/Regression/regression.html#model-level-statistics",
    "title": "Regression in R",
    "section": "Model-level statistics",
    "text": "Model-level statistics\nSome statistics like the coefficient of determination (R2) or the standard error of the estimate (σe) describe the model as a whole.\nThe model-level statistics can be extracted with the performance package’s model_performance function.\n\nperformance(m1)\n\n\n\n\n\nAIC\nAICc\nBIC\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\n196\n197\n201\n0.641\n0.631\n2.61\n2.68\n\n\n\n\n\n\nR2 is in the glance function’s r.squared column, and the standard error of the estimate is in the sigma column. If all you wanted was the R2, you could do this:\n\nr2(m1)\n\n# R2 for Linear Regression\n       R2: 0.641\n  adj. R2: 0.631\n\n\nFor the standard error of the estimate:\n\nsigma(m1)\n\n[1] 2.68\n\n\nWhy would you want just one number instead of reading it from a table? In reproducible research, we intermingle text and code so that it is clear where every number came from. Thus, “hard-coding” your results like this is considered poor practice:\nThe model explains 64% of the variance.\nUsing rmarkdown, instead of typing the numeric results, we type pull the results using an inline code chunk:\nThe model explains 64% of the variance.\nWhich, when rendered, produces the correct output:\n\nThe model explains 64% of the variance.\n\nThat seems like a lot of extra work, right? Yes, it is—unless there is a possibility that your underlying data might change or that you might copy your numbers incorrectly. If you are imperfect, the extra time and effort is worth it. It makes it easy for other scholars to see exactly where each number came from. Hard-coded results are harder to trust."
  },
  {
    "objectID": "tutorials/Regression/regression.html#coefficient-level-statistics",
    "href": "tutorials/Regression/regression.html#coefficient-level-statistics",
    "title": "Regression in R",
    "section": "Coefficient-level statistics",
    "text": "Coefficient-level statistics\nThe regression coefficients—the intercept (b0) and the slope (b1)—have a number of statistics associated with them, which we will discuss later in the course.\nIf you just wanted the intercept and the slope coefficients, use the coef function:\n\ncoef(m1)\n\n(Intercept)     avgphgt \n     -12.88        1.19 \n\n\nThus, the intercept is -12.88 and the slope is 1.19.\nTo get the model parameters (intercept and slope coefficients) along with their p-values and other statistical information\n\nparameters(m1) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n-12.88\n9.779\n0.95\n-32.675\n6.92\n-1.32\n38\n0.196\n\n\navgphgt\n1.19\n0.145\n0.95\n0.899\n1.49\n8.23\n38\n0.000\n\n\n\n\n\n\nIf you want standardized parameters:\n\nstandardise_parameters(m1)\n\n\n\n\n\nParameter\nStd_Coefficient\nCI\nCI_low\nCI_high\n\n\n\n\n(Intercept)\n0.0\n0.95\n-0.194\n0.194\n\n\navgphgt\n0.8\n0.95\n0.603\n0.997"
  },
  {
    "objectID": "tutorials/Regression/regression.html#checking-assumptions-1",
    "href": "tutorials/Regression/regression.html#checking-assumptions-1",
    "title": "Regression in R",
    "section": "Checking assumptions",
    "text": "Checking assumptions\n\ncheck_model(m2)\n\n\n\n\n\n\n\n\nAll looks well."
  },
  {
    "objectID": "tutorials/Regression/regression.html#summarizing-results",
    "href": "tutorials/Regression/regression.html#summarizing-results",
    "title": "Regression in R",
    "section": "Summarizing results",
    "text": "Summarizing results\nTo summarize the results, use the summary function:\n\nsummary(m2)\n\n\nCall:\nlm(formula = height ~ avgphgt + gender, data = d)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.826 -1.094 -0.347  1.656  4.503 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     24.567     10.932    2.25  0.03068 *  \navgphgt          0.671      0.157    4.26  0.00013 ***\ngenderFemales   -4.468      0.920   -4.85  2.2e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.12 on 37 degrees of freedom\nMultiple R-squared:  0.78,  Adjusted R-squared:  0.769 \nF-statistic: 65.7 on 2 and 37 DF,  p-value: 6.6e-13\n\n\nStandardized coefficients:\n\nstandardise_parameters(m2)\n\n\n\n\n\nParameter\nStd_Coefficient\nCI\nCI_low\nCI_high\n\n\n\n\n(Intercept)\n0.506\n0.95\n0.245\n0.768\n\n\navgphgt\n0.450\n0.95\n0.236\n0.664\n\n\ngenderFemales\n-1.012\n0.95\n-1.435\n-0.590\n\n\n\n\n\n\nAn automated report:\n\nreport(m2)\n\nWe fitted a linear model (estimated using OLS) to predict height with avgphgt\nand gender (formula: height ~ avgphgt + gender). The model explains a\nstatistically significant and substantial proportion of variance (R2 = 0.78,\nF(2, 37) = 65.75, p &lt; .001, adj. R2 = 0.77). The model's intercept,\ncorresponding to avgphgt = 0 and gender = Males, is at 24.57 (95% CI [2.42,\n46.72], t(37) = 2.25, p = 0.031). Within this model:\n\n  - The effect of avgphgt is statistically significant and positive (beta = 0.67,\n95% CI [0.35, 0.99], t(37) = 4.26, p &lt; .001; Std. beta = 0.45, 95% CI [0.24,\n0.66])\n  - The effect of gender [Females] is statistically significant and negative\n(beta = -4.47, 95% CI [-6.33, -2.60], t(37) = -4.85, p &lt; .001; Std. beta =\n-1.01, 95% CI [-1.43, -0.59])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "tutorials/Regression/regression.html#comparing-models",
    "href": "tutorials/Regression/regression.html#comparing-models",
    "title": "Regression in R",
    "section": "Comparing models",
    "text": "Comparing models\nTo compare two regression models:\n\ntest_wald(m1, m2)\n\n\n\n\n\nName\nModel\ndf\ndf_diff\nF\np\n\n\n\n\nm1\nlm\n38\n\n\n\n\n\nm2\nlm\n37\n1\n23.6\n0\n\n\n\n\n\n\nThe p-value is significant, meaning that m2 explains more variance than m1.\nThis uses Bayes Factor instead of a p-value:\n\ntest_bf(m1, m2)\n\n\n\n\n\nModel\nlog_BF\nBF\n\n\n\n\navgphgt\n\n\n\n\navgphgt + gender\n8.01\n3010\n\n\n\n\n\n\nA BF &gt; 1 means that m2 is more strongly supported than m1. A BF &lt; 1 means that m1 is more strongly supported than m2. If you are not sure how to interpret the output of test_bf, you can get an automated interpretation:\n\ntest_bf(m1, m2) %&gt;% \n  report()\n\nBayes factors were computed using the BIC approximation, by which BF10 =\nexp((BIC0 - BIC1)/2). Compared to the avgphgt model, we found extreme evidence\n(BF = 3.01e+03) in favour of the avgphgt + gender model (the least supported\nmodel).\n\n\nTo compare many performance statistics at once:\n\ncompare_performance(m1, m2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nAIC\nAIC_wt\nAICc\nAICc_wt\nBIC\nBIC_wt\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\nm1\nlm\n196\n0\n197\n0\n201\n0\n0.641\n0.631\n2.61\n2.68\n\n\nm2\nlm\n179\n1\n180\n1\n185\n1\n0.780\n0.769\n2.04\n2.12"
  },
  {
    "objectID": "tutorials/Regression/regression.html#question-1",
    "href": "tutorials/Regression/regression.html#question-1",
    "title": "Regression in R",
    "section": "Question 1",
    "text": "Question 1\nIs read_1 a significant predictor of read_2?\n\n\nHint 1\n\nMake model fit object called m_read using the lm function.\n\nm_read &lt;- lm(read_2 ~ read_1, data = d_learn)\n\n\n\n\nHint 2\n\nView coefficient-level statistics with the parameters function.\nYou could also use Base R’s summary function.\n\nparameters(m_read)\n\n# or\n\nsummary(m_read)\n\n\n\n\nHint 3\n\nIn the read_1 row, is the p.value column less than 0.05?\n\nparameters(m_read)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n83.591\n10.922\n0.95\n61.44\n105.741\n7.65\n36\n0.000\n\n\nread_1\n0.251\n0.104\n0.95\n0.04\n0.463\n2.41\n36\n0.021"
  },
  {
    "objectID": "tutorials/Regression/regression.html#question-2",
    "href": "tutorials/Regression/regression.html#question-2",
    "title": "Regression in R",
    "section": "Question 2",
    "text": "Question 2\nWhat is the R2 for the m_read model?\n\n\nHint\n\nView model-level statistics with the performance function.\nYou could also use Base R’s summary function.\n\nperformance(m_read)\n\n# or\n\nsummary(m_read)"
  },
  {
    "objectID": "tutorials/Regression/regression.html#question-3",
    "href": "tutorials/Regression/regression.html#question-3",
    "title": "Regression in R",
    "section": "Question 3",
    "text": "Question 3\nWhat does the scatter plot look like when read_1 is on the x-axis and read_2 is on the y-axis? Also plot the regression line. Save your plot using the ggsave function.\n\n\nHint Model\n\nThis will get you started\n\nggplot(d_learn, aes(read_1, read_2))\n\n\n\n\nHint for Adding Points\n\nHere is how you add points.\n\nggplot(d_learn, aes(read_1, read_2)) +\n  geom_point() \n\n\n\n\nHint for Adding Line\n\nHere is how you add a regression line.\n\nggplot(d_learn, aes(read_1, read_2)) +\n  geom_point() + \n  geom_smooth(method = \"lm\") \n\n\n\n\nPlot Polishing\n\nThis is overkill for now. But someday you might want to be able to make your plots as polished as possible.\n\n# I want to put the equation at x = 130\nequation_x &lt;- 130\nequation_y &lt;- predict(m_read, newdata = tibble(read_1 = equation_x))\n\n# Extracting the coefficients\nb_read &lt;- round(coef(m_read),2)\n\n# The angle of the regression line is the inverse tangent of the slope (converted to degrees)\neq_angle &lt;- atan(b_read[2]) * 180 / pi\n\n# Equation\neq_read &lt;- paste0(\"italic(Y) == \", \n                  b_read[1], \n                  \" + \", \n                  b_read[2], \n                  \" *  italic(X) + italic(e)\")\n\nggplot(d_learn, aes(read_1, read_2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Reading at Time 1\",\n       y = \"Reading at Time 2\",\n       title = \"Using Reading at Time 1 to Predict Reading at Time 2\") +\n  coord_fixed() +\n  theme_minimal() +\n  annotate(\n    geom = \"text\",\n    x = equation_x,\n    y = equation_y,\n    angle = eq_angle,\n    label = eq_read,\n    parse = TRUE,\n    vjust = -0.5)"
  },
  {
    "objectID": "tutorials/Regression/regression.html#question-4",
    "href": "tutorials/Regression/regression.html#question-4",
    "title": "Regression in R",
    "section": "Question 4",
    "text": "Question 4\nCreate a regression model in which you predict math at time 2 (math_2) using reading at time 1 (math_1). Call the model fit object m_math1.\nDoes math_1 predict math_2?.\nDoes math_1 still predict math_2 after controlling for read_1?\n\n\nHint\n\n\nm_math &lt;- lm(math_2 ~ math_1 + read_1, data = d_learn)\nsummary(m_math)"
  },
  {
    "objectID": "tutorials/EFA/index.html",
    "href": "tutorials/EFA/index.html",
    "title": "Exploratory Factor Analysis with R",
    "section": "",
    "text": "The following code will install the tidyverse, GPArotation, and psych packages if they are not installed already.\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"GPArotation\")) install.packages(\"GPArotation\")\nif (!require(\"psych\")) install.packages(\"psych\")\nif (!require(\"psych\")) install.packages(\"easystats\")\n\n\nlibrary(tidyverse)\nlibrary(GPArotation)\nlibrary(psych)\nlibrary(easystats)"
  },
  {
    "objectID": "tutorials/EFA/index.html#junk-factors",
    "href": "tutorials/EFA/index.html#junk-factors",
    "title": "Exploratory Factor Analysis with R",
    "section": "Junk Factors",
    "text": "Junk Factors\nWe want to make sure that each factor makes sense theoretically and that there are no “junk” factors. Junk factors group unrelated items that have no underlying theoretical construct that unites them. EFA is “exploratory” and sometimes it groups items together because of chance fluctuations in the correlations. Junk factors typically have low loadings on all items (&lt; 0.4 or so).\nIf you find a junk factor, you can leave it in and not interpret it or you can extract 1 factor fewer.\nIn this case, all the factors are interpretable:\n\nPA1 = Extraversion\nPA2 = Neuroticism\nPA3 = Conscientiousness\nPA4 = Openness\nPA5 = Agreeableness"
  },
  {
    "objectID": "tutorials/EFA/index.html#singleton-factors",
    "href": "tutorials/EFA/index.html#singleton-factors",
    "title": "Exploratory Factor Analysis with R",
    "section": "Singleton Factors",
    "text": "Singleton Factors\nWe also worry about “singleton” factors. These have a high loading (&gt;0.5 or so) on a single item and all other loadings are small (&lt;0.3 or so). EFA is for finding factors that explain variability across items. Singleton factors do not do this. When your solution has a singleton factor, it is generally best to extract 1 factor fewer. However, doing so does not always get rid of the singleton factor. It might collapse two legitimate factors into one factor. In this case, you probably want to return to the previous solution with the singleton (or exclude the problematic item from the analysis)."
  },
  {
    "objectID": "tutorials/EFA/index.html#factor-correlations",
    "href": "tutorials/EFA/index.html#factor-correlations",
    "title": "Exploratory Factor Analysis with R",
    "section": "Factor correlations",
    "text": "Factor correlations\nThe correlation matrrix of the latent factors in a an EFA is sometimes referred to as the Φ (Phi) matrix. To extract the factor correlations from the fa function’s output, we can do like so:\n\nfa(d_bfi, nfactors = 5, fm = \"pa\")$Phi\n\n         PA2     PA1     PA3     PA5      PA4\nPA2  1.00000  0.2282 -0.1897 -0.0733  0.03735\nPA1  0.22822  1.0000 -0.2204 -0.3185 -0.18808\nPA3 -0.18967 -0.2204  1.0000  0.2160  0.18971\nPA5 -0.07330 -0.3185  0.2160  1.0000  0.26729\nPA4  0.03735 -0.1881  0.1897  0.2673  1.00000\n\n\nPlotted, the Φ matrix look like this:\n\n\nCode\nfactor_labels &lt;- c(\"Extraversion\", \"Neuroticism\", \"Conscientiousness\", \"Openness\", \"Agreeableness\")\n\nfa(d_bfi, nfactors = 5, fm = \"pa\")$Phi %&gt;% \n  corrr::as_cordf(diagonal = 1) %&gt;% \n  corrr::stretch() %&gt;% \n  mutate(x = factor(x, labels = factor_labels),\n         y = factor(y, labels = factor_labels)) %&gt;% \n  ggplot(aes(x,y)) + \n  geom_tile(aes(fill = r)) + \n  geom_text(aes(label = WJSmisc::prob_label(r)), family  = \"Roboto Condensed\", hjust = 1, nudge_x = .1, size.unit = \"pt\", size = 12) + \n  scale_fill_gradient2(limits = c(-1,1), labels = \\(x) WJSmisc::prob_label(x, .1), breaks = seq(-1,1,.1)) + \n  theme_minimal(base_family = \"Roboto Condensed\", base_size = 12) + \n  theme(legend.key.height = unit(1, \n                                 units = \"null\"), \n        legend.text = element_text(hjust = 1)) +\n  scale_x_discrete(NULL, expand = expansion()) + \n  scale_y_discrete(NULL, expand = expansion()) +\n  coord_equal()\n\n\n\n\n\n\n\n\n\nThe correlations among these factors are generally low (&lt; 0.3 or so). Thus, the principal components method of parallel analysis is likely more accurate than the factor analysis version of parallel analysis. Thus, we should probably stick with 5 factors.\nHowever, we should probably extract 6 factors and see what we get. After all, this is exploratory work.\n\nfa(d_bfi, nfactors = 6, fm = \"pa\") %&gt;% \n  fa.sort() %&gt;% \n  print(cut = 0.2)\n\nFactor Analysis using method =  pa\nCall: fa(r = d_bfi, nfactors = 6, fm = \"pa\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                                            PA2   PA1   PA3   PA5   PA4   PA6\nGet angry easily.                          0.84                              \nGet irritated easily.                      0.83                              \nHave frequent mood swings.                 0.67                              \nPanic easily.                              0.45  0.24                        \nOften feel blue.                           0.43  0.42                        \nFind it difficult to approach others.            0.69                        \nDon't talk a lot.                                0.59                        \nMake friends easily.                            -0.52        0.21        0.30\nTake charge.                                    -0.40  0.27        0.25      \nContinue until everything is perfect.                  0.67                  \nDo things in a half-way manner.                       -0.65              0.28\nDo things according to a plan.                         0.55                  \nWaste my time.                                        -0.55                  \nAm exacting in my work.                                0.54                  \nInquire about others' well-being.                            0.68            \nKnow how to comfort others.                                  0.62            \nAm indifferent to the feelings of others.                   -0.56        0.29\nMake people feel at ease.                                    0.45        0.23\nLove children.                                               0.40            \nCarry the conversation to a higher level.                          0.65      \nAm full of ideas.                                                  0.57      \nWill not probe deeply into a subject.                             -0.45  0.38\nKnow how to captivate people.                   -0.33              0.39  0.22\nSpend time reflecting on things.                 0.35              0.39      \n                                            h2   u2 com\nGet angry easily.                         0.68 0.32 1.0\nGet irritated easily.                     0.66 0.34 1.0\nHave frequent mood swings.                0.55 0.45 1.2\nPanic easily.                             0.34 0.66 2.3\nOften feel blue.                          0.49 0.51 2.4\nFind it difficult to approach others.     0.56 0.44 1.1\nDon't talk a lot.                         0.39 0.61 1.3\nMake friends easily.                      0.55 0.45 2.0\nTake charge.                              0.40 0.60 2.9\nContinue until everything is perfect.     0.49 0.51 1.3\nDo things in a half-way manner.           0.55 0.45 1.5\nDo things according to a plan.            0.31 0.69 1.1\nWaste my time.                            0.43 0.57 1.5\nAm exacting in my work.                   0.34 0.66 1.3\nInquire about others' well-being.         0.50 0.50 1.1\nKnow how to comfort others.               0.51 0.49 1.2\nAm indifferent to the feelings of others. 0.34 0.66 1.7\nMake people feel at ease.                 0.48 0.52 2.3\nLove children.                            0.28 0.72 2.2\nCarry the conversation to a higher level. 0.47 0.53 1.0\nAm full of ideas.                         0.35 0.65 1.1\nWill not probe deeply into a subject.     0.34 0.66 2.0\nKnow how to captivate people.             0.48 0.52 2.9\nSpend time reflecting on things.          0.27 0.73 2.4\n\n                       PA2  PA1  PA3  PA5  PA4  PA6\nSS loadings           2.46 2.14 2.04 1.88 1.57 0.69\nProportion Var        0.10 0.09 0.08 0.08 0.07 0.03\nCumulative Var        0.10 0.19 0.28 0.35 0.42 0.45\nProportion Explained  0.23 0.20 0.19 0.17 0.15 0.06\nCumulative Proportion 0.23 0.43 0.62 0.79 0.94 1.00\n\n With factor correlations of \n      PA2   PA1   PA3   PA5   PA4   PA6\nPA2  1.00  0.25 -0.19 -0.11  0.03  0.12\nPA1  0.25  1.00 -0.21 -0.29 -0.20 -0.10\nPA3 -0.19 -0.21  1.00  0.19  0.19  0.01\nPA5 -0.11 -0.29  0.19  1.00  0.26  0.15\nPA4  0.03 -0.20  0.19  0.26  1.00  0.08\nPA6  0.12 -0.10  0.01  0.15  0.08  1.00\n\nMean item complexity =  1.7\nTest of the hypothesis that 6 factors are sufficient.\n\ndf null model =  276  with the objective function =  6.99 with Chi Square =  19512\ndf of  the model are 147  and the objective function was  0.34 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic n.obs is  2760 with the empirical chi square  579.3  with prob &lt;  1.2e-52 \nThe total n.obs was  2800  with Likelihood Chi Square =  960.3  with prob &lt;  1.7e-119 \n\nTucker Lewis Index of factoring reliability =  0.921\nRMSEA index =  0.044  and the 90 % confidence intervals are  0.042 0.047\nBIC =  -206.5\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   PA2  PA1  PA3  PA5  PA4  PA6\nCorrelation of (regression) scores with factors   0.93 0.89 0.88 0.87 0.85 0.74\nMultiple R square of scores with factors          0.86 0.79 0.78 0.76 0.72 0.55\nMinimum correlation of possible factor scores     0.72 0.58 0.56 0.53 0.44 0.10\n\n\nThe output does not all fit so the loadings for PA6 are below the loadings of the other factors.\nNotice that all the PA6 loadings are small. Also notice that they do not really make sense to group together. For example, what construct causes people to be “indifferent to the feelings of others” and at the same time “Make people feel at ease”?\nTwo hypotheses:\n\nRead in the right manner, all of the items are consistent with a person who uses charm and guile to manipulate others.\nFour of the six items have words or phrases that require a higher reading level (indifferent, probe, captivate, half-way manner). Perhaps the factor emerged because of differences in literacy.\n\nBoth these hypotheses might be true. They are not mutually exclusive.\nHowever, given that PA6 has no strong loadings, it is a weak factor and possibly a junk factor. Furthermore, our parallel analyses suggested we retain 5 factors, not 6."
  },
  {
    "objectID": "tutorials/ANOVA/ANOVA.html",
    "href": "tutorials/ANOVA/ANOVA.html",
    "title": "ANOVA in R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(easystats)"
  },
  {
    "objectID": "tutorials/ANOVA/ANOVA.html#basic-descriptives",
    "href": "tutorials/ANOVA/ANOVA.html#basic-descriptives",
    "title": "ANOVA in R",
    "section": "Basic descriptives",
    "text": "Basic descriptives\nNumeric variables can be described in terms of means, standard deviations (SD), interquartile ranges (IQR), skewness, and kurtosis:\n\ndescribe_distribution(d)\n\nVariable              |  Mean |    SD |   IQR |          Range | Skewness | Kurtosis |   n | n_Missing\n------------------------------------------------------------------------------------------------------\nAnxiety_Physiological | 60.49 | 12.79 | 16.43 | [22.78, 93.88] |    -0.29 |     0.24 | 300 |         0\nAnxiety_Questionnaire | 55.11 | 13.30 | 18.24 | [21.79, 92.38] |     0.05 |    -0.16 | 300 |         0\n\n\nCategorical variables are often described in terms of frequencies tables.\n\nd %&gt;% \n  select(Attachment) %&gt;% \n  data_tabulate()\n\nAttachment (Attachment) &lt;categorical&gt;\n# total N=300 valid N=300\n\nValue      |   N | Raw % | Valid % | Cumulative %\n-----------+-----+-------+---------+-------------\nSecure     | 110 | 36.67 |   36.67 |        36.67\nAvoidant   |  85 | 28.33 |   28.33 |        65.00\nAmbivalent | 105 | 35.00 |   35.00 |       100.00\n&lt;NA&gt;       |   0 |  0.00 |    &lt;NA&gt; |         &lt;NA&gt;\n\n\nWe can get descriptives statistics of our numeric variables separated by group. There are many ways to do this, the simplest is the describeBy function from the psych package\n\npsych::describeBy(d, group = \"Attachment\")\n\n\n Descriptive statistics by group \nAttachment: 1\n                      vars   n mean sd median trimmed mad min max range  skew\nAttachment               1 110    1  0      1       1   0   1   1     0   NaN\nAnxiety_Physiological    2 110   50 11     51      51  12  23  70    48 -0.44\nAnxiety_Questionnaire    3 110   50 11     51      51  12  23  70    46 -0.36\n                      kurtosis se\nAttachment                 NaN  0\nAnxiety_Physiological    -0.41  1\nAnxiety_Questionnaire    -0.61  1\n------------------------------------------------------------ \nAttachment: 2\n                      vars  n mean   sd median trimmed mad min max range skew\nAttachment               1 85    2  0.0      2       2 0.0   2   2     0  NaN\nAnxiety_Physiological    2 85   67  9.8     67      67 9.5  46  94    48 0.27\nAnxiety_Questionnaire    3 85   48 10.6     48      48 9.9  22  74    52 0.11\n                      kurtosis  se\nAttachment                 NaN 0.0\nAnxiety_Physiological    -0.13 1.1\nAnxiety_Questionnaire    -0.10 1.1\n------------------------------------------------------------ \nAttachment: 3\n                      vars   n mean  sd median trimmed mad min max range skew\nAttachment               1 105    3 0.0      3       3 0.0   3   3     0  NaN\nAnxiety_Physiological    2 105   66 9.3     65      66 7.8  45  94    48 0.19\nAnxiety_Questionnaire    3 105   66 9.9     66      66 9.3  44  92    48 0.31\n                      kurtosis   se\nAttachment                 NaN 0.00\nAnxiety_Physiological     0.08 0.91\nAnxiety_Questionnaire    -0.10 0.97\n\n\nThe datawizard package has some really nice functions for descriptives:\n\nd %&gt;% \n  datawizard::means_by_group(\n    select = contains(\"Anxiety\") , \n    group = \"Attachment\")\n\n# Mean of Anxiety_Physiological by Attachment\n\nCategory   |  Mean |   N |    SD |         95% CI |      p\n----------------------------------------------------------\nSecure     | 49.95 | 110 | 10.67 | [48.08, 51.83] | &lt; .001\nAvoidant   | 67.20 |  85 |  9.84 | [65.07, 69.33] | &lt; .001\nAmbivalent | 66.09 | 105 |  9.33 | [64.17, 68.00] | &lt; .001\nTotal      | 60.49 | 300 | 12.79 |                |       \n\nAnova: R2=0.395; adj.R2=0.391; F=96.988; p&lt;.001\n\n# Mean of Anxiety_Questionnaire by Attachment\n\nCategory   |  Mean |   N |    SD |         95% CI |      p\n----------------------------------------------------------\nSecure     | 50.09 | 110 | 10.90 | [48.13, 52.05] | &lt; .001\nAvoidant   | 47.82 |  85 | 10.57 | [45.59, 50.06] | &lt; .001\nAmbivalent | 66.27 | 105 |  9.90 | [64.26, 68.28] | &lt; .001\nTotal      | 55.11 | 300 | 13.30 |                |       \n\nAnova: R2=0.385; adj.R2=0.381; F=92.975; p&lt;.001\n\n\nThese are convenience functions. If they do exactly what you want, great! However, I often need statistics in a data frame in a format optimal for further processing (e.g., for publication-worthy tables or plots).\n\nd_descriptives &lt;- d %&gt;% \n  pivot_longer(contains(\"Anxiety\"), \n               names_to = \"Measure\") %&gt;% \n  summarise(Mean = mean(value),\n            SD = sd(value),\n            n = n(),\n            .by = c(Attachment, Measure)) %&gt;% \n  mutate(Measure = snakecase::to_title_case(Measure))\n\nd_descriptives\n\n# A tibble: 6 × 5\n  Attachment Measure                Mean    SD     n\n  &lt;fct&gt;      &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Secure     Anxiety Physiological  50.0 10.7    110\n2 Secure     Anxiety Questionnaire  50.1 10.9    110\n3 Ambivalent Anxiety Physiological  66.1  9.33   105\n4 Ambivalent Anxiety Questionnaire  66.3  9.90   105\n5 Avoidant   Anxiety Physiological  67.2  9.84    85\n6 Avoidant   Anxiety Questionnaire  47.8 10.6     85\n\n\nThis output is not pretty, but it was not intended to be. It is in a format that is easy to adapt for other things. For example, I can use the gt (“Great Tables”) package to get exactly what I want to display:\n\n\nCode\nlibrary(gt)\nd_descriptives %&gt;% \n  gt(groupname_col = \"Measure\", \n     rowname_col = \"Attachment\") %&gt;%\n  tab_stub_indent(everything(), indent = 5)\n\n\n\n\n\n\n\n\n\n\nMean\nSD\nn\n\n\n\n\nAnxiety Physiological\n\n\nSecure\n50\n10.7\n110\n\n\nAmbivalent\n66\n9.3\n105\n\n\nAvoidant\n67\n9.8\n85\n\n\nAnxiety Questionnaire\n\n\nSecure\n50\n10.9\n110\n\n\nAmbivalent\n66\n9.9\n105\n\n\nAvoidant\n48\n10.6\n85\n\n\n\n\n\n\n\n\nOr I can use ggplot2 to display means and standard deviations graphically:\n\n\nCode\nd_descriptives %&gt;%\n  mutate(mean_label = scales::number(Mean, .1)) %&gt;% \n  ggplot(aes(Attachment, Mean)) +\n  facet_grid(cols = vars(Measure)) +\n  geom_pointrange(aes(ymin = Mean - SD,\n                      ymax = Mean + SD)) +\n  geom_label(\n    aes(label = mean_label), \n    hjust = -.3)"
  },
  {
    "objectID": "tutorials/ANOVA/ANOVA.html#diagnostic-plots",
    "href": "tutorials/ANOVA/ANOVA.html#diagnostic-plots",
    "title": "ANOVA in R",
    "section": "Diagnostic Plots",
    "text": "Diagnostic Plots\n\ncheck_model(fit)\n\n\n\n\n\n\n\n\nNone of the diagnostic checks raise any alarms. Specifically, the residuals are reasonably normal and have consistent variance acrross groups (i.e., the homogeneeity of variance assumption is reasonable)."
  },
  {
    "objectID": "tutorials/ANOVA/ANOVA.html#summary",
    "href": "tutorials/ANOVA/ANOVA.html#summary",
    "title": "ANOVA in R",
    "section": "Summary",
    "text": "Summary\nThe base R function summary gives us most of what we might want to know.\n\nsummary(fit)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nAttachment    2  20359   10179      93 &lt;2e-16 ***\nResiduals   297  32517     109                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can see that overall model is significant, meaning that at least two groups have reliably different means. At this point we do not know which means differ, but from the plot we have a good guess that the Ambivalent group scores higher on the Anxiety Questionnaire than the other two groups.\nA similar, but tidier display can be found with the parameters function:\n\nparameters(fit)\n\nParameter  | Sum_Squares |  df | Mean_Square |     F |      p\n-------------------------------------------------------------\nAttachment |    20358.60 |   2 |    10179.30 | 92.98 | &lt; .001\nResiduals  |    32516.83 | 297 |      109.48 |       |       \n\nAnova Table (Type 1 tests)"
  },
  {
    "objectID": "tutorials/ANOVA/ANOVA.html#other-summary-functions",
    "href": "tutorials/ANOVA/ANOVA.html#other-summary-functions",
    "title": "ANOVA in R",
    "section": "Other Summary Functions",
    "text": "Other Summary Functions\nThe overall “fit” or “performance” of the model:\n\nperformance(fit)\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |   RMSE |  Sigma\n--------------------------------------------------------------------\n2265.082 | 2265.218 | 2279.897 | 0.385 |     0.381 | 10.411 | 10.463\n\n\n\nAIC: Akaike’s Information Criterion\nAICc: AIC with a correction for small sample sizes\nBIC: Bayesian Information Criterion\nR2: R-squared: Coefficient of Determination\nR2_adj: Adjusted r-squared\nRMSE: Root mean squared error =\\sqrt{\\frac{\\sum_{i = 1}^{n}{e_i}}{n}}\nSIGMA: Residual standard deviation (The SD of the residuals =\\sqrt{\\frac{\\sum_{i = 1}^{n}{e_i}}{n-k-1}})"
  },
  {
    "objectID": "tutorials/ANOVA/ANOVA.html#automated-interpretation",
    "href": "tutorials/ANOVA/ANOVA.html#automated-interpretation",
    "title": "ANOVA in R",
    "section": "Automated Interpretation",
    "text": "Automated Interpretation\n\nreport(fit)\n\nThe ANOVA (formula: Anxiety_Questionnaire ~ Attachment) suggests that:\n\n  - The main effect of Attachment is statistically significant and large (F(2,\n297) = 92.98, p &lt; .001; Eta2 = 0.39, 95% CI [0.32, 1.00])\n\nEffect sizes were labelled following Field's (2013) recommendations."
  },
  {
    "objectID": "tutorials/ANOVA/ANOVA.html#post-hoc-tests",
    "href": "tutorials/ANOVA/ANOVA.html#post-hoc-tests",
    "title": "ANOVA in R",
    "section": "Post-hoc Tests",
    "text": "Post-hoc Tests\nWe would like to compare all means but control for family-wise error.\n\nestimate_contrasts(fit)\n\nNo variable was specified for contrast estimation. Selecting `contrast = \"Attachment\"`.\n\n\nMarginal Contrasts Analysis\n\nLevel1   |     Level2 | Difference |           95% CI |   SE | t(297) |      p\n------------------------------------------------------------------------------\nAvoidant | Ambivalent |     -18.44 | [-22.12, -14.77] | 1.53 | -12.08 | &lt; .001\nSecure   | Ambivalent |     -16.18 | [-19.62, -12.74] | 1.43 | -11.33 | &lt; .001\nSecure   |   Avoidant |       2.27 | [ -1.37,   5.90] | 1.51 |   1.50 | 0.135 \n\nMarginal contrasts estimated at Attachment\np-value adjustment method: Holm (1979)\n\nestimate_contrasts(fit) |&gt; \n  report()\n\nNo variable was specified for contrast estimation. Selecting `contrast = \"Attachment\"`.\n\n\nThe marginal contrasts analysis suggests the following. The difference between\nAvoidant and Ambivalent is negative and statistically significant (difference =\n-18.44, 95% CI [-22.12, -14.77], t(297) = -12.08, p &lt; .001). The difference\nbetween Secure and Ambivalent is negative and statistically significant\n(difference = -16.18, 95% CI [-19.62, -12.74], t(297) = -11.33, p &lt; .001). The\ndifference between Secure and Avoidant is positive and statistically\nnon-significant (difference = 2.27, 95% CI [ -1.37, 5.90], t(297) = 1.50, p =\n0.135)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "A free online book about psychometric tools for psychological assessment\n\nI have been working on a book project off and on since 2012. I have ambitious goals for this book, but it is not nearly complete. The preliminary, unfinished version is here.\n\n\n\n\n\nAn introduction to tests and measurements\nRenée Tobin and I helped update Ronald Cohen’s Psychological Testing and Assessment: An Introduction to Tests and Measurements. It is a comprehensive textbook suitable for introductory psychological assessment courses.\n\n\n\n\n\n\n\n\n\nA book about making reader-friendly assessment reports that instill hope and promote change.\nMy co-authors and I worked hard to fill the second edition of Essentials of Assessment Report Writing with non-obvious guidance about how to write reports so that the meet the needs of the reports’ readers."
  },
  {
    "objectID": "projects.html#individual-psychometrics-an-assessment-toolkit-with-applications-in-r",
    "href": "projects.html#individual-psychometrics-an-assessment-toolkit-with-applications-in-r",
    "title": "Projects",
    "section": "",
    "text": "A free online book about psychometric tools for psychological assessment\n\nI have been working on a book project off and on since 2012. I have ambitious goals for this book, but it is not nearly complete. The preliminary, unfinished version is here."
  },
  {
    "objectID": "projects.html#psychological-testing-and-assessment",
    "href": "projects.html#psychological-testing-and-assessment",
    "title": "Projects",
    "section": "",
    "text": "An introduction to tests and measurements\nRenée Tobin and I helped update Ronald Cohen’s Psychological Testing and Assessment: An Introduction to Tests and Measurements. It is a comprehensive textbook suitable for introductory psychological assessment courses."
  },
  {
    "objectID": "projects.html#essentials-of-assessment-report-writing",
    "href": "projects.html#essentials-of-assessment-report-writing",
    "title": "Projects",
    "section": "",
    "text": "A book about making reader-friendly assessment reports that instill hope and promote change.\nMy co-authors and I worked hard to fill the second edition of Essentials of Assessment Report Writing with non-obvious guidance about how to write reports so that the meet the needs of the reports’ readers."
  },
  {
    "objectID": "posts/splitting-sentence-texts-into-roughly-equal-segments/index.html",
    "href": "posts/splitting-sentence-texts-into-roughly-equal-segments/index.html",
    "title": "Splitting sentence texts into roughly equal segments",
    "section": "",
    "text": "When analyzing questionnaire data, I sometimes put the text of several questions on the y-axis of a plot. Longer text strings will usually make the plot too narrow.\nSuppose we give an admittedly silly questionnaire about preference for darker food. We want to display the percentages of people answering “Yes” to each question.\nHere are the questions and the (completely made-up) percentages:\n# To install the WJSmisc package:\n# remotes::install_github(\"wjschne/WJSmisc\")\nlibrary(WJSmisc)\nlibrary(tidyverse)\nd &lt;- tibble(Item = c(\"When you drink coffee, do you prefer darker roasts?\",\n                     \"Is pumpernickel the best bread?\",\n                     \"Do you like dark chocolate more than you like milk chocolate?\",\n                     \"Are black olives better than green olives?\",\n                     \"Do you enjoy the taste of prunes more than can be prudently admitted aloud?\",\n                     \"Is black licorice the secret to a happly life?\"),\n            Percentage = c(45,4,37, 84,2, 3)) |&gt; \n  mutate(Item = fct_reorder(Item, Percentage))\nA preliminary plot might look like this:\nmyfont &lt;- \"Roboto Condensed\"\nmycolors &lt;- viridis::viridis(n = nrow(d), begin = .2, end = .8) |&gt; tinter::lighten(.5)\nmytextsize &lt;- 17\nmytext &lt;- \"gray60\"\np &lt;- ggplot(d, aes(Percentage, Item)) + \n  geom_col(aes(fill = factor(Percentage))) + \n  geom_richlabel(aes(label = paste0(Percentage, \"%\"), \n                     color = factor(Percentage)), \n                 hjust = 0, fill = \"black\",\n                 text_size = mytextsize,\n                 family = myfont) +\n  labs(y = NULL,\n       caption = \"Note: These numbers were made up for illustrative purposes.\") +\n  theme_minimal(base_size = mytextsize, base_family = myfont) +\n  scale_x_continuous(\"Percent Yes\", \n                     limits = c(0,100), \n                     breaks = NULL,\n                     expand = expansion(mult = c(0,.2))) +\n  scale_fill_manual(values = mycolors) +\n  scale_color_manual(values = mycolors) +\n  theme(plot.title.position = \"plot\", \n        plot.background = element_rect(\"black\"),\n        plot.title = element_text(colour = mytext),\n        plot.caption = element_text(color = \"gray40\"),\n        legend.position = \"none\",\n        axis.title = element_text(color = mytext),\n        panel.grid.major.y = element_blank(),\n        axis.text = element_text(color = mycolors))\np + ggtitle(\"Y-axis labels with no text wrapping\")\nThe questions are taking up a lot of space. We can fix this by using stringr::str_wrap to make the text lines no longer than a specified width:\np + \n  scale_y_discrete(labels = \\(x) str_wrap(x, width = 30)) + \n  ggtitle(\"Splitting Y-axis labels with str_wrap\")\nThis is a definite improvement, but I wish the lines of text were wrapped more evenly. For example, I do not like the fact that “chocolate” and “bread” are by themselves on their own lines. I have spent a fair amount of time breaking lines up by hand and hoped to create a function to automate the process. To achieve a more balanced style of text splitting, I made the str_wrap_equal function.\np + scale_y_discrete(labels = \\(x) str_wrap_equal(x, max_width = 30)) + \n  ggtitle(\"Splitting Y-axis labels with str_wrap_equal\")\nTo each their own, but this style of text wrapping feels better to me here. It is the sort of thing no one but the analyst will notice if it is right but will feel a bit off if not.\nFor your convenience and mine, I put the str_wrap_equal function in the WJSmisc package."
  },
  {
    "objectID": "posts/splitting-sentence-texts-into-roughly-equal-segments/index.html#technical-details",
    "href": "posts/splitting-sentence-texts-into-roughly-equal-segments/index.html#technical-details",
    "title": "Splitting sentence texts into roughly equal segments",
    "section": "Technical Details",
    "text": "Technical Details\nI hesitate to post a new function because the odds are quite high that someone has already done what I did. However, my initial search did not locate a function that does exactly what I needed. If you know of a function have I duplicated unnecessarily, let me know.\nIn deciding whether to break the line, the function does the following:\n\nCalculate the overall length of the text (i.e., number of characters).\nFind k (the number of lines likely needed) by dividing the overall text length by max_width and round up to the nearest integers.\nCalculate the preferred width of the line by dividing the overall text length by the number of lines likely needed.\nAdd to the line one word at a time as long as doing so makes the current line width closer to the preferred width. If not, a new line is started.\nThe function will not make a line longer than what is specified by max_width unless it has to place a single, lonely word that is longer than max_width.\n\nHere is the code for the function. Any suggestions to improve it are welcome.\n\nstr_wrap_equal &lt;- function(x, max_width = 30L, sep = \"\\n\") {\n  purrr::map_chr(x, \\(xi) {\n    # Find overall text length\n    xlen &lt;- stringr::str_length(xi)\n    # Remove any line breaks and allow lines to break at forward slashes\n    xi &lt;- stringr::str_replace(xi, \"\\n\", \" \") |&gt; \n      stringr::str_replace(\"/\", \"/ \")\n    # Number of lines likely needed\n    k &lt;- ceiling(xlen / max_width)\n    # Optimal line length\n    preferred_width &lt;- xlen / k\n    # Split text into words\n    words &lt;- stringr::str_split(xi, pattern = \" \", simplify = F)[[1]]\n    # Number of words in text\n    k_words &lt;- length(words)\n    # Length of each word in text\n    word_len &lt;- stringr::str_length(words)\n    # Create empty text lines with a few extra, if needed\n    textlines &lt;- rep(\"\", k + 10)\n    # Current text line\n    i &lt;- 1\n    # Decide whether to add a word to the current line or to start a new line\n    for (w in seq(k_words)) {\n      # Width of current line before adding a new word\n      current_width &lt;- stringr::str_length(textlines[i])\n      # Width of current line if a new word is added\n      proposed_width &lt;- current_width + word_len[w] + 1\n      # Difference between current width and preferred width\n      current_difference &lt;- abs(current_width - preferred_width)\n      # Difference between proposed width and preferred width\n      proposed_difference &lt;- abs(proposed_width - preferred_width)\n      # Should we start a new line?\n      if (current_difference &lt; proposed_difference | proposed_width &gt; max_width) {\n        i &lt;- i + 1\n      }\n      # Add word to current line, remove spaces, and rejoin words divided by forward slashes\n      textlines[i] &lt;- stringr::str_trim(paste(textlines[i], words[w])) |&gt;\n        stringr::str_replace(\"/ \", \"/\")\n    }\n    # Collapse non-empty lines by separation character\n    paste0(textlines[stringr::str_length(textlines) &gt; 0], collapse = sep)\n  })\n\n}"
  },
  {
    "objectID": "posts/making-text-labels-the-same-size-as-axis-labels-in-ggplot2/index.html",
    "href": "posts/making-text-labels-the-same-size-as-axis-labels-in-ggplot2/index.html",
    "title": "Making text labels the same size as axis labels in ggplot2",
    "section": "",
    "text": "As explained in this ggplot2 vignette, the size parameter in geom_text and geom_label is in millimeters, and the size parameter in all other text elements in ggplot2 is in points.\nIf I specify the base_size of the plot and the size of a label to 16, you can see that the text label is much bigger than 16.\n\nlibrary(tidyverse)\ntextsize &lt;- 16\nggplot() +\n  stat_function(\n    fun = dnorm,\n    xlim = c(-4, 4),\n    geom = \"area\",\n    alpha = .3\n  ) +\n  theme_minimal(base_size = textsize) +\n  annotate(\n    geom = \"text\",\n    x = 0,\n    y = 0,\n    label = \"Mean = 0\",\n    size = textsize,\n    vjust = -.1\n  )\n\n\n\n\n\n\n\n\nIf you do not mind a little trial and error, you can fiddle with the size parameter until you find a value that looks good to you. However, what if we want the axis text labels and the output of geom_text to be exactly the same size?\nWe are in luck because ggplot2 has a .pt constant that will help us convert point sizes to millimeters.\n\nggplot2::.pt\n\n[1] 2.845276\n\n\nWe know that, by default, axis text is .8 times as large as the base_size of the theme.\nLet’s make a function to automate the conversion:\n\nggtext_size &lt;- function(base_size, ratio = 0.8) {\n  ratio * base_size / ggplot2::.pt\n}\n\nNow we can make the label and axis text exactly the same size:\n\nggplot() + \n  stat_function(fun = dnorm, xlim = c(-4,4), geom = \"area\", alpha = .3) +\n  theme_minimal(base_size = textsize) + \n  annotate(\n    geom = \"text\",\n    x = 0,\n    y = 0,\n    label = \"Mean = 0\",\n    size = ggtext_size(textsize),\n    vjust = -.3\n  )\n\n\n\n\n\n\n\n\nFor my own convenience (and possibly yours), I put the ggtext_size function in the WJSmisc package.\nFor more on ggplot font size matters, I found this post by Christophe Nicault to be informative.\n\n\n\nCitationBibTeX citation:@misc{schneider2021,\n  author = {Schneider, W. Joel},\n  title = {Making Text Labels the Same Size as Axis Labels in Ggplot2},\n  date = {2021-08-10},\n  url = {https://wjschne.github.io/posts/making-text-labels-the-same-size-as-axis-labels-in-ggplot2},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2021, August 10). Making text labels the same size as\naxis labels in ggplot2. Schneirographs. https://wjschne.github.io/posts/making-text-labels-the-same-size-as-axis-labels-in-ggplot2"
  },
  {
    "objectID": "posts/creating-collapsible-output-with-knitr-chunk-hooks/index.html",
    "href": "posts/creating-collapsible-output-with-knitr-chunk-hooks/index.html",
    "title": "Creating collapsible output with knitr chunk hooks",
    "section": "",
    "text": "I am writing a (still very much unfinished) psychometrics book with suggested exercises using R. I wanted to provide solutions to the questions, but I wanted to hide the answers until the reader clicks them.\nAn easy way to create collapsible content is with html’s &lt;details&gt; tag. For example:\nCreate a variable `x`, and assign it a value of 5.\n\n&lt;details&gt;\n  &lt;summary&gt;Suggested Solution&lt;/summary&gt;\n\n```{r}\nx &lt;- 5\n```\n\n&lt;/details&gt;\nCreate a variable x, and assign it a value of 5.\n\n\nSuggested Solution\n\n\nx &lt;- 5\n\n\n\nHowever, I did not want to worry about all the &lt;details&gt; every time I wrote a question. To automate this process, I used two knitr hooks: a chunk hook and an option hook.\nChunk hooks are for customizing the output of a chunk.\nHere I create a new chunk hook that is run before and after the chunk. When it is run before the chunk, it adds the &lt;details&gt; and &lt;summary&gt; tags. After the chunk, it closes the &lt;details&gt; tag.\n\n# Add the details tag before running the chunk \n# and close the tag after running the chunk.\nknitr::knit_hooks$set(\n  solutionsetter = function(before,options) {\n  \n  if (before) {\n    \n    \"\\n\\n&lt;details&gt;&lt;summary&gt;Suggested Solution&lt;/summary&gt;\\n\\n\"\n    \n  } else {\n    \n    \"\\n\\n&lt;/details&gt;\\n\\n\"\n    \n  }\n})\n\nIf all we needed to do was to enclose the output into a &lt;details&gt; tag, then just the solutionsetter hook would be needed. However, I also wanted to set echo=TRUE so that the output would be visible even if the default for echo was FALSE.\nOption hooks are great for when you have a kind of chunk you will use often, and it requires setting many chunk options at once (e.g., a plot variant with different dimensions than your default plot has).\nHere I create a new option hook called solution. It sets the current chunk’s echo option to TRUE and also triggers the new solutionsetter code chunk below. The updated options list needs to be returned explicitly, or the hook will not work.\n\nknitr::opts_hooks$set(solution = function(options) {\n  options$echo &lt;- TRUE\n  options$solutionsetter &lt;- TRUE\n  return(options)\n})\n\nNow all need to do is to set my chunk option to solution = TRUE and the output will be collapsible:\n\nCreate a variable `x`, and assign it a value of 5.\n\n```{r, solution = TRUE}\nx &lt;- 5\n```\nCreate a variable x, and assign it a value of 5.\n\n\n\nSuggested Solution\n\nx &lt;- 5\n\n\n\n\nThe details package\nIf you want a function to enclose the output of a chunk or inline code in &lt;details&gt; tags, use the details package by Jonathan Sidi. For example, an inline chunk like this:\n`r details::details(\"Answer\", summary = \"Question\")`\nWill output like so:\n\n\n Question \n\n\nAnswer\n\n\nThe package has many other uses, which are explained in the package’s vignettes.\n\n\n\n\nCitationBibTeX citation:@misc{schneider2023,\n  author = {Schneider, W. Joel},\n  title = {Creating Collapsible Output with Knitr Chunk Hooks},\n  date = {2023-06-30},\n  url = {https://wjschne.github.io/posts/creating-collapsible-output-with-knitr-chunk-hooks},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2023, June 30). Creating collapsible output with knitr\nchunk hooks. Schneirographs. https://wjschne.github.io/posts/creating-collapsible-output-with-knitr-chunk-hooks"
  },
  {
    "objectID": "posts/caption-on-same-line-as-axis-title-in-ggplot2/index.html",
    "href": "posts/caption-on-same-line-as-axis-title-in-ggplot2/index.html",
    "title": "Caption on same line as axis title in ggplot2",
    "section": "",
    "text": "Sometimes I want to put a plot caption in the lower right corner of the plot.\n\nlibrary(tidyverse)\np &lt;- ggplot() +\n  stat_function(xlim = c(-4,4), fun = dnorm, n = 801) + \n  labs(x = \"z-scores\", caption = \"Note: Mean = 0, SD = 1\")\n\np\n\n\n\n\n\n\n\n\nHowever, I want the caption to be a little higher, on the same line as the x-axis title. To do so, set a a negative top margin:\n\np + theme(plot.caption = element_text(margin = margin(t = -10, unit = \"pt\")))\n\n\n\n\n\n\n\n\nHere is a more finished example.\n\nlibrary(ggnormalviolin)\nlibrary(ggtext)\nmy_font_size = 16\nmy_font &lt;- \"Roboto Condensed\"\nupdate_geom_defaults(geom = \"text\",new = list(family = my_font) )\nupdate_geom_defaults(geom = \"label\",new = list(family = my_font) )\nupdate_geom_defaults(\"richtext\", list(family = my_font))\ntheme_set(theme_minimal(base_size = my_font_size, base_family = my_font))\n\nd_rect &lt;- tibble(SS = 100, \n                 width = c(20, 40, 60, 80, 122), \n                 fill = paste0(\"gray\", c(95, 90, 80, 70, 65) - 25)) %&gt;% \n  arrange(-width)\n\ntibble(\n  Scale = c(\n    \"Fluid Reasoning\",\n    \"Verbal Comprehension\",\n    \"Visual-Spatial Processing\",\n    \"Working Memory\",\n    \"Processing Speed\",\n    \"Population\"),\n  y = c(5:1 - 0.5, 0),\n  SS = c(115, 111, 109, 86, 79, 100),\n  rxx = c(.93, .92, .92, .92, .88, 0),\n  width = c(rep(1.4, 5), 10.4),\n  alpha = c(rep(1, 5), .3)\n) %&gt;% \n  mutate(true_hat = rxx * (SS - 100) + 100, \n         see = ifelse(rxx == 0, 15, 15 * sqrt(rxx - rxx ^ 2)),\n         Scale = fct_inorder(Scale) %&gt;% fct_rev()) %&gt;% \n  ggplot(aes(y, SS)) + \n  geom_tile(data = d_rect, aes(width = 5.8, x = 2.9, \n                               fill = fill, \n                               height = width, \n                               y = SS)) + \n  geom_normalviolin(aes(mu = true_hat, \n                                        sigma = see, \n                                        width = width,\n                                        alpha = alpha), \n                                    face_left = F, \n                    fill = \"white\") + \n  geom_richtext(aes(label = ifelse(\n    Scale == \"Population\", \n    \"Population Mean\", \n    paste0(\"&lt;span style='font-size:8.5pt;color:white'&gt;(\", \n           round(100 * pnorm(SS, 100, 15),0),\n           \") &lt;/span&gt;\",\n           SS, \n           \"&lt;span style='font-size:9pt;color:#666666'&gt; (\", \n           round(100 * pnorm(SS, 100, 15),0),\n           \")&lt;/span&gt;\"))), \n    vjust = -0.2, \n    lineheight = .8, \n    fill = NA, \n    color = \"gray20\",\n    label.color = NA,\n    label.padding = unit(0,\"mm\")) +\n  geom_text(aes(y = true_hat, \n                label = ifelse(Scale == \"Population\", \n                               \"\", \n                               as.character(Scale))), \n            vjust = 1.5, \n            color = \"gray15\",\n            lineheight = .8) +\n  geom_linerange(aes(ymin = true_hat - 1.96 * see,\n                      ymax = true_hat + 1.96 * see), \n                 linewidth = .5) +\n  geom_pointrange(aes(ymin = true_hat - see,\n                      ymax = true_hat + see), \n                  size = 1.2, \n                  fatten = 1.5) +\n  geom_text(aes(x = x, y = y, label = label), \n            data = tibble(\n              y = c(49.5, 65, 75, 85, 100, 115, 125, 135, 150.5), \n              x = 5.5, \n              label = c(\"Extremely\\nLow Range\", \n                                  \"Very\\nLow\", \n                                  \"Low\\nRange\", \n                                  \"Low\\nAverage\", \n                                  \"Average\\nRange\", \n                                  \"High\\nAverage\", \n                                  \"High\\nRange\", \n                                  \"Very\\nHigh\", \n                                  \"Extremely\\nHigh Range\")), \n            color = \"white\", lineheight = .8, size = 4.25) +\n  scale_y_continuous(\n    \"Standard Scores &lt;span style='font-size:11.7pt;color:#656565'&gt;&lt;br&gt;\n    (and Percentile Ranks)&lt;/span&gt;\", \n    breaks = seq(40, 160, 10), \n    limits = c(37, 163),\n    labels = \\(x) paste0(x,\n                         \"&lt;br&gt;&lt;span style='font-size:10pt;color:#656565'&gt;(\", \n                         pnorm(x,100, 15) %&gt;% \n                           WJSmisc::proportion2percentile(digits = 2) %&gt;%\n                           str_trim(), \n                         \")&lt;/span&gt;\"),\n    expand = expansion()) +\n  scale_x_continuous(NULL, expand = expansion(), breaks = NULL) +\n  scale_alpha_identity() +\n  scale_fill_identity() +\n  coord_flip(clip = \"off\") + \n  theme(axis.title.x = element_markdown(hjust = 0, \n                                        margin = margin(t = 1.25, \n                                                        l = 2,\n                                                        unit = \"mm\")),\n        axis.text.x = element_markdown(colour = \"gray20\"),\n        plot.caption = element_markdown(hjust = 0, \n                                        size = 10, \n                                        margin = margin(t = -10.25, \n                                                        l = 100, \n                                                        unit = \"mm\"), \n                                        color = \"gray40\"),\n        plot.title = element_text(hjust = .025)) +\n  labs(\n    title = \"Display of Cognitive Test Scores\",\n    caption = \"**Notes:** The white normal curves represent the expected \n       true&lt;br&gt;score distributions for each observed score. The black lines&lt;br&gt;\n       underneath span the 68% and 95% confidence intervals.\")\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2021,\n  author = {Schneider, W. Joel},\n  title = {Caption on Same Line as Axis Title in Ggplot2},\n  date = {2021-07-21},\n  url = {https://wjschne.github.io/posts/caption-on-same-line-as-axis-title-in-ggplot2},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2021, July 21). Caption on same line as axis title in\nggplot2. Schneirographs. https://wjschne.github.io/posts/caption-on-same-line-as-axis-title-in-ggplot2"
  },
  {
    "objectID": "posts/2023-08-26-making-a-custom-arrowhead-for-ggplot2-using-ggarrow-and-arrowheadr/index.html",
    "href": "posts/2023-08-26-making-a-custom-arrowhead-for-ggplot2-using-ggarrow-and-arrowheadr/index.html",
    "title": "Making a custom arrowhead for ggplot2 using ggarrow and arrowheadr",
    "section": "",
    "text": "Base plot for demonstrations\nlibrary(tidyverse)\n# install.packages(\"remotes\")\n# remotes::install_github(\"teunbrand/ggarrow\")\nlibrary(ggarrow)\n\nnode_radius &lt;- .2\nresection_length &lt;- .03\n\nd_node &lt;- tibble(node = c(\"A\", \"B\", \"C\"),\n       x = c(0, .5, 1),\n       y = c(0, sqrt(3) / 2, 0))\n\nd_edge &lt;- d_node |&gt; \n  mutate(theta_next = atan((y - lag(y)) / (x - lag(x))),\n         x_from = lag(x) + cos(theta_next) * (node_radius + resection_length),\n         y_from = lag(y) + sin(theta_next) * (node_radius + resection_length),\n         x_to = x + cos(theta_next + pi) * (node_radius + resection_length),\n         y_to = y + sin(theta_next + pi) * (node_radius + resection_length)) |&gt; \n  filter(!is.na(x_from))\n\np &lt;- d_edge |&gt;\n  ggplot(aes(x = x_from, y = y_from, xend = x_to, yend = y_to)) +\n  ggforce::geom_circle(aes(\n    x0 = x,\n    y0 = y,\n    r = .2,\n    fill = node\n  ),\n  data = d_node,\n  color = NA, inherit.aes = F) +\n  geom_text(aes(label = node, x = x, y = y), \n            data = d_node, \n            size = 20,\n            family = \"Roboto Condensed\",\n            inherit.aes = F) +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.position = \"none\") +\n     scale_fill_viridis_d(\n       option = \"D\",\n       begin = .2,\n       end = .8,\n       alpha = .5\n     )\n\np\n\n\n\n\n\n\n\n\n\nThe default arrows in ggplot2 are perfectly serviceable. There is the open variety:\n\n\nOpen arrow code\np + geom_segment(arrow = arrow())\n\n\n\n\n\n\n\n\nFigure 1: The ggplot2 open arrow\n\n\n\n\n\nYou can specify which ends of the segment have arrowheads, whether the arrows are open or closed, the length of the arrow, and how sharp the arrow’s point is with the angle argument.\nI happen to prefer a longer, sharper, closed arrow:\n\n\nClosed arrow code\np + geom_segment(\n  arrow = arrow(\n    angle = 15,\n    length = unit(8, \"mm\"),\n    type = \"closed\"\n))\n\n\n\n\n\n\n\n\nFigure 2: The ggplot2 closed arrow\n\n\n\n\n\nNevertheless, I miss the variety of arrows in available in TikZ. I particularly like the latex' arrow:\n\n\nTikZ code with latex’ arrow\n\\usetikzlibrary{arrows}\n\\begin{tikzpicture}\n\\node[fill=violet!50!white, circle] (A) at (0,0) {A};\n\\node[fill=cyan!80!black, circle] (B) at (1,1.732) {B};\n\\node[fill=green!40!black!40, circle] (C) at (2,0) {C};\n\\path[-&gt;,\n    draw,\n    shorten &gt;=2pt,\n    shorten &lt;=2pt,\n    &gt;=latex',\n    thick,\n    color = black!40,\n    text = black] (A) -&gt; (B);\n    \\path[-&gt;,\n    draw,\n    shorten &gt;=2pt,\n    shorten &lt;=2pt,\n    &gt;=latex',\n    thick,\n    color = black!40,\n    text = black] (B) -&gt; (C);\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\nFigure 3: The TikZ latex’ arrow\n\n\n\n\n\n\nThe ggarrow package\nIf you want more variety in drawing arrows in ggplot2, Teun van den Brand’s ggarrow package expands your limits to whatever your imagination can provide.\nThe default arrowhead is already a nice improvement:\n\n\nCode for wings arrow (i.e, stealth arrow in TikZ)\np + geom_arrow_segment(length_head = 10)\n\n\n\n\n\n\n\n\nFigure 4: The “wings” arrow\n\n\n\n\n\nYou can play around with the sharpness of the point (offset) and the sharpness of the barb (inset). You can also add feathers.\n\n\nCode for sharp barbs and feathers\np +\n  geom_arrow_segment(length_head = 4,\n                     arrow_head = arrow_head_wings(offset = 20, \n                                                   inset = 10), \n                     arrow_fins = arrow_fins_feather(), \n                     length_fins = 11)\n\n\n\n\n\n\n\n\nFigure 5: Sharp barbs and feathers\n\n\n\n\n\n\n\nCode for kite arrowhead\np +\n  geom_arrow_segment(length_head = 15,\n                     arrow_head = arrow_head_wings(offset = 22.5,\n                                                   inset = 115),\n                     arrow_fins = arrow_head_wings(offset = 22.5,\n                                                   inset = 115),\n                     length_fins = 15)\n\n\n\n\n\n\n\n\nFigure 6: Double-headed arrow with kite arrowhead\n\n\n\n\n\n\n\nCode for reverse kite arrowhead\np +\n  geom_arrow_segment(length_head = 20,\n                     arrow_head = arrow_head_wings(offset = 45,\n                                                   inset = 120))\n\n\n\n\n\n\n\n\nFigure 7: Reverse kite arrowhead\n\n\n\n\n\nIf the arrow goes too far, you can pull it back with the resect arguments.\n\n\nCode for resecting an arrowhead\np +\n  geom_arrow_segment(length_head = 6,\n                     arrow_head = arrow_head_wings(offset = 120,\n                                                   inset = 35),\n                     resect_head = 2)\n\n\n\n\n\n\n\n\nFigure 8: Demonstration of resecting arrowheads\n\n\n\n\n\nThere is much, much more that can be done. See ggarrow’s arrow ornament vignette for more options.\n\n\nCustom Arrowheads\nNot only does ggarrow offer great arrow geoms with excellent features like resection, one can create any custom arrowhead or feather that can be made with a single polygon.\nThe polygon generally falls between -1 and 1 on x and y, though you can plot outside those limits. In most cases, the point is at (0,1), and the line ends at (0,0):\n\n\nCode\npar(pty = \"s\")\nx &lt;- c(0,1)\ny &lt;- c(0,0)\nplot(x , y, xlim = c(-1, 1), ylim = c(-1, 1))\nrect(-1,-.1,0,.1)\ntext(-.5,0, labels = \"Line\")\ntext(1,0, labels = \"Arrow Point\", adj = 1.2)\npolygon(ggarrow::arrow_head_wings() |&gt; `colnames&lt;-`(c(\"a\", \"b\")))\n\n\n\n\n\n\n\n\nFigure 9: Grid for custom arrowheads\n\n\n\n\n\nThe polygon you create should be be a 2-column matrix with named columns (e.g., x and y). Here I make a elliptical arrowhead.\n\n\nCode\nmake_ellipse &lt;- function(a = 1, b = .5){\n  t &lt;- seq(0,2*pi, length.out = 361)\n  cbind(x = a * cos(t), y = b * sin(t)) \n}\n\np +\n  geom_arrow_segment(length_head = 5,\n                     arrow_head = make_ellipse())\n\n\n\n\n\n\n\n\nFigure 10: Make ellipse\n\n\n\n\n\n\n\nThe arrowheadr package\nI made the arrowheadr package to make custom arrowheads quickly. Some of them are admittedly silly. However, it is now easy to make my favorite kind of arrow in ggplot2:\n\n\nCode\nlibrary(arrowheadr)\n\np + \n  geom_arrow_segment(length_head = 5,\n                     arrow_head = arrow_head_latex())\n\n\n\n\n\nMimicking the latex prime arrowhead\n\n\n\n\nMore examples:\nA catenary:\n\n\nCode\nstlouis &lt;- arrow_head_catenary(base_width = .25, thickness = .15)\n\np + \n  geom_arrow_segment(length_head = 5,\n                     arrow_head = stlouis)\n\n\n\n\n\n\n\n\nFigure 11: The Gateway Arch in St. Louis is shaped like a catenary, not a parabola.\n\n\n\n\n\nThe Cauchy function:\n\n\nCode\np + \n  geom_arrow_segment(length_head = 5,\n                     arrow_head = arrow_head_function(dt, df = 1))\n\n\n\n\n\n\n\n\nFigure 12: Any function can make an arrowhead.\n\n\n\n\n\nRazors:\n\n\nCode\nrazors &lt;- c(1,0,\n  0,.5,\n  -.35,.25,\n  -.35, .21,\n  0,.35,\n  .90,0\n  ) |&gt; \n  v2matrix() |&gt; \n  reflecter() \n\np + \n  geom_arrow_segment(length_head = 10,\n                     arrow_head = razors)\n\n\n\n\n\n\n\n\nFigure 13: The reflecter function takes a set of points and makes them symmetrical.\n\n\n\n\n\nA candle flame:\n\n\nCode\ncandleflame &lt;- arrow_head_wittgenstein_rod(\n  fixed_point = c(-2.75, 0),\n  rod_length = 3.75,\n  nudge = c(1, 0),\n  rescale = .95\n)\n\np + \n  geom_arrow_segment(length_head = 12,\n                     arrow_head = candleflame)\n\n\n\n\n\n\n\n\nFigure 14: Wittgenstein’s rod makes some nice shapes\n\n\n\n\n\nUsing bezier curve control points:\n\n\nCode\ncurved_arrowhead &lt;- arrow_head_bezier(list(\n  c(1,  0,\n    .5, .5,\n    .2, .5),\n  c(.2, .5,\n    .2, .1,\n    -.1, .25,\n    -.3, .25),\n  c(-.3, .25,\n    0, 0,\n    -.3, -.25),\n  c(-.3, -.25,\n    -.1, -.25,\n    .2,  -.1,\n    .2, -.5),\n  c(.2, -.5,\n    .5, -.5,\n    1,  0)\n))\n\np + \n  geom_arrow_segment(length_head = 8,\n                     arrow_head = curved_arrowhead)\n\n\n\n\n\n\n\n\nFigure 15: Bezier curves can make almost anything.\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2023,\n  author = {Schneider, W. Joel},\n  title = {Making a Custom Arrowhead for Ggplot2 Using Ggarrow and\n    Arrowheadr},\n  date = {2023-08-26},\n  url = {https://wjschne.github.io/posts/2023-08-26-making-a-custom-arrowhead-for-ggplot2-using-ggarrow-and-arrowheadr},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2023, August 26). Making a custom arrowhead for\nggplot2 using ggarrow and arrowheadr. Schneirographs. https://wjschne.github.io/posts/2023-08-26-making-a-custom-arrowhead-for-ggplot2-using-ggarrow-and-arrowheadr"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "W. Joel Schneider, Ph.D.\nProfessor\nTemple University\nPsychological Studies in Education\n\n\n\nCounseling Psychology\n\n\nSchool Psychology\n\n\n\n\n\n Contact Me\n Curriculum Vitae\n ORCID\n Google Scholar\n Open Science Framework\n ResearchGate\n Mastodon\n Zotero\n Stackoverflow\n YouTube\n GitHub\n\n\n\n\n\n\nAbout Me\nI am a professor at Temple University in the College of Education and Human Development in the Psychological Studies in Education Department. I belong to both the School Psychology and Counseling Psychology programs.\nAlthough I have diverse interests, my primary focus is trying to understand and improve the validity of psychological assessment practices. I teach courses in assessment, counseling, statistics, and research methods.\nI grew up in Southern California (1970–1990) and lived in Córdoba, Argentina (1990–1992). I completed my undergraduate degree in Psychology at the University of California at Berkeley in 1995 and my doctoral studies in clinical psychology at Texas A&M University in 2003. My internship year (2001–2002) was spent in the Dutchess County Department of Behavioral & Community Health in Poughkeepsie, NY. Since then I have been a faculty member at Illinois State University (2002–2017) and Temple University (2017–Present)."
  },
  {
    "objectID": "assessingpsyche.html",
    "href": "assessingpsyche.html",
    "title": "AssessingPsyche",
    "section": "",
    "text": "Overture Redux\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nThe RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs\n\n\nA Review of the Receptive, Expressive & Social Communication Assessment–Elementary\n\n\n\n\n\n\n\n\nOct 26, 2016\n\n\nW. Joel Schneider\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "AssessingPsyche/2010-10-07 Overture/index.html",
    "href": "AssessingPsyche/2010-10-07 Overture/index.html",
    "title": "Overture Redux",
    "section": "",
    "text": "The original title of this blog (Assessing Psyche, Engaging Gauss, Seeking Sophia) was more of an aspiration than a description. To keep things simple, going forward the blog’s title will be just AssessingPsyche.\nThe purpose of this blog remains the same from the original post in 2010:\n\nWhen we are at our best, clinicians skilled in psychological assessment see beyond the obvious and communicate something useful and true to a person who needs our help. Uncovering something true often requires a bit of science, sometimes a little math, and always a lot of empathy. Communicating what is true so that it is useful requires still more empathy and a bit of art. We facilitate in our clients a deeper understanding of what is happening to them and present to them a different vision of what they can be.\nIn this blog, I hope to communicate something useful and true to other professionals about psychological assessment. I hope that in communicating what I know, I will come to know more than I do now.\n\n\n\n\nCitationBibTeX citation:@misc{schneider2023,\n  author = {Schneider, W. Joel},\n  title = {Overture {Redux}},\n  date = {2023-09-12},\n  url = {https://wjschne.github.io/AssessingPsyche/2010-10-07 Overture},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2023, September 12). Overture Redux.\nAssessingPsyche. https://wjschne.github.io/AssessingPsyche/2010-10-07\nOverture"
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "",
    "text": "Conflict of Interest Statement: ATP Assessments, the publisher of the RESCA-E, commissioned me to write a descriptive account of the likely relations among the RESCA-E subtests and CHC Theory constructs. I billed them for the hours that I spent researching this topic and writing my thoughts on the matter. However, I was so impressed the the RESCA-E that I wanted to write a short review of it, and I also conducted additional analyses about its structure. Because ATP Assessments did not ask for my opinion about the quality of the RESCA-E nor for the statistical analyses I conducted, I did not bill for the many additional hours I spent on these activities. If I were not impressed with the RESCA-E, this document would have been much shorter.”)\nThe Receptive, Expressive & Social Communication Assessment–Elementary (RESCA-E) is a new measure of language abilities for children in the elementary school years. The purpose of this review is to evaluate the RESCA-E in terms of the Cattell-Horn-Carroll Theory of Cognitive Abilities (CHC theory; McGrew, 2005; Schneider & McGrew, 2012). However, some preliminary remarks about the test’s design are in order.\n\n\nModest elegance, by its nature, attracts little praise. I will do my part here to rectify this injustice. The RESCA-E test materials, stimuli, and protocols are designed for practical efficiency but sacrifice nothing in aesthetic appeal. This might not seem to matter, but it does. Spending time with ugly, frustrating test materials makes one yearn for early retirement.\nThe application of sound typographical principles has enhanced the readability and ease of use of the protocol; the whole document is thoughtfully coded by font, color, and shading. The protocol does not feel cramped; it has generous space for notes, yet no space is wasted. Sure, the designers could have shortened the protocol by making everything smaller and more compact, but that would have been penny wise, pound foolish. This same care and consistency was extended to everything in the test kit.\n\n\n\nI do not know the test’s authors, Patricia Hamaguchi and Deborah Ross-Swain, and I have had no contact with them. Yet, I can tell something about their work process and their scholarly values. To someone who has never tried to design an ability test, it may not be obvious that the RESCA-E subtest items were labored over for untold hours until they were just right. In most test batteries I find several items (or whole subtests) that seem a bit off, like bum notes in a singer’s solo. I found none here. The items are so smoothly written that they draw no attention to themselves—no small feat.\nEven more importantly, the item content reflects a deep understanding on the part of the authors of what matters in the evaluation of children. No item is merely easy or merely difficult, chosen to meet some psychometric need. No, each item is intended to measure something substantial and relevant to everyday functioning. A rare patience was required to keep working with each item until it was easy to understand, quick to administer, and simple to score, all the while remaining clinically relevant, yet psychometrically sound. For this accomplishment, Patricia Hamaguchi, Deborah Ross-Swain, and their associates at ATP Assessments deserve a tip of the hat and hearty congratulations."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#first-impressions",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#first-impressions",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "",
    "text": "Modest elegance, by its nature, attracts little praise. I will do my part here to rectify this injustice. The RESCA-E test materials, stimuli, and protocols are designed for practical efficiency but sacrifice nothing in aesthetic appeal. This might not seem to matter, but it does. Spending time with ugly, frustrating test materials makes one yearn for early retirement.\nThe application of sound typographical principles has enhanced the readability and ease of use of the protocol; the whole document is thoughtfully coded by font, color, and shading. The protocol does not feel cramped; it has generous space for notes, yet no space is wasted. Sure, the designers could have shortened the protocol by making everything smaller and more compact, but that would have been penny wise, pound foolish. This same care and consistency was extended to everything in the test kit."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#looking-deeper",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#looking-deeper",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "",
    "text": "I do not know the test’s authors, Patricia Hamaguchi and Deborah Ross-Swain, and I have had no contact with them. Yet, I can tell something about their work process and their scholarly values. To someone who has never tried to design an ability test, it may not be obvious that the RESCA-E subtest items were labored over for untold hours until they were just right. In most test batteries I find several items (or whole subtests) that seem a bit off, like bum notes in a singer’s solo. I found none here. The items are so smoothly written that they draw no attention to themselves—no small feat.\nEven more importantly, the item content reflects a deep understanding on the part of the authors of what matters in the evaluation of children. No item is merely easy or merely difficult, chosen to meet some psychometric need. No, each item is intended to measure something substantial and relevant to everyday functioning. A rare patience was required to keep working with each item until it was easy to understand, quick to administer, and simple to score, all the while remaining clinically relevant, yet psychometrically sound. For this accomplishment, Patricia Hamaguchi, Deborah Ross-Swain, and their associates at ATP Assessments deserve a tip of the hat and hearty congratulations."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#comprehension-of-vocabulary",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#comprehension-of-vocabulary",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Comprehension of Vocabulary",
    "text": "Comprehension of Vocabulary\nThe examiner shows four drawings to the examinee and says a word. The examinee points to the one best depicts the word.2 The advantage of this format is that it cleanly measures the CHC narrow ability Lexical Knowledge, a facet of Comprehension/Knowledge (Gc). The “disadvantage” of the multiple choice format is that its loading on general intelligence is likely to be smaller than with tests with more open-ended formats (e.g., WISC-V Vocabulary). Why? No judgment is required to decide which aspects of a definition to emphasize. Wechsler’s (1958) goal was never to measure things like knowledge per se, but to measure intelligence in all its integrative, glorious complexity.3 However, when your goal is to measure word knowledge, not judgment, this item format is exactly what you want.\n2 Similar to the Peabody Picture Vocabulary Test, Fourth Edition and the Receptive One-Word Picture Vocabulary Test-43 Wechsler (1958, p. 15) wrote, “Then, when an examiner employs an arithmetic or a vocabulary test as part of an intelligence scale, the object of the examiner is not to discover the subject’s aptitude for arithmetic or extent of his word knowledge, although these are inevitably involved, but his capacity to function in overall areas which are assumed to require intelligence.” All picture vocabulary tests start with simple objects one encounters frequently (e.g., spoon, ball, dog). Where the test designers go from there matters quite a bit. There are two ways to make a picture vocabulary test more difficult:\n\nShow pictures of increasingly unusual objects (e.g., fob, lappets, ait, manometer).\nShow pictures illustrating increasingly complex concepts (e.g., relieved, hesitant, shrewd, intimacy) or increasingly subtle distinctions between related words (e.g., ask vs. beg, sad vs. sobbing, consider vs. ponder).\n\nWhich approach do you think is more applicable to everyday life? Me, too. Fortunately, this is the approach that the RESCA-E takes, with more difficult items focusing mostly on emotions, interpersonal relations, measurements, and abstractions. None of the words are particularly unusual, technical, or esoteric."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#comprehension-of-oral-directions",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#comprehension-of-oral-directions",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Comprehension of Oral Directions",
    "text": "Comprehension of Oral Directions\nThis measure is similar to the Comprehension of Vocabulary subtest in its format. The examiner shows four drawings to the examinee and says a sentence that contains an instruction. The examinee chooses the picture that is consistent with the instruction. For example, the sentence might be, “You may not ride bicycles in the park.” The four pictures might be various configurations of a child, a bicycle, and a park. The correct answer might be a picture of child in the park with a bicycle chained outside its gates.\nThe items of this test are ingeniously designed to avoid problems in similar tests of oral direction comprehension. Some tests give increasingly long directions that tax working memory instead of comprehension per se.4 This subtest minimizes working memory load by presenting directions that rarely have more than three parts. Items become increasingly difficult mostly because of the complexity of the command rather than its length.5\n4 Which is fine, if we wish to measure the degree to which working memory deficits interfere with comprehension.5 For example, “Write a letter that has only curved lines or only straight lines, but not both kinds of lines. Which letter could be written? [shows letters] B, D, O, or P”6 You might ask, “If CHC theory has holes in it, why not just fill them?” Good question. Theory building, especially the assembly of a comprehensive taxonomy, must be a slow, deliberate, systematic process. Adding new features that are not well validated into the taxonomy will undermine trust in its utility.In terms of CHC theory, it appears that this subtest measures Listening Ability, a facet of Gc. However, it seems likely that Carroll’s Listening Ability factor comprises multiple subfactors, and that this subtest measures a narrow subset of them (e.g., understanding of English syntax). Thus, the model of language that underlies RESCA-E is more elaborated than the model of language ability in CHC theory. Ideally, research using instruments like the RESCA-E will refine and extend CHC theory’s treatment of language abilities.6"
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#comprehension-of-stories-and-questions",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#comprehension-of-stories-and-questions",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Comprehension of Stories and Questions",
    "text": "Comprehension of Stories and Questions\nIn this subtest, the examiner reads a short story to the examinee and asks questions about the examinee’s understanding of it. For each question, there are four possible answers that the examinee can point to (either words that the examiner reads aloud or pictures that are shown). The passages are carefully crafted to be stories rather than barely concealed lists of random details and events. To answer the questions correctly, the examinee must understand the gist of the story rather than recall highly specific details. Thus, it is a true comprehension test rather than a memory test.\nComprehension of Stories and Questions is, like Comprehension of Oral Directions, a measure of Listening Ability, but less about syntax and more about semantics. To some degree, it is also a measure of Meaningful Memory (Gl)."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#comprehsion-of-basic-morphology-and-syntax",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#comprehsion-of-basic-morphology-and-syntax",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Comprehsion of Basic Morphology and Syntax",
    "text": "Comprehsion of Basic Morphology and Syntax\nLike the other receptive language subtests, this supplementary subtest is a multiple-choice test. It is cleverly designed to measure a child’s understanding of syntax and morphology. For example, to measure one’s understanding of plurality, the prompt could be “The birds are swimming.” The child has to choose among pictures of birds swimming, but only one picture has more than one bird. In similar fashion, understanding of a variety of other features of English are tested: past vs. present vs. future tense, negation (e.g., none, not, never), prepositions (e.g., on, in, above, between), gender (he vs. she, him vs. her), singular vs. plural, self vs. other (me vs. her), before vs. after, active vs. passive voice (e.g., the boy touched the dog, the boy was touched by the dog).\nThis subtest is another measure of Listening Ability but with a focus on syntax. From the name of Carroll’s Grammatical Sensitivity factor, it might seem that this is what Comprehension of Basic Morphology and Syntax measures. However, the Grammatical Sensitivity ability factor was a measure of formal knowledge of grammar. Obviously, knowing formal grammatical rules will help on this test but that is not what is being measured here directly."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#executing-oral-directions",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#executing-oral-directions",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Executing Oral Directions",
    "text": "Executing Oral Directions\nThis supplementary test is of obvious importance. It can resolve questions such as, “Does the child understand simple commands?” The subtest is similar to and is most correlated with Comprehension of Oral Directions (r = .52). It differs from that test in that it is not a multiple-choice test. Instead, it requires the examinee to follow simple commands at first (e.g., “Touch your knee. Go.”) and increasingly complex sentences at the end of the test (e.g., “Stand up and walk to the door. When you get there, knock on it three times or open it, but not both. Go.”). Near the end of the test, the commands are long and thus the working memory demands are high."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#expressive-labeling-of-vocabulary",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#expressive-labeling-of-vocabulary",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Expressive Labeling of Vocabulary",
    "text": "Expressive Labeling of Vocabulary\nLike the Comprehension of Vocabulary subtest, this test measures Lexical Knowledge. On some items examinee is asked what object is in a picture. In many confrontational naming tests, the objects in the pictures become increasingly unusual. Not so, here. As with the Comprehension of Vocabulary subtest, the pictures measure knowledge of words related to abstract concepts, emotions, and interpersonal relations.\nAlthough Comprehension of Vocabulary (CV) and Expressive Labeling of Vocabulary (ELV) have the highest correlation of all the RESCA-E subtests (r = .6), the correlation is not so high that they are redundant. In my opinion, the RESCA-E should have a Vocabulary composite score consisting of these tests. Using formulas explained in Schneider (2013), you can compute a custom vocabulary composite score like so:\nVocabulary Composite = 2.795(CV + ELV) − 44.1"
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#expressive-skills-for-describing-and-explaining",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#expressive-skills-for-describing-and-explaining",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Expressive Skills for Describing and Explaining",
    "text": "Expressive Skills for Describing and Explaining\nGuilford (1967) distinguished between tests requiring convergent production (i.e., a single answer is correct) and tests requiring divergent production (e.g., the examinee gives as many correct answers, such as naming as many ice cream flavors as possible within the time allotted). Cattell originally grouped divergent production tests with crystallized intelligence. The reason for this is that his tests had generous time limits so that the tests were measures of how much information was in the person’s knowledge banks instead how fast it could be pulled out of memory. The results will be quite different if one is given 1 minute to name as many words as possible ending in -tion compared to the same task but with a 5-minute time limit. With generous time limits, the test becomes more like a breadth-of-vocabulary test rather than a memory retrieval speed test. Almost all commercially available tests of divergent production have short time limits and thus function as memory retrieval speed tests.\nIn this subtest, the examinee is shown a picture (e.g., a family preparing a meal) or given a scenario (e.g., a child getting ready for bed) and the examinee is prompted to tell the examiner everything he or she knows about this situation. Unlike many divergent processing tests (e.g., COWAT), the child does not get 1 point for every answer. Instead there is a checklist of criteria for scoring points. In general, the examinee is awarded points for mentioning aspects of the picture or scenario that are most salient or of central importance.\nThis is a test paradigm I have never seen before. If it is indeed novel, it is potentially a major advance. It is rare in life to have to name as many exemplars of a category as possible (e.g., sports, furniture, animals, words that begin with H). In contrast, spontaneously describing the most salient aspects of a situation is a hallmark of intelligence. I look forward to seeing validation efforts to evaluate this paradigm’s utility.\nThis subtest has relatively small correlations with the other tests (r in the .2–.3 range), suggesting that it is not merely a Gc test, though it is undoubtedly influenced by Gc. In CHC theory, there is a little-understood narrow ability called Associational Fluency in the Gr (Memory Retrieval Fluency) broad ability cluster. It is distinguished from the better-known Ideational Fluency factor in that the quality of the responses matters more than the number. Given how points are awarded for mentioning important aspects of a picture or scenario, it seems likely that this is what is being measured. It also seems like that having general knowledge, an intermediate factor within Gc, would help a person perform well on this test."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#narrative-skills",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#narrative-skills",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Narrative Skills",
    "text": "Narrative Skills\nIn this subtest, the examinee is prompted to tell the gist of a narrative that the examiner read. Another type of item involves telling the examiner about an experience (e.g., taking a long trip in a car or bus). As with the Expressive Skills for Describing and Explaining subtest, the examiner scores the response for the quality of the answers, not for how much is said. It may sound from this description that this subtest is a bear to score, but it is quite straightforward.\nThe Narrative Skills subtest is most highly correlated with Expressive Skills for Describing and Explaining. It seems likely that it too is a measure of Associational Fluency. It does not seem to draw on background knowledge to the same degree and instead requires Meaningful Memory. Supporting this interpretation is the fact that its second-highest correlation is with Comprehension of Stories and Questions, which is also hypothesized to be influenced by Meaningful Memory."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#expressive-use-of-basic-morphology-and-syntax",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#expressive-use-of-basic-morphology-and-syntax",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Expressive Use of Basic Morphology and Syntax",
    "text": "Expressive Use of Basic Morphology and Syntax\nIn this subtest, the examiner prompts the examinee to answer questions about pictures. The questions are cleverly worded so that the examinee’s answers are expected to conform to certain syntactical rules (e.g., “This girl here is running. What is this other girl doing? Answer: Walking”).\nGiven the similarity in names, it seems reasonable to suppose that this subtest would be strongly correlated with Comprehension of Basic Morphology and Syntax. Its correlation is only r = .45, a value that is similar to the correlations it has with many other subtests. Factor analyses of the RESCA-E reveal no evidence that there is a special relationship between these two measures of morphology and syntax understanding. Examining the pattern of critical difference scores in the Technical Manual, it appears that the correlation between these two tests decreases with age. Why this happens deserves scrutiny. One likely explanation is that both tests have very low ceilings for older children, which means that the scores are imprecise for children with average or better understanding of morphology and syntax.7\n7 A test with a low ceiling does not have enough difficult items to distinguish reliably among high-ability examinees. Low ceilings are a major problem for intelligence tests, but are not so problematic for tests like the RESCA-E, which are designed to identify children with language deficits rather than designed to identify the superstars of syntactic sophistication.In terms of CHC theory, it appears that this subtest measures aspects of Communication Ability, a little-researched factor. As with Comprehension of Basic Morphology & Syntax, it seems likely that Grammatical Sensitivity also influences performance on the test."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#comprehension-of-body-language-and-vocal-emotion",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#comprehension-of-body-language-and-vocal-emotion",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Comprehension of Body Language and Vocal Emotion",
    "text": "Comprehension of Body Language and Vocal Emotion\nIn this subtest, the examiner shows the examinee four pictures of people making different gestures or with different facial expressions. The examiner plays an audio CD that asks a question like, “Which person is thinking, ‘I am confused.’” and the examinee picks the picture of the person who looks confused. The items are well designed and toward the end of the subtest assess fairly subtle social signals. This subtest would fit it with emotional intelligence tests like the MSCEIT’s (Mayer et al., 2002) measures of emotion perception.\nBecause of Guildford’s work in social intelligence, Carroll’s model has a factor called Knowledge of Behavioral Content, an aspect of achievement. However, there is already strong evidence that social and emotional reasoning have several narrow ability factors associated with them (Mayer et al., 2008). It is not clear where these factors belong in CHC theory, but the evidence keeps pouring in that these factors matter. Sooner or later, CHC theory is going to have to provide a more nuanced account of aspects of social and emotional intelligence (MacCann et al., 2014; Schneider et al., 2016)."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#social-and-language-inference",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#social-and-language-inference",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Social and Language Inference",
    "text": "Social and Language Inference\nIn this test, the examiner presents a scenario in which someone uses indirect or idiomatic language. In many items, the examinee selects the correct answer from four answer choices. For example, two children are talking in class and the teacher says, “Hey, knock it off, you two!” The correct inference will be that the teacher wants the children to stop talking.\nIn terms of CHC theory, this seems to be a general Language Development measure with emphasis on Knowledge of Behavioral Content as well."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#situational-language-use",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#situational-language-use",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Situational Language Use",
    "text": "Situational Language Use\nIn this subtest, the examiner presents the examinee with a scenario and prompts the examinee to say how he or she would respond in that situation. For example, “Your mother introduces you to a woman you do not know. The woman smiles and extends her hand, saying ‘Pleased to meet you.’ How would you respond in a friendly and polite manner?” The examinee’s response is scored according to straightforward criteria.\nIn terms of CHC theory, this subtest measures Communication Ability as well as Knowledge of Behavioral Content."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#elicited-body-language",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#elicited-body-language",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Elicited Body Language",
    "text": "Elicited Body Language\nIn this subtest, the examiner asks the examinee to act out various common situations (e.g., Pretend you just took a bite of your favorite food. Pretend you accidentally hurt your finger. Pretend you are listening to someone who whispers a surprising secret.). This test may supplant the SB5 Verbal Absurdities as the most delightful test to administer.\nOnce again, CHC theory’s taxonomy is too sparse in this domain to explain what is going on in this subtest. Nevertheless, the narrow ability that is being measured is mostly likely Knowledge of Behavioral Content."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#exploratory-factor-analysis",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#exploratory-factor-analysis",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Exploratory Factor Analysis",
    "text": "Exploratory Factor Analysis\nI used the RESCA-E correlation matrix for the entire standardization sample (N = 825) to conduct all analyses. I removed the Social Communication Inventory scale from consideration because it is not an ability test. It has low correlations with the other tests and thus would produce an uninformative singleton factor.\nTo see how many factors to extract, I conducted a parallel analysis using the psych package (Revelle, 2016) in R. I used the type of parallel analysis based on principal factors rather than the more commonly-used principal components method because it is more accurate when there is a large general factor (Crawford et al., 2010). As seen in Figure 2, the results suggest that extracting five principal factors is a reasonable choice.\n\n\n\n\n\n\n\n\nFigure 2: Principal Factor Analysis–Based Parallel Analysis of the RESCA-E Subtests\n\n\n\n\n\nI extracted 1, 2, 3, 4, and then 5 principal factors with an oblimin rotation (See Tables 1–5). In no solution did the three language domains—expressive, receptive, and social communication—hang together. Does this result mean that the structure of RESCA-E is not valid? No, but it suggests that the three ability domains are not the same kind of constructs that factor-analysts are used to. The three language domains are not cohesive clusters of relatively unitary abilities, and the subtests in each of the three composite scores will often not hang together as closely as factor-based composites would.\n\n\n\nTable 2. Factor loadings of RESCA-E subtests in a principal factor analysis with oblimin rotation (1-factor solution)\n\n\n\nDomain\nPA1\n\n\n\n\nComprehension of Stories and Questions\nReceptive\n.75\n\n\nExpressive Labeling of Vocabulary\nExpressive\n.75\n\n\nComprehension of Oral Directions\nReceptive\n.73\n\n\nComprehension of Vocabulary\nReceptive\n.72\n\n\nSocial and Language Inference\nSocial\n.69\n\n\nSituational Language Use\nSocial\n.68\n\n\nExpressive Use of Basic Morphology and Syntax\nExpressive\n.66\n\n\nComprehension of Basic Morphology and Syntax\nReceptive\n.65\n\n\nNarrative Skills\nExpressive\n.63\n\n\nExecuting Oral Directions\nReceptive\n.61\n\n\nComprehension of Body Language and Vocal Emotion\nSocial\n.59\n\n\nElicited Body Language\nSocial\n.54\n\n\nExpressive Skills for Describing and Explaining\nExpressive\n.54\n\n\n\n\n\n\nTable 3. Factor loadings of RESCA-E subtests in a principal factor analysis with oblimin rotation (2-factor solution)\n\n\n\n\n\n\n\n\n\nDomain\nPA1\nPA2\n\n\n\n\nSocial and Language Inference\nSocial\n.80\n\n\n\nComprehension of Basic Morphology and Syntax\nReceptive\n.77\n\n\n\nComprehension of Vocabulary\nReceptive\n.73\n\n\n\nComprehension of Oral Directions\nReceptive\n.72\n\n\n\nComprehension of Stories and Questions\nReceptive\n.67\n\n\n\nExpressive Labeling of Vocabulary\nExpressive\n.63\n\n\n\nExecuting Oral Directions\nReceptive\n.59\n\n\n\nExpressive Use of Basic Morphology and Syntax\nExpressive\n.57\n\n\n\nComprehension of Body Language and Vocal Emotion\nSocial\n.55\n\n\n\nSituational Language Use\nSocial\n.47\n.28\n\n\nElicited Body Language\nSocial\n.33\n.28\n\n\nNarrative Skills\nExpressive\n\n.75\n\n\nExpressive Skills for Describing and Explaining\nExpressive\n\n.70\n\n\n\n\n\n\nTable 4. Factor loadings of RESCA-E subtests in a principal factor analysis with oblimin rotation (3-factor solution)\n\n\n\n\n\n\n\n\n\n\nDomain\nPA1\nPA2\nPA3\n\n\n\n\nComprehension of Vocabulary\nReceptive\n.76\n\n\n\n\nExpressive Labeling of Vocabulary\nExpressive\n.74\n\n\n\n\nSocial and Language Inference\nSocial\n.74\n\n\n\n\nComprehension of Stories and Questions\nReceptive\n.69\n\n\n\n\nComprehension of Basic Morphology and Syntax\nReceptive\n.68\n\n\n\n\nComprehension of Oral Directions\nReceptive\n.65\n\n\n\n\nComprehension of Body Language and Vocal Emotion\nSocial\n.53\n\n\n\n\nExpressive Skills for Describing and Explaining\nExpressive\n\n.75\n\n\n\nNarrative Skills\nExpressive\n\n.61\n.23\n\n\nSituational Language Use\nSocial\n\n.20\n.55\n\n\nElicited Body Language\nSocial\n\n.20\n.44\n\n\nExpressive Use of Basic Morphology and Syntax\nExpressive\n.32\n\n.39\n\n\nExecuting Oral Directions\nReceptive\n.36\n\n.37\n\n\n\n\n\n\nTable 5. Factor loadings of RESCA-E subtests in a principal factor analysis with oblimin rotation (4-factor solution)\n\n\n\n\n\n\n\n\n\n\n\nDomain\nPA1\nPA3\nPA2\nPA4\n\n\n\n\nComprehension of Vocabulary\nReceptive\n.78\n\n\n\n\n\nExpressive Labeling of Vocabulary\nExpressive\n.75\n\n\n\n\n\nSocial and Language Inference\nSocial\n.69\n\n\n\n\n\nComprehension of Stories and Questions\nReceptive\n.65\n\n\n\n\n\nComprehension of Basic Morphology and Syntax\nReceptive\n.62\n\n\n\n\n\nComprehension of Oral Directions\nReceptive\n.56\n\n\n.24\n\n\nComprehension of Body Language and Vocal Emotion\nSocial\n.54\n\n\n\n\n\nExpressive Use of Basic Morphology and Syntax\nExpressive\n.32\n.25\n\n\n\n\nSituational Language Use\nSocial\n\n.93\n\n\n\n\nElicited Body Language\nSocial\n\n.34\n\n\n\n\nExpressive Skills for Describing and Explaining\nExpressive\n\n\n.71\n\n\n\nNarrative Skills\nExpressive\n\n\n.68\n\n\n\nExecuting Oral Directions\nReceptive\n\n\n\n.79\n\n\n\n\n\n\nTable 6. Factor loadings of RESCA-E subtests in a principal factor analysis with oblimin rotation (5-factor solution)\n\n\n\n\n\n\n\n\n\n\n\n\nDomain\nPA1\nPA2\nPA5\nPA3\nPA4\n\n\n\n\nComprehension of Vocabulary\nReceptive\n.73\n\n\n\n\n\n\nExpressive Labeling of Vocabulary\nExpressive\n.65\n\n\n\n\n\n\nComprehension of Stories and Questions\nReceptive\n.52\n\n\n\n\n\n\nSocial and Language Inference\nSocial\n.51\n\n.24\n\n\n\n\nComprehension of Body Language and Vocal Emotion\nSocial\n.49\n\n\n\n.23\n\n\nComprehension of Basic Morphology and Syntax\nReceptive\n.39\n\n.31\n\n\n\n\nExpressive Skills for Describing and Explaining\nExpressive\n\n.79\n\n\n\n\n\nNarrative Skills\nExpressive\n\n.65\n\n\n\n\n\nComprehension of Oral Directions\nReceptive\n\n\n.68\n\n\n\n\nExecuting Oral Directions\nReceptive\n\n\n.51\n\n.29\n\n\nElicited Body Language\nSocial\n\n\n\n.83\n\n\n\nSituational Language Use\nSocial\n\n\n\n.33\n.28\n\n\nExpressive Use of Basic Morphology and Syntax\nExpressive\n\n\n\n\n.52"
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#might-mulitdimensional-scaling-detect-a-faceted-relationship-among-the-three-language-domains",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#might-mulitdimensional-scaling-detect-a-faceted-relationship-among-the-three-language-domains",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Might Mulitdimensional Scaling Detect a Faceted Relationship Among the Three Language Domains?",
    "text": "Might Mulitdimensional Scaling Detect a Faceted Relationship Among the Three Language Domains?\nIs there some sort of faceted relationship that the three language domains might have that factor analysis cannot detect (e.g., process × domain)? If so, it should show up in a multidimensional scaling (MDS). Using the correlations subtracted from 1 as distances, a two-dimensional MDS suggests no obvious cohesion of the three domains (See Figure 3).\n\n\n\n\n\n\n\n\nFigure 3: Two-dimensional Multidimensional Scaling of the RESCA-E Subtests. Blue = Receptive, Red = Expressive, Green = Social Communication\n\n\n\n\n\nPerhaps there is order that can be seen in three dimensions that cannot be detected in two. You can grab Figure 4 with your mouse and rotate it. I am unable to detect any patterns that show the three domains to be elegantly cohesive.\n\n\n\n\n\n\n\n\nFigure 4: Three-dimensional Multidimensional Scaling of the RESCA-E Subtests. Blue = Receptive, Red = Expressive, Green = Social Communication"
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#confirmatory-factor-analysis-to-the-rescue",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#confirmatory-factor-analysis-to-the-rescue",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Confirmatory Factor Analysis to the rescue?",
    "text": "Confirmatory Factor Analysis to the rescue?\nThese previous analyses were exploratory analyses. Would confirmatory factor analysis (CFA) have supported the hypothesis that the three domains are cohesive? No, but it might have given the illusion of such a finding.\nIf we conduct a one-factor CFA (Figure 5) and compare it to a three-factor CFA (Figure 6), the model fit improves significantly (p &lt; .001). The loadings all look healthy in Figure 6. What is the problem? The problem is that the factors are almost perfectly correlated, meaning that if the latent variables could be measured without error, there would be little point in distinguishing among them. Had Figure 5.1 of the RESCA-E Technical Manual shown the correlations among the latent factors, this would have been plainly visible (Compare with Figure 7). In contrast, the correlations among the three-factor EFA latent variables are between .62, and .84, which means that they do not yield redundant information.\n\n\n\n\n\n\n\n\nFigure 5: General-factor CFA of RESCA-E Subtests\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Three-factor CFA of RESCA-E Subtests\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Three-factor CFA of RESCA-E Core Subtests"
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#conclusion-the-three-resca-e-language-domains-are-descriptive-categories-not-distinct-abilities",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#conclusion-the-three-resca-e-language-domains-are-descriptive-categories-not-distinct-abilities",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Conclusion: The Three RESCA-E Language Domains Are Descriptive Categories, Not Distinct Abilities",
    "text": "Conclusion: The Three RESCA-E Language Domains Are Descriptive Categories, Not Distinct Abilities\nThe available evidence suggests that the distinction between receptive, expressive, and social communication abilities is mostly descriptive. There are language abilities that are legitimately classified as receptive or expressive, or social, but no cohesive ability constructs we could call receptive language ability, expressive language ability, or social communication ability. If Carroll could not find them with 400+ data sets, we should not be surprised to not find them here in this one. The three RESCA-E composites should be interpreted in this light."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#alternative-structure",
    "href": "AssessingPsyche/2016-10-26 RESCA-E/RESCAE.html#alternative-structure",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Alternative Structure",
    "text": "Alternative Structure\nA cluster analysis can sometimes detect patterns that exploratory factor analyses miss. First, we see the correlation plot (Figure 8), with rectangles around 4 clusters (Ward’s method applied to the correlations subtracted from 1). Again, the pattern is not consistent with the 3 groupings.\n\n\n\n\n\n\n\n\nFigure 8: Correlation plot of the RESCA-E Subtests. Blue = Receptive, Red = Expressive, Green = Social Communication\n\n\n\n\n\nInstead, in the dendrogram in Figure 9, four clusters are highlighted plus a very narrow vocabulary cluster is singled out.\n\n\n\n\n\n\n\n\nFigure 9: Hierarchical Cluster Analysis of the RESCA-E Subtest Correlation Matrix (Ward’s Method). Suggested intepretations of clusters in blue.\n\n\n\n\n\nInstead of a three-part division between receptive, expressive, and social communication, this analysis suggests a 2 × 2 matrix (See Figure 10). There are two expressive clusters, one of which emphasizes social communication whereas the other emphasizes narrative communication (not that narrative is non-social). There are two “receptive” clusters, one of which is more social in that it is about understanding others’ intentions, sometimes via understanding English syntax. The other comprehension cluster consists of tests measuring semantic knowledge (e.g., vocabulary) and narratives.\n\n\n\n\n\n\n\n\nFigure 10: Matrix of Concpetual Groupings\n\n\n\n\n\nFigure 11 shows that the 2 × 2 structure suggested by hierarchical cluster analysis is also present in the the 3D MDS model. Grab the picture with your mouse and move it around to see the structure better.\n\n\n\n\n\n\n\n\nFigure 11: A suggested regrouping of the RESCA-E Subtests.\n\n\n\n\nWhy might there be a division between narrative/semantic vs. social/syntactic abilities? I can only speculate. The clumping of narrative and semantic abilities evokes the classic division of declarative memory into episodic memory and semantic memory. Why would social and syntactic subtests clump together? This is deeply speculative on my part, but there is a developing body of evidence that Broca’s Area not only is vital for the comprehension of syntax, but also the intentions of others (Gentilucci et al., 2006; Hamzei et al., 2003). To oversimplify, perhaps the narrative/semantic vs. social/syntactic abilities distinction reflects to some degree the relative health and functioning of Wernicke’s and Broca’s area, respectively."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Schneirographs",
    "section": "",
    "text": "Making a custom arrowhead for ggplot2 using ggarrow and arrowheadr\n\n\n\n\n\n\nggplot2\n\n\n\nThe ggarrow package allows any polygon to be an arrowhead. \n\n\n\n\n\nAug 26, 2023\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nAnnotated equations in ggplot2\n\n\n\n\n\n\nggplot2\n\n\n\nImporting latex into ggplot2\n\n\n\n\n\nJul 24, 2023\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nCreating collapsible output with knitr chunk hooks\n\n\n\n\n\n\nknitr\n\n\nmarkdown\n\n\n\nHow to make the output of a knitr code chunk collapsible using knitr hooks.\n\n\n\n\n\nJun 30, 2023\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nSplitting sentence texts into roughly equal segments\n\n\n\n\n\n\nggplot2\n\n\n\nFor long text labels on the y-axis, splitting them into roughly equal segments can look better.\n\n\n\n\n\nMay 31, 2023\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nInsert Text from One Markdown Document into Another\n\n\n\n\n\n\nmarkdown\n\n\n\nA function for retrieving a div or span from another markdown document\n\n\n\n\n\nMar 30, 2023\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nMaking text labels the same size as axis labels in ggplot2\n\n\n\n\n\n\nggplot2\n\n\n\nThe geom_text and geom_label functions do not specifiy text size the same way as the rest of ggplot2 elements do. For consistent text sizes, we can apply a simple conversion.\n\n\n\n\n\nAug 10, 2021\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nBar chart labels on smooth paths in ggplot2\n\n\n\n\n\n\nggplot2\n\n\n\nPlacing stacked bar chart labels on a smooth path makes them easier to compare.\n\n\n\n\n\nJul 31, 2021\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nPoint labels perpendicular to a curve in ggplot2\n\n\n\n\n\n\nggplot2\n\n\n\nHow to place legible labels for points on a curve in ggplot2\n\n\n\n\n\nJul 27, 2021\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nCaption on same line as axis title in ggplot2\n\n\n\n\n\n\nggplot2\n\n\n\nYou can put the caption on the line as the x-axis title\n\n\n\n\n\nJul 21, 2021\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nConcatenating vectors unless a vector is empty\n\n\n\n\n\n\nR\n\n\n\nThe paste and paste0 functions have a recycle0 argument that returns an empty vector when any part of the string is empty.\n\n\n\n\n\nMay 13, 2021\n\n\nW. Joel Schneider\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-07-23-latex-equation-in-ggplot2/index.html",
    "href": "posts/2023-07-23-latex-equation-in-ggplot2/index.html",
    "title": "Annotated equations in ggplot2",
    "section": "",
    "text": "R has a mathematical annotation system via plotmath, but I like the look of true \\LaTeX equations better.\nGetting \\LaTeX equations into ggplot2 plots has never been easy. The tikzdevice package is great if you are generating a .pdf document. If you are not, then you might want to consider other options.\nThe easiest, hassle-free option that I know of is to create the equation in a \\TeX editor and then import the resulting .pdf using ggimage. If I need the best possible image quality, I convert the .pdf to .svg using dvisvgm (see example below).\nFor annotated equations, I like using the aptly-named annotate-equations \\LaTeX package. It uses tikz to remember where parts of equations are on the page. The \\eqnmark and \\eqnmarkbox functions work like so:\n\\eqnmark[color]{node_name}{latex equation terms}\nThen use the \\annotate function like so:\n\\annotate[color]{above,left}{node_name}{annotation text}\n\nMy Annotated Equation\nI am going to refer to the same file name with various endings (e.g., .tex, .pdf, .svg), so I will define it here.\n\nmyfile &lt;- \"annotatedequationsimple\"\n\nHere is the code for the annotated equation. It is saved in annotatedequationsimple.tex.\n\n\nLaTeX code\n```{tikz zscorecode}\n#| code-fold: show\n#| code-summary: \"LaTeX code\"\n#| eval: false\n\n\\documentclass[border={25pt 50pt -35pt 52pt}]{standalone}\n%\\documentclass{article}\n\\usepackage{annotate-equations}\n\\usepackage{xcolor}\n\\definecolor{myviolet}{HTML}{440154}\n\\definecolor{myblue}{HTML}{3B528B}\n\\definecolor{myindigo}{HTML}{21908C}\n\\definecolor{mygreen}{HTML}{5DC863}\n\\usepackage[sfdefault,condensed]{roboto}\n\\begin{document}\n    \n\\renewcommand{\\eqnhighlightheight}{\\mathstrut}\n    \n\\huge$\n\\eqnmark[myviolet]{z}{z} = \n\\frac{\n    \\eqnmark[myblue]{x}{X}-\n    \\eqnmark[myindigo]{mu}{\\mu}}{\n    \\eqnmark[mygreen]{sigma}{\\sigma}}\n$\n\n\n\\annotate[\n    yshift=1em, \n    myviolet,\n    align=right]\n    {above, left}\n    {z}\n    {$z$-score}\n    \n\\annotate[\n    yshift=1em, \n    myblue, \n    align=right]\n    {above,left}\n    {x}\n    {Observed\\\\ Score}\n    \n\n\\annotate[\n    yshift=1em, \n    myindigo]\n    {above,right}\n    {mu}\n    {Population\\\\ Mean\\\\ $\\mu = 100$}\n    \n\n    \n\\annotate[\n    yshift=-.4em, \n    mygreen, \n    align=right]\n    {below,left}\n    {sigma}\n    {Population\\\\ Standard\\\\ Deviation\\\\ $\\sigma = 15$}\n    \n\\end{document}\n\n```\n\n\nNow convert the .tex file to .pdf:\n\npaste0('pdflatex -interaction=nonstopmode ', myfile,'.tex') |&gt; \n  shell()\n\nNow we can convert the .pdf to .svg:\n\npaste0(\"dvisvgm --pdf --output=\", myfile,\".svg \", myfile,\".pdf\") |&gt; \n  shell()\n\nFor best image quality, import the .svg file with svgparser. The read_svg function will create a grid grob that can plotted directly using ggplot2::annotation_custom. If you want fine control of the imported grob, you can plot it using the gggrid package. See here for an example.\nIn the simplest case, we can do this:\n\nlibrary(svgparser)\nmy_svg &lt;- svgparser::read_svg(paste0(myfile, \".svg\"))\nggplot() +\n  theme_void() +\n  annotation_custom(my_svg, xmin = 0, xmax = 1, ymin = 0, ymax = 1) \n\n\n\n\n\n\n\nFigure 1: A simple plot with an annotated equation.\n\n\n\n\n\nHowever, this is no better than just displaying the .svg directly. You probably want to embed the equation in a plot. For example:\n\n\nCode\nmu &lt;- 100\nsigma &lt;- 15\nplot_height &lt;- dnorm(mu, mu, sigma)\nlb &lt;- -4 * sigma + mu\nub &lt;- 4 * sigma + mu\n\nggplot() +\n  annotation_custom(my_svg,\n                    xmin = 112,\n                    xmax = 164,\n                    ymin = .33 * plot_height) +\n  stat_function(\n    fun = \\(x) dnorm(x, mean = mu, sd = sigma),\n    geom = \"area\",\n    n = 1000,\n    fill = \"dodgerblue\",\n    alpha = .5\n  ) +\n  theme_classic(base_family = \"Roboto Condensed\",\n                base_size = 18) +\n  theme(\n    axis.text.x = element_markdown(),\n    axis.title.x = element_markdown(),\n    axis.line = element_blank()\n  )  +\n  scale_x_continuous(\n    \"Observed Score *X*&lt;br&gt;*z*\",\n    breaks = seq(lb, ub, sigma),\n    limits = c(lb, ub),\n    labels = \\(x) paste0(\n      signs::signs(x),\n      \"&lt;br&gt;\",\n      ifelse(\n        x == mu,\n        \"&lt;em&gt;&mu;&lt;/em&gt;\",\n        paste0(\n          signs::signs((x - mu) / sigma,\n                       add_plusses = T,\n                       label_at_zero = \"none\"\n          ),\n          \"&lt;em&gt;&sigma;&lt;/em&gt;\"\n        )\n      )\n    )\n  ) +\n  scale_y_continuous(\n    NULL,\n    limits = c(0, plot_height),\n    expand = expansion(),\n    breaks = NULL\n  ) \n\n\n\n\n\n\n\n\nFigure 2: Importing an .svg file via svgparser and annotation_custom.\n\n\n\n\n\nIf you can live with just a little pixelation, the ggimage package can import a .pdf directly with good results and less hassle, provided you render the plot with the ragg package.\n\n\nCode\nggplot() +\n  geom_image(\n    data = tibble(\n      x = 140,\n      y = .65 * plot_height,\n      image = \"annotatedequationsimple.pdf\"\n    ),\n    aes(x, y, image = image),\n    size = .70\n  ) +\n  stat_function(\n    fun = \\(x) dnorm(x, mean = mu, sd = sigma),\n    geom = \"area\",\n    n = 1000,\n    fill = \"dodgerblue\",\n    alpha = .5\n  ) +\n  theme_classic(base_family = \"Roboto Condensed\", \n                base_size = 18) +\n  theme(\n    axis.text.x = element_markdown(),\n    axis.title.x = element_markdown(),\n    axis.line = element_blank()\n  )  +\n  scale_x_continuous(\n    \"Observed Score *X*&lt;br&gt;*z*\",\n    breaks = seq(lb, ub, sigma),\n    limits = c(lb, ub),\n    labels = \\(x) paste0(\n      signs::signs(x),\n      \"&lt;br&gt;\",\n      ifelse(\n        x == mu,\n        \"&lt;em&gt;&mu;&lt;/em&gt;\",\n        paste0(\n          signs::signs((x - mu) / sigma,\n                       add_plusses = T,\n                       label_at_zero = \"none\"\n          ),\n          \"&lt;em&gt;&sigma;&lt;/em&gt;\"\n        )\n      )\n    )\n  ) +\n  scale_y_continuous(\n    NULL,\n    limits = c(0, plot_height),\n    expand = expansion(),\n    breaks = NULL\n  )\n\n\n\n\n\n\n\n\nFigure 3: Importing a .pdf file via ggimage::geom_image.\n\n\n\n\n\n\n\nA more complex example\nIn this example, I used the \\eqnmarkbox function for greater clarity. The .tex file is saved in a file called annotatedequation.tex.\n\n\nLaTeX code\n\\documentclass[border={10pt 48pt -45pt 62pt}]{standalone}\n%\\documentclass{article}\n\\usepackage{annotate-equations}\n\\usepackage{xcolor}\n\\definecolor{myviolet}{HTML}{414487}\n\\definecolor{myblue}{HTML}{2F6C8E}\n\\definecolor{myblue2}{HTML}{21908C}\n\\definecolor{mygreen}{HTML}{2FB47C}\n\\definecolor{mygreen2}{HTML}{7AD151}\n\\usepackage[sfdefault,condensed]{roboto}\n\\begin{document}\n    \n\\renewcommand{\\eqnhighlightheight}{\\mathstrut}\n    \n$\\LARGE\n\\eqnmarkbox[myviolet]{nodeP}{P\\left(T \\le \\tau \\right)} = \n\\eqnmarkbox[myblue]{phi}{\\Phi}\n\\left(\\frac{\n    \\eqnmarkbox[myblue2]{tau}{\\tau}-\n    \\eqnmarkbox[mygreen]{esttrue}{\\hat{T}}}{\n    \\eqnmarkbox[mygreen2]{sigma}{\\sigma_{T - \\hat{T}}}}\n\\right)$\n\n\n\\annotate[\n    yshift=1em, \n    xshift=11mm, \n    myviolet]\n    {above, left}\n    {nodeP}\n    {Probability true score $T$\\\\ is less than threshold $\\tau$ }\n    \n\\annotate[\n    yshift=-.6em,\n    myblue]\n    {below,left}\n    {phi}\n    {Standard Normal Cumulative\\\\ Distribution Function $\\Phi()$}\n    \n\\annotate[\n    yshift=1.4em, \n    xshift=4mm, \n    myblue2]\n    {above,left}\n    {tau}\n    {Threshold $\\tau=70$}\n    \n\\annotate[\n    yshift=3em, \n    xshift=7mm, \n    mygreen]\n    {above,left}\n    {esttrue}\n    {Estimated True Score $\\hat{T}=r_{XX}(X-\\mu)+\\mu$\\\\ \n    Observed Score $X\\sim \\mathcal{N}\\left(\\mu = 100, \\sigma=15\\right)$\\\\ \n    Reliability Coefficient $r_{XX}=\\{.80,.85,.90,.95,.98\\}$}\n    \n\\annotate[\n    yshift=-2em, \n    mygreen2]\n    {below,left}\n    {sigma}\n    {Standard Error of the Estimate\\\\ \n    $\\sigma_{T-\\hat{T}}=\\sigma\\sqrt{r_{XX}-r_{XX}^2}$}\n    \n\\end{document}\n\n\nNow convert .tex to .pdf:\n\n\nCode\nmyfile &lt;- \"annotatedequation\"\n\npaste0('pdflatex -interaction=nonstopmode ', myfile,'.tex') |&gt; \n  shell()\n\n\nAnd we are ready to plot. This plot shows the probability that an observed score will have a true score less than a specific threshold, given a reliability coefficient.\n\n\nCode\nviridis_start &lt;- .2\nviridis_end &lt;- .8\nthreshold &lt;- 70\n\n# Find where a line intersects with the normal cdf\nfind_x &lt;- function(rxx = .8,\n                   slope = 0.0048,\n                   intercept = .66,\n                   mu = 100,\n                   sigma = 15,\n                   start_x = 60,\n                   threshold = 70) {\n  x &lt;- start_x\n  precision &lt;- .00001\n  diff_y &lt;- precision * 10\n  reps &lt;- 0\n  while (abs(diff_y) &gt; precision) {\n    x &lt;- x - diff_y \n    line_y &lt;- x * slope + intercept\n    curve_y &lt;- pnorm(threshold,\n                     mean = rxx * (x - mu) + mu,\n                     sd = sigma * sqrt(rxx - rxx ^ 2))\n    \n    line_y &lt;- x * slope + intercept\n    curve_y &lt;- pnorm(threshold,\n                     mean = rxx * (x - mu) + mu,\n                     sd = sigma * sqrt(rxx - rxx ^ 2))\n    diff_y &lt;- line_y - curve_y\n    reps &lt;- reps + 1\n  }\n  tibble(x = x, p = line_y, mu = mu, sigma = sigma, threshold = threshold, reps = reps)\n}\n\n\n\n\ndimage = data.frame(x = 115, \n                    y = .62, \n                    image = paste0(myfile, \".pdf\"))\nv_rxx &lt;- round(c(seq(0.80, 0.95, 0.05), 0.98), 2)\n\nd_threshold &lt;-\n  crossing(x = round(seq(40, 160, 0.1), 1),\n           rxx = v_rxx,\n           threshold = 70) %&gt;%\n  mutate(\n    see = 15 * sqrt(rxx - rxx ^ 2),\n    mu = (x - 100) * rxx + 100,\n    p = pnorm(threshold, mu, see)\n  ) %&gt;%\n  group_by(rxx) %&gt;%\n  mutate(acceleration = p - lag(p)) %&gt;%\n  ungroup\n\n\n\nd_labels &lt;- tibble(rxx = v_rxx) %&gt;%\n  mutate(x = map_df(rxx, find_x)) |&gt; \n  unnest(x)\n\nd_threshold %&gt;%\n  ggplot(aes(x, p)) +\n  geom_line(aes(color = factor(rxx)), lwd = 1) +\n  geom_vline(\n    aes(xintercept = threshold),\n    lty = 2,\n    lwd = 1,\n    color = \"gray30\"\n  ) +\n  geom_image(data = dimage,\n             aes(x = x,\n                 y = y,\n                 image = image),\n             size = .87) +\n  geom_richtext(\n    aes(label = rxx_label,\n        color = factor(rxx)),\n    data = d_labels %&gt;%\n      mutate(rxx_label = prob_label(rxx)),\n    angle = -67,\n    size = WJSmisc::ggtext_size(13),\n    label.colour = NA,\n    fill = \"#FFFFFF\",\n    family = \"Roboto Condensed\",\n    label.margin = unit(0, \"mm\"),\n    label.r = unit(2, \"mm\"),\n    label.padding = unit(c(0, 0.75, 0, .5), \"mm\")\n  ) +\n  scale_x_continuous(\n    \"Observed Score\",\n    breaks = seq(40, 160, 15),\n    minor_breaks = seq(40, 160, 5),\n    expand = expansion()\n  ) +\n  scale_y_continuous(\n    paste0(\"Probability True Score &lt; \", threshold),\n    expand = expansion(),\n    breaks = seq(0, 1, 0.1),\n    labels = prob_label,\n    limits = c(0, 1)\n  ) +\n  scale_color_viridis_d(begin = viridis_start,\n                        end = viridis_end) +\n  theme_minimal(base_family = \"Roboto Condensed\", \n                base_size = 16) +\n  theme(legend.position = \"none\", \n        plot.margin = unit(c(3, 5, 2, 2), \"mm\")) +\n  coord_fixed(ratio = 100,\n              clip = \"off\",\n              xlim = c(40, 160)) \n\n\n\n\n\n\n\n\nFigure 4: The probability that a true score is less than 70 depends on the observed score’s position and reliability coefficient.\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2023,\n  author = {Schneider, W. Joel},\n  title = {Annotated Equations in Ggplot2},\n  date = {2023-07-24},\n  url = {https://wjschne.github.io/posts/2023-07-23-latex-equation-in-ggplot2},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2023, July 24). Annotated equations in ggplot2.\nSchneirographs. https://wjschne.github.io/posts/2023-07-23-latex-equation-in-ggplot2"
  },
  {
    "objectID": "posts/bar-chart-labels-on-smooth-paths-in-ggplot2/index.html",
    "href": "posts/bar-chart-labels-on-smooth-paths-in-ggplot2/index.html",
    "title": "Bar chart labels on smooth paths in ggplot2",
    "section": "",
    "text": "Setup\nThis trick takes a lot of work. I only use it when I need a plot to look its best.\nSuppose I have a Likert questionnaire item with responses ranging from Strongly Disagree to Strongly Agree. Let’s say I have many groups in my study (e.g., college majors), and I want to compare their responses to the item.\nFirst I will create fake data for 26 groups, A–Z.\n\nlibrary(extrafont)\nlibrary(tidyverse)\nmyfont &lt;- \"Roboto Condensed\"\n\nd &lt;- tibble(Group = LETTERS[1:26],\n            mu = rnorm(26, 0, .5),\n            sigma = 1) %&gt;%\n  mutate(x = pmap(list(mean = mu, sd = sigma), rnorm, n = 500)) %&gt;%\n  unnest(x) %&gt;%\n  mutate(Agree = cut_number(x, 6) %&gt;%\n           factor(\n             labels = c(\n               \"Strongly\\nDisagree\",\n               \"Disagree\",\n               \"Slightly\\nDisagree\",\n               \"Slightly\\nAgree\",\n               \"Agree\",\n               \"Strongly\\nAgree\"\n             )\n           )) %&gt;%\n  group_by(Group, Agree) %&gt;%\n  summarise(n = n(), .groups = \"drop\") %&gt;%\n  group_by(Group) %&gt;%\n  arrange(Group, Agree) %&gt;%\n  mutate(p = n / sum(n)) %&gt;%\n  ungroup()\n\n\n\nFirst Attempts at Plotting\n\nd %&gt;%\n  ggplot(mapping = aes(p, Group)) +\n  geom_col(aes(fill = fct_rev(Agree))) +\n  scale_fill_viridis_d(NULL,\n                       begin = .15,\n                       end = 0.8,\n                       direction = -1) +\n  scale_x_continuous(\"Percent\", expand = expansion()) +\n  scale_y_discrete(\"Group\", expand = expansion()) +\n  coord_fixed(1 / 20.5, clip = \"off\") +\n  geom_text(\n    aes(x = p, label = round(100 * p, 0)),\n    color = \"white\",\n    family = myfont,\n    position = position_stack(vjust = .5)\n  )\n\n\n\n\n\n\n\n\nNot bad, but it would look better if we sorted the groups. We have many sorting options. One is that we can convert the Likert scale to numeric and then sort by the group with the highest mean.\n\nd %&gt;%\n  mutate(Group = fct_reorder(Group, .x = as.numeric(Agree) * p, \n                             .fun = mean)) %&gt;%\n  ggplot(mapping = aes(p, Group)) +\n  geom_col(aes(fill = fct_rev(Agree))) +\n  scale_fill_viridis_d(NULL,\n                       begin = .15,\n                       end = 0.8,\n                       direction = -1) +\n  scale_x_continuous(\"Percent\", expand = expansion()) +\n  scale_y_discrete(\"Group\", expand = expansion()) +\n  coord_fixed(1 / 20.5, clip = \"off\") +\n  geom_text(\n    aes(x = p, label = round(100 * p, 0)),\n    color = \"white\",\n    family = myfont,\n    position = position_stack(vjust = .5)\n  )\n\n\n\n\n\n\n\n\nIf I wanted to sort by the “Strongly Agree” category (or any other category):\n\nd %&gt;%\n  mutate(Group = fct_reorder(\n    Group,\n    .x = (Agree == \"Strongly\\nAgree\") * p,\n    .fun = mean\n  )) %&gt;%\n  ggplot(mapping = aes(p, Group)) +\n  geom_col(aes(fill = fct_rev(Agree))) +\n  scale_fill_viridis_d(NULL,\n                       begin = .15,\n                       end = 0.8,\n                       direction = -1) +\n  scale_x_continuous(\"Percent\", expand = expansion()) +\n  scale_y_discrete(\"Group\", expand = expansion()) +\n  coord_fixed(1 / 20.5, clip = \"off\") +\n  geom_text(\n    aes(x = p, label = round(100 * p, 0)),\n    color = \"white\",\n    family = myfont,\n    position = position_stack(vjust = .5)\n  )\n\n\n\n\n\n\n\n\n\n\nFinal Plot\nSo far, these plots look pretty good. However, I wish that the percentage labels were placed in a more aesthetically pleasing way. I am going to group by each of the response categories and then create a loess regression equation to smooth out the placement. It will mean that the labels are no longer centered in the stacked bars, but I think the sacrifice is worth it. I think that the percentage values are much easier to compare because the eye can follow the smooth line of labels.\n\nd &lt;- tibble(Group = LETTERS[1:26],\n            mu = rnorm(26, 0, .5),\n            sigma = 1) %&gt;%\n  mutate(x = pmap(list(mean = mu, sd = sigma), rnorm, n = 500)) %&gt;%\n  unnest(x) %&gt;%\n  mutate(Agree = cut_number(x, 6) %&gt;%\n           factor(\n             labels = c(\n               \"Strongly\\nDisagree\",\n               \"Disagree\",\n               \"Slightly\\nDisagree\",\n               \"Slightly\\nAgree\",\n               \"Agree\",\n               \"Strongly\\nAgree\"\n             )\n           )) %&gt;%\n  mutate(Group = fct_reorder(Group, as.numeric(Agree), .fun = mean)) %&gt;%\n  group_by(Group, Agree) %&gt;%\n  summarise(n = n(), .groups = \"drop\") %&gt;%\n  mutate(Group_position = as.numeric(Group)) %&gt;%\n  group_by(Group) %&gt;%\n  arrange(Group, Agree) %&gt;%\n  mutate(p = n / sum(n),\n         # proportion in each response category by group\n         cp = cumsum(p),\n         # cumulative proportion\n         xpos = cp - p / 2 # center of each stacked bar\n         ) %&gt;%\n         group_by(Agree) %&gt;%\n           nest() %&gt;%\n           mutate(\n             fit = map(data,\n                       loess,\n                       formula = \"xpos ~ Group_position\",\n                       span = 0.45),\n             # loess regression for each response category\n             xhat = map(fit, predict) # x-axis position on smooth line\n           ) %&gt;%\n           select(-fit) %&gt;%\n           unnest(c(data, xhat)) %&gt;%\n           mutate(Group = factor(Group, labels = rev(LETTERS[1:26])))\n         \n         d %&gt;%\n           ggplot(mapping = aes(p, Group)) +\n           geom_col(aes(fill = fct_rev(Agree))) +\n           geom_text(aes(x = xhat,\n                         label = ifelse(p &gt; .01, # No labels on small bars\n                                        round(100 * p, 0),\n                                        \"\")),\n                     color = \"white\",\n                     family = myfont) +\n           scale_fill_viridis_d(begin = .15,\n                                end = 0.8,\n                                direction = -1) +\n           scale_x_continuous(\"Percent\", expand = expansion()) +\n           scale_y_discrete(\"Group\", expand = expansion()) +\n           coord_fixed(1 / 20.5, clip = \"off\") +\n           theme_minimal(base_size = 13, base_family = myfont) +\n           theme(\n             legend.position = \"top\",\n             legend.box.spacing = unit(0.5, \"mm\"),\n             legend.text = element_text(\n               color = \"white\",\n               size = 13,\n               margin = margin(b = -40),\n               # Lower legend text into box\n               vjust = 0.5\n             ),\n             legend.spacing.x = unit(0, \"mm\"),\n             axis.text.y = element_text(hjust = 0.5)\n           ) +\n           guides(\n             fill = guide_legend(\n               title = NULL,\n               nrow = 1,\n               # Put legend on a single row\n               reverse = T,\n               # Reverse order of legend\n               label.position = \"top\",\n               # Put text atop keys\n               keyheight = unit(15, \"mm\"),\n               # Size of legend rectangles\n               keywidth = unit(22.56, \"mm\")\n             )\n           ) \n\n\n\n\n\n\n\n\nThis feels right to me.\n\n\n\n\nCitationBibTeX citation:@misc{schneider2021,\n  author = {Schneider, W. Joel},\n  title = {Bar Chart Labels on Smooth Paths in Ggplot2},\n  date = {2021-07-31},\n  url = {https://wjschne.github.io/posts/bar-chart-labels-on-smooth-paths-in-ggplot2},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2021, July 31). Bar chart labels on smooth paths in\nggplot2. Schneirographs. https://wjschne.github.io/posts/bar-chart-labels-on-smooth-paths-in-ggplot2"
  },
  {
    "objectID": "posts/concatenating-vectors-unless-a-vector-is-empty/index.html",
    "href": "posts/concatenating-vectors-unless-a-vector-is-empty/index.html",
    "title": "Concatenating vectors unless a vector is empty",
    "section": "",
    "text": "I often need to add a prefix or suffix to a character vector:\n\nlibrary(tidyverse)\nx &lt;- c(\"A\", \"B\", \"C\")\npaste0(x, \"_1\")\n\n[1] \"A_1\" \"B_1\" \"C_1\"\n\n\nHowever, if the vector is empty, I do not want bare prefixes or suffixes like this:\n\nx &lt;- character(0)\npaste0(x, \"_1\")\n\n[1] \"_1\"\n\n\nI used to write a lot of tedious if statements like this:\n\nif (length(x) == 0) {\n  y &lt;- character(0)\n} else {\n  y &lt;- paste0(x, \"_1\")\n}\n\nFor R 4.0.1 and later versions, the paste and paste0 functions acquired the recycle0 argument. Setting recycle0 to TRUE returns an empty vector if at least one string is empty:\n\npaste0(x, \"_1\", recycle0 = TRUE)\n\ncharacter(0)\n\n\nIf you need to work with an earlier version of R, you can use the sprintf function instead:\n\n# Non-empty vector\nx &lt;- c(\"A\", \"B\", \"C\")\nsprintf(\"%s_1\", x)\n\n[1] \"A_1\" \"B_1\" \"C_1\"\n\n# Empty vector\nx &lt;- character(0)\nsprintf(\"%s_1\", x)\n\ncharacter(0)\n\n\nThe glue function from the glue package also works nicely:\n\n# Non-empty vector\nx &lt;- c(\"A\", \"B\", \"C\")\nglue::glue(\"{x}_1\")\n\nA_1\nB_1\nC_1\n\n# Empty vector\nx &lt;- character(0)\nglue::glue(\"{x}_1\")\n\n\n\n\nCitationBibTeX citation:@misc{schneider2021,\n  author = {Schneider, W. Joel},\n  title = {Concatenating Vectors Unless a Vector Is Empty},\n  date = {2021-05-13},\n  url = {https://wjschne.github.io/posts/concatenating-vectors-unless-a-vector-is-empty},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2021, May 13). Concatenating vectors unless a vector\nis empty. Schneirographs. https://wjschne.github.io/posts/concatenating-vectors-unless-a-vector-is-empty"
  },
  {
    "objectID": "posts/insert-text-from-one-markdown-document-into-another/index.html",
    "href": "posts/insert-text-from-one-markdown-document-into-another/index.html",
    "title": "Insert Text from One Markdown Document into Another",
    "section": "",
    "text": "While assembling a reply letter for a revise-and-resubmit document, I was documenting for reviewers how we addressed their concerns. The reply letter quoted numerous sentences and paragraphs that had been changed. Unfortunately, each time my co-authors and I edited the paper, I lost track of which sentences and paragraphs I had previously copied into the reply letter. I did not want to misquote my own paper.\nCopying-and-pasting with each edit was getting tedious, and it was an error-prone process. I decided to write a function that would retrieve a named div or span from the paper and print it as a quote in my reply letter. Here it is:\nget_quote &lt;- function(id, file, blockquote = TRUE) {\n  blocktext &lt;- ifelse(blockquote, \"\\n&gt; \", \"\")\n\n  refreplace &lt;- function(x) {\n    # List of sequence types\n    crossrefs &lt;- c(`@fig-` = \"Figure\",\n                   `@fig-` = \"Table\",\n                   `@eq-` = \"Equation\",\n                   `@thm-` = \"Lemma\",\n                   `@lem-` = \"Corollary\",\n                   `@cor-` = \"Proposition\",\n                   `@prp-` = \"Conjecture\",\n                   `@cnj-` = \"Definition\",\n                   `@def-` = \"Example\",\n                   `@exm-` = \"Example\",\n                   `@exr-` = \"Exercise\",\n                   `apafg-` = \"Figure\",\n                   `apatb-` = \"Figure\"\n    )\n\n    # Find all crossreference types and number them\n    make_replacements &lt;- function(x, reftype, prefix) {\n      regstring &lt;- ifelse(stringr::str_starts(reftype, \"\\\\@\"),\n                          paste0(\"\\\\\", reftype, \"(.*?)(?=[\\\\.\\\\?\\\\!\\\\]\\\\}\\\\s,])\"),\n                          paste0(\"\\\\{\", reftype, \"(.*?)\\\\}\"))\n      patterns &lt;- stringr::str_extract_all(string = x, pattern = regstring) |&gt;\n        unlist() |&gt;\n        unique() |&gt;\n        stringr::str_replace(\"\\\\{\", \"\\\\\\\\{\") |&gt;\n        stringr::str_replace(\"\\\\}\", \"\\\\\\\\}\")\n\n      if (all(is.na(patterns))) return(NULL)\n      replacements &lt;- paste(prefix, seq_along(patterns))\n      names(replacements) &lt;- patterns\n      replacements\n    }\n    allreplacements &lt;- purrr::map2(\n      names(crossrefs),\n      crossrefs,\n      \\(rt, pf) make_replacements(\n        x = x,\n        reftype = rt,\n        prefix = pf)) |&gt;\n      unlist()\n\n    stringr::str_replace_all(x, allreplacements)\n\n  }\n\n  filetext &lt;- readLines(file) |&gt;\n    refreplace()\n\n  idcount &lt;- sum(stringr::str_count(filetext, paste0(\"#\", id)))\n  if (idcount &gt; 1)\n    stop(paste0(\n      \"The id (\",\n      id ,\n      \") is not unique. There are \",\n      idcount,\n      \" instances of id = \",\n      id,\n      \".\"\n    ))\n\n  s &lt;- filetext |&gt;\n    paste0(collapse = \"\\n\") |&gt;\n    stringr::str_match(pattern = paste0(\"(?&lt;=\\\\[).+(?=\\\\]\\\\{\\\\#\",\n                               id,\n                               \"\\\\})\"))  |&gt;\n    getElement(1)\n\n  if (is.na(s)) {\n    s &lt;- filetext |&gt;\n      paste0(collapse = \"|||\") |&gt;\n      stringr::str_match(pattern = paste0(\":::\\\\{\\\\#\",\n                                 id,\n                                 \"\\\\}(.*?):::\"))  |&gt;\n      getElement(2) |&gt;\n      stringr::str_replace_all(\"\\\\|\\\\|\\\\|\", blocktext)\n  } else {\n    s &lt;- paste0(blocktext, s)\n  }\n\n  if (is.na(s)) stop(\"Could not find a div or span with id = \", id)\n\n  s\n}\nFor your convenience and mine, I have added this function into the WJSmisc package."
  },
  {
    "objectID": "posts/insert-text-from-one-markdown-document-into-another/index.html#remove-blockquote-formatting",
    "href": "posts/insert-text-from-one-markdown-document-into-another/index.html#remove-blockquote-formatting",
    "title": "Insert Text from One Markdown Document into Another",
    "section": "Remove blockquote formatting",
    "text": "Remove blockquote formatting\nBy default, the function adds a &gt; before each line in the quoted markdown so that it appears as a block quote. You can turn off the block quote formatting like so:\n`r get_quote(\"id1\", \"index.qmd\", blockquote = FALSE)`\nText in a span"
  },
  {
    "objectID": "posts/insert-text-from-one-markdown-document-into-another/index.html#curly-braces-with-additional-information",
    "href": "posts/insert-text-from-one-markdown-document-into-another/index.html#curly-braces-with-additional-information",
    "title": "Insert Text from One Markdown Document into Another",
    "section": "Curly braces with additional information",
    "text": "Curly braces with additional information\nWhat if you need to put additional information in the curly braces of the span or div (e.g., a css class)?\n[Here is more text with extra stuff in the curly braces.]{#id3 .myclass}\nYou can trick the function into thinking that the extra stuff is part of the id like so:\n`r get_quote(\"id3 .myclass\", \"index.qmd\")`\n\nHere is more text with extra stuff in the curly braces."
  },
  {
    "objectID": "posts/point-labels-perpendicular-to-a-curve-in-ggplot2/index.html",
    "href": "posts/point-labels-perpendicular-to-a-curve-in-ggplot2/index.html",
    "title": "Point labels perpendicular to a curve in ggplot2",
    "section": "",
    "text": "I would like to label points on a sine function so that the labels are always legible. In a sine wave plot in which θ ranges from 0 to 2π, sin(θ) ranges from −1 to +1. Thus, the plot’s xy ratio is\n\\text{plot ratio}= \\frac{2\\pi-0}{1- (-1)}=\\pi\nThe first derivative of the sine function is the cosine function. In my plot, the slope of the tangent line at each point is\n\n\\text{tangent slope} = \\text{plot ratio}\\times \\cos(\\theta)\n The angle of the tangent line’s slope is the arctan of the slope. I would like to place the label perpendicular to the tangent line so I will add 90 degrees (i.e., π/2 radians).\n\n\\text{text angle}=\\tan^{-1}(\\text{tangent slope})+\\pi/2\n\nNow I need a pair of functions that will convert this angle into the right values for ggplot2’s hjust and vjust arguments. I have added the angle2vjust and angle2hjust functions to the WJSmisc package, but I have defined them here as well:\n\nangle2vjust &lt;- function(theta, multiplier = 1, as_degrees = FALSE) {\n  if (as_degrees) theta &lt;- theta * pi / 180\n  (((sin(theta + pi) + 1) / 2) - 0.5) * multiplier + 0.5\n}\n\nangle2hjust &lt;- function(theta, multiplier = 1, as_degrees = FALSE) {\n  if (as_degrees) theta &lt;- theta * pi / 180\n  (((cos(theta + pi) + 1) / 2) - 0.5) * multiplier + 0.5\n}\n\nNow we plot the sine function with labels. I have used geom_richtext from the ggtext package because it allows me to set a white background along with padding and margins.\n\nlibrary(tidyverse)\nlibrary(ggtext)\n\nplot_ratio &lt;- pi\n\ntibble(theta = seq(0, 2 * pi, length.out = 13),\n            y =  sin(theta),\n            tangent_slope = cos(theta) * plot_ratio,\n            text_angle = atan(tangent_slope) + pi / 2) %&gt;%\n  ggplot(aes(theta, y)) +\n  geom_richtext(aes(label = formatC(y, digits = 2, format = \"f\"),\n                    vjust = angle2vjust(text_angle),\n                    hjust = angle2hjust(text_angle)),\n                label.color = NA,\n                label.padding = unit(1, \"pt\"),\n                label.margin = unit(5, \"pt\"),\n                size = 4) +\n  geom_point() +\n  stat_function(fun = sin) +\n  scale_x_continuous(\"&theta;\",\n                     breaks = seq(0, 2 * pi, \n                                  length.out = 13),\n                     minor_breaks = NULL,\n                     labels = function(x) round(x * 180 / pi)) +\n  scale_y_continuous(\"sin(&theta;)\") +\n  coord_fixed(ratio = plot_ratio, clip = \"off\") +\n  theme_minimal(base_size = 16) +\n  theme(axis.title.x = element_markdown(),\n        axis.title.y = element_markdown())\n\n\n\n\n\n\n\n\nIf you do not mind turning your head to one side or the other, a somewhat easier method is to set the label’s vjust to a negative value and rotate the labels by the angle of the tangent line:\n\ntibble(theta = seq(0, 2 * pi, length.out = 13),\n            y = sin(theta),\n            tangent_slope = cos(theta) * plot_ratio,\n            text_angle = atan(tangent_slope)) %&gt;%\n  ggplot(aes(theta, y)) +\n  geom_richtext(aes(label = round(y, 2),\n                    angle = text_angle * 180 / pi),\n                vjust = 0,\n                label.color = NA,\n                label.padding = unit(1, \"pt\"),\n                label.margin = unit(2, \"pt\"),\n                size = 4) +\n  geom_point() +\n  stat_function(fun = sin) +\n  scale_x_continuous(\"&theta;\",\n                     breaks = seq(0, 2 * pi, \n                                  length.out = 13),\n                     minor_breaks = NULL,\n                     labels = function(x) round(x * 180 / pi)) +\n  scale_y_continuous(\"sin(&theta;)\") +\n  coord_fixed(ratio = plot_ratio, clip = \"off\") +\n  theme_minimal(base_size = 16) +\n  theme(axis.title.x = element_markdown(),\n        axis.title.y = element_markdown())\n\n\n\n\n\n\n\n\nWhat if you do not know the function’s first derivative? You can approximate the slope of the tangent line by comparing a function’s output of each point with the output of a slightly deviated point. Here is a plot of the normal cumulative distribution function.\n\n# Small change in x\ndx &lt;- .00000001\nplot_ratio &lt;- 6\n\ntibble(x = seq(-3,3,.5),\n       y = pnorm(x),\n       tangent_slope = plot_ratio * (pnorm(x + dx) - y) / dx,\n       text_angle = atan(tangent_slope) + pi / 2,\n       degrees = text_angle * 180 / pi) %&gt;% \n  ggplot(aes(x,y)) + \n  geom_point() +\n  geom_richtext(aes(label = formatC(y, 2, format = \"f\"),\n                    vjust = angle2vjust(text_angle),\n                    hjust = angle2hjust(text_angle)),\n                label.color = NA,\n                label.padding = unit(1, \"pt\"),\n                label.margin = unit(5, \"pt\"),\n                size = 4) +\n  stat_function(fun = pnorm) + \n  theme_minimal(base_size = 16) + \n  theme(axis.title.x = element_markdown(),\n        axis.title.y = element_markdown()) +\n  coord_fixed(ratio = plot_ratio) + \n  scale_x_continuous(\"*x*\", breaks = -3:3) +\n  scale_y_continuous(\"&Phi;(*x*)\")\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2021,\n  author = {Schneider, W. Joel},\n  title = {Point Labels Perpendicular to a Curve in Ggplot2},\n  date = {2021-07-27},\n  url = {https://wjschne.github.io/posts/point-labels-perpendicular-to-a-curve-in-ggplot2},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2021, July 27). Point labels perpendicular to a curve\nin ggplot2. Schneirographs. https://wjschne.github.io/posts/point-labels-perpendicular-to-a-curve-in-ggplot2"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Nominal, Ordinal, Interval, and Ratio scales\n\n\n\n\n\n\n\n\nHow to visualize which values are most frequent in a distribution\n\n\n\n\n\n\n\n\nFunctions that tell us which values are more likely to occur\n\n\n\n\n\n\n\n\nAn introduction to expected value and how it is related to the concept of the mean of a variable.\n\n\n\n\n\n\n\n\nHow variance is calculated\n\n\n\n\n\n\n\n\nSum of the Reasons Variables Are Normally Normal\n\n\n\n\n\n\n\n\nAn introduction to skewness and its relationship to variability.\n\n\n\n\n\n\n\n\nAn introduction to kurtosis and how it is not just a measure of peakedness.\n\n\n\n\n\n\n\n\nAn introduction to z-scores, stanines, stens, scaled scores, T scores, and index scores\n\n\n\n\n\n\n\n\nCovariance is rarely useful by itself but is indispensable for many statistics."
  },
  {
    "objectID": "presentations.html#psychometrics-from-the-ground-up",
    "href": "presentations.html#psychometrics-from-the-ground-up",
    "title": "Presentations",
    "section": "",
    "text": "Nominal, Ordinal, Interval, and Ratio scales\n\n\n\n\n\n\n\n\nHow to visualize which values are most frequent in a distribution\n\n\n\n\n\n\n\n\nFunctions that tell us which values are more likely to occur\n\n\n\n\n\n\n\n\nAn introduction to expected value and how it is related to the concept of the mean of a variable.\n\n\n\n\n\n\n\n\nHow variance is calculated\n\n\n\n\n\n\n\n\nSum of the Reasons Variables Are Normally Normal\n\n\n\n\n\n\n\n\nAn introduction to skewness and its relationship to variability.\n\n\n\n\n\n\n\n\nAn introduction to kurtosis and how it is not just a measure of peakedness.\n\n\n\n\n\n\n\n\nAn introduction to z-scores, stanines, stens, scaled scores, T scores, and index scores\n\n\n\n\n\n\n\n\nCovariance is rarely useful by itself but is indispensable for many statistics."
  },
  {
    "objectID": "presentations.html#other-assessment-videos",
    "href": "presentations.html#other-assessment-videos",
    "title": "Presentations",
    "section": "Other Assessment Videos",
    "text": "Other Assessment Videos\n\nEmotional Intelligence and CHC Theory\nHow might emotional intelligence relate to psychometric models of intelligence?\n\n\n\n\n\n\n\nMisunderstanding Regression to the Mean\nWhat regression to the mean is and how it is often misunderstood. Examples are provided as to how it is applied to IQ and the death penalty.\n\n\n\n\n\n\n\nTaking latent variable models seriously\nApplying latent score estimates to individuals\n\n\n\n\n\n\n\nSpecific cognitive processing weaknesses are rarely full explanations for academic deficits\nIn which I confess to having had a longstanding misunderstanding about diagnosing learning disorders.\n\n\n\n\n\n\n\nTwo kinds of cognitive ability hierarchies\nSome latent variables represent “hierarchical abstractions.”\n\n\n\n\n\n\n\nA Taxonomy of Influences on Ability Tests\nSome influences are test-specific whereas others are shared across tests. Some influences are transient whereas others are stable. Some influences are relevant to the construct of interest whereas others are irrelevant.\n\n\n\n\n\n\n\nWithin-composite differences: Why measures of the same ability differ?\nThere are a number of lesser-known reasons that two tests that are intended to measure the same ability might differ substantially.\n\n\n\n\n\n\n\nWithin-composite differences: Do large subtest score differences invalidate composite scores?\nIt is often asserted that composite scores should not be interpreted when the scores that make up that composite are discrepant. I show here why this is not typically true, depending on what the composite score is used for.\n\n\n\n\n\n\n\nWriting Assessment Reports People Will Read, Understand, and Remember\nMy appearance on the School Psyched Podcast on October 20, 2019"
  },
  {
    "objectID": "presentations.html#audio-presentations",
    "href": "presentations.html#audio-presentations",
    "title": "Presentations",
    "section": "Audio Presentations",
    "text": "Audio Presentations\n\nThe evolution of cognitive assessment\nMy appearance on the Testing Psychologist Podcast on March 29, 2021"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "R package for simulating data using standardized coefficients\nTutorial\n\n\n\n\nIn the model below, the path coefficients are standardized. You would like to simulate the variables in the model, but you do not know the disturbance and residual variances. The simstandard package can help.\n\n\n\nA standardized latent variable model\n\n\n\n\n\n\n\nAn R package for detecting unusual scores in a test profile\nTutorial\n\n\n\n\nThis package estimates how unusual a multivariate normal profile is.\n\n\n\n\n\n\nA ggplot2 extension package for creating normal violin plots\n\n\n\n\nI needed to show confidence intervals and conditional normal distributions with specific means and standard deviations. I wrote the ggnormalviolin package to make this happen.\nIt makes plots like this:\n\n\n\nExample plot made with ggnormalviolin\n\n\n\n\n\n\n\nFunctions useful for psychological evaluations\nThis package is still in a preliminary state, just like Individual Psychometrics, the book it accompanies.\n\n\n\n\n\n\n\nMulivariate Confindence Intervals\n\n\n\n\n\n\n\nA set of functions I find convenient to have readily available to me\n\n\n\n\n\n\n\nWhoa! How did you place those labels so perfectly?\n\n\n\n\n\n\n\nAn R package for making digital spirographs\nTutorial\nMy Gallery\n\n\n\n\nMaking digital spirographs is fun! I made an R package called spiro that can make animated spirographs like this one:\n\n\n\n\n\n\nR package for making a custom arrowheads for ggplot2 using ggarrow\nTutorial\n\n\n\n\nThe arrowheadr package allows one to create custom arrowheads that can be used with the ggarrow package."
  },
  {
    "objectID": "software.html#simstandard",
    "href": "software.html#simstandard",
    "title": "Software",
    "section": "",
    "text": "R package for simulating data using standardized coefficients\nTutorial\n\n\n\n\nIn the model below, the path coefficients are standardized. You would like to simulate the variables in the model, but you do not know the disturbance and residual variances. The simstandard package can help.\n\n\n\nA standardized latent variable model"
  },
  {
    "objectID": "software.html#unusualprofile",
    "href": "software.html#unusualprofile",
    "title": "Software",
    "section": "",
    "text": "An R package for detecting unusual scores in a test profile\nTutorial\n\n\n\n\nThis package estimates how unusual a multivariate normal profile is."
  },
  {
    "objectID": "software.html#ggnormalviolin",
    "href": "software.html#ggnormalviolin",
    "title": "Software",
    "section": "",
    "text": "A ggplot2 extension package for creating normal violin plots\n\n\n\n\nI needed to show confidence intervals and conditional normal distributions with specific means and standard deviations. I wrote the ggnormalviolin package to make this happen.\nIt makes plots like this:\n\n\n\nExample plot made with ggnormalviolin"
  },
  {
    "objectID": "software.html#psycheval",
    "href": "software.html#psycheval",
    "title": "Software",
    "section": "",
    "text": "Functions useful for psychological evaluations\nThis package is still in a preliminary state, just like Individual Psychometrics, the book it accompanies.\n\n\n\n\n\n\n\nMulivariate Confindence Intervals"
  },
  {
    "objectID": "software.html#wjsmisc",
    "href": "software.html#wjsmisc",
    "title": "Software",
    "section": "",
    "text": "A set of functions I find convenient to have readily available to me\n\n\n\n\n\n\n\nWhoa! How did you place those labels so perfectly?"
  },
  {
    "objectID": "software.html#spiro",
    "href": "software.html#spiro",
    "title": "Software",
    "section": "",
    "text": "An R package for making digital spirographs\nTutorial\nMy Gallery\n\n\n\n\nMaking digital spirographs is fun! I made an R package called spiro that can make animated spirographs like this one:"
  },
  {
    "objectID": "software.html#arrowheadr",
    "href": "software.html#arrowheadr",
    "title": "Software",
    "section": "",
    "text": "R package for making a custom arrowheads for ggplot2 using ggarrow\nTutorial\n\n\n\n\nThe arrowheadr package allows one to create custom arrowheads that can be used with the ggarrow package."
  },
  {
    "objectID": "software.html#apaquarto",
    "href": "software.html#apaquarto",
    "title": "Software",
    "section": "apaquarto",
    "text": "apaquarto\nA Quarto Extension for Creating APA 7 Style Documents\nThis is a quarto article template that creates APA Style 7th Edition documents in .docx, .html. and .pdf. I made this extension for my own workflow. If it helps you, too, I am happy. The output of the template is displayed below:\n\n\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "tutorials/DataWrangling/data_wrangling.html",
    "href": "tutorials/DataWrangling/data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "In programs like SPSS, the data seem to live in a spreadsheet that is always open and available to you. In R, you can see the data whenever you want, but usually it sits unseen inside a variable. It can be disconcerting at first, but think of your data as living in a file somewhere on your hard drive (or online!), and then the data just come for a short visit in R.\nThe major benefit of working with data in R is that all of the changes, transformations, and restructuring happens in code—which can be recreated at any time. There is usually no need to “save” the data after you have transformed it. The next time you work with the data, you just run your code and all the calculations will transform the data exactly the same way as before.\nWhy does this matter? If you feel the need to save your data all the time, you end of having multiple copies of it: data.sav, data_restructured.sav, data_new.sav, data_new_final.sav, fixed_data_new_final.sav, restructued_final_with_missing_cases_removed.sav, and so forth and so on. It can be hard to figure out where to start the next time you work with your data. You might not remember which version has errors and which version has what you need.\nIn general, start with a completely raw data file (one that is exactly the way you started). Resist the temptation to make any changes to it directly. If you must, save a pristine copy somewhere and only then change it. Import your data and make all changes to it with code. One benefit of doing so is that the code documents any changes you would otherwise need to record in your lab notebook."
  },
  {
    "objectID": "tutorials/DataWrangling/data_wrangling.html#restructuring-from-wide-to-long-format-with-pivot_longer",
    "href": "tutorials/DataWrangling/data_wrangling.html#restructuring-from-wide-to-long-format-with-pivot_longer",
    "title": "Data Wrangling",
    "section": "Restructuring from wide to long format with pivot_longer",
    "text": "Restructuring from wide to long format with pivot_longer\nUse the pivot_longer function to pivot the three time variables to long format:\n\nd_small %&gt;% \n  pivot_longer(cols = time_1:time_3)\n\n\n\n\n\nid\nbaseline_sessions_n\nintervention_sessions_n\nname\nvalue\n\n\n\n\n1\n22\n20\ntime_1\n11.481\n\n\n1\n22\n20\ntime_2\n14.402\n\n\n1\n22\n20\ntime_3\n6.151\n\n\n2\n18\n19\ntime_1\n9.114\n\n\n2\n18\n19\ntime_2\n8.586\n\n\n2\n18\n19\ntime_3\n11.010\n\n\n\n\n\n\nTo avoid the hassle of renaming our columns, we can specify what the name and value columns should be called:\n\nd_small %&gt;%\n  pivot_longer(cols = time_1:time_3,\n               names_to = \"time\",\n               values_to = \"on_task\")\n\n\n\n\n\nid\nbaseline_sessions_n\nintervention_sessions_n\ntime\non_task\n\n\n\n\n1\n22\n20\ntime_1\n11.481\n\n\n1\n22\n20\ntime_2\n14.402\n\n\n1\n22\n20\ntime_3\n6.151\n\n\n2\n18\n19\ntime_1\n9.114\n\n\n2\n18\n19\ntime_2\n8.586\n\n\n2\n18\n19\ntime_3\n11.010\n\n\n\n\n\n\nNotice that the time variable is text, not a number, like we want. There are many ways to get rid of the prefix. The simplest is to tell pivot_longer to strip away the prefix “time_”.\n\nd_small %&gt;%\n  pivot_longer(cols = time_1:time_3,\n               names_to = \"time\",\n               values_to = \"on_task\",\n               names_prefix = \"time_\")\n\n\n\n\n\nid\nbaseline_sessions_n\nintervention_sessions_n\ntime\non_task\n\n\n\n\n1\n22\n20\n1\n11.481\n\n\n1\n22\n20\n2\n14.402\n\n\n1\n22\n20\n3\n6.151\n\n\n2\n18\n19\n1\n9.114\n\n\n2\n18\n19\n2\n8.586\n\n\n2\n18\n19\n3\n11.010\n\n\n\n\n\n\nUnfortunately, R thinks that time is a text variable. We want it to be an integer, so we transform it using the as.integer function:\n\nd_small %&gt;%\n  pivot_longer(cols = time_1:time_3,\n               names_to = \"time\",\n               values_to = \"on_task\",\n               names_prefix = \"time_\", \n               names_transform = list(time = as.integer)) \n\n\n\n\n\nid\nbaseline_sessions_n\nintervention_sessions_n\ntime\non_task\n\n\n\n\n1\n22\n20\n1\n11.481\n\n\n1\n22\n20\n2\n14.402\n\n\n1\n22\n20\n3\n6.151\n\n\n2\n18\n19\n1\n9.114\n\n\n2\n18\n19\n2\n8.586\n\n\n2\n18\n19\n3\n11.010\n\n\n\n\n\n\nAlternatively, we can do all these transformations after pivoting:\n\nd_small %&gt;%\n  pivot_longer(cols = time_1:time_3,\n               names_to = \"time\",\n               values_to = \"on_task\") %&gt;% \n  mutate(time = str_remove(time, \"time_\") %&gt;% \n           as.integer())\n\n\n\n\n\nid\nbaseline_sessions_n\nintervention_sessions_n\ntime\non_task\n\n\n\n\n1\n22\n20\n1\n11.481\n\n\n1\n22\n20\n2\n14.402\n\n\n1\n22\n20\n3\n6.151\n\n\n2\n18\n19\n1\n9.114\n\n\n2\n18\n19\n2\n8.586\n\n\n2\n18\n19\n3\n11.010"
  },
  {
    "objectID": "tutorials/DataWrangling/data_wrangling.html#restructuring-from-long-to-wide-format-with-pivot_wider",
    "href": "tutorials/DataWrangling/data_wrangling.html#restructuring-from-long-to-wide-format-with-pivot_wider",
    "title": "Data Wrangling",
    "section": "Restructuring from long to wide format with pivot_wider",
    "text": "Restructuring from long to wide format with pivot_wider\nLet’s pivot the data from long format to wide. First let’s assign the restructured d_small data as d_small_longer:\n\nd_small_longer &lt;- d_small %&gt;%\n  pivot_longer(cols = time_1:time_3,\n               names_to = \"time\",\n               values_to = \"on_task\",\n               names_prefix = \"time_\",\n               names_transform = list(time = as.integer))\n\nNow let’s move it back to where it was with pivot_wider:\n\nd_small_longer %&gt;% \n  pivot_wider(names_from = time, \n              values_from = on_task, \n              names_prefix = \"time_\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nbaseline_sessions_n\nintervention_sessions_n\ntime_1\ntime_2\ntime_3\n\n\n\n\n1\n22\n20\n11.481\n14.402\n6.151\n\n\n2\n18\n19\n9.114\n8.586\n11.010\n\n\n\n\n\n\nPerfect! Now lets move back to the the original large tibble d."
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html",
    "title": "Getting Ready to Use R and RStudio",
    "section": "",
    "text": "“Blooming, buzzing confusion?” I see what you did there. —WJ\n\nFor R to pay off, you have to buy in. Learning challenging material does not have to be “fun” at every moment to be worthwhile. The beginning data analyst, assailed by statistical concepts, coding conventions, and baffling error messages, feels it all as one great blooming, buzzing confusion.\nThe challenge of learning R is indeed going to be challenging at times, but just there will also be many little a-ha moments, several larger check-out-what-I-can-do celebrations, and a few peak experiences of eudaemonic reverie."
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#r-is-a-programming-language-built-for-statistics.",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#r-is-a-programming-language-built-for-statistics.",
    "title": "Getting Ready to Use R and RStudio",
    "section": "R is a programming language built for statistics.",
    "text": "R is a programming language built for statistics.\n\n\n\n\nJohn Chambers\n\n\n\n\nRobert Gentleman\n\n\n\n\nRoss Ihaka\n\n\n\nIn 1976, John Chambers and colleagues at Bell Labs developed S, a programming language specifically designed to facilitate statistical analyses and data visualization.\nIn 1995, Robert Gentleman and Ross Ihaka released an open-source variant of S and named it “R” after their shared first initial.\nMost S code looks identical to R code, but R has a number of subtle enhancements such as better memory management.\nMore about the origins of R"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#r-is-a-computer-program-that-runs-code-in-the-programming-language-r.",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#r-is-a-computer-program-that-runs-code-in-the-programming-language-r.",
    "title": "Getting Ready to Use R and RStudio",
    "section": "R is a computer program that runs code in the programming language R.",
    "text": "R is a computer program that runs code in the programming language R.\nThe primary way that the R language is used is to perform analyses in the R software environment for statistical computing and graphics. This program is free and open source, meaning that not only can anyone download it for free, but its source code can be reused by anyone for any purpose."
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#r-is-a-data-science-ecosystem-in-which-the-r-community-flourishes",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#r-is-a-data-science-ecosystem-in-which-the-r-community-flourishes",
    "title": "Getting Ready to Use R and RStudio",
    "section": "R is a data science ecosystem in which the R community flourishes",
    "text": "R is a data science ecosystem in which the R community flourishes\nStrictly speaking, R is just one program. However, people use R in startlingly creative ways, often in combination with other programs and in coordination with other people. I like to think of R as the entire ecosystem of software, services, standards, and cultural practices shared by the entire community of R users.\n\n\n\nArtwork by [@allison_horst]\n\n\nThe technical merits of the R language and software do matter, but the success of R has more to do with the inclusive, welcoming, diverse, and expanding culture of the R community. When an individual R user has a great idea, the R community has as integrated set of technical standards, web services, and cultural practices such that innovations spread quickly to everyone in the community—often with multiple tutorials aimed at users of all levels of expertise. With the support of the R community, ordinary people can leverage R to accomplish far more than anyone would have thought possible not so very long ago.\n\n\n\nArtwork by [@allison_horst]"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#windows",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#windows",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Windows",
    "text": "Windows\nClick here and download the latest version of R for Windows. Open the file and follow installation instructions.\n\n\n\nScreenshot of Windows Installation Link"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#mac",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#mac",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Mac",
    "text": "Mac\nClick here and download the latest version of R for Mac. Double-click the file and follow installation instructions.\nThe instructions can be overwhelming to newcomers so I have included a picture with the download link highlighted:\n\n\n\nScreenshot of Mac Installation Link"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#linux",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#linux",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Linux",
    "text": "Linux\nInstall R on Ubuntu with this Bash script:\n\nsudo apt-get install r-base \n\nIf you are a Linux user, you probably know what this means. If not, you probably do not. More detailed instructions here."
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#rstudio-desktop",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#rstudio-desktop",
    "title": "Getting Ready to Use R and RStudio",
    "section": "RStudio Desktop",
    "text": "RStudio Desktop\nRStudio is free. Please do not pay for anything. The paid versions of RStudio have nothing you will need for this course. They are not better than the free version. They simply have features tailored to businesses and developers.\nInstall the free version of RStudio Desktop here.\nThe version recommended for your operation system (I’m using Windows) will appear here:\n\n\n\nScreenshot of RStudio Installation Link"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#rstudio-cloud",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#rstudio-cloud",
    "title": "Getting Ready to Use R and RStudio",
    "section": "RStudio Cloud",
    "text": "RStudio Cloud\nOne alternative to installing R and RStudio on your machine is to use RStudio in a web browser in RStudio Cloud. After signing up for a free account, you can use RStudio online. As the internet speeds up, I imagine that this option will become increasingly attractive."
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#the-console",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#the-console",
    "title": "Getting Ready to Use R and RStudio",
    "section": "The Console",
    "text": "The Console\nThe console (lower left) is where you submit quick temporary calculations and run code you have no intention of saving. Hit Enter to submit code to R. R is interactive in the sense that it will display the result of the code in the console where you typed."
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#scripts",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#scripts",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Scripts",
    "text": "Scripts\nThe script pane (upper left) is for writing code you want to preserve in a script file (.R) or RMarkdown file (.Rmd). More on RMarkdown later. Save your script files frequently.\nYou can submit code to the console by hitting Ctrl+Enter (or ⌘+Enter on Macs). You can also submit code by hitting the Run button. If you have code selected, only the selected part will be submitted. If you have no code selected, the current line (wherever the cursor is) will be submitted."
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#environment-variables",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#environment-variables",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Environment Variables",
    "text": "Environment Variables\nIn the Environment tab in the upper right pane, you can see which variables have been created and a preview of what they contain."
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#viewer",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#viewer",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Viewer",
    "text": "Viewer\nThe lower right pane has several tabs:\n\nFiles: Interact with project files\nPlots: Preview plots\nPackages: Install and update packages\nHelp: Search for help\nViewer: View documents created by RStudio"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#give-your-files-long-descriptive-names.",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#give-your-files-long-descriptive-names.",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Give your files long, descriptive names.",
    "text": "Give your files long, descriptive names.\nMake it clear to future-you exactly what is in the file.\n\nBad: data.xlsx\nBetter: dissertation_data.csv\nEven Better: student_questionnaire_time_1.csv"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#separate-words-with-underscores-not-spaces.",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#separate-words-with-underscores-not-spaces.",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Separate words with underscores, not spaces.",
    "text": "Separate words with underscores, not spaces.\nReplace spaces with underscores (_). Spaces often work fine, but sometimes they do not, which can result in hours and hours of debugging. Play it safe and don’t use spaces in file names.\nstudent scores.xlsx → student_scores.xlsx"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#no-special-characters-in-file-names",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#no-special-characters-in-file-names",
    "title": "Getting Ready to Use R and RStudio",
    "section": "No special characters in file names:",
    "text": "No special characters in file names:\nAvoid including in file names characters that have special meanings in many programming languages such as *@^$! and many others. Otherwise unexpected results can make your life complicated.\nparent@emotion*survey.csv → parent_emotion_survey.csv"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#separate-dates-with-hyphens-using-iso-format",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#separate-dates-with-hyphens-using-iso-format",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Separate dates with hyphens using ISO format",
    "text": "Separate dates with hyphens using ISO format\n\n\n\n\nfrom XKCD\n\n\n\nISO format for dates is YYYY-MM-DD, meaning that a four-digit year comes first, followed by a two-digit month, followed by a two-digit day. This format makes sorting order much easier than the formats used in the U.S.\nHere is a data file name that begins with a date, followed by the school district from from the data were collected:\n2020-01-12_district_A.csv\nOne of the benefits of using dates in the ISO format is that they sort chronologically. Which file names would you prefer to deal with?\n\n\n\n\n\nSorted Traditional Dates\nSorted ISO Dates\n\n\n\n\nAugust 13, 2012_District_O.csv\n2007-03-05_District_B.csv\n\n\nJune 08, 2012_District_L.csv\n2010-03-07_District_F.csv\n\n\nMarch 05, 2007_District_B.csv\n2010-03-07_District_R.csv\n\n\nMarch 07, 2010_District_F.csv\n2012-05-10_District_A.csv\n\n\nMarch 07, 2010_District_R.csv\n2012-06-08_District_L.csv\n\n\nMay 02, 2014_District_B.csv\n2012-08-13_District_O.csv\n\n\nMay 10, 2012_District_A.csv\n2014-05-02_District_B.csv\n\n\nMay 23, 2014_District_G.csv\n2014-05-23_District_G.csv"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Getting Ready to Use R and RStudio\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling\n\n\n\n\n\nGetting Data into Useable Formats\n\n\n\n\n\nMar 1, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA in R\n\n\n\n\n\nPlot group means and test to see if their differences are statistically significant.\n\n\n\n\n\nMar 5, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRegression in R\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Factor Analysis with R\n\n\n\n\n\nA step-by-step introduction to exploratory factor analysis in R\n\n\n\n\n\nMar 12, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\nNo matching items"
  }
]