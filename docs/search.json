[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "W. Joel Schneider, Ph.D.\nProfessor\nTemple University\nPsychological Studies in Education\n\n\n\nCounseling Psychology\n\n\nSchool Psychology\n\n\n\n\n\n Contact Me\n Curriculum Vitae\n ORCID\n Google Scholar\n Open Science Framework\n ResearchGate\n Mastodon\n Zotero\n Stackoverflow\n YouTube\n GitHub\n\n\n\n\n\n\n\nI am a professor at Temple University in the College of Education and Human Development in the Psychological Studies in Education Department. I belong to both the School Psychology and Counseling Psychology programs.\nAlthough I have diverse interests, my primary focus is trying to understand and improve the validity of psychological assessment practices. I teach courses in assessment, counseling, statistics, and research methods.\nI grew up in Southern California (1970–1990) and lived in Córdoba, Argentina (1990–1992). I completed my undergraduate degree in Psychology at the University of California at Berkeley in 1995 and my doctoral studies in clinical psychology at Texas A&M University in 2003. My internship year (2001–2002) was spent in the Dutchess County Department of Behavioral & Community Health in Poughkeepsie, NY. Since then I have been a faculty member at Illinois State University (2002–2017) and Temple University (2017–Present)."
  },
  {
    "objectID": "vita.html#w.-joel-schneider",
    "href": "vita.html#w.-joel-schneider",
    "title": "Curriculum Vitae",
    "section": "W. Joel Schneider",
    "text": "W. Joel Schneider\n\n\nTemple University\nCollege of Education & Human Development\nPsychological Studies in Education\n493 Ritter Hall\nPhiladelphia, PA 19122-6091\n\nOffice: (215) 204-8093\nEmail: schneider@temple.edu\nWeb: Faculty Profile\nAssessment Blog: AssessingPsyche R Blog: AssessingPsyche"
  },
  {
    "objectID": "vita.html#publication-indices",
    "href": "vita.html#publication-indices",
    "title": "Curriculum Vitae",
    "section": "Publication Indices",
    "text": "Publication Indices\n\nGoogle Scholar Citations: 5103\nh-index: 28\ni10-index: 42\n\n\n\n\n\n\nGoogle Scholar Citations of My Work"
  },
  {
    "objectID": "vita.html#books",
    "href": "vita.html#books",
    "title": "Curriculum Vitae",
    "section": "Books",
    "text": "Books\n\nCohen, R. J., Schneider, W. J., & Tobin, R. M. (2025). Psychological testing and assessment: An introduction to tests and measurement (11th ed.). McGraw Hill LLC.\n\n\nSchneider, W. J. (2024). Individual psychometrics: An assessment toolkit with applications in R. Author.\n\n\nCohen, R. J., Schneider, W. J., & Tobin, R. M. (2022). Psychological testing and assessment: An introduction to tests and measurement (10th ed.). McGraw Hill LLC.\n\n\nSchneider, W. J., Mather, N., Lichtenberger, E. O., & Kaufman, N. L. (2018). Essentials of assessment report writing (2nd ed.). Wiley."
  },
  {
    "objectID": "vita.html#journal-articles",
    "href": "vita.html#journal-articles",
    "title": "Curriculum Vitae",
    "section": "Journal Articles",
    "text": "Journal Articles\n\nLevi-Nielsen, S., Tobin, R. M., & Schneider, W. J. (2026). A Systematic Review and Single-Case Meta-Analysis of Performance Feedback as Teacher Professional Development. Journal of School Psychology, 115, 101527. https://doi.org/10.1016/j.jsp.2025.101527\n\n\nHajovsky, D. B., Niileksela, C. R., Flanagan, D. P., Alfonso, V. C., Schneider, W. J., & Robbins, J. (2025). Toward a consensus model of cognitive–reading achievement relations using meta-structural equation modeling. Journal of Intelligence, 13(8), 104. https://doi.org/10.3390/jintelligence13080104\n\n\nKane, C., Sandilos, L., Schneider, W. J., & Tobin, R. M. (2025). The influence of social-emotional learning programs on key outcomes for dual language learners in Head Start. Early Education and Development, 1–24. https://doi.org/10.1080/10409289.2025.2526310\n\n\nSchneider, W. J., Flanagan, D. P., Niileksela, C. R., & Engler, J. R. (2024). The effect of measurement error on the positive predictive value of PSW methods for SLD identification: How buffer zones dispel the illusion of inaccuracy. Journal of School Psychology, 103, 101280. https://doi.org/10.1016/j.jsp.2023.101280\n\n\nMcGrew, K. S., Schneider, W. J., Decker, S. L., & Bulut, O. (2023). A psychometric network analysis of CHC intelligence measures: Implications for research, theory, and interpretation of broad CHC scores “beyond g”. Journal of Intelligence, 11(1), 19. https://doi.org/10.3390/jintelligence11010019 \n\n\nSchneider, W. J., & Ji, F. (2023). Detecting unusual score patterns in the context of relevant predictors. Journal of Pediatric Neuropsychology, 9, 1–17. https://doi.org/10.1007/s40817-022-00137-x \n\n\nDowdy, A., Peltier, C., Tincani, M., Schneider, W. J., Hantula, D. A., & Travers, J. C. (2021). Meta‐analyses and effect sizes in applied behavior analysis: A review and discussion. Journal of Applied Behavior Analysis, 54(4), 1317–1340. https://doi.org/10.1002/jaba.862 \n\n\nDowdy, A., Tincani, M., & Schneider, W. J. (2020). Evaluation of publication bias in response interruption and redirection: A meta‐analysis. Journal of Applied Behavior Analysis, 53(4), 2151–2171. https://doi.org/10.1002/jaba.724 \n\n\nHajovsky, D. B., Villeneuve, E. F., Schneider, W. J., & Caemmerer, J. M. (2020). An alternative approach to cognitive and achievement relations research: An introduction to quantile regression. Journal of Pediatric Neuropsychology, 6, 83–95. https://doi.org/10.1007/s40817-020-00086-3 \n\n\nDombrowski, S. C., Beaujean, A. A., McGill, R. J., Benson, N. F., & Schneider, W. J. (2019). Using exploratory bifactor analysis to understand the latent structure of multidimensional psychological measures: An example featuring the WISC-V. Structural Equation Modeling: A Multidisciplinary Journal, 26(6), 847–860. https://doi.org/10.1080/10705511.2019.1622421 \n\n\nSchneider, W. J., & McGrew, K. S. (2019). Process Overlap Theory is a milestone achievement among intelligence theories. Journal of Applied Research in Memory and Cognition, 8(3), 273–276. https://doi.org/https://doi.org/10.1016/j.jarmac.2019.06.006 \n\n\nSchneider, W. J., & Roman, Z. (2018). Fine-tuning Cross-Battery Assessment procedures: After follow-up testing, use all valid scores, cohesive or not. Journal of Psychoeducational Assessment, 36(1), 34–54. https://doi.org/10.1177/0734282917722861 \n\n\nMagoon, M. A., Critchfield, T. S., Merrill, D., Newland, M. C., & Schneider, W. J. (2017). Are positive and negative reinforcement “different”? Insights from a free-operant differential outcomes effect. Journal of the Experimental Analysis of Behavior, 107(1), 39–64. https://doi.org/10.1002/jeab.243 \n\n\nSchneider, W. J., & Kaufman, A. S. (2017). Let’s not do away with comprehensive cognitive assessments just yet. Archives of Clinical Neuropsychology, 32(1), 8–20. https://doi.org/10.1093/arclin/acw104 \n\n\nFlanagan, D. P., & Schneider, W. J. (2016). Cross-Battery Assessment? XBA PSW? A case of mistaken identity: A commentary on Kranzler and colleagues’ “Classification agreement analysis of Cross-Battery Assessment in the identification of specific learning disorders in children and youth”. International Journal of School & Educational Psychology, 4(3), 137–145. https://doi.org/10.1080/21683603.2016.1192852 \n\n\nGadke, D. L., Tobin, R. M., & Schneider, W. J. (2016). Agreeableness, conflict resolution tactics, and school behavior in second graders. Journal of Individual Differences, 37, 145–151. https://doi.org/10.1027/1614-0001/a000199 \n\n\nSchneider, W. J., & Kaufman, A. S. (2016). Commentary on current practices and future directions for the assessment of child and adolescent intelligence in schools around the world. International Journal of School & Educational Psychology, 4(4), 283–288. https://doi.org/10.1080/21683603.2016.1206383 \n\n\nSchneider, W. J., Mayer, J. D., & Newman, D. A. (2016). Integrating hot and cool intelligences: Thinking broadly about broad abilities. Journal of Intelligence, 4(1), 1:1–25. https://doi.org/10.3390/jintelligence4010001 \n\n\nSchneider, W. J., & Newman, D. A. (2015). Intelligence is multidimensional: Theoretical review and implications of specific cognitive abilities. Human Resource Management Review, 25(1), 12–27. https://doi.org/10.1016/j.hrmr.2014.09.004 \n\n\nAbney, D. H., Wagman, J. B., & Schneider, W. J. (2014). Changing grasp position on a wielded object provides self-training for the perception of length. Attention, Perception, & Psychophysics, 76(1), 247–254. https://doi.org/10.3758/s13414-013-0550-x \n\n\nPornprasertmanit, S., & Schneider, W. J. (2014). Accuracy in parameter estimation in cluster randomized designs. Psychological Methods, 19(3), 356–379. https://doi.org/10.1037/a0037036 \n\n\nHerbstrith, J. C., Tobin, R. M., Hesson-McInnis, M. S., & Schneider, W. J. (2013). Preservice teacher attitudes toward gay and lesbian parents. School Psychology Quarterly, 28(3), 183–194. https://doi.org/10.1037/spq0000022 \n\n\nKahn, J. H., & Schneider, W. J. (2013). It’s the destination and it’s the journey: Using multilevel modeling to assess patterns of change in psychotherapy. Journal of Clinical Psychology, 69(6), 543–570. https://doi.org/10.1002/jclp.21964 \n\n\nSchneider, W. J. (2013). What if we took our models seriously? Estimating latent scores in individuals. Journal of Psychoeducational Assessment, 31(2), 186–201. https://doi.org/10.1177/0734282913478046 \n\n\nDecker, S. L., Schneider, W. J., & Hale, J. B. (2012). Estimating base rates of impairment in neuropsychological test batteries: A comparison of quantitative models. Archives of Clinical Neuropsychology, 7(1), 69–84. https://doi.org/10.1093/arclin/acr088 \n\n\nJones, G., & Schneider, W. J. (2010). IQ in the production function: Evidence from immigrant earnings. Economic Inquiry, 48(3), 743–755. https://doi.org/10.1111/j.1465-7295.2008.00206.x \n\n\nGuidry, J. A., Babin, B. J., Graziano, W. G., & Schneider, W. J. (2009). Pride and prejudice in the evaluation of wine? International Journal of Wine Business Research, 21(4), 298–311. https://doi.org/10.1108/17511060911004888 \n\n\nHoff, K. E., Reese-Weber, M., Schneider, W. J., & Stagg, J. W. (2009). The association between high status positions and aggressive behavior in early adolescence. Journal of School Psychology, 47(6), 395–426. https://doi.org/10.1016/j.jsp.2009.07.003 \n\n\nBarr, L. K., Kahn, J. H., & Schneider, W. J. (2008). Individual differences in emotion expression: Hierarchical structure and relations with psychological distress. Journal of Social and Clinical Psychology, 27(10), 1045–1077. https://doi.org/10.1521/jscp.2008.27.10.1045 \n\n\nKahn, J. H., Vogel, D. L., Schneider, W. J., Barr, L. K., & Herrell, K. (2008). The emotional content of client disclosures and session impact: An analogue study. Psychotherapy: Theory, Research, Practice, Training, 45(4), 539–545. https://doi.org/10.1037/a0014337 \n\n\nSchneider, W. J. (2008). Playing statistical Ouija board with commonality analysis: good questions, wrong assumptions. Applied Neuropsychology, 15(1), 44–53. https://doi.org/10.1080/09084280801917566 \n\n\nJones, G., & Schneider, W. J. (2006). Intelligence, human capital, and economic growth: A Bayesian Averaging of Classical Estimates (BACE) approach. Journal of Economic Growth, 11(1), 71–93. https://doi.org/10.2139/ssrn.552481 \n\n\nSchneider, W. J., Timothy Cavell, A., & Hughes, J. N. (2003). A sense of containment: Potential moderator of the relation between parenting practices and children’s externalizing behaviors. Development and Psychopathology, 15(1), 95–117. https://doi.org/10.1017/S0954579403000063"
  },
  {
    "objectID": "vita.html#chapters",
    "href": "vita.html#chapters",
    "title": "Curriculum Vitae",
    "section": "Chapters",
    "text": "Chapters\n\nSchneider, W. J., & Tobin, R. M. (2024). Sophisticated simplicity: Writing reader-friendly assessment reports. In R. Flanagan (Ed.) Clinical guide to effective psychological assessment and report writing (pp. 1–11). Springer International Publishing. https://doi.org/10.1007/978-3-031-67184-5_1\n\n\nSchneider, W. J. (2022). Statistical and clinical interpretation guidelines for school neuropsychological assessment. In D. Miller, D. Maricle, C. Bedford, & J. Gettman (Eds.) Best Practices in School Neuropsychology: Guidelines for Effective Practice, Assessment, and Evidence‐Based Intervention (1st ed., pp. 163–184). Wiley. https://doi.org/10.1002/9781119790563\n\n\nFloyd, R. G., Farmer, R. L., Schneider, W. J., & McGrew, K. S. (2021). Theories and measurement of intelligence. In L. Glidden, L. Abbeduto, L. L. McIntyre, & M. J. Tassé (Eds.) APA handbook of intellectual and developmental disabilities (Vol. 1, pp. 386–424). American Psychological Association. https://doi.org/10.1037/0000194-015 \n\n\nKaufman, A. S., Schneider, W. J., & Kaufman, J. C. (2019). Psychometric approaches to intelligence. In R. J. Sternberg (Ed.) Human intelligence: An introduction (pp. 67–103). Cambridge University Press. \n\n\nSchneider, W. J., & McGrew, K. S. (2018). The Cattell-Horn-Carroll theory of intelligence. In D. P. Flanagan, & E. M. McDonough (Eds.) Contemporary intellectual assessment: Theories, tests, and issues (4th ed., pp. 73–130). Guilford Press. \n\n\nSchneider, W. J. (2016). Case 1—Liam, age 9: Emotionally intelligent testing with the WISC-V and CHC theory. In A. S. Kaufman, S. Raiford, & D. Coalson (Eds.) Intelligent testing with the WISC-V (pp. 265–282). Wiley.\n\n\nSchneider, W. J. (2016). Strengths and weaknesses of the Woodcock-Johnson IV Tests of Cognitive Abilities: Best practice from a scientist-practitioner perspective. In D. P. Flanagan, & V. C. Alfonso (Eds.) WJ IV Clinical Use and Interpretation (pp. 191–210). Academic Press. https://doi.org/10.1016/B978-0-12-802076-0.00007-4 \n\n\nSchneider, W. J., & Flanagan, D. P. (2015). The relationship between theories of intelligence and intelligence tests. In S. Goldstein, D. Princiotta, & J. A. Naglieri (Eds.) Handbook of intelligence: Evolutionary theory, historical perspective, and current concepts (pp. 317–340). Springer. \n\n\nTobin, R. M., Schneider, W. J., & Landau, S. (2014). Best practices in the assessment of youth with attention deficit hyperactivity disorder within a multitiered services framework. In A. Thomas, & P. Harrison (Eds.) Best practices in school psychology: Data-based and collaborative decision making (pp. 391–404). National Association of School Psychologists. \n\n\nSchneider, W. J. (2013). Principles of assessment of aptitude and achievement. In D. Saklofske, C. R. Reynolds, & V. Schwean (Eds.) The Oxford Handbook of Child Psychological Assessment (pp. 286–330). Oxford University Press. \n\n\nSchneider, W. J., & McGrew, K. S. (2013). Cognitive performance models: Individual differences in the ability to process information. In B. Irby, G. Brown, R. Laro-Alecio, & S. Jackson (Eds.) Handbook of educational theories (pp. 767–782). Information Age Publishing. \n\n\nSchneider, W. J., & McGrew, K. S. (2012). The Cattell-Horn-Carroll model of intelligence. In D. P. Flanagan, & P. L. Harrison (Eds.) Contemporary intellectual assessment: Theories, tests, and issues (3rd ed., pp. 99–144). Guilford Press. \n\n\nTobin, R. M., Schneider, W. J., Reck, S. G., & Landau, S. (2008). Best practices in the assessment of children with attention deficit hyperactivity disorder: Linking assessment to response to intervention. In P. Harrison, & A. Thomas (Eds.) Best Practices in School Psychology V (Vol. 2, pp. 617–631). National Association of School Psychologists. \n\n\nSnyder, D. K., Schneider, W. J., & Castellani, A. M. (2003). Tailoring couple therapy to individual differences: A conceptual approach. In D. K. Snyder (Ed.) Treating difficult couples: Helping clients with coexisting mental and relationship disorders (pp. 27–51). Guilford Press. \n\n\nSnyder, D. K., & Schneider, W. J. (2002). Affective reconstruction: A pluralistic, developmental approach. In N. S. Jacobson (Ed.) Clinical handbook of couple therapy (3rd ed., pp. 151–179). Guilford Press."
  },
  {
    "objectID": "vita.html#test-reviews",
    "href": "vita.html#test-reviews",
    "title": "Curriculum Vitae",
    "section": "Test Reviews",
    "text": "Test Reviews\n\nSwerdlik, M. E., & Schneider, W. J. (2010). Review of the PsychProfiler. In R. A. Spies, J. F. Carlson, B. S. Plake, & K. F. Geisinger (Eds.) The eighteenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J. (2007). Review of the Dean-Woodcock Neuropsychological Battery. In K. F. Geisinger, R. A. Spies, J. F. Carlson, & B. S. Plake (Eds.) The seventeenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J. (2007). Review of the Multiple Intelligence Developmental Assessment Scales (MIDAS). In K. F. Geisinger, R. A. Spies, J. F. Carlson, & B. S. Plake (Eds.) The seventeenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J., & Swerdlik, M. E. (2007). Review of the Youth Outcome Questionnaire 30.1. In K. F. Geisinger, R. A. Spies, J. F. Carlson, & B. S. Plake (Eds.) The seventeenth mental measurements yearbook. Buros Institute.\n\n\nSwerdlik, M. E., & Schneider, W. J. (2007). Review of the Behavior Evaluation Scale, Third Edition. In K. F. Geisinger, R. A. Spies, J. F. Carlson, & B. S. Plake (Eds.) The seventeenth mental measurements yearbook. Buros Institute.\n\n\nHoff, K. E., & Schneider, W. J. (2005). Review of the Child Symptom Inventory-4. In B. S. Plake, J. C. Impara, & R. A. Spies (Eds.) The sixteenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J., & Hoff, K. E. (2005). Review of the Word Identification and Spelling Test. In B. S. Plake, J. C. Impara, & R. A. Spies (Eds.) The sixteenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J., & Swerdlik, M. E. (2005). Review of the Memory Test for Older Adults. In B. S. Plake, J. C. Impara, & R. A. Spies (Eds.) The sixteenth mental measurements yearbook. Buros Institute.\n\n\nSwerdlik, M. E., & Schneider, W. J. (2005). Review of the Behavioral and Emotional Rating Scale-Second Edition. In B. S. Plake, J. C. Impara, & R. A. Spies (Eds.) The sixteenth mental measurements yearbook. Buros Institute."
  },
  {
    "objectID": "vita.html#scholarly-reports",
    "href": "vita.html#scholarly-reports",
    "title": "Curriculum Vitae",
    "section": "Scholarly Reports",
    "text": "Scholarly Reports\n\nBlume, J. H., & Paavola, E. C. (2025). Brief of Amici Curiae Randy W. Kamphaus, Kevin S. McGrew, Cecil R. Reynolds, W. Joel Schneider, and Marc J. Tasse, Blaine Keith Milam v. Texas. .\n\n\nGoldrick-Rab, S., Richardson, J., Schneider, W. J., Hernandez, A., & Cady, C. (2018). Still hungry and homeless in college. The Wisconsin HOPE Lab. \n\n\nSchneider, W. J. (2016). The RESCA-E subtests are thoughtfully designed and highly refined measures of CHC constructs: A review of the Receptive, Expressive & Social Communication Assessment–Elementary. Assessing Psyche, Engaging Gauss, Seeking Sophia.\n\n\nSchneider, W. J. (2016). Why are WJ IV cluster scores more extreme than the average of their parts? A gentle explanation of the composite score extremity effect. Houghton Mifflin HarcourtWoodcock-Johnson IV Assessment Service Bulletin.\n\n\nSchneider, W. J., & McGrew, K. S. (2011). “Just say no” to averaging IQ subtest scores. IAP Applied Psychometrics 101 Report."
  },
  {
    "objectID": "vita.html#software",
    "href": "vita.html#software",
    "title": "Curriculum Vitae",
    "section": "Software",
    "text": "Software\n\nSchneider, W. J. (2025). apa7: Facilitate Writing Documents in American Psychological Association Style, Seventh Edition. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.apa7\n\n\nSchneider, W. J. (2025). ggdiagram: Object-oriented diagram plots with ggplot2. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.ggdiagram\n\n\nSchneider, W. J. (2024). condppv: Conditional positive predictive value in the accuracy of specific learning disability identification. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/conditionalppv\n\n\nSchneider, W. J. (2023). apaquarto: A Quarto extension for creating APA 7 style documents. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/apaquarto\n\n\nSchneider, W. J. (2023). arrowheadr: Create custom arrowheads for ggplot2 via ggarrow.. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.arrowheadr\n\n\nSchneider, W. J. (2022). UnusualProfile: A user-friendly web application to impliment functions from the unusualprofile package.. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/unusualprofile_app\n\n\nSchneider, W. J. (2021). Area under the normal curve. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/AreaUnderNormalCurve\n\n\nSchneider, W. J. (2021). psycheval: A psychological evaluation toolkit. [Software] AssessingPsyche.\n\n\nSchneider, W. J. (2021). Simple regression with standard scores. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/simple_regression\n\n\nSchneider, W. J. (2021). ztestvis: A Jamovi module for conducting a one-sample z-test. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/ztest\n\n\nSchneider, W. J., & Ji, F. (2021). unusualprofile: Calculate conditional Mahalanobis distances. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.unusualprofile\n\n\nSchneider, W. J. (2018). ggnormalviolin. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.ggnormalviolin\n\n\nSchneider, W. J. (2018). simstandard. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.simstandard\n\n\nSchneider, W. J. (2012). TableMaker. [Software] AssessingPsyche.\n\n\nSchneider, W. J. (2010). The Compositator 1.0.. [Software] WMF Press."
  },
  {
    "objectID": "vita.html#lectures",
    "href": "vita.html#lectures",
    "title": "Curriculum Vitae",
    "section": "Lectures",
    "text": "Lectures\n\nSchneider, W. J. (2025, November 14). X-BASS 3.0: A Milestone Achievement in Test Interpretation. [Webinar]. X-BASS Institute 2025 Virtual Conference , Comprehensive Assessment for Intervention. https://caipsychs.com/conferences/\n\n\nSchneider, W. J. (2025, October 28). Making survey tests psychometrically sound. [Lecture]. Leadership Education in Neurodevelopmental Disabilities Program , Children’s Hospital of Philadelphia.\n\n\nSchneider, W. J., Tassé, M. J., & Blume, J. H. (2025, April 26). Intellectual disability identification: common problems on prong I. [Panel]. Twenty-Second National Seminar on the Development and Presentation of Mitigating Evidence in Capital Cases , Columbus, OH. https://advancechange.org/events/national-seminar-on-the-development-and-integration-of-mitigation-evidence-in-capital-cases-04-24-2025/\n\n\nTassé, M. J., Schneider, W. J., & Olley, G. (2025, April 24). The APA’s professional guidelines for forensic assessments of ID. [Panel]. Twenty-Second National Seminar on the Development and Presentation of Mitigating Evidence in Capital Cases , Columbus, OH. https://advancechange.org/events/national-seminar-on-the-development-and-integration-of-mitigation-evidence-in-capital-cases-04-24-2025/\n\n\nSchneider, W. J. (2025, February 3). Making survey tests psychometrically sound. [Lecture]. Leadership Education in Neurodevelopmental Disabilities Program , Children’s Hospital of Philadelphia.\n\n\nSchneider, W. J. (2025, January 24). Advanced Interpretation Techniques with the Woodcock-Johnson V. [Webinar]. Comprehensive Assessment for Intervention . https://caipsychs.com/conferences/\n\n\nSchneider, W. J. (2024, October 18). Individual psychometrics. [Webinar]. Illinois School Psychologists Association . https://www.ilispa.org/fall-conference\n\n\nSchneider, W. J. (2024, March 15). Evidence-based assessment and clinical decision-making in school psychology. [Webinar]. 2024 Illinois School Psychologists Association Annual Convention . https://www.ilispa.org/annual-convention\n\n\nSchneider, W. J. (2024, February 26). Making survey tests psychometrically sound. [Lecture]. Leadership Education in Neurodevelopmental Disabilities Program , Children’s Hospital of Philadelphia.\n\n\nEngler, J. R., Schneider, W. J., Flanagan, D. P., & Niileksela, C. R. (2024, February 16). Improve PSW case conceptualization accuracy by taking measurement error seriously. [Mini-Skills Workshop]. National Association of School Psychologists Annual Convention , New Orleans, LA.\n\n\nSchneider, W. J. (2024, January 25). Confidently correct specific learning disability identification. [Webinar]. Comprehensive Assessment for Intervention . https://caipsychs.com/conferences/\n\n\nSchneider, W. J. (2023, November 4). Using arrowheadr with ggarrow and ggplot2. [Webinar]. ggplot2 Extenders .\n\n\nSchneider, W. J. (2023, September 27). Intelligence, critical thinking, and creativity. [Symposium]. In G. Alves (Moderator), Para Além dos Testes Tradicionais: Integrando Inovações à Avaliação de Criatividade e Pensamento Crítico (Beyond Traditional Tests: Integrating Innovations into the Assessment of Creativity and Critical Thinking) , Instituto Ayrton Senna, São Paulo, Brazil. https://institutoayrtonsenna.org.br/nossos-materiais/noticias/catedra-instituto-ayrton-senna-no-iea-usp-promove-live-integrando-inovacao-avaliacao-criatividade-pensamento-critico/\n\n\nSchneider, W. J. (2023, July 29). How recent theories of intelligence can improve the practice of cognitive ability assessment. [Panel]. International Society for Intelligence Research 2023 Annual Conference , Berkelety, CA. https://isironline.org/2023/01/isir-2023-berkeley-california-july-26-29/\n\n\nSchneider, W. J. (2023, March 24). Assessments that restore hope, promote understanding and inspire change. [Invited Lecture]. 42nd Annual School Psychology, Counseling Psychology and Applied Behavior Analysis Conference , Philadelphia, PA. https://education.temple.edu/node/50031\n\n\nSchneider, W. J. (2022, November 15). Using imaginary data to conserve real resources: Study design planning with model-based simulations. [Lecture]. Pearson 4th Developers Conversation .\n\n\nSchneider, W. J. (2022, July 27). What is the current state of affairs of IQ testing? [Keynote Address]. 2022 Knowledge, Innovation & Enterprise (KIE) Conference: Unpacking Creativity: Culture, Innovation, and Motivation in Global Contexts .\n\n\nFlanagan, D. P., Koponen, T., Wagner, R. K., Colvin, M. K., & Schneider, W. J. D. (2022, April 11). How do we diagnose learning disabilities? [Virtual Summit]. Learning Disabilities Summit: A Three-Part Virtual Event Focused on Crticial Learning Disability Issues , Learning Disabilities Association of America. https://ldaamerica.org/lda-to-hold-learning-disabilities-summit/\n\n\nSchneider, W. J. (2022, March 29). How measurement error affects learning disability identification accuracy. [Lecture]. Pearson Scientific Advisory Council .\n\n\nSchneider, W. J. (2022, March 28). Practical psychometrics: Robust interpretation for individuals. [Lecture]. Temple University School Psychology Owl Hour , Philadelphia, PA.\n\n\nSchneider, W. J. (2022, January 27). Life-and-death psychometrics: IQ and the death penalty. [Webinar]. Owl Hour , Temple University. https://events.temple.edu/owl-hour-life-and-death-psychometrics-iq-and-the-death-penalty\n\n\nHajovsky, D. B., & Schneider, W. J. (2021, September 24). Cognitive assessment in the evaluation of specific learning disabilities. [Webinar]. Missouri Association of School Psychologists 2021 Fall Conference , Weldon Spring, MO. https://maosp.wildapricot.org/resources/Documents/Fall%20Conference.pdf\n\n\nSchneider, W. J. (2021, July 29). Advances in test interpretation with the Cattell-Horn-Carroll Theory of Cognitive Abilities. [Webinar]. Training for Region 19 Evaluation Staff , Education Service Center, Region 19, El Paso, TX. https://wjschne.github.io/media/CHCTheoryWebinar.pdf\n\n\nSchneider, W. J. (2021, June 14). Using CHC theory to better understand student needs. [Webinar]. Hill Country Summer Institute . https://esc13.net/events/hill-country-summer-institute-2021\n\n\nSchneider, W. J. (2021, May 2). The accuracy of PSW methods. In J. R. Engler (Chair), PSW for SLD identification: Does cognition matter? [Symposium]. National Association of School Psychologists Annual Convention , Salt Lake City, UT. https://apps.nasponline.org/professional-development/convention/session-detail.aspx?id=20263\n\n\nSchneider, W. J. (2021, January 22). Improving LD identification with fewer myths and more science. [Invited Lecture]. Science to Practice Virtual Conference , Learning Disability Association of America.\n\n\nSchneider, W. J. (2020, July 29). Curious about CHC Theory? Take your evaluations to the next level! [Webinar]. Riverside Insights , Rolling Meadows, IL. https://info.riversideinsights.com/chc-theory-webinar\n\n\nSchneider, W. J. (2019, March 4). The evolution of PSW methods of SLD identification: What I have learned from PSW proponents. [Keynote Address]. Annual School Neuropsychology Conference , Long Beach, CA.\n\n\nFlanagan, D. P., Schneider, W. J., & Alfonso, V. C. (2019, February 26). PSW methods: Comparisons, research, and how to use them responsibly. [Symposium]. National Association of School Psychologists Annual Convention , Atlanta, GA.\n\n\nSchneider, W. J. (2019, February 26). Consumer-oriented and legally defensible psychoeducational reports–Advanced workshop. [Invited Workshop]. National Association of School Psychologists Conference , Atlanta, GA.\n\n\nSchneider, W. J. (2019, February 26). Consumer-oriented and legally defensible psychoeducational reports–Introductory workshop. [Invited Workshop]. National Association of School Psychologists Conference , Atlanta, GA.\n\n\nSchneider, W. J. (2019, January 4). The evolution of PSW methods of SLD identification: What I have learned from PSW critics. [Keynote Address]. Annual School Neuropsychology Conference , Long Beach, CA.\n\n\nSchneider, W. J. (2018, February 16). Re-evaluating the accuracy of the Dual Discrepancy/Consistency Model for SLD Identification. [Lecture]. National Association of School Psychologists Annual Convention , Chicago, IL.\n\n\nMcGrew, K. S., & Schneider, W. J. (2018, February 15). Revisions to the Cattell-Horn-Carroll (CHC) Theory of Intelligence. [Lecture]. National Association of School Psychologists Annual Convention , Chicago, IL.\n\n\nSchneider, W. J. (2016, December 4). How to write psychoeducational reports that people will want to read. [Webinar]. Region 10 Education Service Center , Richardson, TX.\n\n\nCormier, D. C., Bulut, O., Niileksela, C. R., Funamoto, A., & Schneider, W. J. D. (2016, December 2). Revisiting the relationship between CHC abilities and academic achievement. [Symposium]. National Association of School Psychologists Annual Convention , New Orleans, LA.\n\n\nSchneider, W. J. (2016, October 24). How to write psychoeducational reports that people will want to read all the way through. [Webinar]. KIPP Austin Public Schools and Texas State University School Psychology Program .\n\n\nMcGrew, K. S., & Schneider, W. J. (2016, October 19). Porque as habilidades cognitivas importam na educação [Why cognitive abilities matter in education]. [Invited Lecture]. Instituto Ayrton Senna , São Paulo, Brazil.\n\n\nSchneider, W. J., & McGrew, K. S. (2016, October 18). CHC theory, complex problem solving, critical thinking, and creativity. [Invited Lecture]. Instituto Ayrton Senna , São Paulo, Brazil.\n\n\nMcGrew, K. S., & Schneider, W. J. (2016, October 17). CHC theory and education in Brazil. [Invited Lecture]. Instituto Ayrton Senna , São Paulo, Brazil.\n\n\nSchneider, W. J. (2016, July 29). How to write psychoeducational reports that people will want to read all the way through. [Webinar]. New Brunswick Department of Education and Early Childhood Development , Fredericton, NB.\n\n\nSchneider, W. J. (2016, April 8). Writing reports worth reading all the way through. In R. Flanagan (Chair), Sophisticated simplicity: The art of writing reader-friendly assessment reports [Symposium]. Annual American Psychological Association Convention , Denver, CO.\n\n\nSchneider, W. J. (2015, July 15). I didn’t know the WJ IV could do that! Answering practical assessment questions you didn’t know you could ask. [Invited Lecture]. School Neuropsychology Summer Institute , Grapevine, TX.\n\n\nSchneider, W. J. (2014, February 15). What if we took our models seriously? Estimating latent scores in individuals. [Lecture]. National Association of School Psychologists Convention , Washington, D.C..\n\n\nSchneider, W. J. (2013, July 15). Advances in CHC Theory and its application to individuals. [Invited Lecture]. School Neuropsychology Summer Institute , Grapevine, TX.\n\n\nSchneider, W. J. (2013, February 13). Holes in CHC Theory. [Lecture]. National Association of School Psychologists Conference , Seattle, WA.\n\n\nSchneider, W. J., & Taylor, S. J. (2008, January 10). Can the Implicit Association Test be used as a lie detector? Morality, implicit attitudes, and Big Brother’s help measuring illegal file-sharing behavior. [Invited Lecture]. Center for Study of Public Choice at George Mason University , Fairfax, VA.\n\n\nSchneider, W. J. (2006, November 15). We need to change (but I won’t do anything because I’m not the problem): Couples therapy outcomes and the transtheoretical model of change. [Invited Lecture]. Colloquium at Purdue Unversity , West Lafayette, IN.\n\n\nSchneider, W. J., & Huber, B. (2004, March 15). The interactive effects of fluid intelligence and executive functions on behavior disorders in children. [Lecture]. Annual Meeting of the Illinois School Psychology Association , Springfield, IL."
  },
  {
    "objectID": "vita.html#posters-and-papers",
    "href": "vita.html#posters-and-papers",
    "title": "Curriculum Vitae",
    "section": "Posters and Papers",
    "text": "Posters and Papers\n\nAn, C., Schneider, W. J., Campagnolio, A., & Fiorello, C. A. (2025, February 20). Rigorous procedures for measuring broad and narrow abilities in individuals [Poster]. National Association of School Psychologists Convention, Seattle, WA.\n\n\nCampagnolio, A., Schneider, W. J., & An, C. (2025, February 20). Non-linear relations among reading abilities affect score interpretations [Poster]. National Association of School Psychologists Convention, Seattle, WA.\n\n\nHajovsky, D. B., Niileksela, C. R., Flanagan, D. P., Alfonso, V. C., Schneider, W. J., & Zinkiewicz, C. J. (2022, April 8). Toward a consensus model of cognitive-achievement relations using meta-SEM [Poster]. American Psychological Association Convention, Minneapolis, MN.\n\n\nSchneider, W. J., Flanagan, D. P., Niileksela, C. R., & Engler, J. R. (2020, February 20). Reevaluating the accuracy of PSW methods of SLD identification [Poster]. National Association of School Psychologists Convention, Baltimore, MD.\n\n\nSchneider, W. J., & Fiorello, C. A. (2019, August 8). Broad abilities are not doomed to be measured unreliably: A Monte Carlo study of omega statistics [Poster]. American Psychological Association Convention, Chicago, IL. https://github.com/wjschne/APA2019\n\n\nMulderink, T. D., Tobin, R. M., & Schneider, W. J. (2018, October 8). Personality, interactive history, and situational factors during children’s games [Poster]. Annual American Psychological Association Convention, San Francisco, CA.\n\n\nNicholls, C. J., Ross, E., & Schneider, W. J. (2016, April 15). The neuropsychological profiles of twins discordant for Sotos Syndrome: A case study [NA]. American Academy of Pediatric Neuropsychology Annual Conference, Las Vegas, NV.\n\n\nHowe, A. R., Curnock, A. D., Koppenhoefer, S. E., Schneider, W. J., & Tobin, R. M. (2016, April 8). Do vocabulary skills and instructor influence social-emotional learning? [Poster]. Annual American Psychological Association Convention, Denver, CO.\n\n\nCurnock, A. D., Pajor, K. E., Tobin, R. M., & Schneider, W. J. (2016, February 12). Preschoolers’ engagement during Second Step and their social-emotional knowledge [Poster]. National Association of School Psychologists Annual Convention, New Orleans, LA.\n\n\nEngelland, J. L., Tobin, R. M., Meyers, A., Huber, B., Schneider, W. J., Austen, J. H., & Corbin, A. L. (2014, December 8). Longitudinal effects of school climate on middle school students’ development [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nAffrunti, C. L., Schneider, W. J., Tobin, R. M., & Collins, K. D. (2014, August 7). Predictors of academic outcomes in college students suspected of having learning disorders [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nSondalle, A. A., Tobin, R. M., & Schneider, W. J. (2014, August 7). Behavioral engagement, Second Step edition, and children’s social-emotional functioning [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nSondalle, A. A., Probst, K. G., Carreno, C., Tobin, R. M., Schneider, W. J., Moore, N. A., & Mulderink, T. D. (2014, February 15). Engagement during Second Step and kindergarteners’ social-emotional and academic outcomes [Poster]. National Association of School Psychologists Convention, Washington, D.C..\n\n\nSondalle, A. A., Probst, K. G., Tobin, R. M., Schneider, W. J., & Kestian, J. M. (2014, February 15). Second Step and teacher ratings of children’s social-emotional functioning [Poster]. National Association of School Psychologists Conference, Washington, D.C..\n\n\nTobin, R. M., Schneider, W. J., Moore, N. A., Sondalle, A. A., & Willis, M. (2013, August 15). Effortful control and knowledge gains after the Second Step intervention [Poster]. Annual American Psychological Association Convention, Honolulu, HI.\n\n\nMoore, N. A., Sondalle, A. A., Mulderink, T. D., Tobin, R. M., Schneider, W. J., Willis, M., & Probst, K. G. (2013, February 15). Effortful control, Second Step, and behavior in kindergartners [Poster]. National Association of School Psychologists Convention, Seattle, WA.\n\n\nSondalle, A. A., Mulderink, T. D., Moore, N. A., Tobin, R. M., & Schneider, W. J. (2012, August 15). Engagement, dosage, and effectiveness of the kindergarten Second Step curriculum [Poster]. Annual American Psychological Association Convention, Orlando FL.\n\n\nMulderink, T. D., Sondalle, A. A., Moore, N. A., Tobin, R. M., Schneider, W. J., & Sheese, B. E. (2012, February 15). Influences of executive functioning and Second Step on behavior [Poster]. National Association of School Psychologists Conference, Philadelphia, PA.\n\n\nMulderink, T. D., Moore, N. A., Sondalle, A. A., Tobin, R. M., & Schneider, W. J. (2011, August 15). Executive functioning and responsiveness to Second Step [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nTobin, R. M., Mackin, J. M., Babcock, E. A., Mulderink, T. D., Moore, N. A., Carlson, L. A., Gioia, K. A., & Schneider, W. J. (2011, February 15). Intervention frequency and behavior in response to Second Step [Poster]. National Association of School Psychologists Convention, San Francisco, CA.\n\n\nTobin, R. M., Gioia, K. A., Mulderink, T. D., Carlson, L. A., Moore, N. A., & Schneider, W. J. (2010, August 15). Personality, Second Step dosage, and kindergartners’ social skills improvements [Poster]. Annual American Psychological Association Convention, San Diego CA.\n\n\nGioia, K. A., Tobin, R. M., & Schneider, W. J. (2010, January 15). Children’s knowledge gains in response to a social skills intervention [Poster]. Annual Illinois School Psychologist Association Convention, East Peoria, IL.\n\n\nMoore, N. A., Gioia, K. A., Tobin, R. M., & Schneider, W. J. (2010, January 3). Effortful control and emotion regulation in kindergartners [Poster]. National Association of School Psychologists Convention, Chicago, IL.\n\n\nObilade, M. H., Gioia, K. A., Tobin, R. M., & Schneider, W. J. (2010, January 3). Verbal ability and responsiveness to the Second Step curriculum [Poster]. National Association of School Psychologists Convention, Chicago, IL.\n\n\nSchneider, W. J., & Tobin, R. M. (2010, January 3). Previously impossible feats of interpretation and explanation with cognitive and achievement data [Poster]. National Association of School Psychologists Conference, Chicago, IL.\n\n\nTaylor, S. A., & Schneider, W. J. (2009, October 15). Implicit attitudes in folk explanations of digital piracy [Paper]. Frontiers in Service Conference, Honolulu, HI.\n\n\nBooth, B., Lederer, S., Dick, S., Linnell, J., Meyers, A., Schneider, W. J., Swerdlik, M. E., & Anweiler, J. (2009, August 15). Reintegration experiences of returning National Guard war veterans and implications for reintegration programming: Results from two states [Poster]. Annual American Psychological Association Convention, Toronto, ON.\n\n\nGadke, D. L., Tobin, R. M., & Schneider, W. J. (2009, August 15). Agreeableness and prejudice towards overweight men and women [Poster]. Annual American Psychological Association Convention, Toronto, ON.\n\n\nGioia, K. A., Tobin, R. M., & Schneider, W. J. (2009, August 15). Individual differences in children’s responsiveness to a social skills intervention [Poster]. Annual American Psychological Association Convention, Toronto, ON.\n\n\nGioia, K. A., Miller, K. A., Tobin, R. M., & Schneider, W. J. (2009, February 15). Children’s responsiveness to a social skills intervention [Poster]. National Association of School Psychologists Convention, Boston, MA.\n\n\nBaird, S. A., Schneider, W. J., & Tobin, R. M. (2008, August 15). Implicit agreeableness predicts laboratory measures of aggression [Poster]. Annual American Psychological Association Convention, Boston, MA.\n\n\nBarr, L. K., Kahn, J. H., & Schneider, W. J. (2008, March 15). Emotion expression tendencies and mood/anxiety symptoms [Poster]. International Counseling Psychology Conference, Chicago, IL.\n\n\nTobin, R. M., Schneider, W. J., & Landau, S. E. (2008, February 15). Assessing attention-deficit/hyperactivity disorder using an RTI approach [Poster]. National Association of School Psychologists Convention, New Orleans, LA.\n\n\nStagg, J. W., & Schneider, W. J. (2007, November 15). So you flunked the Stroop Test, so what? The utility of single vs. multiple indicators in predicting behavioral outcomes related to executive functioning among non-referred individuals [Poster]. Annual National Academy of Neuropsychology Conference, Scottsdale, AZ.\n\n\nSchneider, W. J. (2007, August 15). Is the TAT helpful in assessing complex aspects of attention and executive functions? An extremely labor-intensive celebration of the null hypothesis [Poster]. Annual American Psychological Association Convention, San Francisco, CA.\n\n\nSchneider, W. J., & Kasson, D. M. (2007, January 3). Do narrow abilities matter more for people with higher IQ? Implications of Spearman’s Law of Diminishing Returns for the discrepancy model of LD [Poster]. National Association of School Psychologists Convention, New York, NY.\n\n\nSchneider, W. J., & McKenna, T. L. (2006, August 15). Using the Implicit Association Test to measure early maladaptive schemas [Poster]. Annual American Psychological Association Convention, New Orleans, LA.\n\n\nKahn, J. H., Vogel, D. L., Schneider, W. J., Barr, L. K., & Henning, K. (2006, April 15). Emotional self-disclosures of college-student clients and counseling session outcome [Poster]. Great Lakes Conference, West Lafayette, IN.\n\n\nJones, G. B., & Schneider, W. J. (2005, August 15). Intelligence, human capital, and economic growth [Paper]. Econometric Society World Congress, London, UK.\n\n\nTobin, R. M., Bodner, A. C., & Schneider, W. J. (2005, August 15). Executive function and emotion regulation in children [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nMarch, A., Funk, K., Schneider, W. J., & Landau, S. E. (2005, January 3). Student and teacher attributions of school shootings [Poster]. National Association of School Psychologists Convention, Atlanta, GA.\n\n\nTobin, R. M., Schneider, W. J., Graziano, W. G., & Pizzitola, K. M. (2002, February 15). Nice kids in competitive situations [Poster]. Annual Society for Personality and Social Psychology Meeting, Savannah, GA.\n\n\nSnyder, D. K., Schneider, W. J., Oxford, M. C., & Quinn, T. (2001, July 15). Secondary prevention group intervention for couples [Paper]. World Congress of Behavioral and Cognitive Therapies, Vancouver, BC.\n\n\nTobin, R. M., Pizzitola, K. M., Schneider, W. J., & Graziano, W. G. (2001, April 15). Goal structures and competitiveness in children’s games [Poster]. Biennial Meeting of the Society for Research in Child Development, Minneapolis, MN.\n\n\nSchneider, W. J., Loss, R. M., Cavell, T. A., & Oxford, M. C. (2000, November 15). Parenting and children with callous-unemotional traits: Relationship quality as a potential socialization mechanism [Poster]. Annual Convention of the Association for the Advancement Behavior Therapy, New Orleans, LA.\n\n\nSnyder, D. K., Schneider, W. J., Oxford, M. C., & Quinn, T. (2000, November 15). SUCCESS: The efficacy of a relationship enhancement group [Poster]. Annual Convention of the Association for the Advancement Behavior Therapy, New Orleans, LA.\n\n\nSchneider, W. J., Cavell, T. A., Hughes, J. N., & Oxford, M. C. (1999, August 15). The development of the Perceived Containment Questionnaire [Poster]. Annual American Psychological Association Convention, Boston, MA."
  },
  {
    "objectID": "vita.html#courses",
    "href": "vita.html#courses",
    "title": "Curriculum Vitae",
    "section": "Courses",
    "text": "Courses\n\nTemple University\n\nCPSY 5519—Group Counseling (Spring 2018–2019)\nCPSY 5694—Introduction to Assessment (Fall 2017–2019, 2023)\nEDUC 5101—Critical Understanding of Social Science Research (Fall 2018)\nEDUC 5325—Introduction to Statistics and Research (Fall 2017, 2021)\nEPSY 5529—Tests and Measurements (Spring 2019–2020, 2022, 2023)\nEPSY 8825—Advanced Data Analysis (Spring 2020, 2022)\nSPSY 9687/8—Seminar in School Psychology/Psychoeducational Clinic (Fall 2019, Spring 2020–2021, Fall 2023)\n\n\n\nIllinois State University\n\nPSY 110—Explaining Human Behavior/Fundamentals of Psychology (Fall 2002–2006, Spring 2003–2004, 2006–2007)\nPSY 138—Social Science Reasoning Using Statistics/Reasoning in Psychology Using Statistics (Spring 2004, 2008–2012, 2014; Fall 2007–2014, 2016)\nPSY 340—Statistics for the Social Sciences (Spring 2005)\nPSY 432—Psychodiagnostics I: Cognitive Assessment/ Theory and Practice of Cognitive Assessment (Fall 2004–2016)\nPSY 436—Clinical/Counseling Practicum (Spring 2004)\nPSY 442—Test Theory (Spring 2015, 2017)\nPSY 443—Regression Analysis (Spring 2016)\nPSY 444—Multivariate Analysis (Fall 2015){.smalldate}\nPSY 464—Theories and Techniques of Counseling: Adults (Spring 2005–2012, 2014–2016)\nPSY 480—Advanced Practicum in Dialectical Behavior Therapy Skills (Fall 2015–2017)\nPSY 480.31—Practicum in Dialectical Behavior Therapy Skills Training (Fall 2014, Spring 2105–2017)\nPSY 480.33—Seminar in Psychology: Supervision of a Dialectical Behavior Therapy Group (Fall 2014, Spring 2105, 2017)"
  },
  {
    "objectID": "vita.html#mentoring",
    "href": "vita.html#mentoring",
    "title": "Curriculum Vitae",
    "section": "Mentoring",
    "text": "Mentoring\n\nDoctoral Dissertation Chair\n\nTemple University\n\nAidan Campagnolio\nChristine An (co-chair)\nMegan Barone\nRandy Taylor\nBernard Dillard (2024)\nStephanie Iaccarino (2023, co-chair)\nJustin Harper (2022, co-chair)\n\n\n\nIllinois State University\n\nC. Lee Affrunti (2013)\nDaniel L. Gadke (co-chair, 2012)\nJonathan W. Stagg (2008)\n\n\n\n\nDoctoral Dissertation Committee Member\n\nTemple University\n\nEmunah Mager-Garfield (2024)\nOctavia Blount (2024)\nValerie Woxholdt (2024)\nCodie Kane (2023)\nPatrick Clancy (2023)\nShana Levi-Nielsen (2022)\nKaiyla Darmer (2022)\nKathryn DeVries (2022)\nTera Gibbs (2022)\nMariah Davis (2022)\nLinda Ruan (2020)\n\n\n\nIllinois State University\n\nJennifer Engelland-Schultz (2015)\nMandi Martinez-Dick (2015)\nThomas Mulderink (2015)\nAlyssa A. Sondalle (2015)\nRachelle Cantin (2013)\nJennifer Wallace (2013)\nSilas Dick (2013)\nTrisha Mann (2012)\nKatherine A. Gioia (2009)\nSarah Reck (2008)\nAnna C. Bodner (2005)\n\n\n\nExternal Reviewer\n\nBrianna Paul (2022) Texas Women’s University\nJake Blair Kraska (2021) Monash University\nPaul Jewsbury (2014) University of Melbourne\n\n\n\n\nMaster’s Thesis Chair\n\nIllinois State University\n\nFeng Ji (2018)\nKatrin Klieme (2016)\nZachary Roman (2016)\nKiera Dymit (2015)\nMeera Afzal (2012)\nRachelle Bauer (2010)\nSunthud Pornprasertmanit (2010)\nDavid Kasson (2006)\nKristina Taylor (2006)\nRobin Van Herrmann (2010)\nMelissa Zygmun (2006)\n\n\n\n\nMaster’s Thesis Committee Member\n\nIllinois State University\n\nRyan Willard (2017)\nRachel Workman (2017)\nHayley Love (2016)\nAnges Strojewska (2016)\nDanielle Freund (2015)\nAmanda Fisher (2015)\nDaniel Nuccio (2014)\nKevin Wallpe (2014)\nNicole Moore (2013)\nDrew Abney (2012)\nThomas Mulderink (2012)\nJames Clinton (2011)\nJamie Hansen (2010)\nYin Ying Ong (2010)\nMelanie Hewett (2010)\nKaty Adler (2008)\nPoonam Joshi (2008)\nRebecca Hoerr (2008)\nArusha Sethi (2008)\nLeah Barr (2008)\nSara Byczek (2007)"
  },
  {
    "objectID": "vita.html#university-service",
    "href": "vita.html#university-service",
    "title": "Curriculum Vitae",
    "section": "University Service",
    "text": "University Service\n\nIllinois State University\n\n\n\n\n\n\n2007–2017\n\n\nCoordinator and Supervisor, College Learning Assessment Service (CLAS), Psychological Services Center"
  },
  {
    "objectID": "vita.html#college-service",
    "href": "vita.html#college-service",
    "title": "Curriculum Vitae",
    "section": "College Service",
    "text": "College Service\n\nTemple University\n\n\n\n\n\n\n2022–2023\n\n\nMember, CEHD Transition Committee\n\n\n\n\n2022–2023\n\n\nMember, CEHD Tenure and Promotion Committee\n\n\n\n\n2020\n\n\nMember, Higher Education Non-Tenure-Track Faculty Search Committee\n\n\n\n\n2019–2022\n\n\nChair, Faculty Merit Committee\n\n\n\n\n2019\n\n\nCo-Chair, Data and Assessment Working Group\n\n\n\n\n2019\n\n\nMember, Higher Education Tenure-Track Faculty Search Committee\n\n\n\n\n2019\n\n\nTraining presenter: Make Your Data POP! How to interpret and present data visually to maximize its impact\n\n\n\n\n2018–2019\n\n\nMember, Faculty Resource and Development Committee\n\n\n\n\n2018\n\n\nMember, Special Education Faculty Search committee\n\n\n\n\n2018\n\n\nTraining presenter: Introduction to Statistical Analysis with R Workshop"
  },
  {
    "objectID": "vita.html#departmental-service",
    "href": "vita.html#departmental-service",
    "title": "Curriculum Vitae",
    "section": "Departmental Service",
    "text": "Departmental Service\n\nTemple University\n\n\n\n\n\n\n2018–\n\n\nMember, School Psychology Program Committee\n\n\n\n\n2017–2020, 2023–\n\n\nMember, Counseling Psychology Program Committee\n\n\n\n\n2020–2023\n\n\nMember, Educational Leadership Program Committee\n\n\n\n\n2018–2020\n\n\nMember, Departmental Promotion and Tenure Committee\n\n\n\n\n\n\nIllinois State University\n\n\n\n\n\n\n2004–2017\n\n\nMember, Clinical/Counseling Psychology Coordinating committee\n\n\n\n\n2004–2017\n\n\nMember, IRB Committee\n\n\n\n\n2004–2017\n\n\nMember, Psychological Services Center Administrative Team\n\n\n\n\n2004–2017\n\n\nMember, Quantitative Psychology Sequence committee\n\n\n\n\n2004–2017\n\n\nMember, Research Committee\n\n\n\n\n2015–2017\n\n\nMember, Department Faculty Status Committee (Evaluates annual merit, tenure, and promotion)\n\n\n\n\n2017\n\n\nChair, Salary Compression Review Team\n\n\n\n\n2015\n\n\nProgram Coordinator, Quantitative Sequence (Fall only)\n\n\n\n\n2014\n\n\nMember, Clinical/Counseling Faculty Search committee\n\n\n\n\n2004–2005\n\n\nMember, Clinical/Counseling Faculty Search committee"
  },
  {
    "objectID": "vita.html#consulting",
    "href": "vita.html#consulting",
    "title": "Curriculum Vitae",
    "section": "Consulting",
    "text": "Consulting\n\n\n\n\n\n\n2019–\n\n\nAcademic Consultant, Empass Learning, developing an online test of cognitive abilities based on CHC Theory, Gurgaon, India\n\n\n\n\n2014–\n\n\nExpert Consultant, providing expert evaluations of death penalty and medical cases for various legal firms, US\n\n\n\n\n2017–2020\n\n\nAcademic Consultant, PT Melintas Cakrawala, developing AJT CogTest?a comprehensive test of cognitive abilities based on CHC Theory, Jakarta, Indonesia\n\n\n\n\n2015–2017\n\n\nAcademic Consultant, Ayrton Senna Institute, developing a comprehensive assessment of 21st century skills, Sao Paulo, Brazil\n\n\n\n\n2016\n\n\nAcademic Consultant, Academic Therapy Publications, developing and reviewing cognitive and oral language assessment batteries, Novato, CA\n\n\n\n\n2007–2010\n\n\nResearch Consultant, Illinois Army National Guard, conducting research examining reintegration of service personnel"
  },
  {
    "objectID": "vita.html#media-appearances",
    "href": "vita.html#media-appearances",
    "title": "Curriculum Vitae",
    "section": "Media Appearances",
    "text": "Media Appearances\n\n\n\n\n\n\n02/03/2014\n\n\nInterviewed in Scientific American Blog Beautiful Minds by Scott Barry Kaufman\n\n\n\n\n08/05/2016\n\n\nQuoted in the Wall Street Journal about IQ tests\n\n\n\n\n10/13/2016\n\n\nQuoted in ScienceNews for Students about IQ tests\n\n\n\n\n10/11/2017\n\n\nOpined in the Guardian about the president’s IQ\n\n\n\n\n04/09/2018\n\n\nAppeared as a guest of Knowledge@Wharton Radio to discuss hunger and homelessness among college students.\n\n\n\n\n04/03/2018\n\n\nAppeared on CNN Headline News (HLN) with MichaeLA to discuss hunger and homelessness among college students.\n\n\n\n\n10/20/2019\n\n\nPresented on Writing Assessment Reports People Will Read, Understand, and Remember on the School Psyched Podcast\n\n\n\n\n03/01/2020\n\n\nQuoted in APA Monitor on writing assessment reports\n\n\n\n\n03/29/2021\n\n\nPresented on the evolution of cognitive assessment on the Testing Psychologist Podcasts"
  },
  {
    "objectID": "vita.html#editorial-positions",
    "href": "vita.html#editorial-positions",
    "title": "Curriculum Vitae",
    "section": "Editorial Positions",
    "text": "Editorial Positions\n\n\n\n\n\n\n\n\n\n\n2019⁠–⁠2020\nJournal of Psychoeducational Assessment\nAssociate Editor\n\n\n2015\nJournal of School Psychology\nGuest Action Editor"
  },
  {
    "objectID": "vita.html#editorial-boards",
    "href": "vita.html#editorial-boards",
    "title": "Curriculum Vitae",
    "section": "Editorial Boards",
    "text": "Editorial Boards\n\n\n\n\n\n\n\n\n\n\n2018⁠–⁠\nJournal of Intelligence\nEditorial Board\n\n\n2011⁠–⁠\nJournal of School Psychology\nEditorial Board\n\n\n2010⁠–⁠\nJournal of Psychoeducational Assessment\nEditorial Board\n\n\n2017⁠–⁠2021\nArchives of Scientific Psychology\nConsulting Editor\n\n\n2009⁠–⁠2018\nPsychological Assessment\nConsulting Editor\n\n\n1997⁠–⁠2000\nClinician's Research Digest\nEditorial Associate"
  },
  {
    "objectID": "vita.html#ad-hoc-reviewing",
    "href": "vita.html#ad-hoc-reviewing",
    "title": "Curriculum Vitae",
    "section": "Ad Hoc Reviewing",
    "text": "Ad Hoc Reviewing\n\nApplied Neuropsychology\nEuropean Journal of Psychological Assessment\nFrontiers in Human Neuroscience\nJournal of Family Psychology\nJournal of Psychoeducational Assessment\nNASP Communiqué\nPsychological Assessment\nPsychology in the Schools"
  },
  {
    "objectID": "vita.html#professional-memberships",
    "href": "vita.html#professional-memberships",
    "title": "Curriculum Vitae",
    "section": "Professional Memberships",
    "text": "Professional Memberships\n\nMember, American Psychological Association\nMember, National Association of School Psychologists"
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html",
    "href": "tutorials/StatsIntro/MeasurementScales.html",
    "title": "Measurement Scales",
    "section": "",
    "text": "In science and mathematics, there are fundamental constants—things that always remain the same (e.g., c the speed of light in a vacuum, or \\pi, the ratio of a circle’s circumference to its diameter). However, most phenomena we study are capable of change. Indeed, much of scientific investigation consists of discovering how change in one process affects changes in other processes. Any process or quantity that can change is called a variable.\n\n\nA constant is a state or quantity that remains the same.\nA variable is an abstract category of information that can assume different values.\n\n\nAn algebraic variable usually takes on any value, but it can be constrained by equations and other kinds of relationships. Consider this equation:\n2x-6=0 \\tag{1}\nIn isolation, the variable x could be any real number. However, once inserted in an equation, x is constrained in some way. In Equation 1, x must equal 3.\n\n\n\nRandom variables are not like algebraic variables. The values that random variables can take on are determined by one or more random processes. For example, the outcome of a coin toss is a random variable. It can either be heads or tails. Think of a random variable as forever spitting out new values. In Figure 1, the random variable is an endless cascade of coins.\n\n\n\n\n\n\n\n\nFigure 1: An endless sequence of fair coin tosses is a random variable.\n\n\n\nIn this case, the set of possible values {heads,tails} is the sample space of the outcome of a coin toss. A random variable's sample space is the set of all possible values that the variable can assume. In the case of the roll of a six-sided die, the sample space is {1,2,3,4,5,6}. This sample space has a finite number of values. However, some sample spaces are infinite. For example, the number of siblings one can have is the set of all non-negative integers {0,1,2,3,...}. Of course, because there are limits as to how many children people can have, the probability of very high numbers is very low. A different kind of infinity is observed in the sample space of variables that are continuous. The length of one's foot in centimeters can be any positive real number. Between any two real numbers, (e.g., 20cm and 30cm) there is an infinite number of possibilities because real numbers can be sliced as thinly as needed (e.g., 28.465570098756642111cm)."
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#algebraic-variables",
    "href": "tutorials/StatsIntro/MeasurementScales.html#algebraic-variables",
    "title": "Measurement Scales",
    "section": "",
    "text": "An algebraic variable usually takes on any value, but it can be constrained by equations and other kinds of relationships. Consider this equation:\n2x-6=0 \\tag{1}\nIn isolation, the variable x could be any real number. However, once inserted in an equation, x is constrained in some way. In Equation 1, x must equal 3."
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#random-variables",
    "href": "tutorials/StatsIntro/MeasurementScales.html#random-variables",
    "title": "Measurement Scales",
    "section": "",
    "text": "Random variables are not like algebraic variables. The values that random variables can take on are determined by one or more random processes. For example, the outcome of a coin toss is a random variable. It can either be heads or tails. Think of a random variable as forever spitting out new values. In Figure 1, the random variable is an endless cascade of coins.\n\n\n\n\n\n\n\n\nFigure 1: An endless sequence of fair coin tosses is a random variable.\n\n\n\nIn this case, the set of possible values {heads,tails} is the sample space of the outcome of a coin toss. A random variable's sample space is the set of all possible values that the variable can assume. In the case of the roll of a six-sided die, the sample space is {1,2,3,4,5,6}. This sample space has a finite number of values. However, some sample spaces are infinite. For example, the number of siblings one can have is the set of all non-negative integers {0,1,2,3,...}. Of course, because there are limits as to how many children people can have, the probability of very high numbers is very low. A different kind of infinity is observed in the sample space of variables that are continuous. The length of one's foot in centimeters can be any positive real number. Between any two real numbers, (e.g., 20cm and 30cm) there is an infinite number of possibilities because real numbers can be sliced as thinly as needed (e.g., 28.465570098756642111cm)."
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#nominal-variables",
    "href": "tutorials/StatsIntro/MeasurementScales.html#nominal-variables",
    "title": "Measurement Scales",
    "section": "Nominal Variables",
    "text": "Nominal Variables\nNominal variables can take on any kind of value, including values that are not numbers. The values must constitute a set of mutually exclusive categories. For example, if I have a set of data about college students, I might record which major each person has. The variable college major consists of different labels (e.g., Accounting, Mathematics, and Psychology). Note that there is no true order to college majors, though we usually alphabetize them for convenience. There is no meaningful sense in which English majors are higher or lower than Biology majors. Nominal values are either the same or they are different. They are not less than or more than anything else.\n\n\nIn a set of mutually exclusive categories, nothing belongs to more than one of the categories in the set at the same time.\n\nExamples of Nominal Variables\n\nBiological sex {male, female}\nRace/Ethnicity {African-American, Asian-American,...}\nType of school {public, private}\nTreatment group {Untreated, Treated}\nDown Syndrome {present, not present}\nAttachment Style {dismissive-avoidant, anxious-preoccupied, secure}\nWhich emotion are you feeling right now? {Happiness, Sadness, Anger, Fear}"
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#ordinal-variables",
    "href": "tutorials/StatsIntro/MeasurementScales.html#ordinal-variables",
    "title": "Measurement Scales",
    "section": "Ordinal Variables",
    "text": "Ordinal Variables\nLike nominal variables, ordinal variables are categorical. Whereas the categories in nominal variables have no meaningful order, the categories in ordinal variables have a natural order. For example, questionnaires often ask multiple-choice questions like so:\nI like chatting with people I do not know.\n\nStrongly disagree\nDisagree\nNeutral\nAgree\nStrongly agree\n\nIt is clear that the response choices have an order that would generate no controversy. Note, however, that there is no meaningful distance between the categories. Is the distance between strongly disagree and disagree the same as the distance between disagree and neutral? It is not a meaningful question because no distance as been defined. All we can do is say is which category is higher than the other.\n\nExamples of Ordinal Variables\n\nDosage {placebo, low dose, high dose}\nOrder of finishing a race {1st place, 2nd place, 3rd place,…}\nISAT category {below standards, meets standards, exceeds standards}\nApgar score {0,1,…,10}"
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#interval-scales",
    "href": "tutorials/StatsIntro/MeasurementScales.html#interval-scales",
    "title": "Measurement Scales",
    "section": "Interval Scales",
    "text": "Interval Scales\nInterval scales are quantitative. The values that interval scales take on are almost always numbers. Furthermore, the distance between the numbers have a consistent meaning. The classic example of an interval scale is temperature on the Celsius or Fahrenheit scale. The distance between 25° and 35° is 10°. The distance between 90° and 100° is also 10°. In both cases, the difference involves the same amount of heat.\nUnlike with nominal and ordinal scales, we can add and subtract scores on an interval scale because there are meaningful distances between the numbers.\nInterestingly, the meaning of 0°C (or 0°F) is not what we are used to thinking about when we encounter the number zero. Usually, the number zero means the absence of something. Unfortunately, the number zero does not have this meaning in interval scales. When something has a temperature of 0°C, it does not mean that there is no heat. It just happens to be the temperature at which water freezes at sea level. It can get much, much colder. Thus, interval scales lack a true zero in which zero indicates a complete absence of something.\n\n\nA true zero indicates the absence of the quantity being measured.\nLacking a true zero, interval scales cannot be used to create meaningful ratios. For example, 20°C is not “twice as hot” as 10°C. Also, 110°F is not “10% hotter” than 100°F.\n\nNearly Interval Scales\nIn truth, there are very, very few examples of variables with a true interval scale. However, a large percentage of variables used in the social sciences are treated as if they are interval scales. It turns out that with a bit of fancy math, many ordinal variables can be transformed, weighted, and summed in such a way that the resulting score is reasonably close to having interval properties. The advantage of doing this is that, unlike with nominal and ordinal scales, you can calculate means, standard deviations, and a host of other statistics that depend on there being meaningful distances between numbers.\nPsychological and educational measures regularly make use of these procedures. For example, on tests like the ACT, we take information about which questions were answered correctly and then transform the scores into a scale that ranges from 1 to 36. As a group, people who score a higher on the ACT tend to perform better in college than people who score lower. Of course, many individuals perform much better than their ACT scores suggest. An equal number of individuals perform much worse than their ACT scores suggest. Among many other things, thirst for knowledge and hard work matter quite a bit. Even so, on average, individuals with a 10 on the ACT are likely to perform worse in college than people with a 20. Roughly by the same amount, people with a 30 on the ACT are likely to perform better in college than people with a 20. Again, we talking about averages, not individuals. Every day, some people beat expectations and some people fail to meet them, often by wide margins.\n\n\nExamples of Interval Scales\n\nTruly interval:\nTemperature on the Celsius and Fahrenheit scale (not on the Kelvin scale)\nCalendar year (e.g., 431BC, 1066AD)\nNotes on an even-tempered instrument such as a piano {A, A#, B, C, C#, D, D#, E, F, F#, G, G#}\nA ratio scale converted to a z-score metric (or any other kind of standard score metric)\nNearly interval:\nMost scores from well-constructed ability tests (e.g., IQ, ACT, GRE) and personality measures (e.g., self-esteem, extroversion).\n\n\n\nA z-score metric has a mean of 0 and a standard deviation of 1. Thus, the zero indicates the mean of the variable, not the absence of the quantity."
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#ratio-scales",
    "href": "tutorials/StatsIntro/MeasurementScales.html#ratio-scales",
    "title": "Measurement Scales",
    "section": "Ratio Scales",
    "text": "Ratio Scales\nA ratio scale has all of the properties of an interval scale. In addition, it has a true zero. When a ratio scale has a value of zero, it indicates the absence of the quantity being measured. For example, if I say that I have 0 coins in my pocket, there are no coins in my pocket. The fact that ratio scales have true zeroes means that ratios are meaningful. For example, if you have 2 coins and I have one, you have twice as many coins as I do. If I have 100 coins and then you give me 10 more, the number of coins I have has increased by 10%.\n\nExamples of Ratio Scales\nRatio scales involve countable quantities, such as:\n\ncoins\nmarbles\ncomputers\nspeeding tickets\npregnancies\nsoldiers\nplanets\n\nCountable quantities are discrete variables like the scale in Figure 3. Countable integers are discrete because they have particular values but not values between those values.\n\n\nA discrete variable can only take on exact, isolated values from a specified list.\nMany physical properties are also ratio scales, such as:\n\ndistance\nmass\nforce\nheat (on the Kelvin scale)\npressure\nvoltage\nacceleration\nproportions\n\nThese dimensions are not discrete countable quantities like cars and bricks but are instead continuous quantities that can be measured with decimals and fractions.\n\n\n\n\n\n\nFigure 3: A continuous scale from 1 to 10 can have any real value between and including 1 and 10. A discrete scale from 1 to 10 can only take on the values shown.\n\n\n\nNotice that even though ratio variables have a true zero, on some of them it is possible to have negative numbers. For example, negative acceleration would indicate a slowing down. A negative value in a checking account means that you owe the bank money.\nIn the social sciences, there are many examples of ratio scales:\n\nIncome\nAge\nYears of education\nReaction time\nFamily size\nHours of study\nPercentage of household chores completed (compared to other members of the household)"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html",
    "href": "tutorials/SEMGrowth/semgrowth.html",
    "title": "Latent Growth Models",
    "section": "",
    "text": "Setup Code\noptions(digits = 4, knitr.kable.NA = '')\n\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(lavaan)\nlibrary(ggdiagram)\nlibrary(gt)\nlibrary(geomtextpath)\nlibrary(sjPlot)\nlibrary(easystats)\n\nmy_font &lt;- \"Roboto Condensed\"\n\nmy_apa &lt;- function(tb) {\n  # amended from https://benediktclaus.github.io/benelib/reference/theme_gt_apa.html\n  tb %&gt;% \n    cols_align(columns = 1, align = \"left\") %&gt;% \n    opt_table_lines(extent = \"none\") %&gt;% \n    tab_options(table.border.top.style = \"solid\",\n                table.border.top.color = \"black\", \n                table.border.top.width = 1,\n                table_body.border.bottom.style = \"solid\", \n                table_body.border.bottom.color = \"black\",\n                table_body.border.bottom.width = 1, \n                column_labels.border.bottom.style = \"solid\", column_labels.border.bottom.color = \"black\", \n            column_labels.border.bottom.width = 1)\n}"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#level-1-measurement-occasion",
    "href": "tutorials/SEMGrowth/semgrowth.html#level-1-measurement-occasion",
    "title": "Latent Growth Models",
    "section": "Level 1 (Measurement Occasion)",
    "text": "Level 1 (Measurement Occasion)\n\nVocabulary_{ti} = b_{0i}+b_{1i}Time_{ti} +b_{2i}Time_{ti}^2+e_{ti}"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#level-2-person",
    "href": "tutorials/SEMGrowth/semgrowth.html#level-2-person",
    "title": "Latent Growth Models",
    "section": "Level 2 (Person)",
    "text": "Level 2 (Person)\n\n\\begin{aligned}\nb_{0i}&=b_{00} + b_{01}Intervention_{i} + u_{0i}\\\\\nb_{1i}&=b_{10} + b_{11}Intervention_{i} + u_{1i}\\\\\nb_{2i}&=b_{20} + b_{21}Intervention_{i}\n\\end{aligned}"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#combined-model",
    "href": "tutorials/SEMGrowth/semgrowth.html#combined-model",
    "title": "Latent Growth Models",
    "section": "Combined Model",
    "text": "Combined Model\n\n\\begin{aligned}\nVocabulary_{ti} &=\n\\underbrace{b_{00} + b_{01}Intervention_{i} + u_{0i}}_{b_{0i}} +\\\\\n&(\\underbrace{b_{10} + b_{11}Intervention_{i} + u_{1i}}_{b_{1i}})Time_{ti} +\\\\\n&(\\underbrace{b_{20}+ b_{21}Intervention_{i}}_{b_{2i}})Time_{ti}^2 + e_{ti}\n\\end{aligned}\n\n\n\nCode\ntibble::tribble(\n     ~Symbol,                                                            ~Meaning,\n  \"$b_{00}$\", \"Intercept (Predicted Vocabulary when Intervention and Time are 0)\",\n  \"$b_{01}$\",                                  \"Slope for Intervention at Time 0\",\n  \"$b_{10}$\",            \"Linear Slope for Time when Intervention and Time are 0\",\n  \"$b_{11}$\",                    \"Interaction of Intervention and Time at Time 0\",\n  \"$b_{20}$\",                   \"Quadratic Effect of Time when Intervention is 0\",\n  \"$b_{21}$\",          \"Interaction of Intervention and Quadratic Effect of Time\",\n  \"$b_{0i}$\",                     \"Predicted Vocabulary at Time 0 for Person *i*\",\n  \"$b_{1i}$\",                    \"Linear Effect of Time at Time 0 for Person *i*\",\n  \"$b_{2i}$\",                           \"Quadratic Effect of Time for Person *i*\",\n  \"$u_{0i}$\",                                             \"Residual for $b_{0i}$\",\n  \"$u_{1i}$\",                                             \"Residual for $b_{1i}$\",\n  \"$e_{ti}$\",                               \"Residual for Person *i* at Time *t*\"\n  ) %&gt;% \n  knitr::kable(align = \"cl\")\n\n\n\n\n\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\nb_{00}\nIntercept (Predicted Vocabulary when Intervention and Time are 0)\n\n\nb_{01}\nSlope for Intervention at Time 0\n\n\nb_{10}\nLinear Slope for Time when Intervention and Time are 0\n\n\nb_{11}\nInteraction of Intervention and Time at Time 0\n\n\nb_{20}\nQuadratic Effect of Time when Intervention is 0\n\n\nb_{21}\nInteraction of Intervention and Quadratic Effect of Time\n\n\nb_{0i}\nPredicted Vocabulary at Time 0 for Person i\n\n\nb_{1i}\nLinear Effect of Time at Time 0 for Person i\n\n\nb_{2i}\nQuadratic Effect of Time for Person i\n\n\nu_{0i}\nResidual for b_{0i}\n\n\nu_{1i}\nResidual for b_{1i}\n\n\ne_{ti}\nResidual for Person i at Time t\n\n\n\n\n\n\nTable 6: Symbols and Meanings of HLM Model"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#analysis",
    "href": "tutorials/SEMGrowth/semgrowth.html#analysis",
    "title": "Latent Growth Models",
    "section": "Analysis",
    "text": "Analysis\nI am going straight to my hypothesized model. In a real analysis, I might have gone through the series of steps we have seen elsewhere in the course. However, I want save time so that I can show how this model is the same as the latent growth model. When I walk through all the latent growth models in sequence, I will show the comparable hierarchical linear models one at a time.\n\nm &lt;- lmer(vocabulary ~ 1 + time * intervention + \n            I(time ^ 2) * intervention + \n            (1 + time | person_id), \n          data = d, \n          REML = FALSE)\n\ntab_model(m)\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n52.00\n46.36 – 57.64\n&lt;0.001\n\n\ntime\n10.50\n9.91 – 11.09\n&lt;0.001\n\n\nintervention\n-0.09\n-7.91 – 7.74\n0.983\n\n\ntime^2\n-1.06\n-1.15 – -0.96\n&lt;0.001\n\n\ntime × intervention\n9.14\n8.33 – 9.95\n&lt;0.001\n\n\nintervention × time^2\n0.09\n-0.05 – 0.22\n0.217\n\n\nRandom Effects\n\n\n\nσ2\n8.99\n\n\n\nτ00 person_id\n786.64\n\n\nτ11 person_id.time\n2.02\n\n\nρ01 person_id\n0.12\n\n\nICC\n0.99\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.381 / 0.993\n\n\n\n\nreport(m)\n\nWe fitted a linear mixed model (estimated using ML and nloptwrap optimizer) to\npredict vocabulary with time and intervention (formula: vocabulary ~ 1 + time *\nintervention + I(time^2) * intervention). The model included time as random\neffects (formula: ~1 + time | person_id). The model's total explanatory power\nis substantial (conditional R2 = 0.99) and the part related to the fixed\neffects alone (marginal R2) is of 0.38. The model's intercept, corresponding to\ntime = 0 and intervention = 0, is at 52.00 (95% CI [46.36, 57.64], t(1190) =\n18.08, p &lt; .001). Within this model:\n\n  - The effect of time is statistically significant and positive (beta = 10.50,\n95% CI [9.91, 11.09], t(1190) = 35.18, p &lt; .001; Std. beta = 0.47, 95% CI\n[0.46, 0.48])\n  - The effect of intervention is statistically non-significant and negative\n(beta = -0.09, 95% CI [-7.91, 7.74], t(1190) = -0.02, p = 0.983; Std. beta =\n0.32, 95% CI [0.21, 0.43])\n  - The effect of time^2 is statistically significant and negative (beta = -1.06,\n95% CI [-1.15, -0.96], t(1190) = -21.10, p &lt; .001; Std. beta = -0.08, 95% CI\n[-0.09, -0.07])\n  - The effect of time × intervention is statistically significant and positive\n(beta = 9.14, 95% CI [8.33, 9.95], t(1190) = 22.08, p &lt; .001; Std. beta = 0.22,\n95% CI [0.21, 0.23])\n  - The effect of intervention × time^2 is statistically non-significant and\npositive (beta = 0.09, 95% CI [-0.05, 0.22], t(1190) = 1.24, p = 0.217; Std.\nbeta = 3.40e-03, 95% CI [-2.00e-03, 8.81e-03])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\nDon’t be fooled by the non-significant effect of the intervention! It is a conditional effect at Time 0—before the intervention began.\nIt is the interaction of time and the intervention that matters.\nLet’s plot the model using sjPlot::plot_model:\n\nplot_model(m, type = \"pred\", terms = c(\"time [all]\", \"intervention\"))\n\n\n\n\n\n\n\n\nInterpretation: As hypothesized, the effect of time has diminishing returns, and the time by interaction interaction effect is evident. The two groups were comparable before the intervention, but the treatment group’s vocabulary grew faster over time. If I needed to publish a plot showing these results, it would look like Figure 10.\n\n\nSidenote: I love the sjPlot::plot_model function. With almost no effort, I can get a plot that helps me see what is going on with my fixed effects. It used to take me so much more code to arrive at where sjPlot::plot_model lands. However, if you want a publication-worthy plot, sjPlot::plot_model tends to impose too much control. Fortunately, the same person who made sjPlot is involved with another easystats package called modelbased that allows for greater flexibility but still reduces what would otherwise be onerous coding.\n\n\nCode\nm_fixed &lt;- fixef(m)\n\nmy_coefs &lt;- tibble(b00 = c(m_fixed[\"(Intercept)\"],\n                          m_fixed[\"(Intercept)\"] + m_fixed[\"intervention\"]),\n                   b10 = c(m_fixed[\"time\"],\n                           m_fixed[\"time\"] + m_fixed[\"time:intervention\"]),\n                   b20 = c(m_fixed[\"I(time^2)\"],\n                           m_fixed[\"I(time^2)\"] + m_fixed[\"intervention:I(time^2)\"])) %&gt;% \n  mutate(across(.fns = \\(x) formatC(x, 2, format = \"f\"))) %&gt;% \n  mutate(intervention = factor(c(\"Control\", \"Treatment\")),\n         eq = glue::glue('{intervention}: {b00} + {b10}Time + {b20}Time&lt;sup&gt;2&lt;/sup&gt;')\n                   )\n\n\n\n# Predicted Data\nd_pred &lt;- modelbased::estimate_relation(m, data = crossing(time = 0:5, intervention = c(0,1))) %&gt;% \n  as_tibble() %&gt;% \n  mutate(intervention = factor(intervention, levels = c(0,1), labels = c(\"Control\", \"Treatment\"))) %&gt;% \n  mutate(vocabulary = Predicted, \n         hjust = ifelse(intervention == \"Treatment\", .93, .91)) %&gt;% \n  left_join(my_coefs, by = join_by(intervention)) \n\nd %&gt;% \n  mutate(intervention = factor(intervention, levels = c(0,1), labels = c(\"Control\", \"Treatment\")),\n         person_id = factor(person_id)) %&gt;% \n  ggplot(aes(time, vocabulary, color = intervention)) +\n  geom_line(aes(group = person_id,\n                x = time + (intervention == \"Treatment\") * 0.2 - 0.1),\n            size = .1, \n            alpha = .5) + \n  ggbeeswarm::geom_quasirandom(\n    aes(x = time + (intervention == \"Treatment\") * 0.2 - 0.1), \n    width = .075, \n    pch = 16, \n    size = .75) +\n  geom_line(data = d_pred,\n            linewidth = 1) +\n  geom_labelline(aes(label = eq,\n                     hjust = hjust),\n                 data = d_pred, \n                 text_only = T,\n                 alpha = .8, size = 5,\n                 # linewidth = 4.25,\n                 linecolour = NA,\n                 vjust = -0.2,\n                 label.padding = unit(0, units = \"pt\"),\n                 family = \"Roboto Condensed\", \n                 rich = TRUE) +\n  scale_x_continuous(\"Time\", \n                     breaks = 0:5, \n                     minor_breaks = NULL) + \n  scale_y_continuous(\"Vocabulary\") +\n  theme_minimal(base_size = 16, \n                base_family = \"Roboto Condensed\") + \n  theme(legend.position = \"none\") + \n  scale_color_brewer(type = \"qual\", palette = 1)\n\n\n\n\n\n\n\n\nFigure 10: The Growth of Vocabulary by Treatment Group"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#restructure-from-long-to-wide-data",
    "href": "tutorials/SEMGrowth/semgrowth.html#restructure-from-long-to-wide-data",
    "title": "Latent Growth Models",
    "section": "Restructure from Long to Wide Data",
    "text": "Restructure from Long to Wide Data\nSEM analyses usually need wide data with each participant’s data on 1 row. We learned how to do so in a previous tutorial.\nUse the pivot_wider function to restructure your data so that the data set d is restructured to look like this:\n\n\n# A tibble: 200 × 8\n   person_id intervention voc_0 voc_1 voc_2 voc_3 voc_4 voc_5\n       &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1         1            0  31.3  38.6  51.2  54.9  50.6  55.3\n 2         2            0  94.1 100.  114.  119.  123.  125. \n 3         3            0  57.0  66.5  73.6  80.9  88.2  93.1\n 4         4            0  60.3  70.2  78.9  78.9  78.4  79.0\n 5         5            0  32.6  39.1  47.3  48.1  51.1  48.3\n 6         6            1  88.6 105.  120.  141.  160.  169. \n 7         7            0  65.6  72.4  76.8  83.3  86.3  87.0\n 8         8            0  35.9  41.7  57.4  64.8  64.1  73.1\n 9         9            1  71.7  78.9  97.4 105.  121.  126. \n10        10            1  75.9  85.0 106.  119.  132.  142. \n# ℹ 190 more rows\n\n\nSee if you can figure out how to restructure the data using pivot_wider. If it takes more than a minute or two, check out the hints.\nCall the new data frame d_wide.\n\n\n\n\n\n\nHintHint\n\n\n\n\n\nSet the values parameter equal to vocabulary.\nvalues = vocabulary\n\n\n\n\n\n\nHintHint 2\n\n\n\n\n\nThe names come from time.\nnames = time\n\n\n\n\n\n\nHintHint 3\n\n\n\n\n\nSpecify voc_ as the names prefix.\nnames_prefix = \"voc_\"\n\n\n\n\n\n\nHintHint 4\n\n\n\n\n\nHere is how I would do it:\n\nd_wide &lt;- d %&gt;% \n  pivot_wider(names_from = time, \n              values_from = vocabulary, \n              names_prefix = \"voc_\")"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-specification",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-specification",
    "title": "Latent Growth Models",
    "section": "Model Specification",
    "text": "Model Specification\nIn general, setting up a latent growth curve model requires much more typing than the corresponding hierarchical linear model. In this model, for all this extra work we get nothing that we did not already have. However, the latent growth modeling approach is extremely flexible, particularly with respect to modeling the simultaneous effects of many other variables. In more complex models, we can evaluate hypotheses that are difficult, if not impossible, in hierarchical linear models.\nNote that intercepts are symbolized with the symbol 1 when it is alone or precided by a coefficient (e.g., b_00 ~ 1)\n\nlatent_model &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Acceleration effect of time (quadratic effect)\nq =~ 0 * voc_0 + 1 * voc_1 + 4 * voc_2 + 9 * voc_3 + 16 * voc_4 + 25 * voc_5\n# Predicting intercept, slope, and quadratic effects\ni ~ b_00 * 1 + b_01 * intervention\ns ~ b_10 * 1 + b_11 * intervention\nq ~ b_20 * 1 + b_21 * intervention\n# Variances and covariances\ni ~~ tau_00 * i + tau_01 * s\ns ~~ tau_11 * s\nq ~~ 0 * q + 0 * s + 0 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n\"\n\nIn the hierarchical model, we did not make b_{2i} (the quadratic effect of time) random. Thus, we will set the \\tau_{22} variance to 0, and also the covariances involving u_{2i} residual (i.e., \\tau_{02} and \\tau_{12}).\nYou can see from the summary output that the parameters are same as before.\n\nfit_latentgrowth &lt;- growth(latent_model, data = d_wide)\n\nsummary(fit_latentgrowth, \n        fit.measures = T, \n        standardized = T)\n\nlavaan 0.6-19 ended normally after 111 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n  Number of equality constraints                     5\n\n  Number of observations                           200\n\nModel Test User Model:\n                                                      \n  Test statistic                                20.473\n  Degrees of freedom                                23\n  P-value (Chi-square)                           0.613\n\nModel Test Baseline Model:\n\n  Test statistic                              4123.548\n  Degrees of freedom                                21\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.001\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3806.202\n  Loglikelihood unrestricted model (H1)      -3795.966\n                                                      \n  Akaike (AIC)                                7632.405\n  Bayesian (BIC)                              7665.388\n  Sample-size adjusted Bayesian (SABIC)       7633.707\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.051\n  P-value H_0: RMSEA &lt;= 0.050                    0.947\n  P-value H_0: RMSEA &gt;= 0.080                    0.001\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.006\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    voc_0             1.000                              28.047    0.994\n    voc_1             1.000                              28.047    0.975\n    voc_2             1.000                              28.047    0.931\n    voc_3             1.000                              28.047    0.870\n    voc_4             1.000                              28.047    0.802\n    voc_5             1.000                              28.047    0.734\n  s =~                                                                  \n    voc_0             0.000                               0.000    0.000\n    voc_1             1.000                               4.781    0.166\n    voc_2             2.000                               9.563    0.317\n    voc_3             3.000                              14.344    0.445\n    voc_4             4.000                              19.126    0.547\n    voc_5             5.000                              23.907    0.625\n  q =~                                                                  \n    voc_0             0.000                               0.000    0.000\n    voc_1             1.000                               0.043    0.001\n    voc_2             4.000                               0.171    0.006\n    voc_3             9.000                               0.386    0.012\n    voc_4            16.000                               0.686    0.020\n    voc_5            25.000                               1.072    0.028\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i ~                                                                   \n    intrvnt (b_01)   -0.086    3.988   -0.022    0.983   -0.003   -0.002\n  s ~                                                                   \n    intrvnt (b_11)    9.138    0.414   22.079    0.000    1.911    0.955\n  q ~                                                                   \n    intrvnt (b_21)    0.086    0.069    1.236    0.217    2.002    1.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .i ~~                                                                  \n   .s       (t_01)    4.602    3.175    1.449    0.147    0.115    0.115\n .s ~~                                                                  \n   .q                 0.000                                 NaN      NaN\n .i ~~                                                                  \n   .q                 0.000                                 NaN      NaN\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .i       (b_00)   52.002    2.876   18.082    0.000    1.854    1.854\n   .s       (b_10)   10.500    0.298   35.181    0.000    2.196    2.196\n   .q       (b_20)   -1.056    0.050  -21.097    0.000  -24.647  -24.647\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .i       (t_00)  786.620   79.133    9.940    0.000    1.000    1.000\n   .s       (t_11)    2.019    0.255    7.932    0.000    0.088    0.088\n   .q                 0.000                               0.000    0.000\n   .voc_0      (e)    8.987    0.449   20.000    0.000    8.987    0.011\n   .voc_1      (e)    8.987    0.449   20.000    0.000    8.987    0.011\n   .voc_2      (e)    8.987    0.449   20.000    0.000    8.987    0.010\n   .voc_3      (e)    8.987    0.449   20.000    0.000    8.987    0.009\n   .voc_4      (e)    8.987    0.449   20.000    0.000    8.987    0.007\n   .voc_5      (e)    8.987    0.449   20.000    0.000    8.987    0.006\n\n\nNotice that we went straight to the final model. We could have done a sequence of steps identical to the hierarchical linear model.\nI will show the sequence of models and compare them."
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-0-random-intercepts",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-0-random-intercepts",
    "title": "Latent Growth Models",
    "section": "Model 0: Random Intercepts",
    "text": "Model 0: Random Intercepts\n\n\nCode\npd0 &lt;- ggdiagram(font_family = my_font, font_size = 14) +\n  {v &lt;- my_observed() %&gt;%\n      ob_array(\n        6,\n        where = \"south\",\n        label = my_observed_label(paste0(\"*V*~\", 0:5, \"~\")),\n        sep = 1.5,\n        fill = class_color(cD)@lighten(seq(0.7, 1, length.out = 6))\n      )} +\n  {e &lt;- my_error(fill = v@fill, label = my_error_label(paste0(\"*e*~\", 0:5, \"~\"))) %&gt;%\n      place(v, \"right\")} +\n  ob_variance(\n    e,\n    \"right\",\n    label = ob_label(\n      \"*&tau;*~1~\",\n      color = cD,\n      label.padding = margin(t = 1)\n    ),\n    color = cD,\n    looseness = 3.15,\n    theta = degree(60)\n  ) +\n  my_connect(e, v, color = v@fill) +\n  {i &lt;- my_latent(\n      label = my_latent_label(\n        \"*i*&lt;br&gt;&lt;span style='font-size: 18pt'&gt;Intercept&lt;/span&gt;&lt;br&gt;*b*~0*i*~\",\n        lineheight = 1,\n        size = 20\n      )\n    ) %&gt;% place(midpoint(v[1], v[2]), \"left\", left)} +\n  {li &lt;- my_connect(i, v@point_at(180 + seq(0, -10, -2)))} +\n  ob_label(\n    1,\n    center = map_ob(li, \\(x) intersection(x, ls[1]@nudge(y = 1.1))),\n    label.padding = margin(1, 0, 0, 0),\n    color = cA\n  ) +\n  {u_0i &lt;- my_error(radius = 1, label = my_error_label(\"*u*~0*i*~\")) %&gt;% place(i, \"left\", left)} +\n  my_connect(u_0i, i) +\n  {t1 &lt;- ob_intercept(\n      intersection(connect(u_0i, s), connect(u_1i, i)) + ob_point(-1.85, 0),\n      width = 3,\n      fill = cD,\n      color = NA,\n      vertex_radius = .004,\n      label = my_observed_label(\"1\", vjust = .4)\n    )} +\n  {b00 &lt;- my_connect(t1, i)} +\n  {vl &lt;- ob_segment(my_connect(u_0i, i)@midpoint(.87),\n                     my_connect(u_2i, q)@midpoint(.87), alpha = 0)} +\n     ob_label(\n      \"*b*~00~\",\n      intersection(b00, vl),\n      label.padding = margin(2),\n      color = cA\n    ) +\n  ob_variance(\n    u_0i,\n    where = \"west\",\n    color = cA,\n    looseness = 1.75,\n    label = ob_label(paste0(\"*&tau;*~00~\"), color = cA))\npd0\n\n\n\n\n\n\n\n\nFigure 12: Path Diagram for Model 0\n\n\n\n\n\nAs usual, we start with a simple model with random intercepts. Time is not yet included in the model. The latent variable i has a mean of b_{00} and a variance of \\tau_{00}. The variances of error terms for each observed variable are constrained to be equal (\\tau_1). The intercepts of the observed variables are not modeled here, so they are assumbed to be 0.\n\nm_0 &lt;- '\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 \n# Variances and covariances\ni ~~ tau_00 * i \nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n'\n\nfit_0 &lt;- growth(m_0, data = d_wide)\n# The standard way to get output from lavaan\nsummary(fit_0, fit.measures = T, standardized = T)\n\nlavaan 0.6-19 ended normally after 18 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         8\n  Number of equality constraints                     5\n\n  Number of observations                           200\n\nModel Test User Model:\n                                                      \n  Test statistic                              3228.267\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              3659.200\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.121\n  Tucker-Lewis Index (TLI)                       0.450\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5642.273\n  Loglikelihood unrestricted model (H1)      -4028.140\n                                                      \n  Akaike (AIC)                               11290.547\n  Bayesian (BIC)                             11300.442\n  Sample-size adjusted Bayesian (SABIC)      11290.938\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.817\n  90 Percent confidence interval - lower         0.793\n  90 Percent confidence interval - upper         0.841\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.357\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    voc_0             1.000                              29.736    0.809\n    voc_1             1.000                              29.736    0.809\n    voc_2             1.000                              29.736    0.809\n    voc_3             1.000                              29.736    0.809\n    voc_4             1.000                              29.736    0.809\n    voc_5             1.000                              29.736    0.809\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    i       (b_00)   80.811    2.193   36.844    0.000    2.718    2.718\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    i       (t_00)  884.245   96.275    9.185    0.000    1.000    1.000\n   .voc_0      (e)  467.273   20.897   22.361    0.000  467.273    0.346\n   .voc_1      (e)  467.273   20.897   22.361    0.000  467.273    0.346\n   .voc_2      (e)  467.273   20.897   22.361    0.000  467.273    0.346\n   .voc_3      (e)  467.273   20.897   22.361    0.000  467.273    0.346\n   .voc_4      (e)  467.273   20.897   22.361    0.000  467.273    0.346\n   .voc_5      (e)  467.273   20.897   22.361    0.000  467.273    0.346\n\n# The easystats way\nparameters(fit_0, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |       95% CI |      p\n-------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\n\n# Mean\n\nLink         | Coefficient |   SE |         95% CI |     z |      p\n-------------------------------------------------------------------\ni ~1  (b_00) |       80.81 | 2.19 | [76.51, 85.11] | 36.84 | &lt; .001\nvoc_0 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_1 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_2 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_3 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_4 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_5 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\n\n# Variance\n\nLink               | Coefficient |    SE |            95% CI |     z |      p\n-----------------------------------------------------------------------------\ni ~~ i (tau_00)    |      884.25 | 96.28 | [695.55, 1072.94] |  9.18 | &lt; .001\nvoc_0 ~~ voc_0 (e) |      467.27 | 20.90 | [426.32,  508.23] | 22.36 | &lt; .001\nvoc_1 ~~ voc_1 (e) |      467.27 | 20.90 | [426.32,  508.23] | 22.36 | &lt; .001\nvoc_2 ~~ voc_2 (e) |      467.27 | 20.90 | [426.32,  508.23] | 22.36 | &lt; .001\nvoc_3 ~~ voc_3 (e) |      467.27 | 20.90 | [426.32,  508.23] | 22.36 | &lt; .001\nvoc_4 ~~ voc_4 (e) |      467.27 | 20.90 | [426.32,  508.23] | 22.36 | &lt; .001\nvoc_5 ~~ voc_5 (e) |      467.27 | 20.90 | [426.32,  508.23] | 22.36 | &lt; .001\n\n# There are many fit indices. https://easystats.github.io/performance/reference/model_performance.lavaan.html\n# Here are some common indices:\nmy_metrics &lt;- c(\"CFI\", \"NNFI\", \"AGFI\", \"RMSEA\", \"AIC\", \"BIC\")\nperformance(fit_0, metrics = my_metrics)\n\n# Indices of model performance\n\nCFI   |  NNFI |  AGFI | RMSEA |       AIC |       BIC\n-----------------------------------------------------\n0.121 | 0.450 | 0.426 | 0.817 | 11290.547 | 11300.442\n\n\nNot surprisingly, the fit statistics of our null model indicate poor fit. The CFI (Comparative Fit Index), NNFI (Non-Normed Fit Index), and AGFI (Adjusted Goodness of Fit) approach 1 when the fit is good. There are no absolute standards for these fit indices, and good values can vary from model to model, but in general look for values better than .90. The RMSEA (Root Mean Squared Error of Approximation) approaches 0 when fit is good. Again, there is no absolute standard for the RMSEA but in general look for values below .10 and be pleased when values are below .05.\nThe reason that the data fit poorly can be seen clearly in Figure 13. Although vocabulary clearly increases over time, Model 0 constrains the prediction to not grow—to be the same for each person across time.\n\n\nCode\nmy_predictions &lt;- function(fit) {\n  lavPredict(fit, type = \"ov\") %&gt;%\n    as_tibble() %&gt;%\n    mutate(across(everything(), .fns = as.numeric)) %&gt;%\n    mutate(person_id = dplyr::row_number()) %&gt;%\n    pivot_longer(\n      starts_with(\"voc_\"),\n      values_to = \"vocabulary\",\n      names_to = \"time\",\n      names_prefix = \"voc_\",\n      names_transform = list(time = as.integer)\n    ) %&gt;%\n    mutate(person_id = factor(person_id) %&gt;% fct_reorder(vocabulary))\n}\n\nggplot(d, aes(time, vocabulary)) +\n  ggbeeswarm::geom_quasirandom(width = .2, size = .5, alpha = .5) +\n  geom_line(\n    data = predict(fit_0) %&gt;%\n      as_tibble() %&gt;%\n      mutate(across(everything(), .fns = as.numeric)) %&gt;%\n      mutate(person_id = dplyr::row_number()) %&gt;%\n      crossing(time = 0:5) %&gt;%\n      mutate(vocabulary = i ),\n    aes(group = person_id),\n    alpha = .25,\n  ) +\n  stat_function(\n    data = tibble(x = 0:5),\n    geom = \"line\",\n    fun = \\(x) coef(fit_0)[\"b_00\"] ,\n    inherit.aes = F,\n    color = \"dodgerblue3\",\n    linewidth = 2\n  ) +\n  theme_minimal(base_size = 20, base_family = my_font) +\n  labs(x = \"Time\", y = \"Vocabulary\") \n\n\n\n\n\n\n\n\nFigure 13: Model 0: Random Intercepts)\n\n\n\n\n\nAs seen in Table 7, the various parameters of the analogous hierarchical linear model are nearly identical—if maximum likelihood is used instead of restricted maximum likelihood (i.e., REML = FALSE)\n\nlmer(vocabulary ~ 1  + (1 | person_id), data = d, REML = FALSE) %&gt;%\n  tab_model()\n\n\n\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n80.81\n76.51 – 85.11\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n467.27\n\n\n\nτ00 person_id\n884.25\n\n\nICC\n0.65\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.000 / 0.654\n\n\n\n\n\n\nTable 7: HLM Model Analogous to Model 0"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-1-model-0-fixed-linear-effect-of-time",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-1-model-0-fixed-linear-effect-of-time",
    "title": "Latent Growth Models",
    "section": "Model 1: Model 0 + Fixed Linear Effect of Time",
    "text": "Model 1: Model 0 + Fixed Linear Effect of Time\n\n\nCode\npd1 &lt;- pd0 +\n  {s &lt;- my_latent(\n      label = my_latent_label(\n        \"*s*&lt;br&gt;&lt;span style='font-size: 18pt'&gt;Slope&lt;/span&gt;&lt;br&gt;*b*~1*i*~\",\n        lineheight = 1,\n        size = 20\n      ),\n      fill = cB\n    ) %&gt;%\n      place(midpoint(v[3], v[4]), \"left\", left)} +\n  {ls &lt;- my_connect(s, v@point_at(\"west\"), color = cB)} +\n  {b10 &lt;- my_connect(t1, s, color = cB)} +\n       ob_label(\n      \"*b*~10~\",\n      intersection(b10, vl),\n      label.padding = margin(2),\n      color = cB\n    ) +\n  ob_label(\n    0:5,\n    center = map_ob(ls, \\(x) intersection(\n      x, ob_circle(s@center, radius = s@radius + .65)\n    )),\n    label.padding = margin(1, 0, 0, 0),\n    color = cB\n  ) \npd1\n\n\n\n\n\n\n\n\nFigure 14: Path Diagram for Model 1\n\n\n\n\n\nWe add a fixed slope for time (latent variable s), setting the residual variance of s and its covariance with the intercept i to 0.\n\nm_1 &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 \ns ~ b_10 * 1\n# Variances and covariances\ni ~~ tau_00 * i \ns ~~ 0 * s + 0 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n\"\n\nfit_1 &lt;- growth(m_1, data = d_wide)\nparameters(fit_1, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |       95% CI |      p\n-------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ns =~ voc_0 |        0.00 | 0.00 | [0.00, 0.00] | &lt; .001\ns =~ voc_1 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ns =~ voc_2 |        2.00 | 0.00 | [2.00, 2.00] | &lt; .001\ns =~ voc_3 |        3.00 | 0.00 | [3.00, 3.00] | &lt; .001\ns =~ voc_4 |        4.00 | 0.00 | [4.00, 4.00] | &lt; .001\ns =~ voc_5 |        5.00 | 0.00 | [5.00, 5.00] | &lt; .001\n\n# Mean\n\nLink         | Coefficient |   SE |         95% CI |     z |      p\n-------------------------------------------------------------------\ni ~1  (b_00) |       55.33 | 2.24 | [50.95, 59.71] | 24.75 | &lt; .001\ns ~1  (b_10) |       10.19 | 0.17 | [ 9.86, 10.53] | 59.22 | &lt; .001\nvoc_0 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_1 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_2 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_3 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_4 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_5 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\n\n# Variance\n\nLink               | Coefficient |    SE |            95% CI |     z |      p\n-----------------------------------------------------------------------------\ni ~~ i (tau_00)    |      944.85 | 96.22 | [756.27, 1133.42] |  9.82 | &lt; .001\ns ~~ s             |        0.00 |  0.00 | [  0.00,    0.00] |       | &lt; .001\nvoc_0 ~~ voc_0 (e) |      103.67 |  4.64 | [ 94.59,  112.76] | 22.36 | &lt; .001\nvoc_1 ~~ voc_1 (e) |      103.67 |  4.64 | [ 94.59,  112.76] | 22.36 | &lt; .001\nvoc_2 ~~ voc_2 (e) |      103.67 |  4.64 | [ 94.59,  112.76] | 22.36 | &lt; .001\nvoc_3 ~~ voc_3 (e) |      103.67 |  4.64 | [ 94.59,  112.76] | 22.36 | &lt; .001\nvoc_4 ~~ voc_4 (e) |      103.67 |  4.64 | [ 94.59,  112.76] | 22.36 | &lt; .001\nvoc_5 ~~ voc_5 (e) |      103.67 |  4.64 | [ 94.59,  112.76] | 22.36 | &lt; .001\n\n# Correlation\n\nLink   | Coefficient |   SE |       95% CI |      p\n---------------------------------------------------\ni ~~ s |        0.00 | 0.00 | [0.00, 0.00] | &lt; .001\n\n\nLet’s see if adding the linear effect of time improved model fit:\n\n# Compare new model with the previous model\ntest_likelihoodratio(fit_0, fit_1)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nModel |   Type | df | df_diff |    Chi2 |      p\n------------------------------------------------\nfit_0 | lavaan | 24 |       1 | 3228.27 | &lt; .001\nfit_1 | lavaan | 23 |         | 1722.61 |       \n\ncompare_performance(fit_0, fit_1, metrics = my_metrics)\n\n# Comparison of Model Performance Indices\n\nName  |  Model |   CFI |  NNFI |  AGFI | RMSEA\n----------------------------------------------\nfit_0 | lavaan | 0.121 | 0.450 | 0.426 | 0.817\nfit_1 | lavaan | 0.534 | 0.696 | 0.726 | 0.608\n\n\nBecause the p-value is significant, adding the linear effect of time improved the model fit.\n\n\nCode\nggplot(d, aes(time, vocabulary)) +\n  ggbeeswarm::geom_quasirandom(width = .2, size = .5, alpha = .5) +\n  geom_line(\n    data = predict(fit_1) %&gt;%\n      as_tibble() %&gt;%\n      mutate(across(everything(), .fns = as.numeric)) %&gt;%\n      mutate(person_id = dplyr::row_number()) %&gt;%\n      crossing(time = 0:5) %&gt;%\n      mutate(vocabulary = i + s * time),\n    aes(group = person_id),\n    alpha = .25,\n  ) +\n  stat_function(\n    data = tibble(x = 0:5),\n    geom = \"line\",\n    fun = \\(x) coef(fit_1)[\"b_00\"] + coef(fit_1)[\"b_10\"] * x ,\n    inherit.aes = F,\n    color = \"dodgerblue3\",\n    linewidth = 2\n  ) +\n  theme_minimal(base_size = 20, base_family = my_font) +\n  labs(x = \"Time\", y = \"Vocabulary\") \n\n\n\n\n\n\n\n\nFigure 15: Model 1: Predicted Vocabulary Over Time (Model 0 + Linear Effect of Time)\n\n\n\n\n\nAs seen in Table 8, the various parameters of the analogous hierarchical linear model are nearly identical.\n\nlmer(vocabulary ~ 1 + time + (1 | person_id), \n               data = d, \n               REML = FALSE) %&gt;% \n  tab_model()\n\n\n\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n55.33\n50.94 – 59.72\n&lt;0.001\n\n\ntime\n10.19\n9.85 – 10.53\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n103.67\n\n\n\nτ00 person_id\n944.84\n\n\nICC\n0.90\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.224 / 0.923\n\n\n\n\n\n\nTable 8: HLM Model Analogous to Model 1"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-2-model-1-quadratic-effect-of-time",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-2-model-1-quadratic-effect-of-time",
    "title": "Latent Growth Models",
    "section": "Model 2: Model 1 + Quadratic Effect of Time",
    "text": "Model 2: Model 1 + Quadratic Effect of Time\n\n\nCode\npd2 &lt;- pd1 +\n  {q &lt;- my_latent(\n      label = my_latent_label(\n        \"*q*&lt;br&gt;&lt;span style='font-size: 18pt'&gt;Slope^2^&lt;/span&gt;&lt;br&gt;*b*~2*i*~\",\n        lineheight = 1,\n        size = 20\n      ),\n      fill = cC\n    ) %&gt;%\n      place(midpoint(v[5], v[6]), \"left\", left)} +\n  {lq &lt;- my_connect(q, v@point_at(180 + seq(10, 0, -2)), color = cC)} +\n  ob_label((0:5)^2,\n           center = map_ob(lq, \\(x) intersection(x, ls[6]@nudge(y = -1.1))),\n           label.padding = margin(1, 0, 0, 0),\n           color = cC\n  ) +\n  {b20 &lt;- my_connect(t1, q, color = cC)} +\n     ob_label(\n      \"*b*~20~\",\n      intersection(b20, vl),\n      label.padding = margin(2),\n      color = cC\n    ) \npd2\n\n\n\n\n\n\n\n\nFigure 16: Path Diagram for Model 2\n\n\n\n\n\nWe set a fixed quadratic effect for time, with the residual variance set to 0.\n\nm_2 &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Acceleration effect of time (quadratic effect)\nq =~ 0 * voc_0 + 1 * voc_1 + 4 * voc_2 + 9 * voc_3 + 16 * voc_4 + 25 * voc_5\n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 \ns ~ b_10 * 1\nq ~ b_20 * 1\n# Variances and covariances\ni ~~ tau_00 * i \ns ~~ 0 * s + 0 * i\nq ~~ 0 * q + 0 * s + 0 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n\"\n\nfit_2 &lt;- growth(m_2, data = d_wide)\nparameters(fit_2, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |         95% CI |      p\n---------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\ns =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_2 |        2.00 | 0.00 | [ 2.00,  2.00] | &lt; .001\ns =~ voc_3 |        3.00 | 0.00 | [ 3.00,  3.00] | &lt; .001\ns =~ voc_4 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\ns =~ voc_5 |        5.00 | 0.00 | [ 5.00,  5.00] | &lt; .001\nq =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\nq =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\nq =~ voc_2 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\nq =~ voc_3 |        9.00 | 0.00 | [ 9.00,  9.00] | &lt; .001\nq =~ voc_4 |       16.00 | 0.00 | [16.00, 16.00] | &lt; .001\nq =~ voc_5 |       25.00 | 0.00 | [25.00, 25.00] | &lt; .001\n\n# Mean\n\nLink         | Coefficient |   SE |         95% CI |     z |      p\n-------------------------------------------------------------------\ni ~1  (b_00) |       51.96 | 2.26 | [47.52, 56.39] | 22.95 | &lt; .001\ns ~1  (b_10) |       15.25 | 0.59 | [14.09, 16.41] | 25.82 | &lt; .001\nq ~1  (b_20) |       -1.01 | 0.11 | [-1.23, -0.79] | -8.92 | &lt; .001\nvoc_0 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_1 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_2 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_3 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_4 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_5 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\n\n# Variance\n\nLink               | Coefficient |    SE |            95% CI |     z |      p\n-----------------------------------------------------------------------------\ni ~~ i (tau_00)    |      946.11 | 96.21 | [757.54, 1134.69] |  9.83 | &lt; .001\ns ~~ s             |        0.00 |  0.00 | [  0.00,    0.00] |       | &lt; .001\nq ~~ q             |        0.00 |  0.00 | [  0.00,    0.00] |       | &lt; .001\nvoc_0 ~~ voc_0 (e) |       96.03 |  4.29 | [ 87.61,  104.45] | 22.36 | &lt; .001\nvoc_1 ~~ voc_1 (e) |       96.03 |  4.29 | [ 87.61,  104.45] | 22.36 | &lt; .001\nvoc_2 ~~ voc_2 (e) |       96.03 |  4.29 | [ 87.61,  104.45] | 22.36 | &lt; .001\nvoc_3 ~~ voc_3 (e) |       96.03 |  4.29 | [ 87.61,  104.45] | 22.36 | &lt; .001\nvoc_4 ~~ voc_4 (e) |       96.03 |  4.29 | [ 87.61,  104.45] | 22.36 | &lt; .001\nvoc_5 ~~ voc_5 (e) |       96.03 |  4.29 | [ 87.61,  104.45] | 22.36 | &lt; .001\n\n# Correlation\n\nLink   | Coefficient |   SE |       95% CI |      p\n---------------------------------------------------\ni ~~ s |        0.00 | 0.00 | [0.00, 0.00] | &lt; .001\ns ~~ q |        0.00 | 0.00 | [0.00, 0.00] | &lt; .001\ni ~~ q |        0.00 | 0.00 | [0.00, 0.00] | &lt; .001\n\n# Compare new model with the previous models\ntest_likelihoodratio(fit_0, fit_1, fit_2)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nModel |   Type | df | df_diff |    Chi2 |      p\n------------------------------------------------\nfit_0 | lavaan | 24 |       1 | 3228.27 | &lt; .001\nfit_1 | lavaan | 23 |       1 | 1722.61 | &lt; .001\nfit_2 | lavaan | 22 |         | 1646.01 |       \n\ncompare_performance(fit_0, fit_1, fit_2, \n                    metrics = my_metrics,\n                    verbose = F)\n\n# Comparison of Model Performance Indices\n\nName  |  Model |   CFI |  NNFI |  AGFI | RMSEA\n----------------------------------------------\nfit_0 | lavaan | 0.121 | 0.450 | 0.426 | 0.817\nfit_1 | lavaan | 0.534 | 0.696 | 0.726 | 0.608\nfit_2 | lavaan | 0.554 | 0.696 | 0.709 | 0.608\n\n\nThe quadratic effect improves fit.\n\n\nCode\nggplot(d, aes(time, vocabulary)) +\n  ggbeeswarm::geom_quasirandom(width = .2, size = .5, alpha = .5) +\n  geom_line(\n    data = predict(fit_2) %&gt;%\n      as_tibble() %&gt;%\n      mutate(across(everything(), .fns = as.numeric)) %&gt;%\n      mutate(person_id = dplyr::row_number()) %&gt;%\n      crossing(time = 0:5) %&gt;%\n      mutate(vocabulary = i + s * time + q * time^2),\n    aes(group = person_id),\n    alpha = .25,\n  ) +\n  stat_function(\n    data = tibble(x = 0:5),\n    geom = \"line\",\n    fun = \\(x) coef(fit_2)[\"b_00\"] + coef(fit_2)[\"b_10\"] * x + coef(fit_2)[\"b_20\"] * (x^2),\n    inherit.aes = F,\n    color = \"dodgerblue3\",\n    linewidth = 2\n  ) +\n  theme_minimal(base_size = 20, base_family = my_font) +\n  labs(x = \"Time\", y = \"Vocabulary\") \n\n\n\n\n\n\n\n\nFigure 17: Model 2: Predicted Vocabulary by Intervention Group Over Time (Model 1 + Quadratic Effect of Time)\n\n\n\n\n\nAgain, Table 9 shows that the same results would have been found with the analogous HLM model—and with much less typing! Why are we torturing ourselves like this? There will a payoff to latent growth modeling, just not yet.\n\nlmer(vocabulary ~ 1 + time + I(time^2) + (1 | person_id),\n     data = d,\n     REML = FALSE) %&gt;%\n  tab_model()\n\n\n\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n51.96\n47.52 – 56.40\n&lt;0.001\n\n\ntime\n15.25\n14.09 – 16.41\n&lt;0.001\n\n\ntime^2\n-1.01\n-1.23 – -0.79\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n96.03\n\n\n\nτ00 person_id\n946.12\n\n\nICC\n0.91\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.229 / 0.929\n\n\n\n\n\n\nTable 9: HLM Model Analogous to Model 2"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-3-model-2-random-linear-time-effect",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-3-model-2-random-linear-time-effect",
    "title": "Latent Growth Models",
    "section": "Model 3: Model 2 + Random Linear Time Effect",
    "text": "Model 3: Model 2 + Random Linear Time Effect\n\n\nCode\npd3 &lt;- pd2 + \n  {u_1i &lt;- my_error(radius = 1,\n                     label = my_error_label(\"*u*~1*i*~\"),\n                     fill = cB) %&gt;% place(s, \"left\", left)} +\n    my_connect(u_1i, s, color = cB) +\n  ob_variance(\n    u_1i,\n    where = \"west\",\n    color = cB,\n    looseness = 1.75,\n    label = ob_label(paste0(\"*&tau;*~11~\"), color = cB))\npd3\n\n\n\n\n\n\n\n\nFigure 18: Path Diagram for Model 3\n\n\n\n\n\n\nm_3 &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Acceleration effect of time (quadratic effect)\nq =~ 0 * voc_0 + 1 * voc_1 + 4 * voc_2 + 9 * voc_3 + 16 * voc_4 + 25 * voc_5\n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 \ns ~ b_10 * 1\nq ~ b_20 * 1\n# Variances and covariances\ni ~~ tau_00 * i \ns ~~ tau_11 * s + 0 * i\nq ~~ 0 * q + 0 * s + 0 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n\"\n\nfit_3 &lt;- growth(m_3, data = d_wide)\nparameters(fit_3, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |         95% CI |      p\n---------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\ns =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_2 |        2.00 | 0.00 | [ 2.00,  2.00] | &lt; .001\ns =~ voc_3 |        3.00 | 0.00 | [ 3.00,  3.00] | &lt; .001\ns =~ voc_4 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\ns =~ voc_5 |        5.00 | 0.00 | [ 5.00,  5.00] | &lt; .001\nq =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\nq =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\nq =~ voc_2 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\nq =~ voc_3 |        9.00 | 0.00 | [ 9.00,  9.00] | &lt; .001\nq =~ voc_4 |       16.00 | 0.00 | [16.00, 16.00] | &lt; .001\nq =~ voc_5 |       25.00 | 0.00 | [25.00, 25.00] | &lt; .001\n\n# Mean\n\nLink         | Coefficient |   SE |         95% CI |      z |      p\n--------------------------------------------------------------------\ni ~1  (b_00) |       51.96 | 1.99 | [48.05, 55.86] |  26.07 | &lt; .001\ns ~1  (b_10) |       15.25 | 0.40 | [14.47, 16.03] |  38.48 | &lt; .001\nq ~1  (b_20) |       -1.01 | 0.03 | [-1.08, -0.94] | -29.14 | &lt; .001\nvoc_0 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_1 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_2 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_3 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_4 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_5 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\n\n# Variance\n\nLink               | Coefficient |    SE |           95% CI |     z |      p\n----------------------------------------------------------------------------\ni ~~ i (tau_00)    |      787.02 | 79.17 | [631.86, 942.19] |  9.94 | &lt; .001\ns ~~ s (tau_11)    |       24.88 |  2.54 | [ 19.90,  29.85] |  9.80 | &lt; .001\nq ~~ q             |        0.00 |  0.00 | [  0.00,   0.00] |       | &lt; .001\nvoc_0 ~~ voc_0 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_1 ~~ voc_1 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_2 ~~ voc_2 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_3 ~~ voc_3 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_4 ~~ voc_4 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_5 ~~ voc_5 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\n\n# Correlation\n\nLink   | Coefficient |   SE |       95% CI |      p\n---------------------------------------------------\ni ~~ s |        0.00 | 0.00 | [0.00, 0.00] | &lt; .001\ns ~~ q |        0.00 | 0.00 | [0.00, 0.00] | &lt; .001\ni ~~ q |        0.00 | 0.00 | [0.00, 0.00] | &lt; .001\n\n# Compare new model with the previous models\ntest_likelihoodratio(fit_0, fit_1, fit_2, fit_3)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nModel |   Type | df | df_diff |    Chi2 |      p\n------------------------------------------------\nfit_0 | lavaan | 24 |       1 | 3228.27 | &lt; .001\nfit_1 | lavaan | 23 |       1 | 1722.61 | &lt; .001\nfit_2 | lavaan | 22 |       1 | 1646.01 | &lt; .001\nfit_3 | lavaan | 21 |         |   19.75 |       \n\ncompare_performance(fit_0, fit_1, fit_2, fit_3, metrics = my_metrics) \n\n# Comparison of Model Performance Indices\n\nName  |  Model |   CFI |  NNFI |  AGFI | RMSEA\n----------------------------------------------\nfit_0 | lavaan | 0.121 | 0.450 | 0.426 | 0.817\nfit_1 | lavaan | 0.534 | 0.696 | 0.726 | 0.608\nfit_2 | lavaan | 0.554 | 0.696 | 0.709 | 0.608\nfit_3 | lavaan | 1.000 | 1.000 | 0.992 | 0.000\n\n\nIncluding the random linear effect of time improves model fit.\nModel 3 plot\n\n\nCode\nggplot(d, aes(time, vocabulary)) +\n  ggbeeswarm::geom_quasirandom(width = .2, size = .5, alpha = .5) +\n  geom_line(\n    data = predict(fit_3) %&gt;%\n      as_tibble() %&gt;%\n      mutate(across(everything(), .fns = as.numeric)) %&gt;%\n      mutate(person_id = dplyr::row_number()) %&gt;%\n      crossing(time = 0:5) %&gt;%\n      mutate(vocabulary = i + s * time + q * time^2),\n    aes(group = person_id),\n    alpha = .25,\n  ) +\n  stat_function(\n    data = tibble(x = 0:5),\n    geom = \"line\",\n    fun = \\(x) coef(fit_3)[\"b_00\"] + coef(fit_3)[\"b_10\"] * x + coef(fit_3)[\"b_20\"] * (x^2),\n    inherit.aes = F,\n    color = \"dodgerblue3\",\n    linewidth = 2\n  ) +\n  theme_minimal(base_size = 20, base_family = my_font) +\n  labs(x = \"Time\", y = \"Vocabulary\") \n\n\n\n\n\n\n\n\nFigure 19: Model 3: Predicted Vocabulary by Intervention Group Over Time (Model 2 + Random Linear Effect of Time)\n\n\n\n\n\n\nlmer(vocabulary ~ 1 + time + I(time ^ 2) + (1 + time || person_id), \n     data = d, \n     REML = FALSE) %&gt;% \n  tab_model()\n\n\n\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n51.96\n48.05 – 55.87\n&lt;0.001\n\n\ntime\n15.25\n14.47 – 16.03\n&lt;0.001\n\n\ntime^2\n-1.01\n-1.08 – -0.94\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n9.00\n\n\n\nτ00 person_id\n787.02\n\n\nτ11 person_id.time\n24.88\n\n\nρ01\n \n\n\nρ01\n \n\n\nICC\n0.99\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.280 / 0.992\n\n\n\n\n\n\nTable 10: HLM Model Analogous to Model 3"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-4-model-3-covariance-between-random-intercepts-and-slopes",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-4-model-3-covariance-between-random-intercepts-and-slopes",
    "title": "Latent Growth Models",
    "section": "Model 4: Model 3 + Covariance between Random Intercepts and Slopes",
    "text": "Model 4: Model 3 + Covariance between Random Intercepts and Slopes\n\n\nCode\npd4 &lt;- pd3 +\n    ob_covariance(u_1i,\n                u_0i,\n                color = cD,\n                label = ob_label(\"*&tau;*~01~\", color = cD)) \npd4\n\n\n\n\n\n\n\n\nFigure 20: Path Diagram for Model 4\n\n\n\n\n\n\nm_4 &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Acceleration effect of time (quadratic effect)\nq =~ 0 * voc_0 + 1 * voc_1 + 4 * voc_2 + 9 * voc_3 + 16 * voc_4 + 25 * voc_5\n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 \ns ~ b_10 * 1\nq ~ b_20 * 1\n# Variances and covariances\ni ~~ tau_00 * i \ns ~~ tau_11 * s + tau_01 * i\nq ~~ 0 * q + 0 * s + 0 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n\"\n\nfit_4 &lt;- growth(m_4, data = d_wide)\nparameters(fit_4, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |         95% CI |      p\n---------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\ns =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_2 |        2.00 | 0.00 | [ 2.00,  2.00] | &lt; .001\ns =~ voc_3 |        3.00 | 0.00 | [ 3.00,  3.00] | &lt; .001\ns =~ voc_4 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\ns =~ voc_5 |        5.00 | 0.00 | [ 5.00,  5.00] | &lt; .001\nq =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\nq =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\nq =~ voc_2 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\nq =~ voc_3 |        9.00 | 0.00 | [ 9.00,  9.00] | &lt; .001\nq =~ voc_4 |       16.00 | 0.00 | [16.00, 16.00] | &lt; .001\nq =~ voc_5 |       25.00 | 0.00 | [25.00, 25.00] | &lt; .001\n\n# Mean\n\nLink         | Coefficient |   SE |         95% CI |      z |      p\n--------------------------------------------------------------------\ni ~1  (b_00) |       51.96 | 1.99 | [48.05, 55.86] |  26.08 | &lt; .001\ns ~1  (b_10) |       15.25 | 0.40 | [14.47, 16.03] |  38.49 | &lt; .001\nq ~1  (b_20) |       -1.01 | 0.03 | [-1.08, -0.94] | -29.14 | &lt; .001\nvoc_0 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_1 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_2 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_3 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_4 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_5 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\n\n# Variance\n\nLink               | Coefficient |    SE |           95% CI |     z |      p\n----------------------------------------------------------------------------\ni ~~ i (tau_00)    |      786.65 | 79.14 | [631.54, 941.75] |  9.94 | &lt; .001\ns ~~ s (tau_11)    |       24.86 |  2.54 | [ 19.89,  29.84] |  9.80 | &lt; .001\nq ~~ q             |        0.00 |  0.00 | [  0.00,   0.00] |       | &lt; .001\nvoc_0 ~~ voc_0 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_1 ~~ voc_1 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_2 ~~ voc_2 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_3 ~~ voc_3 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_4 ~~ voc_4 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_5 ~~ voc_5 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\n\n# Correlation\n\nLink            | Coefficient |    SE |          95% CI |    z |      p\n-----------------------------------------------------------------------\ni ~~ s (tau_01) |        3.71 | 10.02 | [-15.93, 23.36] | 0.37 | 0.711 \ns ~~ q          |        0.00 |  0.00 | [  0.00,  0.00] |      | &lt; .001\ni ~~ q          |        0.00 |  0.00 | [  0.00,  0.00] |      | &lt; .001\n\n# Compare new model with the previous models\ntest_likelihoodratio(fit_0, fit_1, fit_2, fit_3, fit_4)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nModel |   Type | df | df_diff |    Chi2 |      p\n------------------------------------------------\nfit_0 | lavaan | 24 |       1 | 3228.27 | &lt; .001\nfit_1 | lavaan | 23 |       1 | 1722.61 | &lt; .001\nfit_2 | lavaan | 22 |       1 | 1646.01 | &lt; .001\nfit_3 | lavaan | 21 |       1 |   19.75 |  0.711\nfit_4 | lavaan | 20 |         |   19.61 |       \n\ncompare_performance(fit_0, fit_1, fit_2, fit_3, fit_4, metrics = my_metrics)\n\n# Comparison of Model Performance Indices\n\nName  |  Model |   CFI |  NNFI |  AGFI | RMSEA\n----------------------------------------------\nfit_0 | lavaan | 0.121 | 0.450 | 0.426 | 0.817\nfit_1 | lavaan | 0.534 | 0.696 | 0.726 | 0.608\nfit_2 | lavaan | 0.554 | 0.696 | 0.709 | 0.608\nfit_3 | lavaan | 1.000 | 1.000 | 0.992 | 0.000\nfit_4 | lavaan | 1.000 | 1.000 | 0.992 | 0.000\n\n\nThe covariance is not significantly different from 0. I am going to leave it in for now because I want to test the random quadratic effect.\n\n\nCode\nggplot(d, aes(time, vocabulary)) +\n  ggbeeswarm::geom_quasirandom(width = .2, size = .5, alpha = .5) +\n  geom_line(\n    data = predict(fit_4) %&gt;%\n      as_tibble() %&gt;%\n      mutate(across(everything(), .fns = as.numeric)) %&gt;%\n      mutate(person_id = dplyr::row_number()) %&gt;%\n      crossing(time = 0:5) %&gt;%\n      mutate(vocabulary = i + s * time + q * time^2),\n    aes(group = person_id),\n    alpha = .25,\n  ) +\n  stat_function(\n    data = tibble(x = 0:5),\n    geom = \"line\",\n    fun = \\(x) coef(fit_4)[\"b_00\"] + coef(fit_4)[\"b_10\"] * x + coef(fit_4)[\"b_20\"] * (x^2),\n    inherit.aes = F,\n    color = \"dodgerblue3\",\n    linewidth = 2\n  ) +\n  theme_minimal(base_size = 20, base_family = my_font) +\n  labs(x = \"Time\", y = \"Vocabulary\") \n\n\n\n\n\n\n\n\nFigure 21: Model 4: Predicted Vocabulary Over Time (Model 3 + Correlated Slopes and Intercepts)\n\n\n\n\n\n\nlmer(vocabulary ~ 1 + time + I(time ^ 2) + (1 + time | person_id), \n     data = d, \n     REML = FALSE) %&gt;% \n  tab_model()\n\n\n\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n51.96\n48.05 – 55.87\n&lt;0.001\n\n\ntime\n15.25\n14.47 – 16.03\n&lt;0.001\n\n\ntime^2\n-1.01\n-1.08 – -0.94\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n9.00\n\n\n\nτ00 person_id\n786.64\n\n\nτ11 person_id.time\n24.86\n\n\nρ01 person_id\n0.03\n\n\nICC\n0.99\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.229 / 0.993\n\n\n\n\n\n\nTable 11: HLM Model Analogous to Model 4"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-5-model-4-random-quadratic-effect",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-5-model-4-random-quadratic-effect",
    "title": "Latent Growth Models",
    "section": "Model 5: Model 4 + Random Quadratic Effect",
    "text": "Model 5: Model 4 + Random Quadratic Effect\n\n\nCode\npd5 &lt;- pd4 +\n  {u_2i &lt;- my_error(radius = 1,\n                     label = my_error_label(\"*u*~2*i*~\"),\n                     fill = cC) %&gt;% place(q, \"left\", left)} +\n  my_connect(u_2i, q, color = cC) +\n      ob_variance(\n    u_2i,\n    where = \"west\",\n    color = cC,\n    looseness = 1.75,\n    label = ob_label(paste0(\"*&tau;*~22~\"), color = cC)\n  ) \npd5\n\n\n\n\n\n\n\n\nFigure 22: Path Diagram for Model 5\n\n\n\n\n\n\nm_5 &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Acceleration effect of time (quadratic effect)\nq =~ 0 * voc_0 + 1 * voc_1 + 4 * voc_2 + 9 * voc_3 + 16 * voc_4 + 25 * voc_5\n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 \ns ~ b_10 * 1\nq ~ b_20 * 1\n# Variances and covariances\ni ~~ tau_00 * i \ns ~~ tau_11 * s + tau_01 * i\nq ~~ tau_22 * q + 0 * s + 0 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n\"\n\nfit_5 &lt;- growth(m_5, data = d_wide)\nparameters(fit_5, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |         95% CI |      p\n---------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\ns =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_2 |        2.00 | 0.00 | [ 2.00,  2.00] | &lt; .001\ns =~ voc_3 |        3.00 | 0.00 | [ 3.00,  3.00] | &lt; .001\ns =~ voc_4 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\ns =~ voc_5 |        5.00 | 0.00 | [ 5.00,  5.00] | &lt; .001\nq =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\nq =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\nq =~ voc_2 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\nq =~ voc_3 |        9.00 | 0.00 | [ 9.00,  9.00] | &lt; .001\nq =~ voc_4 |       16.00 | 0.00 | [16.00, 16.00] | &lt; .001\nq =~ voc_5 |       25.00 | 0.00 | [25.00, 25.00] | &lt; .001\n\n# Mean\n\nLink         | Coefficient |   SE |         95% CI |      z |      p\n--------------------------------------------------------------------\ni ~1  (b_00) |       51.96 | 1.99 | [48.06, 55.86] |  26.10 | &lt; .001\ns ~1  (b_10) |       15.25 | 0.39 | [14.49, 16.01] |  39.50 | &lt; .001\nq ~1  (b_20) |       -1.01 | 0.04 | [-1.09, -0.94] | -26.85 | &lt; .001\nvoc_0 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_1 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_2 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_3 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_4 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_5 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\n\n# Variance\n\nLink               | Coefficient |    SE |           95% CI |     z |      p\n----------------------------------------------------------------------------\ni ~~ i (tau_00)    |      785.74 | 79.07 | [630.77, 940.71] |  9.94 | &lt; .001\ns ~~ s (tau_11)    |       23.68 |  2.56 | [ 18.66,  28.71] |  9.24 | &lt; .001\nq ~~ q (tau_22)    |        0.06 |  0.03 | [  0.01,   0.11] |  2.25 | 0.024 \nvoc_0 ~~ voc_0 (e) |        8.45 |  0.47 | [  7.53,   9.36] | 18.07 | &lt; .001\nvoc_1 ~~ voc_1 (e) |        8.45 |  0.47 | [  7.53,   9.36] | 18.07 | &lt; .001\nvoc_2 ~~ voc_2 (e) |        8.45 |  0.47 | [  7.53,   9.36] | 18.07 | &lt; .001\nvoc_3 ~~ voc_3 (e) |        8.45 |  0.47 | [  7.53,   9.36] | 18.07 | &lt; .001\nvoc_4 ~~ voc_4 (e) |        8.45 |  0.47 | [  7.53,   9.36] | 18.07 | &lt; .001\nvoc_5 ~~ voc_5 (e) |        8.45 |  0.47 | [  7.53,   9.36] | 18.07 | &lt; .001\n\n# Correlation\n\nLink            | Coefficient |    SE |          95% CI |    z |      p\n-----------------------------------------------------------------------\ni ~~ s (tau_01) |        4.94 | 10.01 | [-14.68, 24.56] | 0.49 | 0.622 \ns ~~ q          |        0.00 |  0.00 | [  0.00,  0.00] |      | &lt; .001\ni ~~ q          |        0.00 |  0.00 | [  0.00,  0.00] |      | &lt; .001\n\n# Compare new model with the previous models\ntest_likelihoodratio(fit_0, fit_1, fit_2, fit_3, fit_4, fit_5)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nModel |   Type | df | df_diff |    Chi2 |      p\n------------------------------------------------\nfit_0 | lavaan | 24 |       1 | 3228.27 | &lt; .001\nfit_1 | lavaan | 23 |       1 | 1722.61 | &lt; .001\nfit_2 | lavaan | 22 |       1 | 1646.01 | &lt; .001\nfit_3 | lavaan | 21 |       1 |   19.75 |  0.711\nfit_4 | lavaan | 20 |       1 |   19.61 |  0.012\nfit_5 | lavaan | 19 |         |   13.23 |       \n\ncompare_performance(fit_0, fit_1, fit_2, fit_3, fit_4, fit_5, metrics = c())\n\n# Comparison of Model Performance Indices\n\nName  |  Model |     Chi2 | Chi2_df | p (Chi2) | Baseline(15) | p (Baseline)\n----------------------------------------------------------------------------\nfit_0 | lavaan | 3228.267 |  24.000 |   &lt; .001 |     3659.200 |       &lt; .001\nfit_1 | lavaan | 1722.613 |  23.000 |   &lt; .001 |     3659.200 |       &lt; .001\nfit_2 | lavaan | 1646.015 |  22.000 |   &lt; .001 |     3659.200 |       &lt; .001\nfit_3 | lavaan |   19.752 |  21.000 |   0.537  |     3659.200 |       &lt; .001\nfit_4 | lavaan |   19.615 |  20.000 |   0.482  |     3659.200 |       &lt; .001\nfit_5 | lavaan |   13.233 |  19.000 |   0.826  |     3659.200 |       &lt; .001\n\nName  |   GFI |  AGFI | NFI |  NNFI |   CFI | RMSEA |      RMSEA  CI\n--------------------------------------------------------------------\nfit_0 | 0.489 | 0.426 |     | 0.450 | 0.121 | 0.817 | [0.793, 0.841]\nfit_1 | 0.767 | 0.726 |     | 0.696 | 0.534 | 0.608 | [0.584, 0.632]\nfit_2 | 0.763 | 0.709 |     | 0.696 | 0.554 | 0.608 | [0.583, 0.633]\nfit_3 | 0.994 | 0.992 |     | 1.000 | 1.000 | 0.000 | [0.000, 0.056]\nfit_4 | 0.994 | 0.992 |     | 1.000 | 1.000 | 0.000 | [0.000, 0.060]\nfit_5 | 0.996 | 0.994 |     | 1.001 | 1.000 | 0.000 | [0.000, 0.038]\n\nName  | p (RMSEA) |     RMR |  SRMR | RFI |  PNFI |   IFI |   RNI\n-----------------------------------------------------------------\nfit_0 |    &lt; .001 | 222.006 | 0.357 |     | 0.188 | 0.119 | 0.121\nfit_1 |    &lt; .001 | 160.723 | 0.154 |     | 0.811 | 0.533 | 0.534\nfit_2 |    &lt; .001 | 160.688 | 0.148 |     | 0.807 | 0.553 | 0.554\nfit_3 |    0.916  |  19.138 | 0.017 |     | 1.392 | 1.000 | 1.000\nfit_4 |    0.889  |   6.180 | 0.006 |     | 1.326 | 1.000 | 1.000\nfit_5 |    0.982  |   7.453 | 0.007 |     | 1.262 | 1.002 | 1.002\n\nName  | Loglikelihood |   AIC (weights) |   BIC (weights) | BIC_adjusted\n------------------------------------------------------------------------\nfit_0 |     -5642.273 | 11290.5 (&lt;.001) | 11300.4 (&lt;.001) |    11290.938\nfit_1 |     -4889.446 |  9786.9 (&lt;.001) |  9800.1 (&lt;.001) |     9787.413\nfit_2 |     -4851.147 |  9712.3 (&lt;.001) |  9728.8 (&lt;.001) |     9712.945\nfit_3 |     -4038.016 |  8088.0 (0.203) |  8107.8 (0.829) |     8088.813\nfit_4 |     -4037.947 |  8089.9 (0.080) |  8113.0 (0.063) |     8090.806\nfit_5 |     -4034.756 |  8085.5 (0.717) |  8111.9 (0.108) |     8086.554\n\n\nLet’s keep the random quadratic effect.\n\n\nCode\nggplot(d, aes(time, vocabulary)) +\n  ggbeeswarm::geom_quasirandom(width = .2, size = .5, alpha = .5) +\n  geom_line(\n    data = predict(fit_5) %&gt;%\n      as_tibble() %&gt;%\n      mutate(across(everything(), .fns = as.numeric)) %&gt;%\n      mutate(person_id = dplyr::row_number()) %&gt;%\n      crossing(time = 0:5) %&gt;%\n      mutate(vocabulary = i + s * time + q * time^2),\n    aes(group = person_id),\n    alpha = .25,\n  ) +\n  stat_function(\n    data = tibble(x = 0:5),\n    geom = \"line\",\n    fun = \\(x) coef(fit_5)[\"b_00\"] + coef(fit_5)[\"b_10\"] * x + coef(fit_5)[\"b_20\"] * (x^2),\n    inherit.aes = F,\n    color = \"dodgerblue3\",\n    linewidth = 2\n  ) +\n  theme_minimal(base_size = 20, base_family = my_font) +\n  labs(x = \"Time\", y = \"Vocabulary\")\n\n\n\n\n\n\n\n\nFigure 23: Model 5: Predicted Vocabulary Over Time (Model 4 + Random Quadratic Effect of Time)\n\n\n\n\n\n\nlmer(vocabulary ~ 1 + time + I(time ^ 2) + \n       (1 + time | person_id) + \n       (0 + I(time ^ 2) | person_id), \n     data = d, \n     REML = FALSE) %&gt;% \n  tab_model()\n\n\n\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n51.96\n48.05 – 55.86\n&lt;0.001\n\n\ntime\n15.25\n14.49 – 16.01\n&lt;0.001\n\n\ntime^2\n-1.01\n-1.09 – -0.94\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n8.45\n\n\n\nτ00 person_id\n785.73\n\n\nτ11 person_id.time\n23.68\n\n\nτ11 person_id.I(time^2)\n0.06\n\n\nρ01 person_id\n0.04\n\n\nICC\n0.99\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.230 / 0.994\n\n\n\n\n\n\nTable 12: HLM Model Analogous to Model 5"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-6-model-5-correlations-with-random-quadratic-effects",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-6-model-5-correlations-with-random-quadratic-effects",
    "title": "Latent Growth Models",
    "section": "Model 6: Model 5 + Correlations with Random Quadratic Effects",
    "text": "Model 6: Model 5 + Correlations with Random Quadratic Effects\n\n\nCode\npd6 &lt;- pd5 +\n  ob_covariance(bind(c(u_2i, u_2i)),\n                bind(c(u_0i, u_1i)),\n                color = cD,\n                label = ob_label(paste0(\"*&tau;*~\", c(0, 1), c(2, 2), \"~\"), color = cD))\npd6\n\n\n\n\n\n\n\n\nFigure 24: Path Diagram for Model 6\n\n\n\n\n\nNow lets allow the covariance between the quadratic effect and the linear and intercept to be non-zero:\n\nm_6 &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Acceleration effect of time (quadratic effect)\nq =~ 0 * voc_0 + 1 * voc_1 + 4 * voc_2 + 9 * voc_3 + 16 * voc_4 + 25 * voc_5\n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 \ns ~ b_10 * 1\nq ~ b_20 * 1\n# Variances and covariances\ni ~~ tau_00 * i \ns ~~ tau_11 * s + tau_01 * i\nq ~~ tau_22 * q + tau_12 * s + tau_02 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n\"\n\nfit_6 &lt;- growth(m_6, data = d_wide)\nparameters(fit_6, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |         95% CI |      p\n---------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\ns =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_2 |        2.00 | 0.00 | [ 2.00,  2.00] | &lt; .001\ns =~ voc_3 |        3.00 | 0.00 | [ 3.00,  3.00] | &lt; .001\ns =~ voc_4 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\ns =~ voc_5 |        5.00 | 0.00 | [ 5.00,  5.00] | &lt; .001\nq =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\nq =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\nq =~ voc_2 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\nq =~ voc_3 |        9.00 | 0.00 | [ 9.00,  9.00] | &lt; .001\nq =~ voc_4 |       16.00 | 0.00 | [16.00, 16.00] | &lt; .001\nq =~ voc_5 |       25.00 | 0.00 | [25.00, 25.00] | &lt; .001\n\n# Mean\n\nLink         | Coefficient |   SE |         95% CI |      z |      p\n--------------------------------------------------------------------\ni ~1  (b_00) |       51.96 | 1.99 | [48.06, 55.85] |  26.13 | &lt; .001\ns ~1  (b_10) |       15.25 | 0.39 | [14.48, 16.02] |  38.78 | &lt; .001\nq ~1  (b_20) |       -1.01 | 0.04 | [-1.09, -0.94] | -26.39 | &lt; .001\nvoc_0 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_1 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_2 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_3 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_4 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_5 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\n\n# Variance\n\nLink               | Coefficient |    SE |           95% CI |     z |      p\n----------------------------------------------------------------------------\ni ~~ i (tau_00)    |      783.75 | 79.06 | [628.79, 938.70] |  9.91 | &lt; .001\ns ~~ s (tau_11)    |       24.86 |  3.11 | [ 18.76,  30.96] |  7.99 | &lt; .001\nq ~~ q (tau_22)    |        0.07 |  0.03 | [  0.01,   0.13] |  2.19 | 0.029 \nvoc_0 ~~ voc_0 (e) |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_1 ~~ voc_1 (e) |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_2 ~~ voc_2 (e) |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_3 ~~ voc_3 (e) |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_4 ~~ voc_4 (e) |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_5 ~~ voc_5 (e) |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\n\n# Correlation\n\nLink            | Coefficient |    SE |          95% CI |     z |     p\n-----------------------------------------------------------------------\ni ~~ s (tau_01) |        6.07 | 11.06 | [-15.61, 27.75] |  0.55 | 0.583\ns ~~ q (tau_12) |       -0.17 |  0.24 | [ -0.64,  0.30] | -0.71 | 0.475\ni ~~ q (tau_02) |       -0.37 |  1.08 | [ -2.48,  1.75] | -0.34 | 0.732\n\n# Compare new model with the previous models\ntest_likelihoodratio(fit_0, fit_1, fit_2, fit_3, fit_4, fit_5, fit_6)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nModel |   Type | df | df_diff |    Chi2 |      p\n------------------------------------------------\nfit_0 | lavaan | 24 |       1 | 3228.27 | &lt; .001\nfit_1 | lavaan | 23 |       1 | 1722.61 | &lt; .001\nfit_2 | lavaan | 22 |       1 | 1646.01 | &lt; .001\nfit_3 | lavaan | 21 |       1 |   19.75 |  0.711\nfit_4 | lavaan | 20 |       1 |   19.61 |  0.012\nfit_5 | lavaan | 19 |       2 |   13.23 |  0.714\nfit_6 | lavaan | 17 |         |   12.56 |       \n\ncompare_performance(fit_0, fit_1, fit_2, fit_3, fit_4, fit_5, fit_6, metrics = my_metrics)\n\n# Comparison of Model Performance Indices\n\nName  |  Model |   CFI |  NNFI |  AGFI | RMSEA\n----------------------------------------------\nfit_0 | lavaan | 0.121 | 0.450 | 0.426 | 0.817\nfit_1 | lavaan | 0.534 | 0.696 | 0.726 | 0.608\nfit_2 | lavaan | 0.554 | 0.696 | 0.709 | 0.608\nfit_3 | lavaan | 1.000 | 1.000 | 0.992 | 0.000\nfit_4 | lavaan | 1.000 | 1.000 | 0.992 | 0.000\nfit_5 | lavaan | 1.000 | 1.001 | 0.994 | 0.000\nfit_6 | lavaan | 1.000 | 1.001 | 0.993 | 0.000\n\n\nThese effects are not needed. We have the option of trimming them now, but I am going to wait a little longer.\n\n\nCode\nggplot(d, aes(time, vocabulary)) +\n  ggbeeswarm::geom_quasirandom(width = .2, size = .5, alpha = .5) +\n  geom_line(\n    data = predict(fit_6) %&gt;%\n      as_tibble() %&gt;%\n      mutate(across(everything(), .fns = as.numeric)) %&gt;%\n      mutate(person_id = dplyr::row_number()) %&gt;%\n      crossing(time = 0:5) %&gt;%\n      mutate(vocabulary = i + s * time + q * time^2),\n    aes(group = person_id),\n    alpha = .25,\n  ) +\n  stat_function(\n    data = tibble(x = 0:5),\n    geom = \"line\",\n    fun = \\(x) coef(fit_6)[\"b_00\"] + coef(fit_6)[\"b_10\"] * x + coef(fit_6)[\"b_20\"] * (x^2),\n    inherit.aes = F,\n    color = \"dodgerblue3\",\n    linewidth = 2\n  ) +\n  theme_minimal(base_size = 20, base_family = my_font) +\n  labs(x = \"Time\", y = \"Vocabulary\")\n\n\n\n\n\n\n\n\nFigure 25: Model 6: Predicted Vocabulary Over Time (Model 5 + All Random Effects Correlated)\n\n\n\n\n\n\nlmer(vocabulary ~ 1 + time + I(time ^ 2) + \n       (1 + time + I(time ^ 2) | person_id), \n     data = d, \n     REML = FALSE) %&gt;% \n  tab_model()\n\n\n\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n51.96\n48.06 – 55.86\n&lt;0.001\n\n\ntime\n15.25\n14.48 – 16.02\n&lt;0.001\n\n\ntime^2\n-1.01\n-1.09 – -0.94\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n8.35\n\n\n\nτ00 person_id\n783.68\n\n\nτ11 person_id.time\n24.87\n\n\nτ11 person_id.I(time^2)\n0.07\n\n\nρ01\n0.04\n\n\n\n-0.05\n\n\nICC\n0.99\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.229 / 0.994\n\n\n\n\n\n\nTable 13: HLM Model Analogous to Model 6"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-7-model-6-treatment-effects",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-7-model-6-treatment-effects",
    "title": "Latent Growth Models",
    "section": "Model 7: Model 6 + Treatment Effects",
    "text": "Model 7: Model 6 + Treatment Effects\n\n\nCode\npd7 &lt;- pd6 +\n  {intervention &lt;- my_observed(\n      intersection(connect(u_2i, s), connect(u_1i, q)) + ob_point(-1.85, 0),\n      a = 1.75,\n      b = 1.75,\n      fill = cD,\n      label = my_observed_label(\"Intervention\", size = 15)\n    )}  +\n  {lt1 &lt;- my_connect(intervention, bind(c(i, s, q)), color = my_colors)} +\n  ob_label(\n    paste0(\"*b*~\", 0:2, \"1~\"),\n    intersection(lt1, vl),\n    label.padding = margin(2),\n    color = my_colors\n  ) \npd7\n\n\n\n\n\n\n\n\nFigure 26: Path Diagram for Model 7\n\n\n\n\n\nI am going to add all three treatment effects at the same time. We could do them one-at-a-time, but no important theoretical question would be answered by doing so. I am mostly interested in the interactions of treatment and the time variables.\n\nm_7 &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Acceleration effect of time (quadratic effect)\nq =~ 0 * voc_0 + 1 * voc_1 + 4 * voc_2 + 9 * voc_3 + 16 * voc_4 + 25 * voc_5\n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 + b_01 * intervention\ns ~ b_10 * 1 + b_11 * intervention\nq ~ b_20 * 1 + b_21 * intervention\n# Variances and covariances\ni ~~ tau_00 * i \ns ~~ tau_11 * s + tau_01 * i\nq ~~ tau_22 * q + tau_12 * s + tau_02 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n\"\n\nfit_7 &lt;- growth(m_7, data = d_wide)\nparameters(fit_7, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |         95% CI |      p\n---------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\ns =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_2 |        2.00 | 0.00 | [ 2.00,  2.00] | &lt; .001\ns =~ voc_3 |        3.00 | 0.00 | [ 3.00,  3.00] | &lt; .001\ns =~ voc_4 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\ns =~ voc_5 |        5.00 | 0.00 | [ 5.00,  5.00] | &lt; .001\nq =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\nq =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\nq =~ voc_2 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\nq =~ voc_3 |        9.00 | 0.00 | [ 9.00,  9.00] | &lt; .001\nq =~ voc_4 |       16.00 | 0.00 | [16.00, 16.00] | &lt; .001\nq =~ voc_5 |       25.00 | 0.00 | [25.00, 25.00] | &lt; .001\n\n# Mean\n\nLink            | Coefficient |   SE |         95% CI |      z |      p\n-----------------------------------------------------------------------\ni ~1  (b_00)    |       52.00 | 2.87 | [46.38, 57.63] |  18.12 | &lt; .001\ns ~1  (b_10)    |       10.50 | 0.32 | [ 9.86, 11.14] |  32.39 | &lt; .001\nq ~1  (b_20)    |       -1.06 | 0.06 | [-1.16, -0.95] | -19.15 | &lt; .001\nvoc_0 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_1 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_2 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_3 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_4 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_5 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nintervention ~1 |        0.52 | 0.00 | [ 0.52,  0.52] |        | &lt; .001\n\n# Regression\n\nLink                    | Coefficient |   SE |         95% CI |     z |      p\n------------------------------------------------------------------------------\ni ~ intervention (b_01) |       -0.09 | 3.98 | [-7.89,  7.71] | -0.02 | 0.983 \ns ~ intervention (b_11) |        9.14 | 0.45 | [ 8.26, 10.02] | 20.33 | &lt; .001\nq ~ intervention (b_21) |        0.09 | 0.08 | [-0.06,  0.24] |  1.12 | 0.262 \n\n# Variance\n\nLink                         | Coefficient |    SE |           95% CI |     z |      p\n--------------------------------------------------------------------------------------\ni ~~ i (tau_00)              |      783.74 | 79.06 | [628.79, 938.70] |  9.91 | &lt; .001\ns ~~ s (tau_11)              |        4.02 |  1.07 | [  1.93,   6.11] |  3.77 | &lt; .001\nq ~~ q (tau_22)              |        0.07 |  0.03 | [  0.01,   0.13] |  2.14 | 0.032 \nvoc_0 ~~ voc_0 (e)           |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_1 ~~ voc_1 (e)           |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_2 ~~ voc_2 (e)           |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_3 ~~ voc_3 (e)           |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_4 ~~ voc_4 (e)           |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_5 ~~ voc_5 (e)           |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nintervention ~~ intervention |        0.25 |  0.00 | [  0.25,   0.25] |       | &lt; .001\n\n# Correlation\n\nLink            | Coefficient |   SE |         95% CI |     z |     p\n---------------------------------------------------------------------\ni ~~ s (tau_01) |        6.26 | 6.32 | [-6.13, 18.65] |  0.99 | 0.322\ns ~~ q (tau_12) |       -0.37 | 0.17 | [-0.71, -0.03] | -2.12 | 0.034\ni ~~ q (tau_02) |       -0.37 | 1.08 | [-2.48,  1.74] | -0.34 | 0.733\n\n# Compare new model with the previous models\ntest_likelihoodratio(fit_6, fit_7)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nModel |   Type | df | df_diff |  Chi2 |     p\n---------------------------------------------\nfit_6 | lavaan | 17 |         | 12.56 |      \nfit_7 | lavaan | 20 |       3 | 14.44 | 0.598\n\ncompare_performance(fit_6, fit_7, metrics = my_metrics)\n\n# Comparison of Model Performance Indices\n\nName  |  Model |   CFI |  NNFI |  AGFI | RMSEA\n----------------------------------------------\nfit_6 | lavaan | 1.000 | 1.001 | 0.993 | 0.000\nfit_7 | lavaan | 1.000 | 1.001 | 0.999 | 0.000\n\n\nSo the intervention effects improve the model fit. Specifically, the intervention predicts the slope such that the slope is steeper for the treatment group.\n\n\nCode\npred_7 &lt;- lavPredictY(\n  fit_7,\n  ynames = d_wide %&gt;% select(starts_with(\"voc_\")) %&gt;%  colnames(),\n  xnames = \"intervention\",\n  newdata = tibble(\n    intervention = c(0, 1),\n    voc_0 = 0,\n    voc_1 = 0,\n    voc_2 = 0,\n    voc_3 = 0,\n    voc_4 = 0,\n    voc_5 = 0\n  )\n) %&gt;%\n  as_tibble() %&gt;%\n  mutate(intervention = factor(c(0, 1), \n                               labels = c(\"Control\", \"Treatment\"))) %&gt;%\n  pivot_longer(\n    starts_with(\"voc_\"),\n    names_to = \"time\",\n    names_prefix = \"voc_\",\n    names_transform = list(time = as.integer),\n    values_to = \"vocabulary\"\n  )\n\n\n\nggplot(\n  data = d %&gt;% \n    mutate(intervention = factor(\n      intervention, \n      labels = c(\"Control\", \"Treatment\"))), \n  aes(time, vocabulary)) +\n  ggbeeswarm::geom_quasirandom(\n    aes(color = intervention, group = intervention),\n    width = .05,\n    data = . %&gt;% \n      mutate(time = time + (as.numeric(intervention) - 1.5) / 6),\n    size = .5,\n    alpha = .5\n  ) +\n  geom_line(\n    data = my_predictions(fit_7) %&gt;% \n      mutate(intervention = factor(\n        intervention, \n        labels = c(\"Control\", \"Treatment\")\n    )),\n    aes(group = person_id, color = intervention),\n    alpha = .3,\n    linewidth = .5\n  ) +\n  geom_line(\n    data = pred_7,\n    aes(group = intervention, color = intervention),\n    linewidth = 3\n  ) +\n  geomtextpath::geom_labelline(\n    data = pred_7,\n    aes(\n      color = intervention,\n      group = intervention,\n      label = intervention\n    ),\n    vjust = -.3,\n    size = 6,\n    family = my_font,\n    fill = \"#FFFFFFBB\",\n    text_only = T,\n    linewidth = 0,\n    label.padding = margin(1, 1, 1, 1),\n    hjust = .95\n  ) +\n  theme_minimal(base_size = 20, base_family = my_font) +\n  theme(legend.position = \"none\") +\n  scale_color_manual(values = c(\n    Control = \"dodgerblue4\",\n    Treatment = \"forestgreen\"\n  )) + \n  labs(x = \"Time\", y = \"Vocabulary\")\n\n\n\n\n\n\n\n\nFigure 27: Model 7: Predicted Vocabulary by Intervention Group Over Time (Model 6 + Effects of Intervention on Intercept, Linear Effect of Time, and Quadratic Effect of Time)\n\n\n\n\n\n\nlmer(vocabulary ~ 1 + time*intervention + I(time ^ 2) * intervention + \n       (1 + time + I(time ^ 2) | person_id), \n     data = d, \n     REML = FALSE) %&gt;% \n  tab_model()\n\n\n\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n52.00\n46.37 – 57.63\n&lt;0.001\n\n\ntime\n10.50\n9.86 – 11.14\n&lt;0.001\n\n\nintervention\n-0.09\n-7.89 – 7.72\n0.983\n\n\ntime^2\n-1.06\n-1.16 – -0.95\n&lt;0.001\n\n\ntime × intervention\n9.14\n8.26 – 10.02\n&lt;0.001\n\n\nintervention × time^2\n0.09\n-0.06 – 0.24\n0.262\n\n\nRandom Effects\n\n\n\nσ2\n8.35\n\n\n\nτ00 person_id\n783.75\n\n\nτ11 person_id.time\n4.02\n\n\nτ11 person_id.I(time^2)\n0.07\n\n\nρ01\n0.11\n\n\n\n-0.05\n\n\nICC\n0.99\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.381 / 0.994\n\n\n\n\n\n\nTable 14: HLM Model Analogous to Model 7"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-8-model-7-autoregression-effect",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-8-model-7-autoregression-effect",
    "title": "Latent Growth Models",
    "section": "Model 8: Model 7 + Autoregression Effect",
    "text": "Model 8: Model 7 + Autoregression Effect\n\n\nCode\npd8 &lt;- pd7 +\n    my_connect(\n    v[1:5],\n    v[2:6],\n    color = v@fill[-6],\n    label = ob_label(\n      \"*r*\",\n      angle = 0,\n      position = .4,\n      size = 14,\n      label.padding = margin()\n    ),\n    resect = 1\n  ) \n\npd8\n\n\n\n\n\n\n\n\nFigure 28: Path Diagram for Model 8\n\n\n\n\n\nSuppose that we want to add an autoregression effect to our latent growth model.\nAn autoregression allows for the residuals of earlier observed variables to have lasting effects on subsequent variables.\n\nm_8 &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Acceleration effect of time (quadratic effect)\nq =~ 0 * voc_0 + 1 * voc_1 + 4 * voc_2 + 9 * voc_3 + 16 * voc_4 + 25 * voc_5\n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 + b_01 * intervention\ns ~ b_10 * 1 + b_11 * intervention\nq ~ b_20 * 1 + b_21 * intervention\n# Variances and covariances\ni ~~ tau_00 * i \ns ~~ tau_11 * s + tau_01 * i\nq ~~ tau_22 * q + tau_12 * s + tau_02 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n# Autoregression\nvoc_1 ~ r * voc_0\nvoc_2 ~ r * voc_1\nvoc_3 ~ r * voc_2\nvoc_4 ~ r * voc_3\nvoc_5 ~ r * voc_4\n\"\n\nfit_8 &lt;- growth(m_8, data = d_wide)\n\nparameters(fit_8, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |         95% CI |      p\n---------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\ns =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_2 |        2.00 | 0.00 | [ 2.00,  2.00] | &lt; .001\ns =~ voc_3 |        3.00 | 0.00 | [ 3.00,  3.00] | &lt; .001\ns =~ voc_4 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\ns =~ voc_5 |        5.00 | 0.00 | [ 5.00,  5.00] | &lt; .001\nq =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\nq =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\nq =~ voc_2 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\nq =~ voc_3 |        9.00 | 0.00 | [ 9.00,  9.00] | &lt; .001\nq =~ voc_4 |       16.00 | 0.00 | [16.00, 16.00] | &lt; .001\nq =~ voc_5 |       25.00 | 0.00 | [25.00, 25.00] | &lt; .001\n\n# Mean\n\nLink            | Coefficient |   SE |         95% CI |      z |      p\n-----------------------------------------------------------------------\ni ~1  (b_00)    |       52.01 | 2.87 | [46.38, 57.64] |  18.11 | &lt; .001\ns ~1  (b_10)    |       10.54 | 0.50 | [ 9.56, 11.52] |  21.03 | &lt; .001\nq ~1  (b_20)    |       -1.06 | 0.07 | [-1.21, -0.92] | -14.39 | &lt; .001\nvoc_0 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_1 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_2 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_3 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_4 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_5 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nintervention ~1 |        0.52 | 0.00 | [ 0.52,  0.52] |        | &lt; .001\n\n# Regression\n\nLink                    | Coefficient |   SE |         95% CI |     z |      p\n------------------------------------------------------------------------------\ni ~ intervention (b_01) |       -0.09 | 3.98 | [-7.89,  7.71] | -0.02 | 0.982 \ns ~ intervention (b_11) |        9.14 | 0.45 | [ 8.26, 10.03] | 20.25 | &lt; .001\nq ~ intervention (b_21) |        0.09 | 0.08 | [-0.06,  0.24] |  1.13 | 0.261 \nvoc_1 ~ voc_0 (r)       |   -1.06e-03 | 0.01 | [-0.02,  0.02] | -0.10 | 0.920 \nvoc_2 ~ voc_1 (r)       |   -1.06e-03 | 0.01 | [-0.02,  0.02] | -0.10 | 0.920 \nvoc_3 ~ voc_2 (r)       |   -1.06e-03 | 0.01 | [-0.02,  0.02] | -0.10 | 0.920 \nvoc_4 ~ voc_3 (r)       |   -1.06e-03 | 0.01 | [-0.02,  0.02] | -0.10 | 0.920 \nvoc_5 ~ voc_4 (r)       |   -1.06e-03 | 0.01 | [-0.02,  0.02] | -0.10 | 0.920 \n\n# Variance\n\nLink                         | Coefficient |    SE |           95% CI |     z |      p\n--------------------------------------------------------------------------------------\ni ~~ i (tau_00)              |      784.05 | 79.15 | [628.92, 939.18] |  9.91 | &lt; .001\ns ~~ s (tau_11)              |        4.04 |  1.09 | [  1.91,   6.18] |  3.71 | &lt; .001\nq ~~ q (tau_22)              |        0.07 |  0.03 | [  0.01,   0.13] |  2.14 | 0.033 \nvoc_0 ~~ voc_0 (e)           |        8.34 |  0.49 | [  7.38,   9.30] | 17.04 | &lt; .001\nvoc_1 ~~ voc_1 (e)           |        8.34 |  0.49 | [  7.38,   9.30] | 17.04 | &lt; .001\nvoc_2 ~~ voc_2 (e)           |        8.34 |  0.49 | [  7.38,   9.30] | 17.04 | &lt; .001\nvoc_3 ~~ voc_3 (e)           |        8.34 |  0.49 | [  7.38,   9.30] | 17.04 | &lt; .001\nvoc_4 ~~ voc_4 (e)           |        8.34 |  0.49 | [  7.38,   9.30] | 17.04 | &lt; .001\nvoc_5 ~~ voc_5 (e)           |        8.34 |  0.49 | [  7.38,   9.30] | 17.04 | &lt; .001\nintervention ~~ intervention |        0.25 |  0.00 | [  0.25,   0.25] |       | &lt; .001\n\n# Correlation\n\nLink            | Coefficient |   SE |         95% CI |     z |     p\n---------------------------------------------------------------------\ni ~~ s (tau_01) |        6.74 | 7.95 | [-8.84, 22.33] |  0.85 | 0.396\ns ~~ q (tau_12) |       -0.37 | 0.18 | [-0.71, -0.03] | -2.11 | 0.034\ni ~~ q (tau_02) |       -0.44 | 1.30 | [-2.98,  2.10] | -0.34 | 0.735\n\n# Compare models\ntest_likelihoodratio(fit_7, fit_8)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nModel |   Type | df | df_diff |  Chi2 |     p\n---------------------------------------------\nfit_7 | lavaan | 20 |       1 | 14.44 | 0.920\nfit_8 | lavaan | 19 |         | 14.43 |      \n\n\nThe autoregression parameter was not statistically significant. Comparing the two models revealed no difference in model fit. Thus, there appears to be no need of the autoregression parameter, once we have accounted for the latent growth."
  },
  {
    "objectID": "tutorials/Regression/regression.html",
    "href": "tutorials/Regression/regression.html",
    "title": "Regression in R",
    "section": "",
    "text": "Here we are going to use a small data set to predict people’s height using the height of their parents."
  },
  {
    "objectID": "tutorials/Regression/regression.html#install-tidyverse",
    "href": "tutorials/Regression/regression.html#install-tidyverse",
    "title": "Regression in R",
    "section": "Install tidyverse",
    "text": "Install tidyverse\nSome packages are designed to work with several other packages as a system. The tidyverse package is a “meta-package” that installs and loads a coherent set of packages designed to help you import, manipulate, visualize, and interpret data. If you do not have a recent version of tidyverse already installed, you can install it with this code:\n\ninstall.packages(\"tidyverse\")"
  },
  {
    "objectID": "tutorials/Regression/regression.html#install-easystats",
    "href": "tutorials/Regression/regression.html#install-easystats",
    "title": "Regression in R",
    "section": "Install easystats",
    "text": "Install easystats\nThe easystats package is another “meta-package” that installs a set of packages designed to work together to make data analysis easier.\nIf you do not have a recent version of easystats already installed, you can install it with this code:\n\ninstall.packages(\"easystats\")"
  },
  {
    "objectID": "tutorials/Regression/regression.html#save-the-plot",
    "href": "tutorials/Regression/regression.html#save-the-plot",
    "title": "Regression in R",
    "section": "Save the plot!",
    "text": "Save the plot!\nYou can save to your hard drive a high-quality plot with the ggsave function. It will save whatever the last plot you created with ggplot. It will guess the file format from the file extension.\nHere I save a .pdf file of the plot to the working directory:\n\nggsave(\"my_plot.pdf\")\n\nWhat is the working directory? Is the folder on your machine that R thinks it should look first if it needs to find files or save files. If you are ever curious about which directory is the working directory, you can see the current working directory with the getwd function:\n\ngetwd()\n\nIf you saved the plot above as \"my_plot.pdf\", you will find the file in the working directory returned by getwd.\nIf you need to set the working working directly to something different from what it is, use the setwd function. In the Session menu in RStudio, you can also set the working directory with a point-an-click dialog box."
  },
  {
    "objectID": "tutorials/Regression/regression.html#vector-based-images",
    "href": "tutorials/Regression/regression.html#vector-based-images",
    "title": "Regression in R",
    "section": "Vector-based images",
    "text": "Vector-based images\nThe .pdf format gives the best image quality but can only be viewed in a .pdf reader. The .svg format is almost as good and can be incorporated into webpages and Office documents. One downside of their near-perfect image quality is that .pdf and .svg image file sizes can become quite large."
  },
  {
    "objectID": "tutorials/Regression/regression.html#raster-images",
    "href": "tutorials/Regression/regression.html#raster-images",
    "title": "Regression in R",
    "section": "Raster images",
    "text": "Raster images\nThe .png format gives good image quality and renders small file sizes. I prefer using the ragg::agg_png device to render a .png because it allows me to use any system font with no extra fuss.\n\nggsave(\"my_plot.png\", device = ragg::agg_png)\n\nThe primary use of the .gif format is to create animated plots. Otherwise stick with .png.\nAlthough the .jpg format is good for photos, it is terrible for plots—it often renders text and sharp corners with pixelated smudges."
  },
  {
    "objectID": "tutorials/Regression/regression.html#posterier-predictions",
    "href": "tutorials/Regression/regression.html#posterier-predictions",
    "title": "Regression in R",
    "section": "Posterier Predictions",
    "text": "Posterier Predictions\n\ndiagnostic_plots &lt;- plot(check_model(m1, panel = FALSE))\n\nFor confidence bands, please install `qqplotr`.\n\ndiagnostic_plots[[1]]\n\nIgnoring unknown labels:\n• size : \"\"\n\n\n\n\n\n\n\n\nFigure 5: Posterior Predictive Check\n\n\n\n\n\nThe Figure 5 generates several sets of random “simulated” data based on the model and plots the distributions as the thin blue lines. Each simulated data set is of the same size as the original data. The thicker green line is based on the observed data. If the blue lines have roughly the same shape as the green line, then the model is likely of the right form.\nThe green line is not that far off from the blue lines, but the green line appears to have 2 peaks, and most of the blue lines have one peak. Because we know that males and females have different mean heights, there is a good chance that we need to model their heights separately. We will do so later in the tutorial.\nThe tutorial for the check_model function gives an example of a posterior prediction check that signals that something is awry."
  },
  {
    "objectID": "tutorials/Regression/regression.html#linearity-assumption",
    "href": "tutorials/Regression/regression.html#linearity-assumption",
    "title": "Regression in R",
    "section": "Linearity Assumption",
    "text": "Linearity Assumption\n\ndiagnostic_plots[[2]]\n\n\n\n\n\n\n\nFigure 6: Linearity assumption\n\n\n\n\n\nIn Figure 6, we see that the fitted values (i.e., \\hat{Y} or predicted values) plotted against the residuals is roughly flat and horizontal. If the green line were clearly not flat, we would consider non-linear models."
  },
  {
    "objectID": "tutorials/Regression/regression.html#homogeneity-of-variance-assumption",
    "href": "tutorials/Regression/regression.html#homogeneity-of-variance-assumption",
    "title": "Regression in R",
    "section": "Homogeneity of Variance Assumption",
    "text": "Homogeneity of Variance Assumption\n\ndiagnostic_plots[[3]]\n\n\n\n\n\n\n\nFigure 7: Homogeneity of variance assumption\n\n\n\n\n\nFigure 7 is similar to Figure 6 except that the fitted values are standardized (i.e., converted to z-scores) and then square root of their absolute values are plotted on the Y-axis. The homogeneity of variance assumption requires that variability of the residuals should be roughly the same across the entire distribution of fitted values. Thus, the line in Figure 7 should be roughly flat and horizontal, which it is. Indeed, we can draw a horizontal line entirely within the gray confidence region around the green line."
  },
  {
    "objectID": "tutorials/Regression/regression.html#influential-observations-and-outlier-detection",
    "href": "tutorials/Regression/regression.html#influential-observations-and-outlier-detection",
    "title": "Regression in R",
    "section": "Influential Observations and Outlier Detection",
    "text": "Influential Observations and Outlier Detection\n\ndiagnostic_plots[[4]]\n\n\n\n\n\n\n\nFigure 8: Influential observations\n\n\n\n\n\nSometimes a single outlier can radically alter a regression model. The plot in Figure 8 shows how influential each point is in creating the regression model. If any point is outside the green dotted lines, we might worry that the point has had undue influence on the model.\nMany additional outlier detection methods are available via the check_outliers function:\n\ncheck_outliers(m1, method = \"all\")\n\nOK: No outliers detected.\n- Based on the following methods and thresholds: zscore_robust (3.291), iqr (2), ci (1), cook (0.706), mahalanobis (13.816), mahalanobis_robust (13.816), mcd (13.816), ics (0.001), optics (4), lof (0.001).\n- For variable: (Whole model)"
  },
  {
    "objectID": "tutorials/Regression/regression.html#normality-assumption",
    "href": "tutorials/Regression/regression.html#normality-assumption",
    "title": "Regression in R",
    "section": "Normality Assumption",
    "text": "Normality Assumption\nThe normality assumption does not require that all variables be normal. It requires that the prediction residuals be approximately normal.\n\ndiagnostic_plots[[5]]\n\n\n\n\n\n\n\nFigure 9: Normality assumption\n\n\n\n\n\nIf the residuals were perfectly normal, the blue dots would fall exactly on the green line in Figure 9. The blue dots are not that far from the green line. Because ordinary least squares is robust to minor violations of the normality assumption, nothing in Figure 9 should make us worry about needing to model the data with some other kind of residuals.\nIf we suspect that the residuals are generated from a different distribution, the check_distribution function will estimate the probability that the residuals (and response/outcome variable) come from 12 major distribution families:\n\ncheck_distribution(m1) \n\n\n\n\n\nDistribution\np_Residuals\np_Response\n\n\n\n\nbernoulli\n0.000\n0.000\n\n\nbeta\n0.031\n0.000\n\n\nbeta-binomial\n0.000\n0.406\n\n\nbinomial\n0.000\n0.406\n\n\ncauchy\n0.094\n0.031\n\n\nchi\n0.031\n0.031\n\n\nexponential\n0.000\n0.000\n\n\nF\n0.000\n0.000\n\n\ngamma\n0.000\n0.000\n\n\nhalf-cauchy\n0.000\n0.000\n\n\ninverse-gamma\n0.000\n0.000\n\n\nlognormal\n0.000\n0.000\n\n\nneg. binomial (zero-infl.)\n0.000\n0.031\n\n\nnegative binomial\n0.062\n0.000\n\n\nnormal\n0.594\n0.031\n\n\npareto\n0.031\n0.000\n\n\npoisson\n0.000\n0.000\n\n\npoisson (zero-infl.)\n0.000\n0.031\n\n\ntweedie\n0.156\n0.000\n\n\nuniform\n0.000\n0.000\n\n\nweibull\n0.000\n0.031\n\n\n\n\n\nplot(check_distribution(m1))\n\n\n\n\n\n\n\n\nHere we see that of the 12 major distribution families tested, the residuals are more likely normal than any of the others. After we control for gender, this probability will increase. If the results had strongly suggested the residuals were generated from one of the other distributions, we might consider switching from a linear model to a generalized linear model."
  },
  {
    "objectID": "tutorials/Regression/regression.html#model-level-statistics",
    "href": "tutorials/Regression/regression.html#model-level-statistics",
    "title": "Regression in R",
    "section": "Model-level statistics",
    "text": "Model-level statistics\nSome statistics like the coefficient of determination (R2) or the standard error of the estimate (σe) describe the model as a whole.\n\nR2\nThe R2 statistic measures the percentage of variance in the outcome explained by the predictors.\n\nR^2=\\frac{\\sigma_{\\hat{Y}}^2}{\\sigma_{Y}^2}=1-\\frac{\\sigma_e^2}{\\sigma_Y^2}\n\nThe model-level statistics can be extracted with the performance package’s model_performance function.\n\nperformance(m1)\n\n\n\n\n\nAIC\nAICc\nBIC\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\n196\n197\n201\n0.641\n0.631\n2.61\n2.68\n\n\n\n\n\n\nR2 is the percentage of variance in the outcome variable (i.e., Height) explained by the predictor variables (i.e., Average Parent Height). In Figure 10, the black line toggles between the best fitting regression line and the horizontal line running through the outcome variable’s mean. The vertical line segments running from each point to the black line are, on average, smaller for the best fitting regression line. Their squared distances shrink, on average, by 64%. Thus, we say that the predictor variance “explains” 64% of the variance in the outcome variable.\n\n\nCode\nr &lt;- d %&gt;% select(avgphgt, height) %&gt;% cor() %&gt;% `[[`(1, 2)\nm &lt;- d %&gt;% select(avgphgt, height) %&gt;% colMeans()\ns &lt;- d %&gt;% select(avgphgt, height) %&gt;% sapply(sd)\nn &lt;- 2\nd_slope &lt;- tibble(id = c(\"0.00\", round(r^2, 2)),\n                  r = seq(0, r, length.out = n)) %&gt;%\n  mutate(i = -1 * r * m[1] * s[2] / s[1] + m[2], \n         b = r * s[2] / s[1])\nlibrary(gganimate)\nanim &lt;- d %&gt;% select(idnum, avgphgt, height) %&gt;%\n  crossing(d_slope) %&gt;%\n  mutate(yhat = b * avgphgt + i, \n         Error = height - yhat) %&gt;%\n  arrange(idnum, Error) %&gt;%\n  ggplot(aes(avgphgt, height)) +\n  geom_abline(aes(slope = b, intercept = i)) +\n  geom_segment(aes(xend = avgphgt, yend = yhat, color = Error)) +\n  geom_point() +\n  transition_states(id, state_length = 3, transition_length = 1) +\n  scale_color_gradient2(\n    low = \"royalblue4\",\n    mid = \"gray\",\n    high = \"firebrick4\",\n    breaks = seq(-6, 6, 2),\n    labels = signs::signs\n  ) +\n  scale_x_continuous(\"Average Parent Height (inches)\", limits = c(60, 75)) +\n  scale_y_continuous(\"Student Height (inches)\", limits = c(60, 75)) +\n  labs(title = \"Variance Explained = {closest_state}\") +\n  coord_equal() +\n  theme_minimal(base_family = \"Roboto Condensed\", base_size = 18) +\n  theme(\n    legend.text.position = \"left\",\n    legend.text = element_text(hjust = 1),\n    legend.key.height = unit(2, \"cm\")\n  )\ngganimate::animate(\n  anim,\n  nframes = 50 * 4,\n  device = \"ragg_png\",\n  width = 10,\n  height = 10,\n  fps = 50,\n  res = 144\n)\n\n\n\n\n\n\n\n\nFigure 10: Percentage of Variance Explained\n\n\n\n\n\nIf all you wanted was the R2, you could do this:\n\nperformance(m1)$R2\n\n[1] 0.641\n\n\nWhy would you want just one number instead of reading it from a table? In reproducible research, we intermingle text and code so that it is clear where every number came from. Thus, “hard-coding” your results like this is considered poor practice:\nThe model explains 64% of the variance.\nUsing rmarkdown, instead of typing the numeric results, we type pull the results using an inline code chunk:\nThe model explains `{r} round(100 * r2(m1)$R2, 0)`% of the variance.\nWhich, when rendered, produces the correct output:\n\nThe model explains 64% of the variance.\n\nThat seems like a lot of extra work, right? Yes, it is—unless there is a possibility that your underlying data might change or that you might copy your numbers incorrectly. If you are imperfect, the extra time and effort is worth it. It makes it easy for other scholars to see exactly where each number came from. Hard-coded results are harder to trust.\nRegression errors are the vertical distances of the outcome variable and regression line. That is, errors are the difference between the outcome and the predicted outcome:\n\ne=Y-\\hat{Y}\n\n\n\nStandard Error of the Estimate (σe)\nThe standard error of the estimate (i.e., Sigma in the performance output table) is the standard deviation of the regression errors \\sigma_{e}. It represents the typical size of the prediction error. That is, when we make a prediction, how far off is that prediction likely to be?\nThe standard error of the estimate can be extracted from a regression fit object with the sigma function:\n\nsigma(m1)\n\n[1] 2.68\n\n\nAlternately, it can be extracted from the performance function’s output:\n\nperformance(m1)$Sigma\n\n[1] 2.68"
  },
  {
    "objectID": "tutorials/Regression/regression.html#coefficient-level-statistics",
    "href": "tutorials/Regression/regression.html#coefficient-level-statistics",
    "title": "Regression in R",
    "section": "Coefficient-level statistics",
    "text": "Coefficient-level statistics\nThe regression coefficients—the intercept (b0) and the slope (b1)—have a number of statistics associated with them, which we will discuss later in the course.\nIf you just wanted the intercept and the slope coefficients, use the coef function:\n\ncoef(m1)\n\n(Intercept)     avgphgt \n     -12.88        1.19 \n\n\nThus, the intercept is -12.88 and the slope is 1.19.\nTo get the model parameters (intercept and slope coefficients) along with their standard errors, confidence intervals, t statistics, and p-values:\n\nparameters(m1) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n-12.88\n9.779\n0.95\n-32.675\n6.92\n-1.32\n38\n0.196\n\n\navgphgt\n1.19\n0.145\n0.95\n0.899\n1.49\n8.23\n38\n0.000\n\n\n\n\n\n\nStandardized parameters are the regression coefficients if all variables in the analysis were standardized (i.e., convernted to z-scores). Standardized coefficients have a straightforward interpretation: They represent the predicted change in the outcome associated with a change of 1 standard deviation in the predictor variable.\n\nstandardize_parameters(m1)\n\n\n\n\n\nParameter\nStd_Coefficient\nCI\nCI_low\nCI_high\n\n\n\n\n(Intercept)\n0.0\n0.95\n-0.194\n0.194\n\n\navgphgt\n0.8\n0.95\n0.603\n0.997"
  },
  {
    "objectID": "tutorials/Regression/regression.html#checking-assumptions-1",
    "href": "tutorials/Regression/regression.html#checking-assumptions-1",
    "title": "Regression in R",
    "section": "Checking assumptions",
    "text": "Checking assumptions\n\ncheck_model(m2)\n\n\n\n\n\n\n\nFigure 11: Assumption check for model 2\n\n\n\n\n\nThe diagnostic plots in Figure 11 looks good. It has one additional plot checking the collinearity of the predictors. If any predictor is strongly predictable from the other predictors, it is difficult for ordinary least squares regression to locate the regression coefficients with precision. Why? Because the regression coefficients represent the independent effect of each variable controlling for all the other variables. If a predictor has little variability left after controlling for the other variables, estimating its independent effect is difficult, which is manifest in “inflated” standardized errors around the coefficient. The VIF (variance inflation factor) statistic estimates how much the standard errors are made large due to collinearity. If the VIF is larger than 10 or so, it is likely that the estimate of the coefficient is not precise."
  },
  {
    "objectID": "tutorials/Regression/regression.html#summarizing-results",
    "href": "tutorials/Regression/regression.html#summarizing-results",
    "title": "Regression in R",
    "section": "Summarizing results",
    "text": "Summarizing results\nTo summarize the results, use the summary function:\n\nsummary(m2)\n\n\nCall:\nlm(formula = height ~ avgphgt + gender, data = d)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.826 -1.094 -0.347  1.656  4.503 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   20.099     10.304    1.95  0.05870 .  \navgphgt        0.671      0.157    4.26  0.00013 ***\ngendermale     4.468      0.920    4.85  2.2e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.12 on 37 degrees of freedom\nMultiple R-squared:  0.78,  Adjusted R-squared:  0.769 \nF-statistic: 65.7 on 2 and 37 DF,  p-value: 6.6e-13\n\n\nModel-level statistics:\n\nperformance(m2)\n\n\n\n\n\nAIC\nAICc\nBIC\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\n179\n180\n185\n0.78\n0.769\n2.04\n2.12\n\n\n\n\n\n\nCoefficient-level statistics:\n\nparameters(m2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n20.099\n10.304\n0.95\n-0.778\n40.976\n1.95\n37\n0.059\n\n\navgphgt\n0.671\n0.157\n0.95\n0.352\n0.989\n4.26\n37\n0.000\n\n\ngendermale\n4.468\n0.920\n0.95\n2.603\n6.333\n4.85\n37\n0.000\n\n\n\n\n\n\nStandardized coefficients:\n\nstandardize_parameters(m2)\n\n\n\n\n\nParameter\nStd_Coefficient\nCI\nCI_low\nCI_high\n\n\n\n\n(Intercept)\n-0.506\n0.95\n-0.768\n-0.245\n\n\navgphgt\n0.450\n0.95\n0.236\n0.664\n\n\ngendermale\n1.012\n0.95\n0.590\n1.435\n\n\n\n\n\n\nAn automated report:\n\nreport(m2)\n\nWe fitted a linear model (estimated using OLS) to predict height with avgphgt\nand gender (formula: height ~ avgphgt + gender). The model explains a\nstatistically significant and substantial proportion of variance (R2 = 0.78,\nF(2, 37) = 65.75, p &lt; .001, adj. R2 = 0.77). The model's intercept,\ncorresponding to avgphgt = 0 and gender = female, is at 20.10 (95% CI [-0.78,\n40.98], t(37) = 1.95, p = 0.059). Within this model:\n\n  - The effect of avgphgt is statistically significant and positive (beta = 0.67,\n95% CI [0.35, 0.99], t(37) = 4.26, p &lt; .001; Std. beta = 0.45, 95% CI [0.24,\n0.66])\n  - The effect of gender [male] is statistically significant and positive (beta =\n4.47, 95% CI [2.60, 6.33], t(37) = 4.85, p &lt; .001; Std. beta = 1.01, 95% CI\n[0.59, 1.43])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "tutorials/Regression/regression.html#comparing-models",
    "href": "tutorials/Regression/regression.html#comparing-models",
    "title": "Regression in R",
    "section": "Comparing models",
    "text": "Comparing models\nIn the first model, there was only one predictor. The second model had an additional predictor. To test whether the second predictor has incremental validity (predicts variance in the outcome beyond what is predicted by other predictors), we can compare the two models using the Wald test:\n\ntest_wald(m1, m2)\n\n\n\n\n\nName\nModel\ndf\ndf_diff\nF\np\n\n\n\n\nm1\nlm\n38\n\n\n\n\n\nm2\nlm\n37\n1\n23.6\n0\n\n\n\n\n\n\nThe Wald test can also be conducted in base R with the anova function:\n\nanova(m1, m2)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n38\n273\n\n\n\n\n\n\n37\n167\n1\n106\n23.6\n0\n\n\n\n\n\n\nEither way, the p-value is significant, meaning that m2 explains more variance than m1.\nThe semi-partial correlation coefficient squared tells us how much incremental variance each predictor has over the other:\n\nr2_semipartial(m2)\n\n\n\n\n\nTerm\nr2_semipartial\nCI\nCI_low\nCI_high\n\n\n\n\navgphgt\n0.108\n0.95\n0.019\n1\n\n\ngender\n0.140\n0.95\n0.037\n1\n\n\n\n\n\n\nThus, gender explains 14% variance beyond avgphgt. Likewise, avgphgt explains 11% variance beyond gender.\n\nBayes Factors\nAn alternate method of comparing two models uses the Bayes factor. It tells us under which model the observed data are more probable.\n\ntest_bf(m1, m2)\n\n\n\n\n\nModel\nlog_BF\nBF\n\n\n\n\navgphgt\n\n\n\n\navgphgt + gender\n8.01\n3010\n\n\n\n\n\n\nA BF &gt; 1 means that m2 is more strongly supported than m1. A BF &lt; 1 means that m1 is more strongly supported than m2. If you are not sure how to interpret the output of test_bf, you can get an automated interpretation:\n\ntest_bf(m1, m2) %&gt;% \n  report()\n\nBayes factors were computed using the BIC approximation, by which BF10 =\nexp((BIC0 - BIC1)/2). Compared to the avgphgt model, we found extreme evidence\n(BF = 3.01e+03) in favour of the avgphgt + gender model (the least supported\nmodel).\n\n\nTo compare many performance statistics at once:\n\ncompare_performance(m1, m2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nAIC\nAIC_wt\nAICc\nAICc_wt\nBIC\nBIC_wt\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\nm1\nlm\n196\n0\n197\n0\n201\n0\n0.641\n0.631\n2.61\n2.68\n\n\nm2\nlm\n179\n1\n180\n1\n185\n1\n0.780\n0.769\n2.04\n2.12"
  },
  {
    "objectID": "tutorials/Regression/regression.html#question-1",
    "href": "tutorials/Regression/regression.html#question-1",
    "title": "Regression in R",
    "section": "Question 1",
    "text": "Question 1\nIs read_1 a significant predictor of read_2?\n\n\n\n\n\n\nNoteHint 1\n\n\n\n\n\nMake model fit object called m_read using the lm function.\n\nm_read &lt;- lm(read_2 ~ read_1, data = d_learn)\n\n\n\n\n\n\n\n\n\n\nNoteHint 2\n\n\n\n\n\nView coefficient-level statistics with the parameters function.\nYou could also use Base R’s summary function.\n\nparameters(m_read)\n\n# or\n\nsummary(m_read)\n\n\n\n\n\n\n\n\n\n\nNoteHint 3\n\n\n\n\n\nIn the read_1 row, is the p.value column less than 0.05?\n\nparameters(m_read)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n83.591\n10.922\n0.95\n61.44\n105.741\n7.65\n36\n0.000\n\n\nread_1\n0.251\n0.104\n0.95\n0.04\n0.463\n2.41\n36\n0.021"
  },
  {
    "objectID": "tutorials/Regression/regression.html#question-2",
    "href": "tutorials/Regression/regression.html#question-2",
    "title": "Regression in R",
    "section": "Question 2",
    "text": "Question 2\nWhat is the R2 for the m_read model?\n\n\n\n\n\n\nNoteHint\n\n\n\n\n\nView model-level statistics with the performance function.\nYou could also use Base R’s summary function.\n\nperformance(m_read)\n\n# or\n\nsummary(m_read)"
  },
  {
    "objectID": "tutorials/Regression/regression.html#question-3",
    "href": "tutorials/Regression/regression.html#question-3",
    "title": "Regression in R",
    "section": "Question 3",
    "text": "Question 3\nWhat does the scatter plot look like when read_1 is on the x-axis and read_2 is on the y-axis? Also plot the regression line. Save your plot using the ggsave function.\n\n\n\n\n\n\nNoteHint for Getting Started\n\n\n\n\n\nThis will get you started\n\nggplot(d_learn, aes(read_1, read_2))\n\n\n\n\n\n\n\n\n\n\nNoteHint for Adding Points\n\n\n\n\n\nHere is how you add points.\n\nggplot(d_learn, aes(read_1, read_2)) +\n  geom_point() \n\n\n\n\n\n\n\n\n\n\nNoteHint for Adding a Regression Line\n\n\n\n\n\nHere is how you add a regression line.\n\nggplot(d_learn, aes(read_1, read_2)) +\n  geom_point() + \n  geom_smooth(method = \"lm\") \n\n\n\n\n\n\n\n\n\n\nNotePlot Polishing\n\n\n\n\n\nThis is overkill for now. But someday you might want to be able to make your plots as polished as possible.\n\n# I want to put the equation at x = 130\nequation_x &lt;- 130\nequation_y &lt;- predict(m_read, newdata = tibble(read_1 = equation_x))\n\n# Extracting the coefficients\nb_read &lt;- round(coef(m_read),2)\n\n# The angle of the regression line is the inverse tangent of the slope (converted to degrees)\neq_angle &lt;- atan(b_read[2]) * 180 / pi\n\n# Equation\neq_read &lt;- paste0(\"italic(Y) == \", \n                  b_read[1], \n                  \" + \", \n                  b_read[2], \n                  \" *  italic(X) + italic(e)\")\n\nggplot(d_learn, aes(read_1, read_2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Reading at Time 1\",\n       y = \"Reading at Time 2\",\n       title = \"Using Reading at Time 1 to Predict Reading at Time 2\") +\n  coord_fixed() +\n  theme_minimal() +\n  annotate(\n    geom = \"text\",\n    x = equation_x,\n    y = equation_y,\n    angle = eq_angle,\n    label = eq_read,\n    parse = TRUE,\n    vjust = -0.5)"
  },
  {
    "objectID": "tutorials/Regression/regression.html#question-4",
    "href": "tutorials/Regression/regression.html#question-4",
    "title": "Regression in R",
    "section": "Question 4",
    "text": "Question 4\nCreate a regression model in which you predict math at time 2 (math_2) using reading at time 1 (math_1). Call the model fit object m_math1.\nDoes math_1 predict math_2?.\nDoes math_1 still predict math_2 after controlling for read_1?\n\n\n\n\n\n\nNoteHint\n\n\n\n\n\n\nm_math &lt;- lm(math_2 ~ math_1 + read_1, data = d_learn)\nsummary(m_math)"
  },
  {
    "objectID": "tutorials/StatsIntro/Distributions.html",
    "href": "tutorials/StatsIntro/Distributions.html",
    "title": "Distributions",
    "section": "",
    "text": "The statistical program R is a free, open-source project that is used by millions of people all over the world. It is so good that I would prefer it over other programs even if it were not free. The fact that it will always be free is simply amazing.\nThe downside of R is that it can be intimidating for beginners. It can be finicky in ways that can frustrate even advanced users. Fortunately, R can be extended and adapted by anyone (including you). These extensions are called packages. The program we use, Jamovi, is built atop R, and has a number of R packages associated with it to make Jamovi a free and easy-to-use alternative to commercial statistical programs that can be quite costly. Think of Jamovi as an interface that makes R much easier way to interact with R. In fact, to use Jamovi, you do not need to know anything about R at all. However, if you need Jamovi to do something that it does not do out of the box, you can ask Jamovi to ask R to do it."
  },
  {
    "objectID": "tutorials/StatsIntro/Distributions.html#making-frequency-distribution-tables-with-jamovi",
    "href": "tutorials/StatsIntro/Distributions.html#making-frequency-distribution-tables-with-jamovi",
    "title": "Distributions",
    "section": "Making Frequency Distribution Tables with Jamovi",
    "text": "Making Frequency Distribution Tables with Jamovi\nIn the menu, select Analyses→Exploration→Descriptions. That is, select the Analyses tab, then the Exploration icon, then the Descriptives menu item.\n\nNow drag the Quiz1 variable into the Variables box. Alternately, you can select Quiz1 and then click the arrow next to the Variables box.\nCheck the Frequency tables checkbox.\n\nIn the Results pane, you should see Descriptives with some summary statistics for Quiz1 and a Frequencies table for Quiz1\n\nThe Levels are the values the variables has. The Counts are the frequencies of the each level. The % of Total column is the Counts divided by the sample size. The Cumulative % column tells the percentage of scores at that level or less."
  },
  {
    "objectID": "tutorials/StatsIntro/Distributions.html#bar-plots",
    "href": "tutorials/StatsIntro/Distributions.html#bar-plots",
    "title": "Distributions",
    "section": "Bar Plots",
    "text": "Bar Plots\nTo display the distribution of a categorical variable one should use a bar plot. These are used most often to display the distribution of subjects or cases in certain categories, such as the number of A, B, C, D, and F grades in a given class.\nLet’s start with looking at the distribution of ethnicity in our data file. So what our graph will show are the counts (or frequency) for each of ethnic category.\n\nFrom the menu click Analyses→Exploration→Descriptives.\nDrag the Ethnicity variable into the Variables box.\nClick Plots\nCheck Bar plot.\n\nYou should get a bar chart that looks something like this.\n\nMake a bar graph of the counts of the final grades (called FinalClassGrade in the file) in the class (i.e. A, B, C,…).\nMake a bar graph of the counts of the final grades in the class (i.e. A, B, C,…), further broken down by whether they attended the review session or not.\nDrag the Review variable into the SplitBy box."
  },
  {
    "objectID": "tutorials/StatsIntro/Distributions.html#histograms",
    "href": "tutorials/StatsIntro/Distributions.html#histograms",
    "title": "Distributions",
    "section": "Histograms",
    "text": "Histograms\nSuppose that we wish to know how the students did on quiz 1. We could try looking at all of the scores, but that’s a lot of numbers. Instead, it is better to try to look at the entire distribution, rather than all of the individual scores. We should use a histogram because our variable (score on Quiz1) is a continuous variable.\nHistogram: A histogram is a pictorial representation of the distribution of values for a particular variable. The bars represent the number of occurrences of each value. These look similar to bar graphs except they are used more often to indicate the number of subjects or cases in ranges of values for a continuous variable, such as the number of subjects or cases in ranges of values for a continuous variable."
  },
  {
    "objectID": "tutorials/StatsIntro/Distributions.html#using-jamovi-to-create-a-histogram",
    "href": "tutorials/StatsIntro/Distributions.html#using-jamovi-to-create-a-histogram",
    "title": "Distributions",
    "section": "Using Jamovi to create a histogram:",
    "text": "Using Jamovi to create a histogram:\n\nFrom the menu click Analyses→Exploration→Descriptives.\nDrag the TotalPercent variable into the Variables box.\nClick Plots\nCheck Histogram."
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Getting Ready to Use R and RStudio\n\n\n\n\n\nTips for Installation and Configuration\n\n\n\n\n\nJan 20, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling\n\n\n\n\n\nGetting Data into Useable Formats\n\n\n\n\n\nMar 1, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA in R\n\n\n\n\n\nPlot group means and test to see if their differences are statistically significant.\n\n\n\n\n\nMar 5, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRegression in R\n\n\n\n\n\nA step-by-step introduction to regresion in R\n\n\n\n\n\nMar 7, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Factor Analysis with R\n\n\n\n\n\nA step-by-step introduction to exploratory factor analysis in R\n\n\n\n\n\nMar 12, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Intercept Models in R\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nRegression in which each group has its own intercept\n\n\n\n\n\nJan 27, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Slopes\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nRegression in which each group has its own slope for some predictors\n\n\n\n\n\nFeb 6, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nMultilevel Models with Three Levels\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nRegression in which there are groups within groups\n\n\n\n\n\nFeb 11, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Mixed Models\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nMultilevel models in which the residuals are not normally distributed\n\n\n\n\n\nFeb 18, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nData Simulation for Multilevel Models\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nSimulating data that conforms to a specified model. Useful for power analysis and other kinds of “what-if” thought experiments.\n\n\n\n\n\nFeb 25, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nMultilevel Modeling of Longitudinal Data in R\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nMultilevel models about how and why variables change over time\n\n\n\n\n\nMar 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLatent Growth Models\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nUsing structural equations to understand change over time\n\n\n\n\n\nMar 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBivariate Change Models\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nHow two variables influence each other over time\n\n\n\n\n\nMar 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPower Analysis with Multilevel Models\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nWhat sample size is needed to reliably find effects in hypothesized models?\n\n\n\n\n\nApr 1, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurement Scales\n\n\nIntroduction to Statistics and Research (EDUC 5325 )\n\n\n\n\n\n\n\n\nJan 12, 2026\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nDistributions\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2026\n\n\n\n\n\nNo matching items"
  }
]