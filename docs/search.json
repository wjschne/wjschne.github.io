[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "W. Joel Schneider, Ph.D.\nProfessor\nTemple University\nPsychological Studies in Education\n\n\n\nCounseling Psychology\n\n\nSchool Psychology\n\n\n\n\n\n Contact Me\n Curriculum Vitae\n ORCID\n Google Scholar\n Open Science Framework\n ResearchGate\n Mastodon\n Zotero\n Stackoverflow\n YouTube\n GitHub\n\n\n\n\n\n\n\nI am a professor at Temple University in the College of Education and Human Development in the Psychological Studies in Education Department. I belong to both the School Psychology and Counseling Psychology programs.\nAlthough I have diverse interests, my primary focus is trying to understand and improve the validity of psychological assessment practices. I teach courses in assessment, counseling, statistics, and research methods.\nI grew up in Southern California (1970–1990) and lived in Córdoba, Argentina (1990–1992). I completed my undergraduate degree in Psychology at the University of California at Berkeley in 1995 and my doctoral studies in clinical psychology at Texas A&M University in 2003. My internship year (2001–2002) was spent in the Dutchess County Department of Behavioral & Community Health in Poughkeepsie, NY. Since then I have been a faculty member at Illinois State University (2002–2017) and Temple University (2017–Present)."
  },
  {
    "objectID": "tutorials/Regression/regression.html",
    "href": "tutorials/Regression/regression.html",
    "title": "Regression in R",
    "section": "",
    "text": "Here we are going to use a small data set to predict people’s height using the height of their parents."
  },
  {
    "objectID": "tutorials/Regression/regression.html#install-tidyverse",
    "href": "tutorials/Regression/regression.html#install-tidyverse",
    "title": "Regression in R",
    "section": "Install tidyverse",
    "text": "Install tidyverse\nSome packages are designed to work with several other packages as a system. The tidyverse package is a “meta-package” that installs and loads a coherent set of packages designed to help you import, manipulate, visualize, and interpret data. If you do not have a recent version of tidyverse already installed, you can install it with this code:\n\ninstall.packages(\"tidyverse\")"
  },
  {
    "objectID": "tutorials/Regression/regression.html#install-easystats",
    "href": "tutorials/Regression/regression.html#install-easystats",
    "title": "Regression in R",
    "section": "Install easystats",
    "text": "Install easystats\nThe easystats package is another “meta-package” that installs a set of packages designed to work together to make data analysis easier.\nIf you do not have a recent version of easystats already installed, you can install it with this code:\n\ninstall.packages(\"easystats\")"
  },
  {
    "objectID": "tutorials/Regression/regression.html#save-the-plot",
    "href": "tutorials/Regression/regression.html#save-the-plot",
    "title": "Regression in R",
    "section": "Save the plot!",
    "text": "Save the plot!\nYou can save to your hard drive a high-quality plot with the ggsave function. It will save whatever the last plot you created with ggplot. It will guess the file format from the file extension.\nHere I save a .pdf file of the plot to the working directory:\n\nggsave(\"my_plot.pdf\")\n\nWhat is the working directory? Is the folder on your machine that R thinks it should look first if it needs to find files or save files. If you are ever curious about which directory is the working directory, you can see the current working directory with the getwd function:\n\ngetwd()\n\nIf you saved the plot above as \"my_plot.pdf\", you will find the file in the working directory returned by getwd.\nIf you need to set the working working directly to something different from what it is, use the setwd function. In the Session menu in RStudio, you can also set the working directory with a point-an-click dialog box."
  },
  {
    "objectID": "tutorials/Regression/regression.html#vector-based-images",
    "href": "tutorials/Regression/regression.html#vector-based-images",
    "title": "Regression in R",
    "section": "Vector-based images",
    "text": "Vector-based images\nThe .pdf format gives the best image quality but can only be viewed in a .pdf reader. The .svg format is almost as good and can be incorporated into webpages and Office documents. One downside of their near-perfect image quality is that .pdf and .svg image file sizes can become quite large."
  },
  {
    "objectID": "tutorials/Regression/regression.html#raster-images",
    "href": "tutorials/Regression/regression.html#raster-images",
    "title": "Regression in R",
    "section": "Raster images",
    "text": "Raster images\nThe .png format gives good image quality and renders small file sizes. I prefer using the ragg::agg_png device to render a .png because it allows me to use any system font with no extra fuss.\n\nggsave(\"my_plot.png\", device = ragg::agg_png)\n\nThe primary use of the .gif format is to create animated plots. Otherwise stick with .png.\nAlthough the .jpg format is good for photos, it is terrible for plots—it often renders text and sharp corners with pixelated smudges."
  },
  {
    "objectID": "tutorials/Regression/regression.html#posterier-predictions",
    "href": "tutorials/Regression/regression.html#posterier-predictions",
    "title": "Regression in R",
    "section": "Posterier Predictions",
    "text": "Posterier Predictions\n\ndiagnostic_plots &lt;- plot(check_model(m1, panel = FALSE))\n\nFor confidence bands, please install `qqplotr`.\n\ndiagnostic_plots[[1]]\n\nIgnoring unknown labels:\n• size : \"\"\n\n\n\n\n\n\n\n\nFigure 5: Posterior Predictive Check\n\n\n\n\n\nThe Figure 5 generates several sets of random “simulated” data based on the model and plots the distributions as the thin blue lines. Each simulated data set is of the same size as the original data. The thicker green line is based on the observed data. If the blue lines have roughly the same shape as the green line, then the model is likely of the right form.\nThe green line is not that far off from the blue lines, but the green line appears to have 2 peaks, and most of the blue lines have one peak. Because we know that males and females have different mean heights, there is a good chance that we need to model their heights separately. We will do so later in the tutorial.\nThe tutorial for the check_model function gives an example of a posterior prediction check that signals that something is awry."
  },
  {
    "objectID": "tutorials/Regression/regression.html#linearity-assumption",
    "href": "tutorials/Regression/regression.html#linearity-assumption",
    "title": "Regression in R",
    "section": "Linearity Assumption",
    "text": "Linearity Assumption\n\ndiagnostic_plots[[2]]\n\n\n\n\n\n\n\nFigure 6: Linearity assumption\n\n\n\n\n\nIn Figure 6, we see that the fitted values (i.e., \\hat{Y} or predicted values) plotted against the residuals is roughly flat and horizontal. If the green line were clearly not flat, we would consider non-linear models."
  },
  {
    "objectID": "tutorials/Regression/regression.html#homogeneity-of-variance-assumption",
    "href": "tutorials/Regression/regression.html#homogeneity-of-variance-assumption",
    "title": "Regression in R",
    "section": "Homogeneity of Variance Assumption",
    "text": "Homogeneity of Variance Assumption\n\ndiagnostic_plots[[3]]\n\n\n\n\n\n\n\nFigure 7: Homogeneity of variance assumption\n\n\n\n\n\nFigure 7 is similar to Figure 6 except that the fitted values are standardized (i.e., converted to z-scores) and then square root of their absolute values are plotted on the Y-axis. The homogeneity of variance assumption requires that variability of the residuals should be roughly the same across the entire distribution of fitted values. Thus, the line in Figure 7 should be roughly flat and horizontal, which it is. Indeed, we can draw a horizontal line entirely within the gray confidence region around the green line."
  },
  {
    "objectID": "tutorials/Regression/regression.html#influential-observations-and-outlier-detection",
    "href": "tutorials/Regression/regression.html#influential-observations-and-outlier-detection",
    "title": "Regression in R",
    "section": "Influential Observations and Outlier Detection",
    "text": "Influential Observations and Outlier Detection\n\ndiagnostic_plots[[4]]\n\n\n\n\n\n\n\nFigure 8: Influential observations\n\n\n\n\n\nSometimes a single outlier can radically alter a regression model. The plot in Figure 8 shows how influential each point is in creating the regression model. If any point is outside the green dotted lines, we might worry that the point has had undue influence on the model.\nMany additional outlier detection methods are available via the check_outliers function:\n\ncheck_outliers(m1, method = \"all\")\n\nOK: No outliers detected.\n- Based on the following methods and thresholds: zscore_robust (3.291), iqr (2), ci (1), cook (0.706), mahalanobis (13.816), mahalanobis_robust (13.816), mcd (13.816), ics (0.001), optics (4), lof (0.001).\n- For variable: (Whole model)"
  },
  {
    "objectID": "tutorials/Regression/regression.html#normality-assumption",
    "href": "tutorials/Regression/regression.html#normality-assumption",
    "title": "Regression in R",
    "section": "Normality Assumption",
    "text": "Normality Assumption\nThe normality assumption does not require that all variables be normal. It requires that the prediction residuals be approximately normal.\n\ndiagnostic_plots[[5]]\n\n\n\n\n\n\n\nFigure 9: Normality assumption\n\n\n\n\n\nIf the residuals were perfectly normal, the blue dots would fall exactly on the green line in Figure 9. The blue dots are not that far from the green line. Because ordinary least squares is robust to minor violations of the normality assumption, nothing in Figure 9 should make us worry about needing to model the data with some other kind of residuals.\nIf we suspect that the residuals are generated from a different distribution, the check_distribution function will estimate the probability that the residuals (and response/outcome variable) come from 12 major distribution families:\n\ncheck_distribution(m1) \n\n\n\n\n\nDistribution\np_Residuals\np_Response\n\n\n\n\nbernoulli\n0.00000\n0.00000\n\n\nbeta\n0.03125\n0.00000\n\n\nbeta-binomial\n0.00000\n0.40625\n\n\nbinomial\n0.00000\n0.40625\n\n\ncauchy\n0.09375\n0.03125\n\n\nchi\n0.03125\n0.03125\n\n\nexponential\n0.00000\n0.00000\n\n\nF\n0.00000\n0.00000\n\n\ngamma\n0.00000\n0.00000\n\n\nhalf-cauchy\n0.00000\n0.00000\n\n\ninverse-gamma\n0.00000\n0.00000\n\n\nlognormal\n0.00000\n0.00000\n\n\nneg. binomial (zero-infl.)\n0.00000\n0.03125\n\n\nnegative binomial\n0.06250\n0.00000\n\n\nnormal\n0.59375\n0.03125\n\n\npareto\n0.03125\n0.00000\n\n\npoisson\n0.00000\n0.00000\n\n\npoisson (zero-infl.)\n0.00000\n0.03125\n\n\ntweedie\n0.15625\n0.00000\n\n\nuniform\n0.00000\n0.00000\n\n\nweibull\n0.00000\n0.03125\n\n\n\n\n\nplot(check_distribution(m1))\n\n\n\n\n\n\n\n\nHere we see that of the 12 major distribution families tested, the residuals are more likely normal than any of the others. After we control for gender, this probability will increase. If the results had strongly suggested the residuals were generated from one of the other distributions, we might consider switching from a linear model to a generalized linear model."
  },
  {
    "objectID": "tutorials/Regression/regression.html#model-level-statistics",
    "href": "tutorials/Regression/regression.html#model-level-statistics",
    "title": "Regression in R",
    "section": "Model-level statistics",
    "text": "Model-level statistics\nSome statistics like the coefficient of determination (R2) or the standard error of the estimate (σe) describe the model as a whole.\n\nR2\nThe R2 statistic measures the percentage of variance in the outcome explained by the predictors.\n\nR^2=\\frac{\\sigma_{\\hat{Y}}^2}{\\sigma_{Y}^2}=1-\\frac{\\sigma_e^2}{\\sigma_Y^2}\n\nThe model-level statistics can be extracted with the performance package’s model_performance function.\n\nperformance(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAIC\nAICc\nBIC\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\n196.3596\n197.0262\n201.4262\n0.640591\n0.6311328\n2.613149\n2.681034\n\n\n\n\n\n\nR2 is the percentage of variance in the outcome variable (i.e., Height) explained by the predictor variables (i.e., Average Parent Height). In Figure 10, the black line toggles between the best fitting regression line and the horizontal line running through the outcome variable’s mean. The vertical line segments running from each point to the black line are, on average, smaller for the best fitting regression line. Their squared distances shrink, on average, by 64%. Thus, we say that the predictor variance “explains” 64% of the variance in the outcome variable.\n\n\nCode\nr &lt;- d %&gt;% select(avgphgt, height) %&gt;% cor() %&gt;% `[[`(1, 2)\nm &lt;- d %&gt;% select(avgphgt, height) %&gt;% colMeans()\ns &lt;- d %&gt;% select(avgphgt, height) %&gt;% sapply(sd)\nn &lt;- 2\nd_slope &lt;- tibble(id = c(\"0.00\", round(r^2, 2)),\n                  r = seq(0, r, length.out = n)) %&gt;%\n  mutate(i = -1 * r * m[1] * s[2] / s[1] + m[2], \n         b = r * s[2] / s[1])\nlibrary(gganimate)\nanim &lt;- d %&gt;% select(idnum, avgphgt, height) %&gt;%\n  crossing(d_slope) %&gt;%\n  mutate(yhat = b * avgphgt + i, \n         Error = height - yhat) %&gt;%\n  arrange(idnum, Error) %&gt;%\n  ggplot(aes(avgphgt, height)) +\n  geom_abline(aes(slope = b, intercept = i)) +\n  geom_segment(aes(xend = avgphgt, yend = yhat, color = Error)) +\n  geom_point() +\n  transition_states(id, state_length = 3, transition_length = 1) +\n  scale_color_gradient2(\n    low = \"royalblue4\",\n    mid = \"gray\",\n    high = \"firebrick4\",\n    breaks = seq(-6, 6, 2),\n    labels = signs::signs\n  ) +\n  scale_x_continuous(\"Average Parent Height (inches)\", limits = c(60, 75)) +\n  scale_y_continuous(\"Student Height (inches)\", limits = c(60, 75)) +\n  labs(title = \"Variance Explained = {closest_state}\") +\n  coord_equal() +\n  theme_minimal(base_family = \"Roboto Condensed\", base_size = 18) +\n  theme(\n    legend.text.position = \"left\",\n    legend.text = element_text(hjust = 1),\n    legend.key.height = unit(2, \"cm\")\n  )\ngganimate::animate(\n  anim,\n  nframes = 50 * 4,\n  device = \"ragg_png\",\n  width = 10,\n  height = 10,\n  fps = 50,\n  res = 144\n)\n\n\n\n\n\n\n\n\nFigure 10: Percentage of Variance Explained\n\n\n\n\n\nIf all you wanted was the R2, you could do this:\n\nperformance(m1)$R2\n\n[1] 0.640591\n\n\nWhy would you want just one number instead of reading it from a table? In reproducible research, we intermingle text and code so that it is clear where every number came from. Thus, “hard-coding” your results like this is considered poor practice:\nThe model explains 64% of the variance.\nUsing rmarkdown, instead of typing the numeric results, we type pull the results using an inline code chunk:\nThe model explains `{r} round(100 * r2(m1)$R2, 0)`% of the variance.\nWhich, when rendered, produces the correct output:\n\nThe model explains 64% of the variance.\n\nThat seems like a lot of extra work, right? Yes, it is—unless there is a possibility that your underlying data might change or that you might copy your numbers incorrectly. If you are imperfect, the extra time and effort is worth it. It makes it easy for other scholars to see exactly where each number came from. Hard-coded results are harder to trust.\nRegression errors are the vertical distances of the outcome variable and regression line. That is, errors are the difference between the outcome and the predicted outcome:\n\ne=Y-\\hat{Y}\n\n\n\nStandard Error of the Estimate (σe)\nThe standard error of the estimate (i.e., Sigma in the performance output table) is the standard deviation of the regression errors \\sigma_{e}. It represents the typical size of the prediction error. That is, when we make a prediction, how far off is that prediction likely to be?\nThe standard error of the estimate can be extracted from a regression fit object with the sigma function:\n\nsigma(m1)\n\n[1] 2.681034\n\n\nAlternately, it can be extracted from the performance function’s output:\n\nperformance(m1)$Sigma\n\n[1] 2.681034"
  },
  {
    "objectID": "tutorials/Regression/regression.html#coefficient-level-statistics",
    "href": "tutorials/Regression/regression.html#coefficient-level-statistics",
    "title": "Regression in R",
    "section": "Coefficient-level statistics",
    "text": "Coefficient-level statistics\nThe regression coefficients—the intercept (b0) and the slope (b1)—have a number of statistics associated with them, which we will discuss later in the course.\nIf you just wanted the intercept and the slope coefficients, use the coef function:\n\ncoef(m1)\n\n(Intercept)     avgphgt \n -12.878215    1.192926 \n\n\nThus, the intercept is -12.88 and the slope is 1.19.\nTo get the model parameters (intercept and slope coefficients) along with their standard errors, confidence intervals, t statistics, and p-values:\n\nparameters(m1) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n-12.878215\n9.7789931\n0.95\n-32.674752\n6.918321\n-1.316927\n38\n0.1957502\n\n\navgphgt\n1.192926\n0.1449525\n0.95\n0.899485\n1.486367\n8.229770\n38\n0.0000000\n\n\n\n\n\n\nStandardized parameters are the regression coefficients if all variables in the analysis were standardized (i.e., convernted to z-scores). Standardized coefficients have a straightforward interpretation: They represent the predicted change in the outcome associated with a change of 1 standard deviation in the predictor variable.\n\nstandardize_parameters(m1)\n\n\n\n\n\nParameter\nStd_Coefficient\nCI\nCI_low\nCI_high\n\n\n\n\n(Intercept)\n0.0000000\n0.95\n-0.1944017\n0.1944017\n\n\navgphgt\n0.8003693\n0.95\n0.6034910\n0.9972475"
  },
  {
    "objectID": "tutorials/Regression/regression.html#checking-assumptions-1",
    "href": "tutorials/Regression/regression.html#checking-assumptions-1",
    "title": "Regression in R",
    "section": "Checking assumptions",
    "text": "Checking assumptions\n\ncheck_model(m2)\n\n\n\n\n\n\n\nFigure 11: Assumption check for model 2\n\n\n\n\n\nThe diagnostic plots in Figure 11 looks good. It has one additional plot checking the collinearity of the predictors. If any predictor is strongly predictable from the other predictors, it is difficult for ordinary least squares regression to locate the regression coefficients with precision. Why? Because the regression coefficients represent the independent effect of each variable controlling for all the other variables. If a predictor has little variability left after controlling for the other variables, estimating its independent effect is difficult, which is manifest in “inflated” standardized errors around the coefficient. The VIF (variance inflation factor) statistic estimates how much the standard errors are made large due to collinearity. If the VIF is larger than 10 or so, it is likely that the estimate of the coefficient is not precise."
  },
  {
    "objectID": "tutorials/Regression/regression.html#summarizing-results",
    "href": "tutorials/Regression/regression.html#summarizing-results",
    "title": "Regression in R",
    "section": "Summarizing results",
    "text": "Summarizing results\nTo summarize the results, use the summary function:\n\nsummary(m2)\n\n\nCall:\nlm(formula = height ~ avgphgt + gender, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8260 -1.0937 -0.3465  1.6563  4.5035 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.0986    10.3035   1.951 0.058704 .  \navgphgt       0.6705     0.1574   4.260 0.000134 ***\ngendermale    4.4680     0.9205   4.854 2.21e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.124 on 37 degrees of freedom\nMultiple R-squared:  0.7804,    Adjusted R-squared:  0.7685 \nF-statistic: 65.75 on 2 and 37 DF,  p-value: 6.603e-13\n\n\nModel-level statistics:\n\nperformance(m2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAIC\nAICc\nBIC\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\n178.6511\n179.7939\n185.4066\n0.780413\n0.7685434\n2.042551\n2.123744\n\n\n\n\n\n\nCoefficient-level statistics:\n\nparameters(m2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n20.0985997\n10.3035446\n0.95\n-0.7783648\n40.9755641\n1.950649\n37\n0.0587039\n\n\navgphgt\n0.6705107\n0.1573791\n0.95\n0.3516303\n0.9893911\n4.260480\n37\n0.0001345\n\n\ngendermale\n4.4679572\n0.9204998\n0.95\n2.6028475\n6.3330669\n4.853838\n37\n0.0000221\n\n\n\n\n\n\nStandardized coefficients:\n\nstandardize_parameters(m2)\n\n\n\n\n\nParameter\nStd_Coefficient\nCI\nCI_low\nCI_high\n\n\n\n\n(Intercept)\n-0.5060712\n0.95\n-0.7675756\n-0.2445668\n\n\navgphgt\n0.4498654\n0.95\n0.2359191\n0.6638117\n\n\ngendermale\n1.0121425\n0.95\n0.5896324\n1.4346525\n\n\n\n\n\n\nAn automated report:\n\nreport(m2)\n\nWe fitted a linear model (estimated using OLS) to predict height with avgphgt\nand gender (formula: height ~ avgphgt + gender). The model explains a\nstatistically significant and substantial proportion of variance (R2 = 0.78,\nF(2, 37) = 65.75, p &lt; .001, adj. R2 = 0.77). The model's intercept,\ncorresponding to avgphgt = 0 and gender = female, is at 20.10 (95% CI [-0.78,\n40.98], t(37) = 1.95, p = 0.059). Within this model:\n\n  - The effect of avgphgt is statistically significant and positive (beta = 0.67,\n95% CI [0.35, 0.99], t(37) = 4.26, p &lt; .001; Std. beta = 0.45, 95% CI [0.24,\n0.66])\n  - The effect of gender [male] is statistically significant and positive (beta =\n4.47, 95% CI [2.60, 6.33], t(37) = 4.85, p &lt; .001; Std. beta = 1.01, 95% CI\n[0.59, 1.43])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "tutorials/Regression/regression.html#comparing-models",
    "href": "tutorials/Regression/regression.html#comparing-models",
    "title": "Regression in R",
    "section": "Comparing models",
    "text": "Comparing models\nIn the first model, there was only one predictor. The second model had an additional predictor. To test whether the second predictor has incremental validity (predicts variance in the outcome beyond what is predicted by other predictors), we can compare the two models using the Wald test:\n\ntest_wald(m1, m2)\n\n\n\n\n\nName\nModel\ndf\ndf_diff\nF\np\n\n\n\n\nm1\nlm\n38\nNA\nNA\nNA\n\n\nm2\nlm\n37\n1\n23.55975\n2.21e-05\n\n\n\n\n\n\nThe Wald test can also be conducted in base R with the anova function:\n\nanova(m1, m2)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n38\n273.1419\nNA\nNA\nNA\nNA\n\n\n37\n166.8806\n1\n106.2612\n23.55975\n2.21e-05\n\n\n\n\n\n\nEither way, the p-value is significant, meaning that m2 explains more variance than m1.\nThe semi-partial correlation coefficient squared tells us how much incremental variance each predictor has over the other:\n\nr2_semipartial(m2)\n\n\n\n\n\nTerm\nr2_semipartial\nCI\nCI_low\nCI_high\n\n\n\n\navgphgt\n0.1077264\n0.95\n0.0192805\n1\n\n\ngender\n0.1398220\n0.95\n0.0371881\n1\n\n\n\n\n\n\nThus, gender explains 14% variance beyond avgphgt. Likewise, avgphgt explains 11% variance beyond gender.\n\nBayes Factors\nAn alternate method of comparing two models uses the Bayes factor. It tells us under which model the observed data are more probable.\n\ntest_bf(m1, m2)\n\n\n\n\n\nModel\nlog_BF\nBF\n\n\n\n\navgphgt\nNA\nNA\n\n\navgphgt + gender\n8.009811\n3010.348\n\n\n\n\n\n\nA BF &gt; 1 means that m2 is more strongly supported than m1. A BF &lt; 1 means that m1 is more strongly supported than m2. If you are not sure how to interpret the output of test_bf, you can get an automated interpretation:\n\ntest_bf(m1, m2) %&gt;% \n  report()\n\nBayes factors were computed using the BIC approximation, by which BF10 =\nexp((BIC0 - BIC1)/2). Compared to the avgphgt model, we found extreme evidence\n(BF = 3.01e+03) in favour of the avgphgt + gender model (the least supported\nmodel).\n\n\nTo compare many performance statistics at once:\n\ncompare_performance(m1, m2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nAIC\nAIC_wt\nAICc\nAICc_wt\nBIC\nBIC_wt\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\nm1\nlm\n196.3596\n0.0001428\n197.0262\n0.0001811\n201.4262\n0.0003321\n0.640591\n0.6311328\n2.613149\n2.681034\n\n\nm2\nlm\n178.6511\n0.9998572\n179.7939\n0.9998189\n185.4066\n0.9996679\n0.780413\n0.7685434\n2.042551\n2.123744"
  },
  {
    "objectID": "tutorials/Regression/regression.html#question-1",
    "href": "tutorials/Regression/regression.html#question-1",
    "title": "Regression in R",
    "section": "Question 1",
    "text": "Question 1\nIs read_1 a significant predictor of read_2?\n\n\n\n\n\n\nNoteHint 1\n\n\n\n\n\nMake model fit object called m_read using the lm function.\n\nm_read &lt;- lm(read_2 ~ read_1, data = d_learn)\n\n\n\n\n\n\n\n\n\n\nNoteHint 2\n\n\n\n\n\nView coefficient-level statistics with the parameters function.\nYou could also use Base R’s summary function.\n\nparameters(m_read)\n\n# or\n\nsummary(m_read)\n\n\n\n\n\n\n\n\n\n\nNoteHint 3\n\n\n\n\n\nIn the read_1 row, is the p.value column less than 0.05?\n\nparameters(m_read)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n83.5910408\n10.9217563\n0.95\n61.4406923\n105.7413892\n7.653626\n36\n0.0000000\n\n\nread_1\n0.2514884\n0.1044825\n0.95\n0.0395881\n0.4633888\n2.406991\n36\n0.0213387"
  },
  {
    "objectID": "tutorials/Regression/regression.html#question-2",
    "href": "tutorials/Regression/regression.html#question-2",
    "title": "Regression in R",
    "section": "Question 2",
    "text": "Question 2\nWhat is the R2 for the m_read model?\n\n\n\n\n\n\nNoteHint\n\n\n\n\n\nView model-level statistics with the performance function.\nYou could also use Base R’s summary function.\n\nperformance(m_read)\n\n# or\n\nsummary(m_read)"
  },
  {
    "objectID": "tutorials/Regression/regression.html#question-3",
    "href": "tutorials/Regression/regression.html#question-3",
    "title": "Regression in R",
    "section": "Question 3",
    "text": "Question 3\nWhat does the scatter plot look like when read_1 is on the x-axis and read_2 is on the y-axis? Also plot the regression line. Save your plot using the ggsave function.\n\n\n\n\n\n\nNoteHint for Getting Started\n\n\n\n\n\nThis will get you started\n\nggplot(d_learn, aes(read_1, read_2))\n\n\n\n\n\n\n\n\n\n\nNoteHint for Adding Points\n\n\n\n\n\nHere is how you add points.\n\nggplot(d_learn, aes(read_1, read_2)) +\n  geom_point() \n\n\n\n\n\n\n\n\n\n\nNoteHint for Adding a Regression Line\n\n\n\n\n\nHere is how you add a regression line.\n\nggplot(d_learn, aes(read_1, read_2)) +\n  geom_point() + \n  geom_smooth(method = \"lm\") \n\n\n\n\n\n\n\n\n\n\nNotePlot Polishing\n\n\n\n\n\nThis is overkill for now. But someday you might want to be able to make your plots as polished as possible.\n\n# I want to put the equation at x = 130\nequation_x &lt;- 130\nequation_y &lt;- predict(m_read, newdata = tibble(read_1 = equation_x))\n\n# Extracting the coefficients\nb_read &lt;- round(coef(m_read),2)\n\n# The angle of the regression line is the inverse tangent of the slope (converted to degrees)\neq_angle &lt;- atan(b_read[2]) * 180 / pi\n\n# Equation\neq_read &lt;- paste0(\"italic(Y) == \", \n                  b_read[1], \n                  \" + \", \n                  b_read[2], \n                  \" *  italic(X) + italic(e)\")\n\nggplot(d_learn, aes(read_1, read_2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Reading at Time 1\",\n       y = \"Reading at Time 2\",\n       title = \"Using Reading at Time 1 to Predict Reading at Time 2\") +\n  coord_fixed() +\n  theme_minimal() +\n  annotate(\n    geom = \"text\",\n    x = equation_x,\n    y = equation_y,\n    angle = eq_angle,\n    label = eq_read,\n    parse = TRUE,\n    vjust = -0.5)"
  },
  {
    "objectID": "tutorials/Regression/regression.html#question-4",
    "href": "tutorials/Regression/regression.html#question-4",
    "title": "Regression in R",
    "section": "Question 4",
    "text": "Question 4\nCreate a regression model in which you predict math at time 2 (math_2) using reading at time 1 (math_1). Call the model fit object m_math1.\nDoes math_1 predict math_2?.\nDoes math_1 still predict math_2 after controlling for read_1?\n\n\n\n\n\n\nNoteHint\n\n\n\n\n\n\nm_math &lt;- lm(math_2 ~ math_1 + read_1, data = d_learn)\nsummary(m_math)"
  },
  {
    "objectID": "vita.html#w.-joel-schneider",
    "href": "vita.html#w.-joel-schneider",
    "title": "Curriculum Vitae",
    "section": "W. Joel Schneider",
    "text": "W. Joel Schneider\n\n\nTemple University\nCollege of Education & Human Development\nPsychological Studies in Education\n493 Ritter Hall\nPhiladelphia, PA 19122-6091\n\nOffice: (215) 204-8093\nEmail: schneider@temple.edu\nWeb: Faculty Profile\nAssessment Blog: AssessingPsyche R Blog: AssessingPsyche"
  },
  {
    "objectID": "vita.html#publication-indices",
    "href": "vita.html#publication-indices",
    "title": "Curriculum Vitae",
    "section": "Publication Indices",
    "text": "Publication Indices\n\nGoogle Scholar Citations: 5098\nh-index: 28\ni10-index: 42\n\n\n\n\n\n\nGoogle Scholar Citations of My Work"
  },
  {
    "objectID": "vita.html#books",
    "href": "vita.html#books",
    "title": "Curriculum Vitae",
    "section": "Books",
    "text": "Books\n\nCohen, R. J., Schneider, W. J., & Tobin, R. M. (2025). Psychological testing and assessment: An introduction to tests and measurement (11th ed.). McGraw Hill LLC.\n\n\nSchneider, W. J. (2024). Individual psychometrics: An assessment toolkit with applications in R. Author.\n\n\nCohen, R. J., Schneider, W. J., & Tobin, R. M. (2022). Psychological testing and assessment: An introduction to tests and measurement (10th ed.). McGraw Hill LLC.\n\n\nSchneider, W. J., Mather, N., Lichtenberger, E. O., & Kaufman, N. L. (2018). Essentials of assessment report writing (2nd ed.). Wiley."
  },
  {
    "objectID": "vita.html#journal-articles",
    "href": "vita.html#journal-articles",
    "title": "Curriculum Vitae",
    "section": "Journal Articles",
    "text": "Journal Articles\n\nLevi-Nielsen, S., Tobin, R. M., & Schneider, W. J. (2026). A Systematic Review and Single-Case Meta-Analysis of Performance Feedback as Teacher Professional Development. Journal of School Psychology, 115, 101527. https://doi.org/10.1016/j.jsp.2025.101527\n\n\nHajovsky, D. B., Niileksela, C. R., Flanagan, D. P., Alfonso, V. C., Schneider, W. J., & Robbins, J. (2025). Toward a consensus model of cognitive–reading achievement relations using meta-structural equation modeling. Journal of Intelligence, 13(8), 104. https://doi.org/10.3390/jintelligence13080104\n\n\nKane, C., Sandilos, L., Schneider, W. J., & Tobin, R. M. (2025). The influence of social-emotional learning programs on key outcomes for dual language learners in Head Start. Early Education and Development, 1–24. https://doi.org/10.1080/10409289.2025.2526310\n\n\nSchneider, W. J., Flanagan, D. P., Niileksela, C. R., & Engler, J. R. (2024). The effect of measurement error on the positive predictive value of PSW methods for SLD identification: How buffer zones dispel the illusion of inaccuracy. Journal of School Psychology, 103, 101280. https://doi.org/10.1016/j.jsp.2023.101280\n\n\nMcGrew, K. S., Schneider, W. J., Decker, S. L., & Bulut, O. (2023). A psychometric network analysis of CHC intelligence measures: Implications for research, theory, and interpretation of broad CHC scores “beyond g”. Journal of Intelligence, 11(1), 19. https://doi.org/10.3390/jintelligence11010019 \n\n\nSchneider, W. J., & Ji, F. (2023). Detecting unusual score patterns in the context of relevant predictors. Journal of Pediatric Neuropsychology, 9, 1–17. https://doi.org/10.1007/s40817-022-00137-x \n\n\nDowdy, A., Peltier, C., Tincani, M., Schneider, W. J., Hantula, D. A., & Travers, J. C. (2021). Meta‐analyses and effect sizes in applied behavior analysis: A review and discussion. Journal of Applied Behavior Analysis, 54(4), 1317–1340. https://doi.org/10.1002/jaba.862 \n\n\nDowdy, A., Tincani, M., & Schneider, W. J. (2020). Evaluation of publication bias in response interruption and redirection: A meta‐analysis. Journal of Applied Behavior Analysis, 53(4), 2151–2171. https://doi.org/10.1002/jaba.724 \n\n\nHajovsky, D. B., Villeneuve, E. F., Schneider, W. J., & Caemmerer, J. M. (2020). An alternative approach to cognitive and achievement relations research: An introduction to quantile regression. Journal of Pediatric Neuropsychology, 6, 83–95. https://doi.org/10.1007/s40817-020-00086-3 \n\n\nDombrowski, S. C., Beaujean, A. A., McGill, R. J., Benson, N. F., & Schneider, W. J. (2019). Using exploratory bifactor analysis to understand the latent structure of multidimensional psychological measures: An example featuring the WISC-V. Structural Equation Modeling: A Multidisciplinary Journal, 26(6), 847–860. https://doi.org/10.1080/10705511.2019.1622421 \n\n\nSchneider, W. J., & McGrew, K. S. (2019). Process Overlap Theory is a milestone achievement among intelligence theories. Journal of Applied Research in Memory and Cognition, 8(3), 273–276. https://doi.org/https://doi.org/10.1016/j.jarmac.2019.06.006 \n\n\nSchneider, W. J., & Roman, Z. (2018). Fine-tuning Cross-Battery Assessment procedures: After follow-up testing, use all valid scores, cohesive or not. Journal of Psychoeducational Assessment, 36(1), 34–54. https://doi.org/10.1177/0734282917722861 \n\n\nMagoon, M. A., Critchfield, T. S., Merrill, D., Newland, M. C., & Schneider, W. J. (2017). Are positive and negative reinforcement “different”? Insights from a free-operant differential outcomes effect. Journal of the Experimental Analysis of Behavior, 107(1), 39–64. https://doi.org/10.1002/jeab.243 \n\n\nSchneider, W. J., & Kaufman, A. S. (2017). Let’s not do away with comprehensive cognitive assessments just yet. Archives of Clinical Neuropsychology, 32(1), 8–20. https://doi.org/10.1093/arclin/acw104 \n\n\nFlanagan, D. P., & Schneider, W. J. (2016). Cross-Battery Assessment? XBA PSW? A case of mistaken identity: A commentary on Kranzler and colleagues’ “Classification agreement analysis of Cross-Battery Assessment in the identification of specific learning disorders in children and youth”. International Journal of School & Educational Psychology, 4(3), 137–145. https://doi.org/10.1080/21683603.2016.1192852 \n\n\nGadke, D. L., Tobin, R. M., & Schneider, W. J. (2016). Agreeableness, conflict resolution tactics, and school behavior in second graders. Journal of Individual Differences, 37, 145–151. https://doi.org/10.1027/1614-0001/a000199 \n\n\nSchneider, W. J., & Kaufman, A. S. (2016). Commentary on current practices and future directions for the assessment of child and adolescent intelligence in schools around the world. International Journal of School & Educational Psychology, 4(4), 283–288. https://doi.org/10.1080/21683603.2016.1206383 \n\n\nSchneider, W. J., Mayer, J. D., & Newman, D. A. (2016). Integrating hot and cool intelligences: Thinking broadly about broad abilities. Journal of Intelligence, 4(1), 1:1–25. https://doi.org/10.3390/jintelligence4010001 \n\n\nSchneider, W. J., & Newman, D. A. (2015). Intelligence is multidimensional: Theoretical review and implications of specific cognitive abilities. Human Resource Management Review, 25(1), 12–27. https://doi.org/10.1016/j.hrmr.2014.09.004 \n\n\nAbney, D. H., Wagman, J. B., & Schneider, W. J. (2014). Changing grasp position on a wielded object provides self-training for the perception of length. Attention, Perception, & Psychophysics, 76(1), 247–254. https://doi.org/10.3758/s13414-013-0550-x \n\n\nPornprasertmanit, S., & Schneider, W. J. (2014). Accuracy in parameter estimation in cluster randomized designs. Psychological Methods, 19(3), 356–379. https://doi.org/10.1037/a0037036 \n\n\nHerbstrith, J. C., Tobin, R. M., Hesson-McInnis, M. S., & Schneider, W. J. (2013). Preservice teacher attitudes toward gay and lesbian parents. School Psychology Quarterly, 28(3), 183–194. https://doi.org/10.1037/spq0000022 \n\n\nKahn, J. H., & Schneider, W. J. (2013). It’s the destination and it’s the journey: Using multilevel modeling to assess patterns of change in psychotherapy. Journal of Clinical Psychology, 69(6), 543–570. https://doi.org/10.1002/jclp.21964 \n\n\nSchneider, W. J. (2013). What if we took our models seriously? Estimating latent scores in individuals. Journal of Psychoeducational Assessment, 31(2), 186–201. https://doi.org/10.1177/0734282913478046 \n\n\nDecker, S. L., Schneider, W. J., & Hale, J. B. (2012). Estimating base rates of impairment in neuropsychological test batteries: A comparison of quantitative models. Archives of Clinical Neuropsychology, 7(1), 69–84. https://doi.org/10.1093/arclin/acr088 \n\n\nJones, G., & Schneider, W. J. (2010). IQ in the production function: Evidence from immigrant earnings. Economic Inquiry, 48(3), 743–755. https://doi.org/10.1111/j.1465-7295.2008.00206.x \n\n\nGuidry, J. A., Babin, B. J., Graziano, W. G., & Schneider, W. J. (2009). Pride and prejudice in the evaluation of wine? International Journal of Wine Business Research, 21(4), 298–311. https://doi.org/10.1108/17511060911004888 \n\n\nHoff, K. E., Reese-Weber, M., Schneider, W. J., & Stagg, J. W. (2009). The association between high status positions and aggressive behavior in early adolescence. Journal of School Psychology, 47(6), 395–426. https://doi.org/10.1016/j.jsp.2009.07.003 \n\n\nBarr, L. K., Kahn, J. H., & Schneider, W. J. (2008). Individual differences in emotion expression: Hierarchical structure and relations with psychological distress. Journal of Social and Clinical Psychology, 27(10), 1045–1077. https://doi.org/10.1521/jscp.2008.27.10.1045 \n\n\nKahn, J. H., Vogel, D. L., Schneider, W. J., Barr, L. K., & Herrell, K. (2008). The emotional content of client disclosures and session impact: An analogue study. Psychotherapy: Theory, Research, Practice, Training, 45(4), 539–545. https://doi.org/10.1037/a0014337 \n\n\nSchneider, W. J. (2008). Playing statistical Ouija board with commonality analysis: good questions, wrong assumptions. Applied Neuropsychology, 15(1), 44–53. https://doi.org/10.1080/09084280801917566 \n\n\nJones, G., & Schneider, W. J. (2006). Intelligence, human capital, and economic growth: A Bayesian Averaging of Classical Estimates (BACE) approach. Journal of Economic Growth, 11(1), 71–93. https://doi.org/10.2139/ssrn.552481 \n\n\nSchneider, W. J., Timothy Cavell, A., & Hughes, J. N. (2003). A sense of containment: Potential moderator of the relation between parenting practices and children’s externalizing behaviors. Development and Psychopathology, 15(1), 95–117. https://doi.org/10.1017/S0954579403000063"
  },
  {
    "objectID": "vita.html#chapters",
    "href": "vita.html#chapters",
    "title": "Curriculum Vitae",
    "section": "Chapters",
    "text": "Chapters\n\nSchneider, W. J., & Tobin, R. M. (2024). Sophisticated simplicity: Writing reader-friendly assessment reports. In R. Flanagan (Ed.) Clinical guide to effective psychological assessment and report writing (pp. 1–11). Springer International Publishing. https://doi.org/10.1007/978-3-031-67184-5_1\n\n\nSchneider, W. J. (2022). Statistical and clinical interpretation guidelines for school neuropsychological assessment. In D. Miller, D. Maricle, C. Bedford, & J. Gettman (Eds.) Best Practices in School Neuropsychology: Guidelines for Effective Practice, Assessment, and Evidence‐Based Intervention (1st ed., pp. 163–184). Wiley. https://doi.org/10.1002/9781119790563\n\n\nFloyd, R. G., Farmer, R. L., Schneider, W. J., & McGrew, K. S. (2021). Theories and measurement of intelligence. In L. Glidden, L. Abbeduto, L. L. McIntyre, & M. J. Tassé (Eds.) APA handbook of intellectual and developmental disabilities (Vol. 1, pp. 386–424). American Psychological Association. https://doi.org/10.1037/0000194-015 \n\n\nKaufman, A. S., Schneider, W. J., & Kaufman, J. C. (2019). Psychometric approaches to intelligence. In R. J. Sternberg (Ed.) Human intelligence: An introduction (pp. 67–103). Cambridge University Press. \n\n\nSchneider, W. J., & McGrew, K. S. (2018). The Cattell-Horn-Carroll theory of intelligence. In D. P. Flanagan, & E. M. McDonough (Eds.) Contemporary intellectual assessment: Theories, tests, and issues (4th ed., pp. 73–130). Guilford Press. \n\n\nSchneider, W. J. (2016). Case 1—Liam, age 9: Emotionally intelligent testing with the WISC-V and CHC theory. In A. S. Kaufman, S. Raiford, & D. Coalson (Eds.) Intelligent testing with the WISC-V (pp. 265–282). Wiley.\n\n\nSchneider, W. J. (2016). Strengths and weaknesses of the Woodcock-Johnson IV Tests of Cognitive Abilities: Best practice from a scientist-practitioner perspective. In D. P. Flanagan, & V. C. Alfonso (Eds.) WJ IV Clinical Use and Interpretation (pp. 191–210). Academic Press. https://doi.org/10.1016/B978-0-12-802076-0.00007-4 \n\n\nSchneider, W. J., & Flanagan, D. P. (2015). The relationship between theories of intelligence and intelligence tests. In S. Goldstein, D. Princiotta, & J. A. Naglieri (Eds.) Handbook of intelligence: Evolutionary theory, historical perspective, and current concepts (pp. 317–340). Springer. \n\n\nTobin, R. M., Schneider, W. J., & Landau, S. (2014). Best practices in the assessment of youth with attention deficit hyperactivity disorder within a multitiered services framework. In A. Thomas, & P. Harrison (Eds.) Best practices in school psychology: Data-based and collaborative decision making (pp. 391–404). National Association of School Psychologists. \n\n\nSchneider, W. J. (2013). Principles of assessment of aptitude and achievement. In D. Saklofske, C. R. Reynolds, & V. Schwean (Eds.) The Oxford Handbook of Child Psychological Assessment (pp. 286–330). Oxford University Press. \n\n\nSchneider, W. J., & McGrew, K. S. (2013). Cognitive performance models: Individual differences in the ability to process information. In B. Irby, G. Brown, R. Laro-Alecio, & S. Jackson (Eds.) Handbook of educational theories (pp. 767–782). Information Age Publishing. \n\n\nSchneider, W. J., & McGrew, K. S. (2012). The Cattell-Horn-Carroll model of intelligence. In D. P. Flanagan, & P. L. Harrison (Eds.) Contemporary intellectual assessment: Theories, tests, and issues (3rd ed., pp. 99–144). Guilford Press. \n\n\nTobin, R. M., Schneider, W. J., Reck, S. G., & Landau, S. (2008). Best practices in the assessment of children with attention deficit hyperactivity disorder: Linking assessment to response to intervention. In P. Harrison, & A. Thomas (Eds.) Best Practices in School Psychology V (Vol. 2, pp. 617–631). National Association of School Psychologists. \n\n\nSnyder, D. K., Schneider, W. J., & Castellani, A. M. (2003). Tailoring couple therapy to individual differences: A conceptual approach. In D. K. Snyder (Ed.) Treating difficult couples: Helping clients with coexisting mental and relationship disorders (pp. 27–51). Guilford Press. \n\n\nSnyder, D. K., & Schneider, W. J. (2002). Affective reconstruction: A pluralistic, developmental approach. In N. S. Jacobson (Ed.) Clinical handbook of couple therapy (3rd ed., pp. 151–179). Guilford Press."
  },
  {
    "objectID": "vita.html#test-reviews",
    "href": "vita.html#test-reviews",
    "title": "Curriculum Vitae",
    "section": "Test Reviews",
    "text": "Test Reviews\n\nSwerdlik, M. E., & Schneider, W. J. (2010). Review of the PsychProfiler. In R. A. Spies, J. F. Carlson, B. S. Plake, & K. F. Geisinger (Eds.) The eighteenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J. (2007). Review of the Dean-Woodcock Neuropsychological Battery. In K. F. Geisinger, R. A. Spies, J. F. Carlson, & B. S. Plake (Eds.) The seventeenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J. (2007). Review of the Multiple Intelligence Developmental Assessment Scales (MIDAS). In K. F. Geisinger, R. A. Spies, J. F. Carlson, & B. S. Plake (Eds.) The seventeenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J., & Swerdlik, M. E. (2007). Review of the Youth Outcome Questionnaire 30.1. In K. F. Geisinger, R. A. Spies, J. F. Carlson, & B. S. Plake (Eds.) The seventeenth mental measurements yearbook. Buros Institute.\n\n\nSwerdlik, M. E., & Schneider, W. J. (2007). Review of the Behavior Evaluation Scale, Third Edition. In K. F. Geisinger, R. A. Spies, J. F. Carlson, & B. S. Plake (Eds.) The seventeenth mental measurements yearbook. Buros Institute.\n\n\nHoff, K. E., & Schneider, W. J. (2005). Review of the Child Symptom Inventory-4. In B. S. Plake, J. C. Impara, & R. A. Spies (Eds.) The sixteenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J., & Hoff, K. E. (2005). Review of the Word Identification and Spelling Test. In B. S. Plake, J. C. Impara, & R. A. Spies (Eds.) The sixteenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J., & Swerdlik, M. E. (2005). Review of the Memory Test for Older Adults. In B. S. Plake, J. C. Impara, & R. A. Spies (Eds.) The sixteenth mental measurements yearbook. Buros Institute.\n\n\nSwerdlik, M. E., & Schneider, W. J. (2005). Review of the Behavioral and Emotional Rating Scale-Second Edition. In B. S. Plake, J. C. Impara, & R. A. Spies (Eds.) The sixteenth mental measurements yearbook. Buros Institute."
  },
  {
    "objectID": "vita.html#scholarly-reports",
    "href": "vita.html#scholarly-reports",
    "title": "Curriculum Vitae",
    "section": "Scholarly Reports",
    "text": "Scholarly Reports\n\nBlume, J. H., & Paavola, E. C. (2025). Brief of Amici Curiae Randy W. Kamphaus, Kevin S. McGrew, Cecil R. Reynolds, W. Joel Schneider, and Marc J. Tasse, Blaine Keith Milam v. Texas. .\n\n\nGoldrick-Rab, S., Richardson, J., Schneider, W. J., Hernandez, A., & Cady, C. (2018). Still hungry and homeless in college. The Wisconsin HOPE Lab. \n\n\nSchneider, W. J. (2016). The RESCA-E subtests are thoughtfully designed and highly refined measures of CHC constructs: A review of the Receptive, Expressive & Social Communication Assessment–Elementary. Assessing Psyche, Engaging Gauss, Seeking Sophia.\n\n\nSchneider, W. J. (2016). Why are WJ IV cluster scores more extreme than the average of their parts? A gentle explanation of the composite score extremity effect. Houghton Mifflin HarcourtWoodcock-Johnson IV Assessment Service Bulletin.\n\n\nSchneider, W. J., & McGrew, K. S. (2011). “Just say no” to averaging IQ subtest scores. IAP Applied Psychometrics 101 Report."
  },
  {
    "objectID": "vita.html#software",
    "href": "vita.html#software",
    "title": "Curriculum Vitae",
    "section": "Software",
    "text": "Software\n\nSchneider, W. J. (2025). apa7: Facilitate Writing Documents in American Psychological Association Style, Seventh Edition. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.apa7\n\n\nSchneider, W. J. (2025). ggdiagram: Object-oriented diagram plots with ggplot2. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.ggdiagram\n\n\nSchneider, W. J. (2024). condppv: Conditional positive predictive value in the accuracy of specific learning disability identification. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/conditionalppv\n\n\nSchneider, W. J. (2023). apaquarto: A Quarto extension for creating APA 7 style documents. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/apaquarto\n\n\nSchneider, W. J. (2023). arrowheadr: Create custom arrowheads for ggplot2 via ggarrow.. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.arrowheadr\n\n\nSchneider, W. J. (2022). UnusualProfile: A user-friendly web application to impliment functions from the unusualprofile package.. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/unusualprofile_app\n\n\nSchneider, W. J. (2021). Area under the normal curve. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/AreaUnderNormalCurve\n\n\nSchneider, W. J. (2021). psycheval: A psychological evaluation toolkit. [Software] AssessingPsyche.\n\n\nSchneider, W. J. (2021). Simple regression with standard scores. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/simple_regression\n\n\nSchneider, W. J. (2021). ztestvis: A Jamovi module for conducting a one-sample z-test. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/ztest\n\n\nSchneider, W. J., & Ji, F. (2021). unusualprofile: Calculate conditional Mahalanobis distances. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.unusualprofile\n\n\nSchneider, W. J. (2018). ggnormalviolin. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.ggnormalviolin\n\n\nSchneider, W. J. (2018). simstandard. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.simstandard\n\n\nSchneider, W. J. (2012). TableMaker. [Software] AssessingPsyche.\n\n\nSchneider, W. J. (2010). The Compositator 1.0.. [Software] WMF Press."
  },
  {
    "objectID": "vita.html#lectures",
    "href": "vita.html#lectures",
    "title": "Curriculum Vitae",
    "section": "Lectures",
    "text": "Lectures\n\nSchneider, W. J. (2025, November 14). X-BASS 3.0: A Milestone Achievement in Test Interpretation. [Webinar]. X-BASS Institute 2025 Virtual Conference , Comprehensive Assessment for Intervention. https://caipsychs.com/conferences/\n\n\nSchneider, W. J. (2025, October 28). Making survey tests psychometrically sound. [Lecture]. Leadership Education in Neurodevelopmental Disabilities Program , Children’s Hospital of Philadelphia.\n\n\nSchneider, W. J., Tassé, M. J., & Blume, J. H. (2025, April 26). Intellectual disability identification: common problems on prong I. [Panel]. Twenty-Second National Seminar on the Development and Presentation of Mitigating Evidence in Capital Cases , Columbus, OH. https://advancechange.org/events/national-seminar-on-the-development-and-integration-of-mitigation-evidence-in-capital-cases-04-24-2025/\n\n\nTassé, M. J., Schneider, W. J., & Olley, G. (2025, April 24). The APA’s professional guidelines for forensic assessments of ID. [Panel]. Twenty-Second National Seminar on the Development and Presentation of Mitigating Evidence in Capital Cases , Columbus, OH. https://advancechange.org/events/national-seminar-on-the-development-and-integration-of-mitigation-evidence-in-capital-cases-04-24-2025/\n\n\nSchneider, W. J. (2025, February 3). Making survey tests psychometrically sound. [Lecture]. Leadership Education in Neurodevelopmental Disabilities Program , Children’s Hospital of Philadelphia.\n\n\nSchneider, W. J. (2025, January 24). Advanced Interpretation Techniques with the Woodcock-Johnson V. [Webinar]. Comprehensive Assessment for Intervention . https://caipsychs.com/conferences/\n\n\nSchneider, W. J. (2024, October 18). Individual psychometrics. [Webinar]. Illinois School Psychologists Association . https://www.ilispa.org/fall-conference\n\n\nSchneider, W. J. (2024, March 15). Evidence-based assessment and clinical decision-making in school psychology. [Webinar]. 2024 Illinois School Psychologists Association Annual Convention . https://www.ilispa.org/annual-convention\n\n\nSchneider, W. J. (2024, February 26). Making survey tests psychometrically sound. [Lecture]. Leadership Education in Neurodevelopmental Disabilities Program , Children’s Hospital of Philadelphia.\n\n\nEngler, J. R., Schneider, W. J., Flanagan, D. P., & Niileksela, C. R. (2024, February 16). Improve PSW case conceptualization accuracy by taking measurement error seriously. [Mini-Skills Workshop]. National Association of School Psychologists Annual Convention , New Orleans, LA.\n\n\nSchneider, W. J. (2024, January 25). Confidently correct specific learning disability identification. [Webinar]. Comprehensive Assessment for Intervention . https://caipsychs.com/conferences/\n\n\nSchneider, W. J. (2023, November 4). Using arrowheadr with ggarrow and ggplot2. [Webinar]. ggplot2 Extenders .\n\n\nSchneider, W. J. (2023, September 27). Intelligence, critical thinking, and creativity. [Symposium]. In G. Alves (Moderator), Para Além dos Testes Tradicionais: Integrando Inovações à Avaliação de Criatividade e Pensamento Crítico (Beyond Traditional Tests: Integrating Innovations into the Assessment of Creativity and Critical Thinking) , Instituto Ayrton Senna, São Paulo, Brazil. https://institutoayrtonsenna.org.br/nossos-materiais/noticias/catedra-instituto-ayrton-senna-no-iea-usp-promove-live-integrando-inovacao-avaliacao-criatividade-pensamento-critico/\n\n\nSchneider, W. J. (2023, July 29). How recent theories of intelligence can improve the practice of cognitive ability assessment. [Panel]. International Society for Intelligence Research 2023 Annual Conference , Berkelety, CA. https://isironline.org/2023/01/isir-2023-berkeley-california-july-26-29/\n\n\nSchneider, W. J. (2023, March 24). Assessments that restore hope, promote understanding and inspire change. [Invited Lecture]. 42nd Annual School Psychology, Counseling Psychology and Applied Behavior Analysis Conference , Philadelphia, PA. https://education.temple.edu/node/50031\n\n\nSchneider, W. J. (2022, November 15). Using imaginary data to conserve real resources: Study design planning with model-based simulations. [Lecture]. Pearson 4th Developers Conversation .\n\n\nSchneider, W. J. (2022, July 27). What is the current state of affairs of IQ testing? [Keynote Address]. 2022 Knowledge, Innovation & Enterprise (KIE) Conference: Unpacking Creativity: Culture, Innovation, and Motivation in Global Contexts .\n\n\nFlanagan, D. P., Koponen, T., Wagner, R. K., Colvin, M. K., & Schneider, W. J. D. (2022, April 11). How do we diagnose learning disabilities? [Virtual Summit]. Learning Disabilities Summit: A Three-Part Virtual Event Focused on Crticial Learning Disability Issues , Learning Disabilities Association of America. https://ldaamerica.org/lda-to-hold-learning-disabilities-summit/\n\n\nSchneider, W. J. (2022, March 29). How measurement error affects learning disability identification accuracy. [Lecture]. Pearson Scientific Advisory Council .\n\n\nSchneider, W. J. (2022, March 28). Practical psychometrics: Robust interpretation for individuals. [Lecture]. Temple University School Psychology Owl Hour , Philadelphia, PA.\n\n\nSchneider, W. J. (2022, January 27). Life-and-death psychometrics: IQ and the death penalty. [Webinar]. Owl Hour , Temple University. https://events.temple.edu/owl-hour-life-and-death-psychometrics-iq-and-the-death-penalty\n\n\nHajovsky, D. B., & Schneider, W. J. (2021, September 24). Cognitive assessment in the evaluation of specific learning disabilities. [Webinar]. Missouri Association of School Psychologists 2021 Fall Conference , Weldon Spring, MO. https://maosp.wildapricot.org/resources/Documents/Fall%20Conference.pdf\n\n\nSchneider, W. J. (2021, July 29). Advances in test interpretation with the Cattell-Horn-Carroll Theory of Cognitive Abilities. [Webinar]. Training for Region 19 Evaluation Staff , Education Service Center, Region 19, El Paso, TX. https://wjschne.github.io/media/CHCTheoryWebinar.pdf\n\n\nSchneider, W. J. (2021, June 14). Using CHC theory to better understand student needs. [Webinar]. Hill Country Summer Institute . https://esc13.net/events/hill-country-summer-institute-2021\n\n\nSchneider, W. J. (2021, May 2). The accuracy of PSW methods. In J. R. Engler (Chair), PSW for SLD identification: Does cognition matter? [Symposium]. National Association of School Psychologists Annual Convention , Salt Lake City, UT. https://apps.nasponline.org/professional-development/convention/session-detail.aspx?id=20263\n\n\nSchneider, W. J. (2021, January 22). Improving LD identification with fewer myths and more science. [Invited Lecture]. Science to Practice Virtual Conference , Learning Disability Association of America.\n\n\nSchneider, W. J. (2020, July 29). Curious about CHC Theory? Take your evaluations to the next level! [Webinar]. Riverside Insights , Rolling Meadows, IL. https://info.riversideinsights.com/chc-theory-webinar\n\n\nSchneider, W. J. (2019, March 4). The evolution of PSW methods of SLD identification: What I have learned from PSW proponents. [Keynote Address]. Annual School Neuropsychology Conference , Long Beach, CA.\n\n\nFlanagan, D. P., Schneider, W. J., & Alfonso, V. C. (2019, February 26). PSW methods: Comparisons, research, and how to use them responsibly. [Symposium]. National Association of School Psychologists Annual Convention , Atlanta, GA.\n\n\nSchneider, W. J. (2019, February 26). Consumer-oriented and legally defensible psychoeducational reports–Advanced workshop. [Invited Workshop]. National Association of School Psychologists Conference , Atlanta, GA.\n\n\nSchneider, W. J. (2019, February 26). Consumer-oriented and legally defensible psychoeducational reports–Introductory workshop. [Invited Workshop]. National Association of School Psychologists Conference , Atlanta, GA.\n\n\nSchneider, W. J. (2019, January 4). The evolution of PSW methods of SLD identification: What I have learned from PSW critics. [Keynote Address]. Annual School Neuropsychology Conference , Long Beach, CA.\n\n\nSchneider, W. J. (2018, February 16). Re-evaluating the accuracy of the Dual Discrepancy/Consistency Model for SLD Identification. [Lecture]. National Association of School Psychologists Annual Convention , Chicago, IL.\n\n\nMcGrew, K. S., & Schneider, W. J. (2018, February 15). Revisions to the Cattell-Horn-Carroll (CHC) Theory of Intelligence. [Lecture]. National Association of School Psychologists Annual Convention , Chicago, IL.\n\n\nSchneider, W. J. (2016, December 4). How to write psychoeducational reports that people will want to read. [Webinar]. Region 10 Education Service Center , Richardson, TX.\n\n\nCormier, D. C., Bulut, O., Niileksela, C. R., Funamoto, A., & Schneider, W. J. D. (2016, December 2). Revisiting the relationship between CHC abilities and academic achievement. [Symposium]. National Association of School Psychologists Annual Convention , New Orleans, LA.\n\n\nSchneider, W. J. (2016, October 24). How to write psychoeducational reports that people will want to read all the way through. [Webinar]. KIPP Austin Public Schools and Texas State University School Psychology Program .\n\n\nMcGrew, K. S., & Schneider, W. J. (2016, October 19). Porque as habilidades cognitivas importam na educação [Why cognitive abilities matter in education]. [Invited Lecture]. Instituto Ayrton Senna , São Paulo, Brazil.\n\n\nSchneider, W. J., & McGrew, K. S. (2016, October 18). CHC theory, complex problem solving, critical thinking, and creativity. [Invited Lecture]. Instituto Ayrton Senna , São Paulo, Brazil.\n\n\nMcGrew, K. S., & Schneider, W. J. (2016, October 17). CHC theory and education in Brazil. [Invited Lecture]. Instituto Ayrton Senna , São Paulo, Brazil.\n\n\nSchneider, W. J. (2016, July 29). How to write psychoeducational reports that people will want to read all the way through. [Webinar]. New Brunswick Department of Education and Early Childhood Development , Fredericton, NB.\n\n\nSchneider, W. J. (2016, April 8). Writing reports worth reading all the way through. In R. Flanagan (Chair), Sophisticated simplicity: The art of writing reader-friendly assessment reports [Symposium]. Annual American Psychological Association Convention , Denver, CO.\n\n\nSchneider, W. J. (2015, July 15). I didn’t know the WJ IV could do that! Answering practical assessment questions you didn’t know you could ask. [Invited Lecture]. School Neuropsychology Summer Institute , Grapevine, TX.\n\n\nSchneider, W. J. (2014, February 15). What if we took our models seriously? Estimating latent scores in individuals. [Lecture]. National Association of School Psychologists Convention , Washington, D.C..\n\n\nSchneider, W. J. (2013, July 15). Advances in CHC Theory and its application to individuals. [Invited Lecture]. School Neuropsychology Summer Institute , Grapevine, TX.\n\n\nSchneider, W. J. (2013, February 13). Holes in CHC Theory. [Lecture]. National Association of School Psychologists Conference , Seattle, WA.\n\n\nSchneider, W. J., & Taylor, S. J. (2008, January 10). Can the Implicit Association Test be used as a lie detector? Morality, implicit attitudes, and Big Brother’s help measuring illegal file-sharing behavior. [Invited Lecture]. Center for Study of Public Choice at George Mason University , Fairfax, VA.\n\n\nSchneider, W. J. (2006, November 15). We need to change (but I won’t do anything because I’m not the problem): Couples therapy outcomes and the transtheoretical model of change. [Invited Lecture]. Colloquium at Purdue Unversity , West Lafayette, IN.\n\n\nSchneider, W. J., & Huber, B. (2004, March 15). The interactive effects of fluid intelligence and executive functions on behavior disorders in children. [Lecture]. Annual Meeting of the Illinois School Psychology Association , Springfield, IL."
  },
  {
    "objectID": "vita.html#posters-and-papers",
    "href": "vita.html#posters-and-papers",
    "title": "Curriculum Vitae",
    "section": "Posters and Papers",
    "text": "Posters and Papers\n\nAn, C., Schneider, W. J., Campagnolio, A., & Fiorello, C. A. (2025, February 20). Rigorous procedures for measuring broad and narrow abilities in individuals [Poster]. National Association of School Psychologists Convention, Seattle, WA.\n\n\nCampagnolio, A., Schneider, W. J., & An, C. (2025, February 20). Non-linear relations among reading abilities affect score interpretations [Poster]. National Association of School Psychologists Convention, Seattle, WA.\n\n\nHajovsky, D. B., Niileksela, C. R., Flanagan, D. P., Alfonso, V. C., Schneider, W. J., & Zinkiewicz, C. J. (2022, April 8). Toward a consensus model of cognitive-achievement relations using meta-SEM [Poster]. American Psychological Association Convention, Minneapolis, MN.\n\n\nSchneider, W. J., Flanagan, D. P., Niileksela, C. R., & Engler, J. R. (2020, February 20). Reevaluating the accuracy of PSW methods of SLD identification [Poster]. National Association of School Psychologists Convention, Baltimore, MD.\n\n\nSchneider, W. J., & Fiorello, C. A. (2019, August 8). Broad abilities are not doomed to be measured unreliably: A Monte Carlo study of omega statistics [Poster]. American Psychological Association Convention, Chicago, IL. https://github.com/wjschne/APA2019\n\n\nMulderink, T. D., Tobin, R. M., & Schneider, W. J. (2018, October 8). Personality, interactive history, and situational factors during children’s games [Poster]. Annual American Psychological Association Convention, San Francisco, CA.\n\n\nNicholls, C. J., Ross, E., & Schneider, W. J. (2016, April 15). The neuropsychological profiles of twins discordant for Sotos Syndrome: A case study [NA]. American Academy of Pediatric Neuropsychology Annual Conference, Las Vegas, NV.\n\n\nHowe, A. R., Curnock, A. D., Koppenhoefer, S. E., Schneider, W. J., & Tobin, R. M. (2016, April 8). Do vocabulary skills and instructor influence social-emotional learning? [Poster]. Annual American Psychological Association Convention, Denver, CO.\n\n\nCurnock, A. D., Pajor, K. E., Tobin, R. M., & Schneider, W. J. (2016, February 12). Preschoolers’ engagement during Second Step and their social-emotional knowledge [Poster]. National Association of School Psychologists Annual Convention, New Orleans, LA.\n\n\nEngelland, J. L., Tobin, R. M., Meyers, A., Huber, B., Schneider, W. J., Austen, J. H., & Corbin, A. L. (2014, December 8). Longitudinal effects of school climate on middle school students’ development [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nAffrunti, C. L., Schneider, W. J., Tobin, R. M., & Collins, K. D. (2014, August 7). Predictors of academic outcomes in college students suspected of having learning disorders [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nSondalle, A. A., Tobin, R. M., & Schneider, W. J. (2014, August 7). Behavioral engagement, Second Step edition, and children’s social-emotional functioning [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nSondalle, A. A., Probst, K. G., Carreno, C., Tobin, R. M., Schneider, W. J., Moore, N. A., & Mulderink, T. D. (2014, February 15). Engagement during Second Step and kindergarteners’ social-emotional and academic outcomes [Poster]. National Association of School Psychologists Convention, Washington, D.C..\n\n\nSondalle, A. A., Probst, K. G., Tobin, R. M., Schneider, W. J., & Kestian, J. M. (2014, February 15). Second Step and teacher ratings of children’s social-emotional functioning [Poster]. National Association of School Psychologists Conference, Washington, D.C..\n\n\nTobin, R. M., Schneider, W. J., Moore, N. A., Sondalle, A. A., & Willis, M. (2013, August 15). Effortful control and knowledge gains after the Second Step intervention [Poster]. Annual American Psychological Association Convention, Honolulu, HI.\n\n\nMoore, N. A., Sondalle, A. A., Mulderink, T. D., Tobin, R. M., Schneider, W. J., Willis, M., & Probst, K. G. (2013, February 15). Effortful control, Second Step, and behavior in kindergartners [Poster]. National Association of School Psychologists Convention, Seattle, WA.\n\n\nSondalle, A. A., Mulderink, T. D., Moore, N. A., Tobin, R. M., & Schneider, W. J. (2012, August 15). Engagement, dosage, and effectiveness of the kindergarten Second Step curriculum [Poster]. Annual American Psychological Association Convention, Orlando FL.\n\n\nMulderink, T. D., Sondalle, A. A., Moore, N. A., Tobin, R. M., Schneider, W. J., & Sheese, B. E. (2012, February 15). Influences of executive functioning and Second Step on behavior [Poster]. National Association of School Psychologists Conference, Philadelphia, PA.\n\n\nMulderink, T. D., Moore, N. A., Sondalle, A. A., Tobin, R. M., & Schneider, W. J. (2011, August 15). Executive functioning and responsiveness to Second Step [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nTobin, R. M., Mackin, J. M., Babcock, E. A., Mulderink, T. D., Moore, N. A., Carlson, L. A., Gioia, K. A., & Schneider, W. J. (2011, February 15). Intervention frequency and behavior in response to Second Step [Poster]. National Association of School Psychologists Convention, San Francisco, CA.\n\n\nTobin, R. M., Gioia, K. A., Mulderink, T. D., Carlson, L. A., Moore, N. A., & Schneider, W. J. (2010, August 15). Personality, Second Step dosage, and kindergartners’ social skills improvements [Poster]. Annual American Psychological Association Convention, San Diego CA.\n\n\nGioia, K. A., Tobin, R. M., & Schneider, W. J. (2010, January 15). Children’s knowledge gains in response to a social skills intervention [Poster]. Annual Illinois School Psychologist Association Convention, East Peoria, IL.\n\n\nMoore, N. A., Gioia, K. A., Tobin, R. M., & Schneider, W. J. (2010, January 3). Effortful control and emotion regulation in kindergartners [Poster]. National Association of School Psychologists Convention, Chicago, IL.\n\n\nObilade, M. H., Gioia, K. A., Tobin, R. M., & Schneider, W. J. (2010, January 3). Verbal ability and responsiveness to the Second Step curriculum [Poster]. National Association of School Psychologists Convention, Chicago, IL.\n\n\nSchneider, W. J., & Tobin, R. M. (2010, January 3). Previously impossible feats of interpretation and explanation with cognitive and achievement data [Poster]. National Association of School Psychologists Conference, Chicago, IL.\n\n\nTaylor, S. A., & Schneider, W. J. (2009, October 15). Implicit attitudes in folk explanations of digital piracy [Paper]. Frontiers in Service Conference, Honolulu, HI.\n\n\nBooth, B., Lederer, S., Dick, S., Linnell, J., Meyers, A., Schneider, W. J., Swerdlik, M. E., & Anweiler, J. (2009, August 15). Reintegration experiences of returning National Guard war veterans and implications for reintegration programming: Results from two states [Poster]. Annual American Psychological Association Convention, Toronto, ON.\n\n\nGadke, D. L., Tobin, R. M., & Schneider, W. J. (2009, August 15). Agreeableness and prejudice towards overweight men and women [Poster]. Annual American Psychological Association Convention, Toronto, ON.\n\n\nGioia, K. A., Tobin, R. M., & Schneider, W. J. (2009, August 15). Individual differences in children’s responsiveness to a social skills intervention [Poster]. Annual American Psychological Association Convention, Toronto, ON.\n\n\nGioia, K. A., Miller, K. A., Tobin, R. M., & Schneider, W. J. (2009, February 15). Children’s responsiveness to a social skills intervention [Poster]. National Association of School Psychologists Convention, Boston, MA.\n\n\nBaird, S. A., Schneider, W. J., & Tobin, R. M. (2008, August 15). Implicit agreeableness predicts laboratory measures of aggression [Poster]. Annual American Psychological Association Convention, Boston, MA.\n\n\nBarr, L. K., Kahn, J. H., & Schneider, W. J. (2008, March 15). Emotion expression tendencies and mood/anxiety symptoms [Poster]. International Counseling Psychology Conference, Chicago, IL.\n\n\nTobin, R. M., Schneider, W. J., & Landau, S. E. (2008, February 15). Assessing attention-deficit/hyperactivity disorder using an RTI approach [Poster]. National Association of School Psychologists Convention, New Orleans, LA.\n\n\nStagg, J. W., & Schneider, W. J. (2007, November 15). So you flunked the Stroop Test, so what? The utility of single vs. multiple indicators in predicting behavioral outcomes related to executive functioning among non-referred individuals [Poster]. Annual National Academy of Neuropsychology Conference, Scottsdale, AZ.\n\n\nSchneider, W. J. (2007, August 15). Is the TAT helpful in assessing complex aspects of attention and executive functions? An extremely labor-intensive celebration of the null hypothesis [Poster]. Annual American Psychological Association Convention, San Francisco, CA.\n\n\nSchneider, W. J., & Kasson, D. M. (2007, January 3). Do narrow abilities matter more for people with higher IQ? Implications of Spearman’s Law of Diminishing Returns for the discrepancy model of LD [Poster]. National Association of School Psychologists Convention, New York, NY.\n\n\nSchneider, W. J., & McKenna, T. L. (2006, August 15). Using the Implicit Association Test to measure early maladaptive schemas [Poster]. Annual American Psychological Association Convention, New Orleans, LA.\n\n\nKahn, J. H., Vogel, D. L., Schneider, W. J., Barr, L. K., & Henning, K. (2006, April 15). Emotional self-disclosures of college-student clients and counseling session outcome [Poster]. Great Lakes Conference, West Lafayette, IN.\n\n\nJones, G. B., & Schneider, W. J. (2005, August 15). Intelligence, human capital, and economic growth [Paper]. Econometric Society World Congress, London, UK.\n\n\nTobin, R. M., Bodner, A. C., & Schneider, W. J. (2005, August 15). Executive function and emotion regulation in children [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nMarch, A., Funk, K., Schneider, W. J., & Landau, S. E. (2005, January 3). Student and teacher attributions of school shootings [Poster]. National Association of School Psychologists Convention, Atlanta, GA.\n\n\nTobin, R. M., Schneider, W. J., Graziano, W. G., & Pizzitola, K. M. (2002, February 15). Nice kids in competitive situations [Poster]. Annual Society for Personality and Social Psychology Meeting, Savannah, GA.\n\n\nSnyder, D. K., Schneider, W. J., Oxford, M. C., & Quinn, T. (2001, July 15). Secondary prevention group intervention for couples [Paper]. World Congress of Behavioral and Cognitive Therapies, Vancouver, BC.\n\n\nTobin, R. M., Pizzitola, K. M., Schneider, W. J., & Graziano, W. G. (2001, April 15). Goal structures and competitiveness in children’s games [Poster]. Biennial Meeting of the Society for Research in Child Development, Minneapolis, MN.\n\n\nSchneider, W. J., Loss, R. M., Cavell, T. A., & Oxford, M. C. (2000, November 15). Parenting and children with callous-unemotional traits: Relationship quality as a potential socialization mechanism [Poster]. Annual Convention of the Association for the Advancement Behavior Therapy, New Orleans, LA.\n\n\nSnyder, D. K., Schneider, W. J., Oxford, M. C., & Quinn, T. (2000, November 15). SUCCESS: The efficacy of a relationship enhancement group [Poster]. Annual Convention of the Association for the Advancement Behavior Therapy, New Orleans, LA.\n\n\nSchneider, W. J., Cavell, T. A., Hughes, J. N., & Oxford, M. C. (1999, August 15). The development of the Perceived Containment Questionnaire [Poster]. Annual American Psychological Association Convention, Boston, MA."
  },
  {
    "objectID": "vita.html#courses",
    "href": "vita.html#courses",
    "title": "Curriculum Vitae",
    "section": "Courses",
    "text": "Courses\n\nTemple University\n\nCPSY 5519—Group Counseling (Spring 2018–2019)\nCPSY 5694—Introduction to Assessment (Fall 2017–2019, 2023)\nEDUC 5101—Critical Understanding of Social Science Research (Fall 2018)\nEDUC 5325—Introduction to Statistics and Research (Fall 2017, 2021)\nEPSY 5529—Tests and Measurements (Spring 2019–2020, 2022, 2023)\nEPSY 8825—Advanced Data Analysis (Spring 2020, 2022)\nSPSY 9687/8—Seminar in School Psychology/Psychoeducational Clinic (Fall 2019, Spring 2020–2021, Fall 2023)\n\n\n\nIllinois State University\n\nPSY 110—Explaining Human Behavior/Fundamentals of Psychology (Fall 2002–2006, Spring 2003–2004, 2006–2007)\nPSY 138—Social Science Reasoning Using Statistics/Reasoning in Psychology Using Statistics (Spring 2004, 2008–2012, 2014; Fall 2007–2014, 2016)\nPSY 340—Statistics for the Social Sciences (Spring 2005)\nPSY 432—Psychodiagnostics I: Cognitive Assessment/ Theory and Practice of Cognitive Assessment (Fall 2004–2016)\nPSY 436—Clinical/Counseling Practicum (Spring 2004)\nPSY 442—Test Theory (Spring 2015, 2017)\nPSY 443—Regression Analysis (Spring 2016)\nPSY 444—Multivariate Analysis (Fall 2015){.smalldate}\nPSY 464—Theories and Techniques of Counseling: Adults (Spring 2005–2012, 2014–2016)\nPSY 480—Advanced Practicum in Dialectical Behavior Therapy Skills (Fall 2015–2017)\nPSY 480.31—Practicum in Dialectical Behavior Therapy Skills Training (Fall 2014, Spring 2105–2017)\nPSY 480.33—Seminar in Psychology: Supervision of a Dialectical Behavior Therapy Group (Fall 2014, Spring 2105, 2017)"
  },
  {
    "objectID": "vita.html#mentoring",
    "href": "vita.html#mentoring",
    "title": "Curriculum Vitae",
    "section": "Mentoring",
    "text": "Mentoring\n\nDoctoral Dissertation Chair\n\nTemple University\n\nAidan Campagnolio\nChristine An (co-chair)\nMegan Barone\nRandy Taylor\nBernard Dillard (2024)\nStephanie Iaccarino (2023, co-chair)\nJustin Harper (2022, co-chair)\n\n\n\nIllinois State University\n\nC. Lee Affrunti (2013)\nDaniel L. Gadke (co-chair, 2012)\nJonathan W. Stagg (2008)\n\n\n\n\nDoctoral Dissertation Committee Member\n\nTemple University\n\nEmunah Mager-Garfield (2024)\nOctavia Blount (2024)\nValerie Woxholdt (2024)\nCodie Kane (2023)\nPatrick Clancy (2023)\nShana Levi-Nielsen (2022)\nKaiyla Darmer (2022)\nKathryn DeVries (2022)\nTera Gibbs (2022)\nMariah Davis (2022)\nLinda Ruan (2020)\n\n\n\nIllinois State University\n\nJennifer Engelland-Schultz (2015)\nMandi Martinez-Dick (2015)\nThomas Mulderink (2015)\nAlyssa A. Sondalle (2015)\nRachelle Cantin (2013)\nJennifer Wallace (2013)\nSilas Dick (2013)\nTrisha Mann (2012)\nKatherine A. Gioia (2009)\nSarah Reck (2008)\nAnna C. Bodner (2005)\n\n\n\nExternal Reviewer\n\nBrianna Paul (2022) Texas Women’s University\nJake Blair Kraska (2021) Monash University\nPaul Jewsbury (2014) University of Melbourne\n\n\n\n\nMaster’s Thesis Chair\n\nIllinois State University\n\nFeng Ji (2018)\nKatrin Klieme (2016)\nZachary Roman (2016)\nKiera Dymit (2015)\nMeera Afzal (2012)\nRachelle Bauer (2010)\nSunthud Pornprasertmanit (2010)\nDavid Kasson (2006)\nKristina Taylor (2006)\nRobin Van Herrmann (2010)\nMelissa Zygmun (2006)\n\n\n\n\nMaster’s Thesis Committee Member\n\nIllinois State University\n\nRyan Willard (2017)\nRachel Workman (2017)\nHayley Love (2016)\nAnges Strojewska (2016)\nDanielle Freund (2015)\nAmanda Fisher (2015)\nDaniel Nuccio (2014)\nKevin Wallpe (2014)\nNicole Moore (2013)\nDrew Abney (2012)\nThomas Mulderink (2012)\nJames Clinton (2011)\nJamie Hansen (2010)\nYin Ying Ong (2010)\nMelanie Hewett (2010)\nKaty Adler (2008)\nPoonam Joshi (2008)\nRebecca Hoerr (2008)\nArusha Sethi (2008)\nLeah Barr (2008)\nSara Byczek (2007)"
  },
  {
    "objectID": "vita.html#university-service",
    "href": "vita.html#university-service",
    "title": "Curriculum Vitae",
    "section": "University Service",
    "text": "University Service\n\nIllinois State University\n\n\n\n\n\n\n2007–2017\n\n\nCoordinator and Supervisor, College Learning Assessment Service (CLAS), Psychological Services Center"
  },
  {
    "objectID": "vita.html#college-service",
    "href": "vita.html#college-service",
    "title": "Curriculum Vitae",
    "section": "College Service",
    "text": "College Service\n\nTemple University\n\n\n\n\n\n\n2022–2023\n\n\nMember, CEHD Transition Committee\n\n\n\n\n2022–2023\n\n\nMember, CEHD Tenure and Promotion Committee\n\n\n\n\n2020\n\n\nMember, Higher Education Non-Tenure-Track Faculty Search Committee\n\n\n\n\n2019–2022\n\n\nChair, Faculty Merit Committee\n\n\n\n\n2019\n\n\nCo-Chair, Data and Assessment Working Group\n\n\n\n\n2019\n\n\nMember, Higher Education Tenure-Track Faculty Search Committee\n\n\n\n\n2019\n\n\nTraining presenter: Make Your Data POP! How to interpret and present data visually to maximize its impact\n\n\n\n\n2018–2019\n\n\nMember, Faculty Resource and Development Committee\n\n\n\n\n2018\n\n\nMember, Special Education Faculty Search committee\n\n\n\n\n2018\n\n\nTraining presenter: Introduction to Statistical Analysis with R Workshop"
  },
  {
    "objectID": "vita.html#departmental-service",
    "href": "vita.html#departmental-service",
    "title": "Curriculum Vitae",
    "section": "Departmental Service",
    "text": "Departmental Service\n\nTemple University\n\n\n\n\n\n\n2018–\n\n\nMember, School Psychology Program Committee\n\n\n\n\n2017–2020, 2023–\n\n\nMember, Counseling Psychology Program Committee\n\n\n\n\n2020–2023\n\n\nMember, Educational Leadership Program Committee\n\n\n\n\n2018–2020\n\n\nMember, Departmental Promotion and Tenure Committee\n\n\n\n\n\n\nIllinois State University\n\n\n\n\n\n\n2004–2017\n\n\nMember, Clinical/Counseling Psychology Coordinating committee\n\n\n\n\n2004–2017\n\n\nMember, IRB Committee\n\n\n\n\n2004–2017\n\n\nMember, Psychological Services Center Administrative Team\n\n\n\n\n2004–2017\n\n\nMember, Quantitative Psychology Sequence committee\n\n\n\n\n2004–2017\n\n\nMember, Research Committee\n\n\n\n\n2015–2017\n\n\nMember, Department Faculty Status Committee (Evaluates annual merit, tenure, and promotion)\n\n\n\n\n2017\n\n\nChair, Salary Compression Review Team\n\n\n\n\n2015\n\n\nProgram Coordinator, Quantitative Sequence (Fall only)\n\n\n\n\n2014\n\n\nMember, Clinical/Counseling Faculty Search committee\n\n\n\n\n2004–2005\n\n\nMember, Clinical/Counseling Faculty Search committee"
  },
  {
    "objectID": "vita.html#consulting",
    "href": "vita.html#consulting",
    "title": "Curriculum Vitae",
    "section": "Consulting",
    "text": "Consulting\n\n\n\n\n\n\n2019–\n\n\nAcademic Consultant, Empass Learning, developing an online test of cognitive abilities based on CHC Theory, Gurgaon, India\n\n\n\n\n2014–\n\n\nExpert Consultant, providing expert evaluations of death penalty and medical cases for various legal firms, US\n\n\n\n\n2017–2020\n\n\nAcademic Consultant, PT Melintas Cakrawala, developing AJT CogTest?a comprehensive test of cognitive abilities based on CHC Theory, Jakarta, Indonesia\n\n\n\n\n2015–2017\n\n\nAcademic Consultant, Ayrton Senna Institute, developing a comprehensive assessment of 21st century skills, Sao Paulo, Brazil\n\n\n\n\n2016\n\n\nAcademic Consultant, Academic Therapy Publications, developing and reviewing cognitive and oral language assessment batteries, Novato, CA\n\n\n\n\n2007–2010\n\n\nResearch Consultant, Illinois Army National Guard, conducting research examining reintegration of service personnel"
  },
  {
    "objectID": "vita.html#media-appearances",
    "href": "vita.html#media-appearances",
    "title": "Curriculum Vitae",
    "section": "Media Appearances",
    "text": "Media Appearances\n\n\n\n\n\n\n02/03/2014\n\n\nInterviewed in Scientific American Blog Beautiful Minds by Scott Barry Kaufman\n\n\n\n\n08/05/2016\n\n\nQuoted in the Wall Street Journal about IQ tests\n\n\n\n\n10/13/2016\n\n\nQuoted in ScienceNews for Students about IQ tests\n\n\n\n\n10/11/2017\n\n\nOpined in the Guardian about the president’s IQ\n\n\n\n\n04/09/2018\n\n\nAppeared as a guest of Knowledge@Wharton Radio to discuss hunger and homelessness among college students.\n\n\n\n\n04/03/2018\n\n\nAppeared on CNN Headline News (HLN) with MichaeLA to discuss hunger and homelessness among college students.\n\n\n\n\n10/20/2019\n\n\nPresented on Writing Assessment Reports People Will Read, Understand, and Remember on the School Psyched Podcast\n\n\n\n\n03/01/2020\n\n\nQuoted in APA Monitor on writing assessment reports\n\n\n\n\n03/29/2021\n\n\nPresented on the evolution of cognitive assessment on the Testing Psychologist Podcasts"
  },
  {
    "objectID": "vita.html#editorial-positions",
    "href": "vita.html#editorial-positions",
    "title": "Curriculum Vitae",
    "section": "Editorial Positions",
    "text": "Editorial Positions\n\n\n\n\n\n\n\n\n\n\n2019⁠–⁠2020\nJournal of Psychoeducational Assessment\nAssociate Editor\n\n\n2015\nJournal of School Psychology\nGuest Action Editor"
  },
  {
    "objectID": "vita.html#editorial-boards",
    "href": "vita.html#editorial-boards",
    "title": "Curriculum Vitae",
    "section": "Editorial Boards",
    "text": "Editorial Boards\n\n\n\n\n\n\n\n\n\n\n2018⁠–⁠\nJournal of Intelligence\nEditorial Board\n\n\n2011⁠–⁠\nJournal of School Psychology\nEditorial Board\n\n\n2010⁠–⁠\nJournal of Psychoeducational Assessment\nEditorial Board\n\n\n2017⁠–⁠2021\nArchives of Scientific Psychology\nConsulting Editor\n\n\n2009⁠–⁠2018\nPsychological Assessment\nConsulting Editor\n\n\n1997⁠–⁠2000\nClinician's Research Digest\nEditorial Associate"
  },
  {
    "objectID": "vita.html#ad-hoc-reviewing",
    "href": "vita.html#ad-hoc-reviewing",
    "title": "Curriculum Vitae",
    "section": "Ad Hoc Reviewing",
    "text": "Ad Hoc Reviewing\n\nApplied Neuropsychology\nEuropean Journal of Psychological Assessment\nFrontiers in Human Neuroscience\nJournal of Family Psychology\nJournal of Psychoeducational Assessment\nNASP Communiqué\nPsychological Assessment\nPsychology in the Schools"
  },
  {
    "objectID": "vita.html#professional-memberships",
    "href": "vita.html#professional-memberships",
    "title": "Curriculum Vitae",
    "section": "Professional Memberships",
    "text": "Professional Memberships\n\nMember, American Psychological Association\nMember, National Association of School Psychologists"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Getting Ready to Use R and RStudio\n\n\n\n\n\nTips for Installation and Configuration\n\n\n\n\n\nJan 20, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling\n\n\n\n\n\nGetting Data into Useable Formats\n\n\n\n\n\nMar 1, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA in R\n\n\n\n\n\nPlot group means and test to see if their differences are statistically significant.\n\n\n\n\n\nMar 5, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRegression in R\n\n\n\n\n\nA step-by-step introduction to regresion in R\n\n\n\n\n\nMar 7, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Factor Analysis with R\n\n\n\n\n\nA step-by-step introduction to exploratory factor analysis in R\n\n\n\n\n\nMar 12, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Intercept Models in R\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nRegression in which each group has its own intercept\n\n\n\n\n\nJan 27, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Slopes\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nRegression in which each group has its own slope for some predictors\n\n\n\n\n\nFeb 6, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nMultilevel Models with Three Levels\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nRegression in which there are groups within groups\n\n\n\n\n\nFeb 11, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Mixed Models\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nMultilevel models in which the residuals are not normally distributed\n\n\n\n\n\nFeb 18, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nData Simulation for Multilevel Models\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nSimulating data that conforms to a specified model. Useful for power analysis and other kinds of “what-if” thought experiments.\n\n\n\n\n\nFeb 25, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nMultilevel Modeling of Longitudinal Data in R\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nMultilevel models about how and why variables change over time\n\n\n\n\n\nMar 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLatent Growth Models\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nUsing structural equations to understand change over time\n\n\n\n\n\nMar 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBivariate Change Models\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nHow two variables influence each other over time\n\n\n\n\n\nMar 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPower Analysis with Multilevel Models\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nWhat sample size is needed to reliably find effects in hypothesized models?\n\n\n\n\n\nApr 1, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nDistributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurement Scales\n\n\nIntroduction to Statistics and Research (EDUC 5325 )\n\n\n\n\n\n\n\n\nW. Joel Schneider\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html",
    "href": "tutorials/RandomIntercepts/random_intercepts.html",
    "title": "Random Intercept Models in R",
    "section": "",
    "text": "In a one-way ANOVA, there is a fixed variable and a random variable. Usually, our focus is on the fixed variable—so much so that we might not realize that the random variable is a variable at all.\nA fixed effect is used when the comparisons between groups of scores are meaningful. There is a small, defined number of groups, and all of them are sampled. A random effect is one where the number of groups is open-ended, and the differences between specific groups are not of theoretical interest.\nFor example, suppose that we compare an outcome variable collected from children who have been given no treatment, a short intervention, and a long-lasting intervention.\n\nOutcome_i=Treatment_i+e_i\n\n\nThe fixed effect is the treatment variable with 3 group means for the control group, the short intervention group, and the long intervention group.\nThe random effect is the difference of each child (e). It is a random effect because there is an open-ended number of children who could have been in our study, and we have no theoretical interest in comparing specific children with other specific children.\n\nThe one-way ANOVA comparing the three groups of children will reveal something about the group mean differences on the outcome variable, but nothing substantive about individual children."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#lme4",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#lme4",
    "title": "Random Intercept Models in R",
    "section": "lme4",
    "text": "lme4\nThe primary package for multilevel modeling we will use in this class is lme4."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#easystats",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#easystats",
    "title": "Random Intercept Models in R",
    "section": "Easystats",
    "text": "Easystats\nThe easystats package, like the tidyverse package, loads a family of packages that can make many aspects of analysis much, much easier than they used to be. You should know that the packages are still very much in development.\n\n parameters gets information about model parameters.\n performance helps you evaluate the performance of your models.\n modelbased computes marginal means and model-based predictions.\n insight extracts fundamental information about your model.\n effectsize computes model effect sizes.\n correlation makes working with correlations easier.\n datawizard makes transforming your data easier.\n see makes publication-ready data visualizations.\n report automates model interpretation.\n\n\n# Install packages, if needed.\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"lme4\")) install.packages(\"lme4\")\nif (!require(\"easystats\")) install.packages(\"easystats\")\nif (!require(\"marginaleffects\")) install.packages(\"marginaleffects\")\n\n\n# Load packages\nlibrary(lme4) # Multilevel modeling\nlibrary(tidyverse) # General data management and plotting\nlibrary(easystats) # General modeling enhancements\nlibrary(marginaleffects) # Predicted values"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#lme4-formula-syntax",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#lme4-formula-syntax",
    "title": "Random Intercept Models in R",
    "section": "lme4 Formula Syntax",
    "text": "lme4 Formula Syntax\nThe lmer function from the lme4 package is our primary tool for estimating multilevel models. The same syntax for OLS regression applies to lmer formulas, with one addition.\n\n\n\nlme4 syntax for a random intercepts model\n\n\n\nThe variable on the left is the outcome variable, reading_comprehension.\nThe ~ means “predicted by.”\nThe first 1 is the overall intercept. In this model it represents the weighted mean of all the classrooms on the outcome variable (reading_comprehension).\nThe parentheses are for specifying a random variable.\nThe 1 inside the parentheses represents the random intercepts for each classroom.\nThe | means “nested in.”"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#model-fit-object",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#model-fit-object",
    "title": "Random Intercept Models in R",
    "section": "Model Fit Object",
    "text": "Model Fit Object\nYou can assign the model’s fit object to a variable. I am calling mine m_0.\n\nm_0 &lt;- lmer(reading_comprehension ~ 1 + (1 | classroom_id), data = d)\n\nThus far nothing appears to have happened. However, the m_0 variable contains all the information we need to evaluate the model."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#checking-model-assumptions",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#checking-model-assumptions",
    "title": "Random Intercept Models in R",
    "section": "Checking Model Assumptions",
    "text": "Checking Model Assumptions\nThe performance package has a nice, overall check:\n\ncheck_model(m_0)\n\n\n\n\n\n\n\n\nEverything looks good. If we want, we can check things one at a time.\n\nNormality Assumptions\nWe want the level 1 and level 2 residuals to be normal. Here we check the level 1 residuals:\n\nnorm_check &lt;- check_normality(m_0)\nnorm_check\n\nOK: residuals appear as normally distributed (p = 0.663).\n\n\nThe output says that the residuals are approximately normal. If the output says that the residuals are not normal, it might not be a major concern. Small departures from normality can trigger a significant test result. In general, we worry about large departures from normality, not statistically significant small ones. They best way to detect large departures from normality is to plot the residuals.\n\nplot(norm_check)\n\n\n\n\n\n\n\nFigure 1: Checking the normality assumption with a Q-Q plot\n\n\n\n\n\nFigure 1 is called a Q-Q plot (Quantile-Quantile plot), which plots the standard normal distribution against the observed residuals. If the residuals are perfectly normal, the dots will be on a diagonal line. If a few points are outside the confidence interval in gray, there probably is nothing to worry about. However, if the points are not even close to the line or the gray confidence region, then maybe you need a generalized linear mixed model (lme4::glmer) instead of the linear mixed model (lme4:lmer). More on that later in the course.\nThe Level 1 residuals look approximately normal in Figure 1.\nWe can also check the normality of the level-2 residuals.\n\nnorm_check_level2 &lt;- check_normality(m_0, effects = \"random\")\nnorm_check_level2\n\nOK: Random effects 'classroom_id: (Intercept)' appear as normally distributed (p = 0.342).\n\n\nFigure 2 suggests that the level-2 random effects are approximately normal. If the error bars did not straddle the regression line, then we would start to worry about the plausibility of the normality assumption at level 2.\n\nplot(norm_check_level2)\n\n[[1]]\n\n\n\n\n\n\n\n\nFigure 2: The Q-Q plot of the level-2 random effects\n\n\n\n\n\nThe performance package has an admittedly experimental function that uses machine learning to predict the distribution of your residuals and response variable (i.e., Y, the variable you are predicting):\n\ncheck_distribution(m_0)\n\n# Distribution of Model Family\n\nPredicted Distribution of Residuals\n\n Distribution Probability\n       cauchy         75%\n       normal         19%\n  exponential          3%\n\nPredicted Distribution of Response\n\n Distribution Probability\n       normal         44%\n       cauchy         41%\n    lognormal          6%\n\n\nFigure 3 plots these estimates along with showing plots of the two residual distributions.\n\nplot(check_distribution(m_0))\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nThis function will become useful when we look at generalized linear mixed models and need to select a distribution family for our residuals.\n\n\nCheck for auto-correlation\nIf your data were gathered independently, the order of the cases should show no discernible trends. If adjacent cases are more similar to each other than to non-adjacent cases, then you have auto-correlation in your data. That might be a sign that the independence assumption has been violated.\n\ncheck_autocorrelation(m_0) \n\nOK: Residuals appear to be independent and not autocorrelated (p = 0.240).\n\n\nFortunately, there is no evidence of autocorrelation here.\n\n\nCheck for non-constant error variance (heteroscedasticity)\n\ncheck_heteroscedasticity(m_0)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p &lt; .001).\n\n\nThere is some of evidence of non-constant error variance. By itself, the warning that heteroscedasticity has been detected is not a big deal. In a large sample, statistically significant results sometimes can be triggered by trivially small departures from homoscedasticity. A plot can help us decide if this violation is worrisome.\n\nplot(check_heteroscedasticity(m_0))\n\n\n\n\n\n\n\nFigure 4: Checking the homoscedasticity assumption\n\n\n\n\n\nFigure 4 shows a mostly flat line. If you see major departures from a flat line, we might need to worry about why the residuals have larger variances for some predicted values than for others. This plot suggests that there is little to worry about.\n\n\nCheck for outliers\nWhen the sample size is small, outliers can distort your findings.\n\ncheck_outliers(m_0)\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.5).\n- For variable: (Whole model)\n\n\n\n# Checking for outliers\nplot(check_outliers(m_0))\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\nThis plot looks squished because none of the points have extreme leverage. Observations with high leverage are not necessarily bad. What we do not want to see is the combination of extreme residuals with extreme leverage.\nAn extreme residual is, by definition, far from the regression line. An observation with extreme leverage, by definition, is an outlier in terms of the predictor variables (i.e., in multiple regression, it is a multivariate outlier in the predictor space).\nIn the plot below, the blue line is the estimated regression line before the red outlier is added. The red line hows how the estimated regression line changes in the presence of the red outlier point.\n\n\n\n\n\n\nFigure 6: A point with high leverage and a large residual can cause large changes in the regession coefficients.\n\n\n\nOutliers in the predictor space are said to have leverage (the ability to influence coefficients). Notice that the red point barely changes the regression line when the predictor is near the mean of the predictor variable (i.e., when it has little leverage). When the red point is at either extreme of the predictor distribution, it has a lot of leverage to change the regression line. If the red point is near the original regression line, there is no harm done. However, if the red point has the combination of high leverage and a large residual (i.e., it is an outlier in terms of the predictors and also in terms of the residuals), the red regression line can become quite misleading."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#model-summary",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#model-summary",
    "title": "Random Intercept Models in R",
    "section": "Model Summary",
    "text": "Model Summary\nAs with OLS (ordinary least squares) models generated with the lm function, we can see the overall results with the summary function.\n\nsummary(m_0)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_comprehension ~ 1 + (1 | classroom_id)\n   Data: d\n\nREML criterion at convergence: 8078\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.613 -0.676  0.037  0.651  2.971 \n\nRandom effects:\n Groups       Name        Variance Std.Dev.\n classroom_id (Intercept) 304      17.4    \n Residual                 420      20.5    \nNumber of obs: 900, groups:  classroom_id, 30\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)    57.04       3.25    17.5\n\n\nHere we are given the method by which the parameters were estimated: REML (Restricted Maximum Likelihood). We are told the range and interquartile range of the residuals. We are told the variance and SD of the random effects. We are given the estimates, standard errors, and t statistics for the fixed variables, though in this case the only fixed effect is the overall intercept."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#model-statistics",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#model-statistics",
    "title": "Random Intercept Models in R",
    "section": "Model Statistics",
    "text": "Model Statistics\n\nperformance(m_0)\n\n# Indices of model performance\n\nAIC    |   AICc |    BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n----------------------------------------------------------------------------\n8084.3 | 8084.3 | 8098.7 |      0.420 |          0 | 0.420 | 20.159 | 20.488\n\n\nHere we see the Sigma, the individual-level variance. The root-mean squared error is also a measure of individual-level variance but does not correct for degrees of freedom. The AIC and BIC statistics are useful for comparing models that are non-nested (i.e., models that differ in ways other than merely adding predictors). The R2 (cond.) is the variance explained by all terms, including random effects. The R2 (marg.) is the variance explained by the fixed effect effects. Because there are no fixed effects, this statistic is 0. ICC is the proportion of variance explained by the random effects, which in this case is equal to the R2 (cond.)\nThe report package has an automated interpretation function:\n\nreport(m_0)\n\n We fitted a constant (intercept-only) linear mixed model (estimated using REML\nand nloptwrap optimizer) to predict reading_comprehension (formula:\nreading_comprehension ~ 1). The model included classroom_id as random effect\n(formula: ~1 | classroom_id). The model's intercept is at 57.04 (95% CI [50.66,\n63.43], t(897) = 17.53, p &lt; .001).\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\nIf you do not want the entire report, it can be broken down into several components:\n\nreport_info(m_0)\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using a Wald t-distribution approximation.\n\nreport_model(m_0)\n\nconstant (intercept-only) linear mixed model (estimated using REML and nloptwrap optimizer) to predict reading_comprehension (formula: reading_comprehension ~ 1). The model included classroom_id as random effect (formula: ~1 | classroom_id)\n\nreport_effectsize(m_0)\n\nEffect sizes were labelled following Cohen's (1988) recommendations. \n\nvery small (Std. beta = 1.27e-15, 95% CI [-0.24, 0.24])\n\nreport_random(m_0)\n\nThe model included classroom_id as random effect (formula: ~1 | classroom_id)\n\nreport_intercept(m_0)\n\nThe model's intercept is at 57.04 (95% CI [50.66, 63.43], t(897) = 17.53, p &lt; .001).\n\nreport_parameters(m_0)\n\n  - The intercept is statistically significant and positive (beta = 57.04, 95% CI [50.66, 63.43], t(897) = 17.53, p &lt; .001; Std. beta = 1.27e-15, 95% CI [-0.24, 0.24])\n\nreport_statistics(m_0)\n\nbeta = 57.04, 95% CI [50.66, 63.43], t(897) = 17.53, p &lt; .001; Std. beta = 1.27e-15, 95% CI [-0.24, 0.24]"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#intraclass-correlation-coefficient-icc",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#intraclass-correlation-coefficient-icc",
    "title": "Random Intercept Models in R",
    "section": "Intraclass Correlation Coefficient (ICC)",
    "text": "Intraclass Correlation Coefficient (ICC)\nIf there is little variability in the level-2 intercepts, then a multilevel model might not be all that important. The intraclass correlation coefficient estimates how much variability in the outcome variable is explained by the level-2 intercepts:\n\\text{ICC}=\\frac{\\tau_{00}}{\\tau_{00}+\\sigma^2} = \\frac{\\text{Level 2 Residual Variance}}{\\text{Level 2 + Level 1 Residual Variance}}\nThe performance package extracts the intraclass correlation coefficient.\n\nicc(m_0)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.420\n  Unadjusted ICC: 0.420\n\n\nThe ICC here means that letting each classroom have its own mean explained 42% of the residual variance. Had the value\nThe regular ICC (“adjusted”) takes into account all random effects. The unadjusted ICC takes both fixed and random effects into account. Because there are no fixed effects in this model, the adjusted and conditional ICC are the same.\n\\text{ICC}_{\\text{Unadjusted}}=\\frac{\\tau_{00}}{\\tau_{00}+\\sigma_e^2+\\sigma_{\\text{Fixed}}^2} = \\frac{\\text{Level 2 Residual Variance}}{\\text{Total Variance}}\n\n\n\n\n\n\n\nICC\nMeaning\n\n\n\n\nAdjusted\nThe proportion of random variance explained by the grouping structure.\n\n\nUnadjusted\nThe proportion of all variance (fixed and random) explained by the grouping structure.\n\n\n\nThe performance package also gives us an alternate set of model statistics along with the ICC."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#variance-explained-r2",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#variance-explained-r2",
    "title": "Random Intercept Models in R",
    "section": "Variance Explained (R2)",
    "text": "Variance Explained (R2)\nThe variance explained by a model is one of the ways we evaluate its explanatory scope.\n\nperformance::r2(m_0)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.420\n     Marginal R2: 0.000\n\n\nThe Conditional R2 is the variance explained by both the fixed and random variables. Note that in this model, because \\sigma_{\\text{fixed}}^2=0, \\text{R}_{\\text{Conditional}}^2=\\text{ICC}.\n\\text{R}_{\\text{Conditional}}^2=\\frac{\\tau_{00}+\\sigma_{\\text{Fixed}}^2}{\\tau_{00}+\\sigma_e^2+\\sigma_{\\text{Fixed}}^2} = \\frac{\\text{Level 2 Residual Variance + Fixed Variance}}{\\text{Total Variance}}\nThe Marginal R2 is the variance explained by just the fixed variables. In this case, there are no fixed variables. Thus, Marginal R2 = 0.\n\\text{R}_{\\text{Marginal}}^2=\\frac{\\sigma_{\\text{Fixed}}^2}{\\tau_{00}+\\sigma_e^2+\\sigma_{\\text{Fixed}}^2} = \\frac{\\text{Fixed Variance}}{\\text{Total Variance}}"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#fixed-and-random-coefficients",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#fixed-and-random-coefficients",
    "title": "Random Intercept Models in R",
    "section": "Fixed and Random Coefficients",
    "text": "Fixed and Random Coefficients\nCoefficient-level statistics can be viewed with the parameters function.\n\nparameters(m_0)\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |         95% CI | t(897) |      p\n-------------------------------------------------------------------\n(Intercept) |       57.04 | 3.25 | [50.66, 63.43] |  17.53 | &lt; .001\n\n# Random Effects\n\nParameter                    | Coefficient\n------------------------------------------\nSD (Intercept: classroom_id) |       17.42\nSD (Residual)                |       20.49\n\n\nThe only fixed coefficient is the overall intercept, which is not usually of theoretical interest.\nIf you only want fixed effect printed:\n\nparameters(m_0, effects = \"fixed\")\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |         95% CI | t(897) |      p\n-------------------------------------------------------------------\n(Intercept) |       57.04 | 3.25 | [50.66, 63.43] |  17.53 | &lt; .001\n\n\nIf you only want random effect printed:\n\nparameters(m_0, effects = \"random\")\n\n# Random Effects\n\nParameter                    | Coefficient | 95% CI\n---------------------------------------------------\nSD (Intercept: classroom_id) |       17.42 |       \nSD (Residual)                |       20.49 |       \n\n\nThe random parameters are standard deviations. Squaring the estimate for sd__(Intercept) will give \\tau_{00}. Squaring the estimate for sd__Observation will give \\sigma^2.\nIf you want estimated parameters for each group:\n\nparameters(m_0, effects = \"random\", group_level = TRUE)\n\n# Random Effects\n\nParameter        | Coefficient |   SE |           95% CI\n--------------------------------------------------------\n(Intercept) [11] |      -28.30 | 3.66 | [-35.46, -21.13]\n(Intercept) [26] |      -25.27 | 3.66 | [-32.44, -18.11]\n(Intercept) [7]  |      -23.33 | 3.66 | [-30.50, -16.17]\n(Intercept) [13] |      -22.16 | 3.66 | [-29.32, -14.99]\n(Intercept) [21] |      -21.82 | 3.66 | [-28.99, -14.65]\n(Intercept) [3]  |      -21.29 | 3.66 | [-28.46, -14.12]\n(Intercept) [15] |      -13.16 | 3.66 | [-20.33,  -5.99]\n(Intercept) [23] |      -11.85 | 3.66 | [-19.02,  -4.69]\n(Intercept) [29] |       -9.81 | 3.66 | [-16.98,  -2.64]\n(Intercept) [30] |       -8.60 | 3.66 | [-15.77,  -1.43]\n(Intercept) [5]  |       -7.86 | 3.66 | [-15.02,  -0.69]\n(Intercept) [9]  |       -7.08 | 3.66 | [-14.24,   0.09]\n(Intercept) [28] |       -6.95 | 3.66 | [-14.12,   0.22]\n(Intercept) [6]  |       -5.40 | 3.66 | [-12.57,   1.77]\n(Intercept) [27] |       -3.05 | 3.66 | [-10.22,   4.12]\n(Intercept) [12] |        1.61 | 3.66 | [ -5.56,   8.78]\n(Intercept) [18] |        1.83 | 3.66 | [ -5.34,   9.00]\n(Intercept) [17] |        4.50 | 3.66 | [ -2.66,  11.67]\n(Intercept) [10] |        7.02 | 3.66 | [ -0.14,  14.19]\n(Intercept) [4]  |        7.55 | 3.66 | [  0.38,  14.72]\n(Intercept) [24] |        8.07 | 3.66 | [  0.91,  15.24]\n(Intercept) [14] |       10.14 | 3.66 | [  2.97,  17.30]\n(Intercept) [25] |       17.51 | 3.66 | [ 10.34,  24.68]\n(Intercept) [8]  |       17.86 | 3.66 | [ 10.69,  25.03]\n(Intercept) [16] |       18.11 | 3.66 | [ 10.94,  25.28]\n(Intercept) [20] |       18.43 | 3.66 | [ 11.26,  25.60]\n(Intercept) [19] |       21.06 | 3.66 | [ 13.89,  28.22]\n(Intercept) [22] |       23.32 | 3.66 | [ 16.15,  30.49]\n(Intercept) [1]  |       27.11 | 3.66 | [ 19.94,  34.28]\n(Intercept) [2]  |       31.81 | 3.66 | [ 24.64,  38.98]\n\n\nWhy would you want the estimated values? Maybe you want to make a plot of the random intercepts like in Figure 7.\n\nparameters(m_0, effects = \"random\", group_level = TRUE) %&gt;% \n  mutate(Level = fct_inorder(Level)) %&gt;%  # Reorder classroom id\n  ggplot(aes(Level, Coefficient)) + \n  geom_pointrange(aes(ymin = CI_low,\n                      ymax = CI_high )) + \n  labs(x = \"Classroom ID\", \n       y = \"Random Intercepts\")\n\n\n\n\n\n\n\nFigure 7: Group coefficients"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#basic-plot",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#basic-plot",
    "title": "Random Intercept Models in R",
    "section": "Basic Plot",
    "text": "Basic Plot\nSuppose we want the predicted value for each classroom. There are a variety of ways to do this. We are going to get case-level predictions from the marginaleffects package’s predictions function. A nice feature of his function is that it will average the predictions over any variable. In this case, the prediction is the same for all 30 groups, so instead of 900 predictions (one for each person), we only need 30 predictions (one for each classroom).\n\nd_predicted &lt;- marginaleffects::predictions(\n  m_0, \n  by = \"classroom_id\") \n\n\n# plot distributions and predicted values\nd %&gt;%\n  ggplot(aes(x = classroom_id,\n             y = reading_comprehension)) +\n  geom_violin(color = NA, fill = \"dodgerblue\", alpha = .25) +\n  geom_point(data = d_predicted,\n             aes(y = estimate)) +\n  geom_hline(aes(yintercept = get_intercept(m_0)))\n\n\n\n\n\n\n\n\nHere a fancier version of the same plot.\n\n# ggtext displays rich text\nlibrary(ggtext)\n\nd %&gt;% \n  ggplot(aes(x = classroom_id,\n             y = reading_comprehension)) + \n  # Within-group variability\n  geom_violin(color = NA, scale = \"width\", fill = \"dodgerblue\", alpha = .25) +\n  # Display raw data\n  ggbeeswarm::geom_quasirandom(size = .5, alpha = 0.5, pch = 16) +\n  # Group means as horizontal segments\n  geom_segment(\n    data = d_predicted,\n    aes(\n      x = as.numeric(classroom_id) - 0.5,\n      xend = as.numeric(classroom_id) + 0.5,\n      y = estimate ,\n      yend = estimate ),\n    linewidth =  .1) +\n  # Group means as text\n  geom_label(aes(\n    y = estimate, \n    label = scales::number(estimate,.1)), \n    data = d_predicted,\n    fill = tinter::lighten('dodgerblue', .25),\n    vjust = -0.3, \n    size = 2.8, \n    color = \"gray30\", \n    label.padding = unit(0,\"mm\"), label.size = 0) +\n  # Display Overall intercept coefficient as text\n  geom_richtext(data = tibble(\n    classroom_id = 1,\n    reading_comprehension = get_intercept(m_0),\n    label =  glue::glue(\"*&gamma;*&lt;sub&gt;00&lt;/sub&gt; = {round(fixef(m_0), 2)}\")\n    ),\n    aes(label = label),\n    label.margin = unit(1,\"mm\"),\n    label.padding = unit(0,\"mm\"),\n    label.color = NA,\n    fill = scales::alpha(\"white\", 0.5), \n    vjust = 0,\n    hjust = 0\n  ) +\n  # Display Overall intercept as horizontal line\n  geom_hline(yintercept = get_intercept(m_0)) +\n  # Axis labels\n  labs(x = \"Classrooms\", y = \"Reading Comprehension\")\n\n\n\n\n\n\n\nFigure 8: Model 0 Group intercepts"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#fit-object",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#fit-object",
    "title": "Random Intercept Models in R",
    "section": "Fit object",
    "text": "Fit object\nRepresenting the fixed intercept, the initial 1 is optional. I included it to be explicit, but most of the time it is omitted.\n\nm_1 &lt;- lmer(reading_comprehension ~ 1 + treatment + (1 | classroom_id), \n              data = d)\n\nSometimes these models can become large and unwieldy, making you vulnerable to coding errors. A safer way to create a nested model is to use the update function that only changes the part you specify. In this syntax, a . means that everything is the same.\n\nm_1 &lt;- update(m_0, . ~ . + treatment)\n\nThe formula above means “Make a new model based on the m_0 model. On the left-hand side, the outcome variable is the same. On the right-hand side, everything is the same except that we want to add the treatment variable as a predictor. Also, use the same data.”\nIt is best practice to use the update function wherever possible, because it reduces errors by making you explicitly state which part of the model will change."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#assumption-checks",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#assumption-checks",
    "title": "Random Intercept Models in R",
    "section": "Assumption Checks",
    "text": "Assumption Checks\n\ncheck_model(m_1)\n\n\n\n\n\n\n\n\nStill no problems."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#summary",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#summary",
    "title": "Random Intercept Models in R",
    "section": "Summary",
    "text": "Summary\n\nsummary(m_1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_comprehension ~ (1 | classroom_id) + treatment\n   Data: d\n\nREML criterion at convergence: 8053\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.580 -0.679  0.042  0.649  3.003 \n\nRandom effects:\n Groups       Name        Variance Std.Dev.\n classroom_id (Intercept) 149      12.2    \n Residual                 420      20.5    \nNumber of obs: 900, groups:  classroom_id, 30\n\nFixed effects:\n                   Estimate Std. Error t value\n(Intercept)           45.42       3.20   14.21\ntreatmentTreatment    24.91       4.68    5.32\n\nCorrelation of Fixed Effects:\n            (Intr)\ntrtmntTrtmn -0.683\n\n\nWith a t value of 5.32, the effect of treatment is surely significant.\nNote that lmer displays the treatment variable as treatmentTreatment. This means that it is how much the Treatment group differs from the reference group, which in this case the control group. R is not smart enough to know in advance that the control group is the reference group. Instead, it assumes that the first factor level listed is the reference group. In this case, the control group is listed first. If you want a different reference group, you need to change the order of the factor levels. The base R way is to use the relevel function, but see the forcats package for other useful ways to do this.\nThe report package has an automated interpretation function. Note that unlike the summary function, the report function includes p-values for the fixed effects.\n\n# Overall report\nreport(m_1)\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading_comprehension with treatment (formula: reading_comprehension\n~ treatment). The model included classroom_id as random effect (formula: ~1 |\nclassroom_id). The model's total explanatory power is substantial (conditional\nR2 = 0.42) and the part related to the fixed effects alone (marginal R2) is of\n0.21. The model's intercept, corresponding to treatment = Control, is at 45.42\n(95% CI [39.14, 51.69], t(896) = 14.21, p &lt; .001). Within this model:\n\n  - The effect of treatment [Treatment] is statistically significant and positive\n(beta = 24.91, 95% CI [15.73, 34.09], t(896) = 5.32, p &lt; .001; Std. beta =\n0.93, 95% CI [0.59, 1.28])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#plot-model",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#plot-model",
    "title": "Random Intercept Models in R",
    "section": "Plot Model",
    "text": "Plot Model\nFrom Figure 9, it is easy to see that the treatment group has higher reading comprehension, on average, than the control group.\n\nmodelbased::estimate_relation(m_1) %&gt;% \n  plot() + \n  labs(x = NULL, y = \"Reading Comprehension\")\n\n\n\n\n\n\n\nFigure 9: Model-based estimate of the relationship between treatment and reading comprehension"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#fixed-and-random-coefficients-1",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#fixed-and-random-coefficients-1",
    "title": "Random Intercept Models in R",
    "section": "Fixed and Random Coefficients",
    "text": "Fixed and Random Coefficients\n\nparameters(m_1)\n\n# Fixed Effects\n\nParameter             | Coefficient |   SE |         95% CI | t(896) |      p\n-----------------------------------------------------------------------------\n(Intercept)           |       45.42 | 3.20 | [39.14, 51.69] |  14.21 | &lt; .001\ntreatment [Treatment] |       24.91 | 4.68 | [15.73, 34.09] |   5.32 | &lt; .001\n\n# Random Effects\n\nParameter                    | Coefficient\n------------------------------------------\nSD (Intercept: classroom_id) |       12.22\nSD (Residual)                |       20.49\n\n\nHere we see that the treatment variable’s estimate is statistically significant.\nIf we want to see what the estimated means are for the 2 treatment groups:\n\nmodelbased::estimate_means(m_1)\n\nEstimated Marginal Means\n\ntreatment |  Mean |   SE |         95% CI | t(896)\n--------------------------------------------------\nControl   | 45.42 | 3.20 | [39.14, 51.69] |  14.21\nTreatment | 70.32 | 3.42 | [63.62, 77.03] |  20.58\n\nVariable predicted: reading_comprehension\nPredictors modulated: treatment\nPredictors averaged: classroom_id"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#intraclass-correlation-coefficient",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#intraclass-correlation-coefficient",
    "title": "Random Intercept Models in R",
    "section": "Intraclass Correlation Coefficient",
    "text": "Intraclass Correlation Coefficient\n\nperformance::icc(m_1)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.262\n  Unadjusted ICC: 0.206\n\n\nCompare these values to Model 0’s ICC\n\nperformance::icc(m_0)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.420\n  Unadjusted ICC: 0.420\n\n\nHere we see that the ICC’s are reduced—because treatment explained some of the between-group variability."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#variance-explained-r2-1",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#variance-explained-r2-1",
    "title": "Random Intercept Models in R",
    "section": "Variance Explained (R2)",
    "text": "Variance Explained (R2)\nIn OLS regression, the variance explained is fairly straightforward because there is only one random residual. Its size changes with each new predictor added to the model. With multilevel models, variance explained is more complicated because there are multiple residuals to worry about.\nOur model consists of fixed effects (intercept and predictors) and random effects at level 1 and level 1:\n\nReading_{ij}=\\underbrace{\\gamma_{00}+\\gamma_{01}Treatment_{j}}_{\\text{Fixed}}+\\underbrace{u_{0j}}_{\\text{Random}_2}+\\underbrace{e_{ij}}_{\\text{Random}_1}\n\nWe can calculate the variance of the fixed effects, and the 2 random effects.\n\n\\begin{align}\n\\sigma_{\\text{Fixed}}^2&=\\text{Var}(\\gamma_{00}+\\gamma_{01}Treatment_{j})\\\\\n\\sigma_{1}^2&=\\text{Var}(e_{ij})=\\sigma_e^2\\\\\n\\sigma_{2}^2&=\\text{Var}(u_{0j})=\\tau_{00}\n\\end{align}\n\nThe marginal R^2 is the ratio of the variance of the fixed effects and the total variance (fixed, level 1, and level 2). That is, what percentage of variance is explained by the fixed effects.\n\nR^2_{\\text{Marginal}}=\\frac{\\sigma_{\\text{Fixed}}^2}{\\sigma_{\\text{Fixed}}^2+\\sigma_{2}^2+\\sigma_{1}^2}\n\nThe conditional R^2 compares the fixed and level 2 variances to the total variance (fixed, level 1, and level 2). In this context, what percentage of variance is explained by the fixed effects and the random intercepts.\n\nR^2_{\\text{Conditional}}=\\frac{\\sigma_{\\text{Fixed}}^2+\\sigma_{2}^2}{\\sigma_{\\text{Fixed}}^2+\\sigma_{2}^2+\\sigma_{1}^2}\n\n\nperformance::r2(m_1)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.420\n     Marginal R2: 0.214\n\n\nHere we see that Marginal R2 increased from 0 to 0.21, meaning that a substantial portion of the between-group variability is accounted for by treatment.\nThe Conditional R2 is the same as it was in model 0, because treatment cannot account for additional variability beyond classroom_id (which “accounts” for 100% of the between-group variability)."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#compare-models",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#compare-models",
    "title": "Random Intercept Models in R",
    "section": "Compare Models",
    "text": "Compare Models\nTo compare model coefficients, use the parameters::compare_models function:\n\ncompare_models(m_0, m_1, effects = \"all\")\n\n# Fixed Effects\n\nParameter             |                  m_0 |                  m_1\n-------------------------------------------------------------------\n(Intercept)           | 57.04 (50.66, 63.43) | 45.42 (39.14, 51.69)\ntreatment [Treatment] |                      | 24.91 (15.73, 34.09)\n\n# Random Effects\n\nParameter                    |   m_0 |   m_1\n--------------------------------------------\nSD (Intercept: classroom_id) | 17.42 | 12.22\nSD (Residual)                | 20.49 | 20.49\n\n\nTo test the whether one variable explains more variance than a different nested variable, use the anova function.\n\nanova(m_0, m_1)\n\nData: d\nModels:\nm_0: reading_comprehension ~ 1 + (1 | classroom_id)\nm_1: reading_comprehension ~ (1 | classroom_id) + treatment\n    npar  AIC  BIC logLik -2*log(L) Chisq Df Pr(&gt;Chisq)    \nm_0    3 8088 8103  -4041      8082                        \nm_1    4 8069 8089  -4031      8061    21  1    4.6e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value is in Pr(&gt;Chisq) column. It means that m_1’s deviance statistic is significantly smaller than that of m_0.\nThe performance function compare_peformance gives many metrics of change. The Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) can be used to compare non-nested models. The AIC_wt and BIC_wt columns tell you the conditional probabilities in favor of each model.\n\ncompare_performance(m_0, m_1) \n\n# Comparison of Model Performance Indices\n\nName |   Model |  AIC (weights) | AICc (weights) |  BIC (weights) | R2 (cond.)\n------------------------------------------------------------------------------\nm_0  | lmerMod | 8088.5 (&lt;.001) | 8088.5 (&lt;.001) | 8102.9 (&lt;.001) |      0.420\nm_1  | lmerMod | 8069.6 (&gt;.999) | 8069.6 (&gt;.999) | 8088.8 (0.999) |      0.420\n\nName | R2 (marg.) |   ICC |   RMSE |  Sigma\n-------------------------------------------\nm_0  |      0.000 | 0.420 | 20.159 | 20.488\nm_1  |      0.214 | 0.262 | 20.172 | 20.488\n\n\nIn this case, it is clear that model m_1 is preferred compared to m_0.\n\ntest_performance(m_0, m_1) \n\nName |   Model |     BF | df | df_diff |  Chi2 |      p\n-------------------------------------------------------\nm_0  | lmerMod |        |  3 |         |       |       \nm_1  | lmerMod | &gt; 1000 |  4 |       1 | 20.93 | &lt; .001\nModels were detected as nested (in terms of fixed parameters) and are compared in sequential order.\n\n\nWe can also compare the models with a Bayes Factor (BF). If BF &gt; 1, there is more evidence for the second model. If BF &lt; 1, there is greater evidence for the first model.\nHere are some guidelines for interpreting BF Raftery (1995):\n\nbf = 1–3: Weak\nbf = 3–20: Positive\nbf = 20–150: Strong\nbf &gt; 150: Very strong\n\nRaftery, A. E. (1995). Bayesian model selection in social research. Sociological Methodology 25, 111–64.\n\ntest_bf(m_0, m_1) %&gt;% \n  report()\n\nBayes factors were computed using the BIC approximation, by which BF10 =\nexp((BIC0 - BIC1)/2). Compared to the 1 + (1 | classroom_id) model, we found\nextreme evidence (BF = 1.17e+03) in favour of the treatment + (1 |\nclassroom_id) model (the least supported model).\n\n\nThus we see that there is strong evidence that m_1 is a better model than m_0."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#reduction-in-random-variable-variance",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#reduction-in-random-variable-variance",
    "title": "Random Intercept Models in R",
    "section": "Reduction in random variable variance",
    "text": "Reduction in random variable variance\nAlthough the Model 0 \\tau_{00} has the same symbol as the Model 1 \\tau_{00}, they are not the same. I therefore will distinguish the Model 0 and Model1 \\tau_{00} with second-level subscripts: \\tau_{00_0} and \\tau_{00_1}, respectively.\n\\Delta R_2^2(\\text{approx})=\\frac{\\tau_{00_0}-\\tau_{00_1}}{\\tau_{00_0}}\n\ntau_00_0 &lt;- get_variance_random(m_0)\ntau_00_1 &lt;- get_variance_random(m_1)\n\n(tau_00_0 - tau_00_1) / tau_00_0\n\nvar.random \n    0.5077 \n\n100*round((tau_00_0 - tau_00_1) / tau_00_0, 3)\n\nvar.random \n      50.8 \n\n\nThis means that τ00/1 = 149.41 is 50.8% smaller than τ00/0 = 303.51.\nWe can likewise see if there is any reduction in level-1 variance by comparing the residual variance of both models:\n\\Delta R_1^2(\\text{approx})=\\frac{\\sigma_{0}^2-\\sigma_{1}^2}{\\sigma_{0}^2}\n\nsigma_e_0 &lt;- get_variance_residual(m_0)\nsigma_e_1 &lt;- get_variance_residual(m_1)\n\n(sigma_e_0 - sigma_e_1) / sigma_e_0\n\nvar.residual \n    2.37e-09 \n\n\nYou can see that the reduction is very close to 0. Why? Because treatment was cluster-randomized. As a level-2 variable, it would be hard to see how it could influence individual variability. That is, everyone in the same classroom has the same treatment. The level-1 residuals (eij) are created by subtracting from the classroom means. Thus, the treatment variable cannot distinguish between anyone in the same class and cannot explain any individual variability.\nLater we will see that treatment might interact with an individual difference variable such that the treatment works for some students better than for others. In such a model, a level-2 predictor could influence individual variability."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#not-so-basic-plot",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#not-so-basic-plot",
    "title": "Random Intercept Models in R",
    "section": "Not-So Basic Plot",
    "text": "Not-So Basic Plot\nFigure 10 makes it quite clear that the treatment is associated with higher overall scores.\n\n\nCode\nd_augmented_1 &lt;- augment(m_1, conf.int = TRUE)\n\n# Get unique values of the fitted responses\nd_classroom_1 &lt;- d_augmented_1 %&gt;% \n  select(classroom_id, .fitted, treatment) %&gt;%\n  unique() %&gt;% \n  ungroup()\n\n# Get the mean values (.fixed) for each treatment condition\nd_treatment_1 &lt;- d_augmented_1 %&gt;% \n  select(.fixed, treatment) %&gt;% \n  unique()\n\n# plot distributions and predicted values\naugment(m_1) %&gt;%\n  mutate(treatment = fct_rev(treatment)) %&gt;% \n  ggplot(aes(x = classroom_id,\n             y = reading_comprehension)) +\n  geom_violin(color = NA, \n              aes(fill = treatment), \n              alpha = 0.5) +\n  geom_point(data = d_classroom_1,\n             aes(y = .fitted)) +\n  geom_hline(data = d_treatment_1, \n             aes(yintercept = .fixed)) +\n  facet_grid(cols = vars(treatment), \n             scales = \"free\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 10: Treatment effects for Model 1"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Facilitate writing documents in APA Style\n\n\n\n\n\n\n\n\n\nAn object-oriented approach to making diagrams via ggplot2\n\n\n\n\nComplex diagrams can take a long time to get right. The ggdiagram package can take away much of the burden of tedious calculation.\n\n\n\nA regression path model\n\n\n\n\n\nLatent variable path diagram with all variables scaled to their variance sizes.\n\n\n\n\n\nA hierarchical model of abilities\n\n\n\n\n\n\n\nR package for simulating data using standardized coefficients\nTutorial\n\n\n\n\nIn the model below, the path coefficients are standardized. You would like to simulate the variables in the model, but you do not know the disturbance and residual variances. The simstandard package can help.\n\n\n\nA standardized latent variable model\n\n\n\n\n\n\n\nAn R package for detecting unusual scores in a test profile\nTutorial\n\n\n\n\nThis package estimates how unusual a multivariate normal profile is.\n\n\n\n\n\n\nA ggplot2 extension package for creating normal violin plots\n\n\n\n\nI needed to show confidence intervals and conditional normal distributions with specific means and standard deviations. I wrote the ggnormalviolin package to make this happen.\nIt makes plots like this:\n\n\n\nExample plot made with ggnormalviolin\n\n\n\n\n\n\n\nFunctions useful for psychological evaluations\nThis package is still in a preliminary state, just like Individual Psychometrics, the book it accompanies.\n\n\n\n\n\n\n\nMulivariate Confindence Intervals\n\n\n\n\n\n\n\nA set of functions I find convenient to have readily available to me\n\n\n\n\n\n\n\nWhoa! How did you place those labels so perfectly?\n\n\n\n\n\n\n\nAn R package for making digital spirographs\nTutorial\nMy Gallery\n\n\n\n\nMaking digital spirographs is fun! I made an R package called spiro that can make animated spirographs like this one:\n\n\n\n\n\n\nR package for making a custom arrowheads for ggplot2 using ggarrow\nTutorial\n\n\n\n\nThe arrowheadr package allows one to create custom arrowheads that can be used with the ggarrow package."
  },
  {
    "objectID": "software.html#apa7",
    "href": "software.html#apa7",
    "title": "Software",
    "section": "",
    "text": "Facilitate writing documents in APA Style"
  },
  {
    "objectID": "software.html#ggdiagram",
    "href": "software.html#ggdiagram",
    "title": "Software",
    "section": "",
    "text": "An object-oriented approach to making diagrams via ggplot2\n\n\n\n\nComplex diagrams can take a long time to get right. The ggdiagram package can take away much of the burden of tedious calculation.\n\n\n\nA regression path model\n\n\n\n\n\nLatent variable path diagram with all variables scaled to their variance sizes.\n\n\n\n\n\nA hierarchical model of abilities"
  },
  {
    "objectID": "software.html#simstandard",
    "href": "software.html#simstandard",
    "title": "Software",
    "section": "",
    "text": "R package for simulating data using standardized coefficients\nTutorial\n\n\n\n\nIn the model below, the path coefficients are standardized. You would like to simulate the variables in the model, but you do not know the disturbance and residual variances. The simstandard package can help.\n\n\n\nA standardized latent variable model"
  },
  {
    "objectID": "software.html#unusualprofile",
    "href": "software.html#unusualprofile",
    "title": "Software",
    "section": "",
    "text": "An R package for detecting unusual scores in a test profile\nTutorial\n\n\n\n\nThis package estimates how unusual a multivariate normal profile is."
  },
  {
    "objectID": "software.html#ggnormalviolin",
    "href": "software.html#ggnormalviolin",
    "title": "Software",
    "section": "",
    "text": "A ggplot2 extension package for creating normal violin plots\n\n\n\n\nI needed to show confidence intervals and conditional normal distributions with specific means and standard deviations. I wrote the ggnormalviolin package to make this happen.\nIt makes plots like this:\n\n\n\nExample plot made with ggnormalviolin"
  },
  {
    "objectID": "software.html#psycheval",
    "href": "software.html#psycheval",
    "title": "Software",
    "section": "",
    "text": "Functions useful for psychological evaluations\nThis package is still in a preliminary state, just like Individual Psychometrics, the book it accompanies.\n\n\n\n\n\n\n\nMulivariate Confindence Intervals"
  },
  {
    "objectID": "software.html#wjsmisc",
    "href": "software.html#wjsmisc",
    "title": "Software",
    "section": "",
    "text": "A set of functions I find convenient to have readily available to me\n\n\n\n\n\n\n\nWhoa! How did you place those labels so perfectly?"
  },
  {
    "objectID": "software.html#spiro",
    "href": "software.html#spiro",
    "title": "Software",
    "section": "",
    "text": "An R package for making digital spirographs\nTutorial\nMy Gallery\n\n\n\n\nMaking digital spirographs is fun! I made an R package called spiro that can make animated spirographs like this one:"
  },
  {
    "objectID": "software.html#arrowheadr",
    "href": "software.html#arrowheadr",
    "title": "Software",
    "section": "",
    "text": "R package for making a custom arrowheads for ggplot2 using ggarrow\nTutorial\n\n\n\n\nThe arrowheadr package allows one to create custom arrowheads that can be used with the ggarrow package."
  },
  {
    "objectID": "software.html#apaquarto",
    "href": "software.html#apaquarto",
    "title": "Software",
    "section": "apaquarto",
    "text": "apaquarto\nA Quarto Extension for Creating APA 7 Style Documents\nThis is a quarto article template that creates APA Style 7th Edition documents in .docx, .html. and .pdf. I made this extension for my own workflow. If it helps you, too, I am happy. The output of the template is displayed below:\n\n\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "tutorials/StatsIntro/Distributions.html",
    "href": "tutorials/StatsIntro/Distributions.html",
    "title": "Distributions",
    "section": "",
    "text": "The statistical program R is a free, open-source project that is used by millions of people all over the world. It is so good that I would prefer it over other programs even if it were not free. The fact that it will always be free is simply amazing.\nThe downside of R is that it can be intimidating for beginners. It can be finicky in ways that can frustrate even advanced users. Fortunately, R can be extended and adapted by anyone (including you). These extensions are called packages. The program we use, Jamovi, is built atop R, and has a number of R packages associated with it to make Jamovi a free and easy-to-use alternative to commercial statistical programs that can be quite costly. Think of Jamovi as an interface that makes R much easier way to interact with R. In fact, to use Jamovi, you do not need to know anything about R at all. However, if you need Jamovi to do something that it does not do out of the box, you can ask Jamovi to ask R to do it."
  },
  {
    "objectID": "tutorials/StatsIntro/Distributions.html#making-frequency-distribution-tables-with-jamovi",
    "href": "tutorials/StatsIntro/Distributions.html#making-frequency-distribution-tables-with-jamovi",
    "title": "Distributions",
    "section": "Making Frequency Distribution Tables with Jamovi",
    "text": "Making Frequency Distribution Tables with Jamovi\nIn the menu, select Analyses→Exploration→Descriptions. That is, select the Analyses tab, then the Exploration icon, then the Descriptives menu item.\n\nNow drag the Quiz1 variable into the Variables box. Alternately, you can select Quiz1 and then click the arrow next to the Variables box.\nCheck the Frequency tables checkbox.\n\nIn the Results pane, you should see Descriptives with some summary statistics for Quiz1 and a Frequencies table for Quiz1\n\nThe Levels are the values the variables has. The Counts are the frequencies of the each level. The % of Total column is the Counts divided by the sample size. The Cumulative % column tells the percentage of scores at that level or less."
  },
  {
    "objectID": "tutorials/StatsIntro/Distributions.html#bar-plots",
    "href": "tutorials/StatsIntro/Distributions.html#bar-plots",
    "title": "Distributions",
    "section": "Bar Plots",
    "text": "Bar Plots\nTo display the distribution of a categorical variable one should use a bar plot. These are used most often to display the distribution of subjects or cases in certain categories, such as the number of A, B, C, D, and F grades in a given class.\nLet’s start with looking at the distribution of ethnicity in our data file. So what our graph will show are the counts (or frequency) for each of ethnic category.\n\nFrom the menu click Analyses→Exploration→Descriptives.\nDrag the Ethnicity variable into the Variables box.\nClick Plots\nCheck Bar plot.\n\nYou should get a bar chart that looks something like this.\n\nMake a bar graph of the counts of the final grades (called FinalClassGrade in the file) in the class (i.e. A, B, C,…).\nMake a bar graph of the counts of the final grades in the class (i.e. A, B, C,…), further broken down by whether they attended the review session or not.\nDrag the Review variable into the SplitBy box."
  },
  {
    "objectID": "tutorials/StatsIntro/Distributions.html#histograms",
    "href": "tutorials/StatsIntro/Distributions.html#histograms",
    "title": "Distributions",
    "section": "Histograms",
    "text": "Histograms\nSuppose that we wish to know how the students did on quiz 1. We could try looking at all of the scores, but that’s a lot of numbers. Instead, it is better to try to look at the entire distribution, rather than all of the individual scores. We should use a histogram because our variable (score on Quiz1) is a continuous variable.\nHistogram: A histogram is a pictorial representation of the distribution of values for a particular variable. The bars represent the number of occurrences of each value. These look similar to bar graphs except they are used more often to indicate the number of subjects or cases in ranges of values for a continuous variable, such as the number of subjects or cases in ranges of values for a continuous variable."
  },
  {
    "objectID": "tutorials/StatsIntro/Distributions.html#using-jamovi-to-create-a-histogram",
    "href": "tutorials/StatsIntro/Distributions.html#using-jamovi-to-create-a-histogram",
    "title": "Distributions",
    "section": "Using Jamovi to create a histogram:",
    "text": "Using Jamovi to create a histogram:\n\nFrom the menu click Analyses→Exploration→Descriptives.\nDrag the TotalPercent variable into the Variables box.\nClick Plots\nCheck Histogram."
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html",
    "href": "tutorials/StatsIntro/MeasurementScales.html",
    "title": "Measurement Scales",
    "section": "",
    "text": "In science and mathematics, there are fundamental constants—things that always remain the same (e.g., c the speed of light in a vacuum, or \\pi, the ratio of a circle’s circumference to its diameter). However, most phenomena we study are capable of change. Indeed, much of scientific investigation consists of discovering how change in one process affects changes in other processes. Any process or quantity that can change is called a variable.\n\n\nA constant is a state or quantity that remains the same.\nA variable is an abstract category of information that can assume different values.\n\n\nAn algebraic variable usually takes on any value, but it can be constrained by equations and other kinds of relationships. Consider this equation:\n2x-6=0 \\tag{1}\nIn isolation, the variable x could be any real number. However, once inserted in an equation, x is constrained in some way. In Equation 1, x must equal 3.\n\n\n\nRandom variables are not like algebraic variables. The values that random variables can take on are determined by one or more random processes. For example, the outcome of a coin toss is a random variable. It can either be heads or tails. Think of a random variable as forever spitting out new values. In Figure 1, the random variable is an endless cascade of coins.\n\n\n\n\n\n\n\n\nFigure 1: An endless sequence of fair coin tosses is a random variable.\n\n\n\nIn this case, the set of possible values {heads,tails} is the sample space of the outcome of a coin toss. A random variable's sample space is the set of all possible values that the variable can assume. In the case of the roll of a six-sided die, the sample space is {1,2,3,4,5,6}. This sample space has a finite number of values. However, some sample spaces are infinite. For example, the number of siblings one can have is the set of all non-negative integers {0,1,2,3,...}. Of course, because there are limits as to how many children people can have, the probability of very high numbers is very low. A different kind of infinity is observed in the sample space of variables that are continuous. The length of one's foot in centimeters can be any positive real number. Between any two real numbers, (e.g., 20cm and 30cm) there is an infinite number of possibilities because real numbers can be sliced as thinly as needed (e.g., 28.465570098756642111cm)."
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#algebraic-variables",
    "href": "tutorials/StatsIntro/MeasurementScales.html#algebraic-variables",
    "title": "Measurement Scales",
    "section": "",
    "text": "An algebraic variable usually takes on any value, but it can be constrained by equations and other kinds of relationships. Consider this equation:\n2x-6=0 \\tag{1}\nIn isolation, the variable x could be any real number. However, once inserted in an equation, x is constrained in some way. In Equation 1, x must equal 3."
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#random-variables",
    "href": "tutorials/StatsIntro/MeasurementScales.html#random-variables",
    "title": "Measurement Scales",
    "section": "",
    "text": "Random variables are not like algebraic variables. The values that random variables can take on are determined by one or more random processes. For example, the outcome of a coin toss is a random variable. It can either be heads or tails. Think of a random variable as forever spitting out new values. In Figure 1, the random variable is an endless cascade of coins.\n\n\n\n\n\n\n\n\nFigure 1: An endless sequence of fair coin tosses is a random variable.\n\n\n\nIn this case, the set of possible values {heads,tails} is the sample space of the outcome of a coin toss. A random variable's sample space is the set of all possible values that the variable can assume. In the case of the roll of a six-sided die, the sample space is {1,2,3,4,5,6}. This sample space has a finite number of values. However, some sample spaces are infinite. For example, the number of siblings one can have is the set of all non-negative integers {0,1,2,3,...}. Of course, because there are limits as to how many children people can have, the probability of very high numbers is very low. A different kind of infinity is observed in the sample space of variables that are continuous. The length of one's foot in centimeters can be any positive real number. Between any two real numbers, (e.g., 20cm and 30cm) there is an infinite number of possibilities because real numbers can be sliced as thinly as needed (e.g., 28.465570098756642111cm)."
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#nominal-variables",
    "href": "tutorials/StatsIntro/MeasurementScales.html#nominal-variables",
    "title": "Measurement Scales",
    "section": "Nominal Variables",
    "text": "Nominal Variables\nNominal variables can take on any kind of value, including values that are not numbers. The values must constitute a set of mutually exclusive categories. For example, if I have a set of data about college students, I might record which major each person has. The variable college major consists of different labels (e.g., Accounting, Mathematics, and Psychology). Note that there is no true order to college majors, though we usually alphabetize them for convenience. There is no meaningful sense in which English majors are higher or lower than Biology majors. Nominal values are either the same or they are different. They are not less than or more than anything else.\n\n\nIn a set of mutually exclusive categories, nothing belongs to more than one of the categories in the set at the same time.\n\nExamples of nominal variables\n\nBiological sex {male, female}\nRace/Ethnicity {African-American, Asian-American,...}\nType of school {public, private}\nTreatment group {Untreated, Treated}\nDown Syndrome {present, not present}\nAttachment Style {dismissive-avoidant, anxious-preoccupied, secure}\nWhich emotion are you feeling right now? {Happiness, Sadness, Anger, Fear}"
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#ordinal-variables",
    "href": "tutorials/StatsIntro/MeasurementScales.html#ordinal-variables",
    "title": "Measurement Scales",
    "section": "Ordinal Variables",
    "text": "Ordinal Variables\nLike nominal variables, ordinal variables are categorical. Whereas the categories in nominal variables have no meaningful order, the categories in ordinal variables have a natural order. For example, questionnaires often ask multiple-choice questions like so:\nI like chatting with people I do not know.\n\nStrongly disagree\nDisagree\nNeutral\nAgree\nStrongly agree\n\nIt is clear that the response choices have an order that would generate no controversy. Note, however, that there is no meaningful distance between the categories. Is the distance between strongly disagree and disagree the same as the distance between disagree and neutral? It is not a meaningful question because no distance as been defined. All we can do is say is which category is higher than the other.\n\nExamples of ordinal variables\n\nDosage {placebo, low dose, high dose}\nOrder of finishing a race {1st place, 2nd place, 3rd place,…}\nISAT category {below standards, meets standards, exceeds standards}\nApgar score {0,1,…,10}"
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#interval-scales",
    "href": "tutorials/StatsIntro/MeasurementScales.html#interval-scales",
    "title": "Measurement Scales",
    "section": "Interval scales",
    "text": "Interval scales\nInterval scales are quantitative. The values that interval scales take on are almost always numbers. Furthermore, the distance between the numbers have a consistent meaning. The classic example of an interval scale is temperature on the Celsius or Fahrenheit scale. The distance between 25° and 35° is 10°. The distance between 90° and 100° is also 10°. In both cases, the difference involves the same amount of heat.\nUnlike with nominal and ordinal scales, we can add and subtract scores on an interval scale because there are meaningful distances between the numbers.\nInterestingly, the meaning of 0°C (or 0°F) is not what we are used to thinking about when we encounter the number zero. Usually, the number zero means the absence of something. Unfortunately, the number zero does not have this meaning in interval scales. When something has a temperature of 0°C, it does not mean that there is no heat. It just happens to be the temperature at which water freezes at sea level. It can get much, much colder. Thus, interval scales lack a true zero in which zero indicates a complete absence of something.\n\n\nA true zero indicates the absence of the quantity being measured.\nLacking a true zero, interval scales cannot be used to create meaningful ratios. For example, 20°C is not “twice as hot” as 10°C. Also, 110°F is not “10% hotter” than 100°F.\n\nNearly interval scales\nIn truth, there are very, very few examples of variables with a true interval scale. However, a large percentage of variables used in the social sciences are treated as if they are interval scales. It turns out that with a bit of fancy math, many ordinal variables can be transformed, weighted, and summed in such a way that the resulting score is reasonably close to having interval properties. The advantage of doing this is that, unlike with nominal and ordinal scales, you can calculate means, standard deviations, and a host of other statistics that depend on there being meaningful distances between numbers.\nPsychological and educational measures regularly make use of these procedures. For example, on tests like the ACT, we take information about which questions were answered correctly and then transform the scores into a scale that ranges from 1 to 36. As a group, people who score a higher on the ACT tend to perform better in college than people who score lower. Of course, many individuals perform much better than their ACT scores suggest. An equal number of individuals perform much worse than their ACT scores suggest. Among many other things, thirst for knowledge and hard work matter quite a bit. Even so, on average, individuals with a 10 on the ACT are likely to perform worse in college than people with a 20. Roughly by the same amount, people with a 30 on the ACT are likely to perform better in college than people with a 20. Again, we talking about averages, not individuals. Every day, some people beat expectations and some people fail to meet them, often by wide margins.\n\n\nExamples of interval scales\n\nTruly interval:\nTemperature on the Celsius and Fahrenheit scale (not on the Kelvin scale)\nCalendar year (e.g., 431BC, 1066AD)\nNotes on an even-tempered instrument such as a piano {A, A#, B, C, C#, D, D#, E, F, F#, G, G#}\nA ratio scale converted to a z-score metric (or any other kind of standard score metric)\nNearly interval:\nMost scores from well-constructed ability tests (e.g., IQ, ACT, GRE) and personality measures (e.g., self-esteem, extroversion).\n\n\n\nA z-score metric has a mean of 0 and a standard deviation of 1. Thus, the zero indicates the mean of the variable, not the absence of the quantity."
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#ratio-scales",
    "href": "tutorials/StatsIntro/MeasurementScales.html#ratio-scales",
    "title": "Measurement Scales",
    "section": "Ratio Scales",
    "text": "Ratio Scales\nA ratio scale has all of the properties of an interval scale. In addition, it has a true zero. When a ratio scale has a value of zero, it indicates the absence of the quantity being measured. For example, if I say that I have 0 coins in my pocket, there are no coins in my pocket. The fact that ratio scales have true zeroes means that ratios are meaningful. For example, if you have 2 coins and I have one, you have twice as many coins as I do. If I have 100 coins and then you give me 10 more, the number of coins I have has increased by 10%.\n\nExamples of Ratio Scales\nRatio scales involve countable quantities, such as:\n\ncoins\nmarbles\ncomputers\nspeeding tickets\npregnancies\nsoldiers\nplanets\n\nCountable quantities are discrete variables like the scale in Figure 3. Countable integers are discrete because they have particular values but not values between those values.\n\n\nA discrete variable can only take on exact, isolated values from a specified list.\nMany physical properties are also ratio scales, such as:\n\ndistance\nmass\nforce\nheat (on the Kelvin scale)\npressure\nvoltage\nacceleration\nproportions\n\nThese dimensions are not discrete countable quantities like cars and bricks but are instead continuous quantities that can be measured with decimals and fractions.\n\n\n\n\n\n\nFigure 3: A continuous scale from 1 to 10 can have any real value between and including 1 and 10. A discrete scale from 1 to 10 can only take on the values shown.\n\n\n\nNotice that even though ratio variables have a true zero, on some of them it is possible to have negative numbers. For example, negative acceleration would indicate a slowing down. A negative value in a checking account means that you owe the bank money.\nIn the social sciences, there are many examples of ratio scales:\n\nIncome\nAge\nYears of education\nReaction time\nFamily size\nHours of study\nPercentage of household chores completed (compared to other members of the household)"
  }
]