[
  {
    "objectID": "vita.html#w.-joel-schneider",
    "href": "vita.html#w.-joel-schneider",
    "title": "Curriculum Vitae",
    "section": "W. Joel Schneider",
    "text": "W. Joel Schneider\n\n\nTemple University\nCollege of Education & Human Development\nPsychological Studies in Education\n493 Ritter Hall\nPhiladelphia, PA 19122-6091\n\nOffice: (215) 204-8093\nEmail: schneider@temple.edu\nWeb: Faculty Profile\nAssessment Blog: AssessingPsyche R Blog: AssessingPsyche"
  },
  {
    "objectID": "vita.html#publication-indices",
    "href": "vita.html#publication-indices",
    "title": "Curriculum Vitae",
    "section": "Publication Indices",
    "text": "Publication Indices\n\nGoogle Scholar Citations: 5068\nh-index: 28\ni10-index: 42\n\n\n\n\n\n\nGoogle Scholar Citations of My Work"
  },
  {
    "objectID": "vita.html#books",
    "href": "vita.html#books",
    "title": "Curriculum Vitae",
    "section": "Books",
    "text": "Books\n\nCohen, R. J., Schneider, W. J., & Tobin, R. M. (2025). Psychological testing and assessment: An introduction to tests and measurement (11th ed.). McGraw Hill LLC.\n\n\nSchneider, W. J. (2024). Individual psychometrics: An assessment toolkit with applications in R. Author.\n\n\nCohen, R. J., Schneider, W. J., & Tobin, R. M. (2022). Psychological testing and assessment: An introduction to tests and measurement (10th ed.). McGraw Hill LLC.\n\n\nSchneider, W. J., Mather, N., Lichtenberger, E. O., & Kaufman, N. L. (2018). Essentials of assessment report writing (2nd ed.). Wiley."
  },
  {
    "objectID": "vita.html#journal-articles",
    "href": "vita.html#journal-articles",
    "title": "Curriculum Vitae",
    "section": "Journal Articles",
    "text": "Journal Articles\n\nHajovsky, D. B., Niileksela, C. R., Flanagan, D. P., Alfonso, V. C., Schneider, W. J., & Robbins, J. (2025). Toward a consensus model of cognitive–reading achievement relations using meta-structural equation modeling. Journal of Intelligence, 13(8), 104. https://doi.org/10.3390/jintelligence13080104\n\n\nKane, C., Sandilos, L., Schneider, W. J., & Tobin, R. M. (2025). The influence of social-emotional learning programs on key outcomes for dual language learners in Head Start. Early Education and Development, 1–24. https://doi.org/10.1080/10409289.2025.2526310\n\n\nSchneider, W. J., Flanagan, D. P., Niileksela, C. R., & Engler, J. R. (2024). The effect of measurement error on the positive predictive value of PSW methods for SLD identification: How buffer zones dispel the illusion of inaccuracy. Journal of School Psychology, 103, 101280. https://doi.org/10.1016/j.jsp.2023.101280\n\n\nMcGrew, K. S., Schneider, W. J., Decker, S. L., & Bulut, O. (2023). A psychometric network analysis of CHC intelligence measures: Implications for research, theory, and interpretation of broad CHC scores “beyond g”. Journal of Intelligence, 11(1), 19. https://doi.org/10.3390/jintelligence11010019 \n\n\nSchneider, W. J., & Ji, F. (2023). Detecting unusual score patterns in the context of relevant predictors. Journal of Pediatric Neuropsychology, 9, 1–17. https://doi.org/10.1007/s40817-022-00137-x \n\n\nDowdy, A., Peltier, C., Tincani, M., Schneider, W. J., Hantula, D. A., & Travers, J. C. (2021). Meta‐analyses and effect sizes in applied behavior analysis: A review and discussion. Journal of Applied Behavior Analysis, 54(4), 1317–1340. https://doi.org/10.1002/jaba.862 \n\n\nDowdy, A., Tincani, M., & Schneider, W. J. (2020). Evaluation of publication bias in response interruption and redirection: A meta‐analysis. Journal of Applied Behavior Analysis, 53(4), 2151–2171. https://doi.org/10.1002/jaba.724 \n\n\nHajovsky, D. B., Villeneuve, E. F., Schneider, W. J., & Caemmerer, J. M. (2020). An alternative approach to cognitive and achievement relations research: An introduction to quantile regression. Journal of Pediatric Neuropsychology, 6, 83–95. https://doi.org/10.1007/s40817-020-00086-3 \n\n\nDombrowski, S. C., Beaujean, A. A., McGill, R. J., Benson, N. F., & Schneider, W. J. (2019). Using exploratory bifactor analysis to understand the latent structure of multidimensional psychological measures: An example featuring the WISC-V. Structural Equation Modeling: A Multidisciplinary Journal, 26(6), 847–860. https://doi.org/10.1080/10705511.2019.1622421 \n\n\nSchneider, W. J., & McGrew, K. S. (2019). Process Overlap Theory is a milestone achievement among intelligence theories. Journal of Applied Research in Memory and Cognition, 8(3), 273–276. https://doi.org/https://doi.org/10.1016/j.jarmac.2019.06.006 \n\n\nSchneider, W. J., & Roman, Z. (2018). Fine-tuning Cross-Battery Assessment procedures: After follow-up testing, use all valid scores, cohesive or not. Journal of Psychoeducational Assessment, 36(1), 34–54. https://doi.org/10.1177/0734282917722861 \n\n\nMagoon, M. A., Critchfield, T. S., Merrill, D., Newland, M. C., & Schneider, W. J. (2017). Are positive and negative reinforcement “different”? Insights from a free-operant differential outcomes effect. Journal of the Experimental Analysis of Behavior, 107(1), 39–64. https://doi.org/10.1002/jeab.243 \n\n\nSchneider, W. J., & Kaufman, A. S. (2017). Let’s not do away with comprehensive cognitive assessments just yet. Archives of Clinical Neuropsychology, 32(1), 8–20. https://doi.org/10.1093/arclin/acw104 \n\n\nFlanagan, D. P., & Schneider, W. J. (2016). Cross-Battery Assessment? XBA PSW? A case of mistaken identity: A commentary on Kranzler and colleagues’ “Classification agreement analysis of Cross-Battery Assessment in the identification of specific learning disorders in children and youth”. International Journal of School & Educational Psychology, 4(3), 137–145. https://doi.org/10.1080/21683603.2016.1192852 \n\n\nGadke, D. L., Tobin, R. M., & Schneider, W. J. (2016). Agreeableness, conflict resolution tactics, and school behavior in second graders. Journal of Individual Differences, 37, 145–151. https://doi.org/10.1027/1614-0001/a000199 \n\n\nSchneider, W. J., & Kaufman, A. S. (2016). Commentary on current practices and future directions for the assessment of child and adolescent intelligence in schools around the world. International Journal of School & Educational Psychology, 4(4), 283–288. https://doi.org/10.1080/21683603.2016.1206383 \n\n\nSchneider, W. J., Mayer, J. D., & Newman, D. A. (2016). Integrating hot and cool intelligences: Thinking broadly about broad abilities. Journal of Intelligence, 4(1), 1:1–25. https://doi.org/10.3390/jintelligence4010001 \n\n\nSchneider, W. J., & Newman, D. A. (2015). Intelligence is multidimensional: Theoretical review and implications of specific cognitive abilities. Human Resource Management Review, 25(1), 12–27. https://doi.org/10.1016/j.hrmr.2014.09.004 \n\n\nAbney, D. H., Wagman, J. B., & Schneider, W. J. (2014). Changing grasp position on a wielded object provides self-training for the perception of length. Attention, Perception, & Psychophysics, 76(1), 247–254. https://doi.org/10.3758/s13414-013-0550-x \n\n\nPornprasertmanit, S., & Schneider, W. J. (2014). Accuracy in parameter estimation in cluster randomized designs. Psychological Methods, 19(3), 356–379. https://doi.org/10.1037/a0037036 \n\n\nHerbstrith, J. C., Tobin, R. M., Hesson-McInnis, M. S., & Schneider, W. J. (2013). Preservice teacher attitudes toward gay and lesbian parents. School Psychology Quarterly, 28(3), 183–194. https://doi.org/10.1037/spq0000022 \n\n\nKahn, J. H., & Schneider, W. J. (2013). It’s the destination and it’s the journey: Using multilevel modeling to assess patterns of change in psychotherapy. Journal of Clinical Psychology, 69(6), 543–570. https://doi.org/10.1002/jclp.21964 \n\n\nSchneider, W. J. (2013). What if we took our models seriously? Estimating latent scores in individuals. Journal of Psychoeducational Assessment, 31(2), 186–201. https://doi.org/10.1177/0734282913478046 \n\n\nDecker, S. L., Schneider, W. J., & Hale, J. B. (2012). Estimating base rates of impairment in neuropsychological test batteries: A comparison of quantitative models. Archives of Clinical Neuropsychology, 7(1), 69–84. https://doi.org/10.1093/arclin/acr088 \n\n\nJones, G., & Schneider, W. J. (2010). IQ in the production function: Evidence from immigrant earnings. Economic Inquiry, 48(3), 743–755. https://doi.org/10.1111/j.1465-7295.2008.00206.x \n\n\nGuidry, J. A., Babin, B. J., Graziano, W. G., & Schneider, W. J. (2009). Pride and prejudice in the evaluation of wine? International Journal of Wine Business Research, 21(4), 298–311. https://doi.org/10.1108/17511060911004888 \n\n\nHoff, K. E., Reese-Weber, M., Schneider, W. J., & Stagg, J. W. (2009). The association between high status positions and aggressive behavior in early adolescence. Journal of School Psychology, 47(6), 395–426. https://doi.org/10.1016/j.jsp.2009.07.003 \n\n\nBarr, L. K., Kahn, J. H., & Schneider, W. J. (2008). Individual differences in emotion expression: Hierarchical structure and relations with psychological distress. Journal of Social and Clinical Psychology, 27(10), 1045–1077. https://doi.org/10.1521/jscp.2008.27.10.1045 \n\n\nKahn, J. H., Vogel, D. L., Schneider, W. J., Barr, L. K., & Herrell, K. (2008). The emotional content of client disclosures and session impact: An analogue study. Psychotherapy: Theory, Research, Practice, Training, 45(4), 539–545. https://doi.org/10.1037/a0014337 \n\n\nSchneider, W. J. (2008). Playing statistical Ouija board with commonality analysis: good questions, wrong assumptions. Applied Neuropsychology, 15(1), 44–53. https://doi.org/10.1080/09084280801917566 \n\n\nJones, G., & Schneider, W. J. (2006). Intelligence, human capital, and economic growth: A Bayesian Averaging of Classical Estimates (BACE) approach. Journal of Economic Growth, 11(1), 71–93. https://doi.org/10.2139/ssrn.552481 \n\n\nSchneider, W. J., Timothy Cavell, A., & Hughes, J. N. (2003). A sense of containment: Potential moderator of the relation between parenting practices and children’s externalizing behaviors. Development and Psychopathology, 15(1), 95–117. https://doi.org/10.1017/S0954579403000063"
  },
  {
    "objectID": "vita.html#chapters",
    "href": "vita.html#chapters",
    "title": "Curriculum Vitae",
    "section": "Chapters",
    "text": "Chapters\n\nSchneider, W. J., & Tobin, R. M. (2024). Sophisticated simplicity: Writing reader-friendly assessment reports. In R. Flanagan (Ed.) Clinical guide to effective psychological assessment and report writing (pp. 1–11). Springer International Publishing. https://doi.org/10.1007/978-3-031-67184-5_1\n\n\nSchneider, W. J. (2022). Statistical and clinical interpretation guidelines for school neuropsychological assessment. In D. Miller, D. Maricle, C. Bedford, & J. Gettman (Eds.) Best Practices in School Neuropsychology: Guidelines for Effective Practice, Assessment, and Evidence‐Based Intervention (1st ed., pp. 163–184). Wiley. https://doi.org/10.1002/9781119790563\n\n\nFloyd, R. G., Farmer, R. L., Schneider, W. J., & McGrew, K. S. (2021). Theories and measurement of intelligence. In L. Glidden, L. Abbeduto, L. L. McIntyre, & M. J. Tassé (Eds.) APA handbook of intellectual and developmental disabilities (Vol. 1, pp. 386–424). American Psychological Association. https://doi.org/10.1037/0000194-015 \n\n\nKaufman, A. S., Schneider, W. J., & Kaufman, J. C. (2019). Psychometric approaches to intelligence. In R. J. Sternberg (Ed.) Human intelligence: An introduction (pp. 67–103). Cambridge University Press. \n\n\nSchneider, W. J., & McGrew, K. S. (2018). The Cattell-Horn-Carroll theory of intelligence. In D. P. Flanagan, & E. M. McDonough (Eds.) Contemporary intellectual assessment: Theories, tests, and issues (4th ed., pp. 73–130). Guilford Press. \n\n\nSchneider, W. J. (2016). Case 1—Liam, age 9: Emotionally intelligent testing with the WISC-V and CHC theory. In A. S. Kaufman, S. Raiford, & D. Coalson (Eds.) Intelligent testing with the WISC-V (pp. 265–282). Wiley.\n\n\nSchneider, W. J. (2016). Strengths and weaknesses of the Woodcock-Johnson IV Tests of Cognitive Abilities: Best practice from a scientist-practitioner perspective. In D. P. Flanagan, & V. C. Alfonso (Eds.) WJ IV Clinical Use and Interpretation (pp. 191–210). Academic Press. https://doi.org/10.1016/B978-0-12-802076-0.00007-4 \n\n\nSchneider, W. J., & Flanagan, D. P. (2015). The relationship between theories of intelligence and intelligence tests. In S. Goldstein, D. Princiotta, & J. A. Naglieri (Eds.) Handbook of intelligence: Evolutionary theory, historical perspective, and current concepts (pp. 317–340). Springer. \n\n\nTobin, R. M., Schneider, W. J., & Landau, S. (2014). Best practices in the assessment of youth with attention deficit hyperactivity disorder within a multitiered services framework. In A. Thomas, & P. Harrison (Eds.) Best practices in school psychology: Data-based and collaborative decision making (pp. 391–404). National Association of School Psychologists. \n\n\nSchneider, W. J. (2013). Principles of assessment of aptitude and achievement. In D. Saklofske, C. R. Reynolds, & V. Schwean (Eds.) The Oxford Handbook of Child Psychological Assessment (pp. 286–330). Oxford University Press. \n\n\nSchneider, W. J., & McGrew, K. S. (2013). Cognitive performance models: Individual differences in the ability to process information. In B. Irby, G. Brown, R. Laro-Alecio, & S. Jackson (Eds.) Handbook of educational theories (pp. 767–782). Information Age Publishing. \n\n\nSchneider, W. J., & McGrew, K. S. (2012). The Cattell-Horn-Carroll model of intelligence. In D. P. Flanagan, & P. L. Harrison (Eds.) Contemporary intellectual assessment: Theories, tests, and issues (3rd ed., pp. 99–144). Guilford Press. \n\n\nTobin, R. M., Schneider, W. J., Reck, S. G., & Landau, S. (2008). Best practices in the assessment of children with attention deficit hyperactivity disorder: Linking assessment to response to intervention. In P. Harrison, & A. Thomas (Eds.) Best Practices in School Psychology V (Vol. 2, pp. 617–631). National Association of School Psychologists. \n\n\nSnyder, D. K., Schneider, W. J., & Castellani, A. M. (2003). Tailoring couple therapy to individual differences: A conceptual approach. In D. K. Snyder (Ed.) Treating difficult couples: Helping clients with coexisting mental and relationship disorders (pp. 27–51). Guilford Press. \n\n\nSnyder, D. K., & Schneider, W. J. (2002). Affective reconstruction: A pluralistic, developmental approach. In N. S. Jacobson (Ed.) Clinical handbook of couple therapy (3rd ed., pp. 151–179). Guilford Press."
  },
  {
    "objectID": "vita.html#test-reviews",
    "href": "vita.html#test-reviews",
    "title": "Curriculum Vitae",
    "section": "Test Reviews",
    "text": "Test Reviews\n\nSwerdlik, M. E., & Schneider, W. J. (2010). Review of the PsychProfiler. In R. A. Spies, J. F. Carlson, B. S. Plake, & K. F. Geisinger (Eds.) The eighteenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J. (2007). Review of the Dean-Woodcock Neuropsychological Battery. In K. F. Geisinger, R. A. Spies, J. F. Carlson, & B. S. Plake (Eds.) The seventeenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J. (2007). Review of the Multiple Intelligence Developmental Assessment Scales (MIDAS). In K. F. Geisinger, R. A. Spies, J. F. Carlson, & B. S. Plake (Eds.) The seventeenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J., & Swerdlik, M. E. (2007). Review of the Youth Outcome Questionnaire 30.1. In K. F. Geisinger, R. A. Spies, J. F. Carlson, & B. S. Plake (Eds.) The seventeenth mental measurements yearbook. Buros Institute.\n\n\nSwerdlik, M. E., & Schneider, W. J. (2007). Review of the Behavior Evaluation Scale, Third Edition. In K. F. Geisinger, R. A. Spies, J. F. Carlson, & B. S. Plake (Eds.) The seventeenth mental measurements yearbook. Buros Institute.\n\n\nHoff, K. E., & Schneider, W. J. (2005). Review of the Child Symptom Inventory-4. In B. S. Plake, J. C. Impara, & R. A. Spies (Eds.) The sixteenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J., & Hoff, K. E. (2005). Review of the Word Identification and Spelling Test. In B. S. Plake, J. C. Impara, & R. A. Spies (Eds.) The sixteenth mental measurements yearbook. Buros Institute.\n\n\nSchneider, W. J., & Swerdlik, M. E. (2005). Review of the Memory Test for Older Adults. In B. S. Plake, J. C. Impara, & R. A. Spies (Eds.) The sixteenth mental measurements yearbook. Buros Institute.\n\n\nSwerdlik, M. E., & Schneider, W. J. (2005). Review of the Behavioral and Emotional Rating Scale-Second Edition. In B. S. Plake, J. C. Impara, & R. A. Spies (Eds.) The sixteenth mental measurements yearbook. Buros Institute."
  },
  {
    "objectID": "vita.html#scholarly-reports",
    "href": "vita.html#scholarly-reports",
    "title": "Curriculum Vitae",
    "section": "Scholarly Reports",
    "text": "Scholarly Reports\n\nBlume, J. H., & Paavola, E. C. (2025). Brief of Amici Curiae Randy W. Kamphaus, Kevin S. McGrew, Cecil R. Reynolds, W. Joel Schneider, and Marc J. Tasse, Blaine Keith Milam v. Texas. .\n\n\nGoldrick-Rab, S., Richardson, J., Schneider, W. J., Hernandez, A., & Cady, C. (2018). Still hungry and homeless in college. The Wisconsin HOPE Lab. \n\n\nSchneider, W. J. (2016). The RESCA-E subtests are thoughtfully designed and highly refined measures of CHC constructs: A review of the Receptive, Expressive & Social Communication Assessment–Elementary. Assessing Psyche, Engaging Gauss, Seeking Sophia.\n\n\nSchneider, W. J. (2016). Why are WJ IV cluster scores more extreme than the average of their parts? A gentle explanation of the composite score extremity effect. Houghton Mifflin HarcourtWoodcock-Johnson IV Assessment Service Bulletin.\n\n\nSchneider, W. J., & McGrew, K. S. (2011). “Just say no” to averaging IQ subtest scores. IAP Applied Psychometrics 101 Report."
  },
  {
    "objectID": "vita.html#software",
    "href": "vita.html#software",
    "title": "Curriculum Vitae",
    "section": "Software",
    "text": "Software\n\nSchneider, W. J. (2025). apa7: Facilitate Writing Documents in American Psychological Association Style, Seventh Edition. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.apa7\n\n\nSchneider, W. J. (2025). ggdiagram: Object-oriented diagram plots with ggplot2. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.ggdiagram\n\n\nSchneider, W. J. (2024). condppv: Conditional positive predictive value in the accuracy of specific learning disability identification. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/conditionalppv\n\n\nSchneider, W. J. (2023). apaquarto: A Quarto extension for creating APA 7 style documents. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/apaquarto\n\n\nSchneider, W. J. (2023). arrowheadr: Create custom arrowheads for ggplot2 via ggarrow.. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.arrowheadr\n\n\nSchneider, W. J. (2022). UnusualProfile: A user-friendly web application to impliment functions from the unusualprofile package.. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/unusualprofile_app\n\n\nSchneider, W. J. (2021). Area under the normal curve. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/AreaUnderNormalCurve\n\n\nSchneider, W. J. (2021). psycheval: A psychological evaluation toolkit. [Software] AssessingPsyche.\n\n\nSchneider, W. J. (2021). Simple regression with standard scores. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/simple_regression\n\n\nSchneider, W. J. (2021). ztestvis: A Jamovi module for conducting a one-sample z-test. [Software] AssessingPsyche. Retrieved from https://github.com/wjschne/ztest\n\n\nSchneider, W. J., & Ji, F. (2021). unusualprofile: Calculate conditional Mahalanobis distances. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.unusualprofile\n\n\nSchneider, W. J. (2018). ggnormalviolin. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.ggnormalviolin\n\n\nSchneider, W. J. (2018). simstandard. [Software] AssessingPsyche. Retrieved from https://doi.org/10.32614/CRAN.package.simstandard\n\n\nSchneider, W. J. (2012). TableMaker. [Software] AssessingPsyche.\n\n\nSchneider, W. J. (2010). The Compositator 1.0.. [Software] WMF Press."
  },
  {
    "objectID": "vita.html#lectures",
    "href": "vita.html#lectures",
    "title": "Curriculum Vitae",
    "section": "Lectures",
    "text": "Lectures\n\nSchneider, W. J. (2025, November 14). X-BASS 3.0: A Milestone Achievement in Test Interpretation. [Webinar]. X-BASS Institute 2025 Virtual Conference , Comprehensive Assessment for Intervention. https://caipsychs.com/conferences/\n\n\nSchneider, W. J. (2025, October 28). Making survey tests psychometrically sound. [Lecture]. Leadership Education in Neurodevelopmental Disabilities Program , Children’s Hospital of Philadelphia.\n\n\nSchneider, W. J., Tassé, M. J., & Blume, J. H. (2025, April 26). Intellectual disability identification: common problems on prong I. [Panel]. Twenty-Second National Seminar on the Development and Presentation of Mitigating Evidence in Capital Cases , Columbus, OH. https://advancechange.org/events/national-seminar-on-the-development-and-integration-of-mitigation-evidence-in-capital-cases-04-24-2025/\n\n\nTassé, M. J., Schneider, W. J., & Olley, G. (2025, April 24). The APA’s professional guidelines for forensic assessments of ID. [Panel]. Twenty-Second National Seminar on the Development and Presentation of Mitigating Evidence in Capital Cases , Columbus, OH. https://advancechange.org/events/national-seminar-on-the-development-and-integration-of-mitigation-evidence-in-capital-cases-04-24-2025/\n\n\nSchneider, W. J. (2025, February 3). Making survey tests psychometrically sound. [Lecture]. Leadership Education in Neurodevelopmental Disabilities Program , Children’s Hospital of Philadelphia.\n\n\nSchneider, W. J. (2025, January 24). Advanced Interpretation Techniques with the Woodcock-Johnson V. [Webinar]. Comprehensive Assessment for Intervention . https://caipsychs.com/conferences/\n\n\nSchneider, W. J. (2024, October 18). Individual psychometrics. [Webinar]. Illinois School Psychologists Association . https://www.ilispa.org/fall-conference\n\n\nSchneider, W. J. (2024, March 15). Evidence-based assessment and clinical decision-making in school psychology. [Webinar]. 2024 Illinois School Psychologists Association Annual Convention . https://www.ilispa.org/annual-convention\n\n\nSchneider, W. J. (2024, February 26). Making survey tests psychometrically sound. [Lecture]. Leadership Education in Neurodevelopmental Disabilities Program , Children’s Hospital of Philadelphia.\n\n\nEngler, J. R., Schneider, W. J., Flanagan, D. P., & Niileksela, C. R. (2024, February 16). Improve PSW case conceptualization accuracy by taking measurement error seriously. [Mini-Skills Workshop]. National Association of School Psychologists Annual Convention , New Orleans, LA.\n\n\nSchneider, W. J. (2024, January 25). Confidently correct specific learning disability identification. [Webinar]. Comprehensive Assessment for Intervention . https://caipsychs.com/conferences/\n\n\nSchneider, W. J. (2023, November 4). Using arrowheadr with ggarrow and ggplot2. [Webinar]. ggplot2 Extenders .\n\n\nSchneider, W. J. (2023, September 27). Intelligence, critical thinking, and creativity. [Symposium]. In G. Alves (Moderator), Para Além dos Testes Tradicionais: Integrando Inovações à Avaliação de Criatividade e Pensamento Crítico (Beyond Traditional Tests: Integrating Innovations into the Assessment of Creativity and Critical Thinking) , Instituto Ayrton Senna, São Paulo, Brazil. https://institutoayrtonsenna.org.br/nossos-materiais/noticias/catedra-instituto-ayrton-senna-no-iea-usp-promove-live-integrando-inovacao-avaliacao-criatividade-pensamento-critico/\n\n\nSchneider, W. J. (2023, July 29). How recent theories of intelligence can improve the practice of cognitive ability assessment. [Panel]. International Society for Intelligence Research 2023 Annual Conference , Berkelety, CA. https://isironline.org/2023/01/isir-2023-berkeley-california-july-26-29/\n\n\nSchneider, W. J. (2023, March 24). Assessments that restore hope, promote understanding and inspire change. [Invited Lecture]. 42nd Annual School Psychology, Counseling Psychology and Applied Behavior Analysis Conference , Philadelphia, PA. https://education.temple.edu/node/50031\n\n\nSchneider, W. J. (2022, November 15). Using imaginary data to conserve real resources: Study design planning with model-based simulations. [Lecture]. Pearson 4th Developers Conversation .\n\n\nSchneider, W. J. (2022, July 27). What is the current state of affairs of IQ testing? [Keynote Address]. 2022 Knowledge, Innovation & Enterprise (KIE) Conference: Unpacking Creativity: Culture, Innovation, and Motivation in Global Contexts .\n\n\nFlanagan, D. P., Koponen, T., Wagner, R. K., Colvin, M. K., & Schneider, W. J. D. (2022, April 11). How do we diagnose learning disabilities? [Virtual Summit]. Learning Disabilities Summit: A Three-Part Virtual Event Focused on Crticial Learning Disability Issues , Learning Disabilities Association of America. https://ldaamerica.org/lda-to-hold-learning-disabilities-summit/\n\n\nSchneider, W. J. (2022, March 29). How measurement error affects learning disability identification accuracy. [Lecture]. Pearson Scientific Advisory Council .\n\n\nSchneider, W. J. (2022, March 28). Practical psychometrics: Robust interpretation for individuals. [Lecture]. Temple University School Psychology Owl Hour , Philadelphia, PA.\n\n\nSchneider, W. J. (2022, January 27). Life-and-death psychometrics: IQ and the death penalty. [Webinar]. Owl Hour , Temple University. https://events.temple.edu/owl-hour-life-and-death-psychometrics-iq-and-the-death-penalty\n\n\nHajovsky, D. B., & Schneider, W. J. (2021, September 24). Cognitive assessment in the evaluation of specific learning disabilities. [Webinar]. Missouri Association of School Psychologists 2021 Fall Conference , Weldon Spring, MO. https://maosp.wildapricot.org/resources/Documents/Fall%20Conference.pdf\n\n\nSchneider, W. J. (2021, July 29). Advances in test interpretation with the Cattell-Horn-Carroll Theory of Cognitive Abilities. [Webinar]. Training for Region 19 Evaluation Staff , Education Service Center, Region 19, El Paso, TX. https://wjschne.github.io/media/CHCTheoryWebinar.pdf\n\n\nSchneider, W. J. (2021, June 14). Using CHC theory to better understand student needs. [Webinar]. Hill Country Summer Institute . https://esc13.net/events/hill-country-summer-institute-2021\n\n\nSchneider, W. J. (2021, May 2). The accuracy of PSW methods. In J. R. Engler (Chair), PSW for SLD identification: Does cognition matter? [Symposium]. National Association of School Psychologists Annual Convention , Salt Lake City, UT. https://apps.nasponline.org/professional-development/convention/session-detail.aspx?id=20263\n\n\nSchneider, W. J. (2021, January 22). Improving LD identification with fewer myths and more science. [Invited Lecture]. Science to Practice Virtual Conference , Learning Disability Association of America.\n\n\nSchneider, W. J. (2020, July 29). Curious about CHC Theory? Take your evaluations to the next level! [Webinar]. Riverside Insights , Rolling Meadows, IL. https://info.riversideinsights.com/chc-theory-webinar\n\n\nSchneider, W. J. (2019, March 4). The evolution of PSW methods of SLD identification: What I have learned from PSW proponents. [Keynote Address]. Annual School Neuropsychology Conference , Long Beach, CA.\n\n\nFlanagan, D. P., Schneider, W. J., & Alfonso, V. C. (2019, February 26). PSW methods: Comparisons, research, and how to use them responsibly. [Symposium]. National Association of School Psychologists Annual Convention , Atlanta, GA.\n\n\nSchneider, W. J. (2019, February 26). Consumer-oriented and legally defensible psychoeducational reports–Advanced workshop. [Invited Workshop]. National Association of School Psychologists Conference , Atlanta, GA.\n\n\nSchneider, W. J. (2019, February 26). Consumer-oriented and legally defensible psychoeducational reports–Introductory workshop. [Invited Workshop]. National Association of School Psychologists Conference , Atlanta, GA.\n\n\nSchneider, W. J. (2019, January 4). The evolution of PSW methods of SLD identification: What I have learned from PSW critics. [Keynote Address]. Annual School Neuropsychology Conference , Long Beach, CA.\n\n\nSchneider, W. J. (2018, February 16). Re-evaluating the accuracy of the Dual Discrepancy/Consistency Model for SLD Identification. [Lecture]. National Association of School Psychologists Annual Convention , Chicago, IL.\n\n\nMcGrew, K. S., & Schneider, W. J. (2018, February 15). Revisions to the Cattell-Horn-Carroll (CHC) Theory of Intelligence. [Lecture]. National Association of School Psychologists Annual Convention , Chicago, IL.\n\n\nSchneider, W. J. (2016, December 4). How to write psychoeducational reports that people will want to read. [Webinar]. Region 10 Education Service Center , Richardson, TX.\n\n\nCormier, D. C., Bulut, O., Niileksela, C. R., Funamoto, A., & Schneider, W. J. D. (2016, December 2). Revisiting the relationship between CHC abilities and academic achievement. [Symposium]. National Association of School Psychologists Annual Convention , New Orleans, LA.\n\n\nSchneider, W. J. (2016, October 24). How to write psychoeducational reports that people will want to read all the way through. [Webinar]. KIPP Austin Public Schools and Texas State University School Psychology Program .\n\n\nMcGrew, K. S., & Schneider, W. J. (2016, October 19). Porque as habilidades cognitivas importam na educação [Why cognitive abilities matter in education]. [Invited Lecture]. Instituto Ayrton Senna , São Paulo, Brazil.\n\n\nSchneider, W. J., & McGrew, K. S. (2016, October 18). CHC theory, complex problem solving, critical thinking, and creativity. [Invited Lecture]. Instituto Ayrton Senna , São Paulo, Brazil.\n\n\nMcGrew, K. S., & Schneider, W. J. (2016, October 17). CHC theory and education in Brazil. [Invited Lecture]. Instituto Ayrton Senna , São Paulo, Brazil.\n\n\nSchneider, W. J. (2016, July 29). How to write psychoeducational reports that people will want to read all the way through. [Webinar]. New Brunswick Department of Education and Early Childhood Development , Fredericton, NB.\n\n\nSchneider, W. J. (2016, April 8). Writing reports worth reading all the way through. In R. Flanagan (Chair), Sophisticated simplicity: The art of writing reader-friendly assessment reports [Symposium]. Annual American Psychological Association Convention , Denver, CO.\n\n\nSchneider, W. J. (2015, July 15). I didn’t know the WJ IV could do that! Answering practical assessment questions you didn’t know you could ask. [Invited Lecture]. School Neuropsychology Summer Institute , Grapevine, TX.\n\n\nSchneider, W. J. (2014, February 15). What if we took our models seriously? Estimating latent scores in individuals. [Lecture]. National Association of School Psychologists Convention , Washington, D.C..\n\n\nSchneider, W. J. (2013, July 15). Advances in CHC Theory and its application to individuals. [Invited Lecture]. School Neuropsychology Summer Institute , Grapevine, TX.\n\n\nSchneider, W. J. (2013, February 13). Holes in CHC Theory. [Lecture]. National Association of School Psychologists Conference , Seattle, WA.\n\n\nSchneider, W. J., & Taylor, S. J. (2008, January 10). Can the Implicit Association Test be used as a lie detector? Morality, implicit attitudes, and Big Brother’s help measuring illegal file-sharing behavior. [Invited Lecture]. Center for Study of Public Choice at George Mason University , Fairfax, VA.\n\n\nSchneider, W. J. (2006, November 15). We need to change (but I won’t do anything because I’m not the problem): Couples therapy outcomes and the transtheoretical model of change. [Invited Lecture]. Colloquium at Purdue Unversity , West Lafayette, IN.\n\n\nSchneider, W. J., & Huber, B. (2004, March 15). The interactive effects of fluid intelligence and executive functions on behavior disorders in children. [Lecture]. Annual Meeting of the Illinois School Psychology Association , Springfield, IL."
  },
  {
    "objectID": "vita.html#posters-and-papers",
    "href": "vita.html#posters-and-papers",
    "title": "Curriculum Vitae",
    "section": "Posters and Papers",
    "text": "Posters and Papers\n\nAn, C., Schneider, W. J., Campagnolio, A., & Fiorello, C. A. (2025, February 20). Rigorous procedures for measuring broad and narrow abilities in individuals [Poster]. National Association of School Psychologists Convention, Seattle, WA.\n\n\nCampagnolio, A., Schneider, W. J., & An, C. (2025, February 20). Non-linear relations among reading abilities affect score interpretations [Poster]. National Association of School Psychologists Convention, Seattle, WA.\n\n\nHajovsky, D. B., Niileksela, C. R., Flanagan, D. P., Alfonso, V. C., Schneider, W. J., & Zinkiewicz, C. J. (2022, April 8). Toward a consensus model of cognitive-achievement relations using meta-SEM [Poster]. American Psychological Association Convention, Minneapolis, MN.\n\n\nSchneider, W. J., Flanagan, D. P., Niileksela, C. R., & Engler, J. R. (2020, February 20). Reevaluating the accuracy of PSW methods of SLD identification [Poster]. National Association of School Psychologists Convention, Baltimore, MD.\n\n\nSchneider, W. J., & Fiorello, C. A. (2019, August 8). Broad abilities are not doomed to be measured unreliably: A Monte Carlo study of omega statistics [Poster]. American Psychological Association Convention, Chicago, IL. https://github.com/wjschne/APA2019\n\n\nMulderink, T. D., Tobin, R. M., & Schneider, W. J. (2018, October 8). Personality, interactive history, and situational factors during children’s games [Poster]. Annual American Psychological Association Convention, San Francisco, CA.\n\n\nNicholls, C. J., Ross, E., & Schneider, W. J. (2016, April 15). The neuropsychological profiles of twins discordant for Sotos Syndrome: A case study [NA]. American Academy of Pediatric Neuropsychology Annual Conference, Las Vegas, NV.\n\n\nHowe, A. R., Curnock, A. D., Koppenhoefer, S. E., Schneider, W. J., & Tobin, R. M. (2016, April 8). Do vocabulary skills and instructor influence social-emotional learning? [Poster]. Annual American Psychological Association Convention, Denver, CO.\n\n\nCurnock, A. D., Pajor, K. E., Tobin, R. M., & Schneider, W. J. (2016, February 12). Preschoolers’ engagement during Second Step and their social-emotional knowledge [Poster]. National Association of School Psychologists Annual Convention, New Orleans, LA.\n\n\nEngelland, J. L., Tobin, R. M., Meyers, A., Huber, B., Schneider, W. J., Austen, J. H., & Corbin, A. L. (2014, December 8). Longitudinal effects of school climate on middle school students’ development [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nAffrunti, C. L., Schneider, W. J., Tobin, R. M., & Collins, K. D. (2014, August 7). Predictors of academic outcomes in college students suspected of having learning disorders [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nSondalle, A. A., Tobin, R. M., & Schneider, W. J. (2014, August 7). Behavioral engagement, Second Step edition, and children’s social-emotional functioning [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nSondalle, A. A., Probst, K. G., Carreno, C., Tobin, R. M., Schneider, W. J., Moore, N. A., & Mulderink, T. D. (2014, February 15). Engagement during Second Step and kindergarteners’ social-emotional and academic outcomes [Poster]. National Association of School Psychologists Convention, Washington, D.C..\n\n\nSondalle, A. A., Probst, K. G., Tobin, R. M., Schneider, W. J., & Kestian, J. M. (2014, February 15). Second Step and teacher ratings of children’s social-emotional functioning [Poster]. National Association of School Psychologists Conference, Washington, D.C..\n\n\nTobin, R. M., Schneider, W. J., Moore, N. A., Sondalle, A. A., & Willis, M. (2013, August 15). Effortful control and knowledge gains after the Second Step intervention [Poster]. Annual American Psychological Association Convention, Honolulu, HI.\n\n\nMoore, N. A., Sondalle, A. A., Mulderink, T. D., Tobin, R. M., Schneider, W. J., Willis, M., & Probst, K. G. (2013, February 15). Effortful control, Second Step, and behavior in kindergartners [Poster]. National Association of School Psychologists Convention, Seattle, WA.\n\n\nSondalle, A. A., Mulderink, T. D., Moore, N. A., Tobin, R. M., & Schneider, W. J. (2012, August 15). Engagement, dosage, and effectiveness of the kindergarten Second Step curriculum [Poster]. Annual American Psychological Association Convention, Orlando FL.\n\n\nMulderink, T. D., Sondalle, A. A., Moore, N. A., Tobin, R. M., Schneider, W. J., & Sheese, B. E. (2012, February 15). Influences of executive functioning and Second Step on behavior [Poster]. National Association of School Psychologists Conference, Philadelphia, PA.\n\n\nMulderink, T. D., Moore, N. A., Sondalle, A. A., Tobin, R. M., & Schneider, W. J. (2011, August 15). Executive functioning and responsiveness to Second Step [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nTobin, R. M., Mackin, J. M., Babcock, E. A., Mulderink, T. D., Moore, N. A., Carlson, L. A., Gioia, K. A., & Schneider, W. J. (2011, February 15). Intervention frequency and behavior in response to Second Step [Poster]. National Association of School Psychologists Convention, San Francisco, CA.\n\n\nTobin, R. M., Gioia, K. A., Mulderink, T. D., Carlson, L. A., Moore, N. A., & Schneider, W. J. (2010, August 15). Personality, Second Step dosage, and kindergartners’ social skills improvements [Poster]. Annual American Psychological Association Convention, San Diego CA.\n\n\nGioia, K. A., Tobin, R. M., & Schneider, W. J. (2010, January 15). Children’s knowledge gains in response to a social skills intervention [Poster]. Annual Illinois School Psychologist Association Convention, East Peoria, IL.\n\n\nMoore, N. A., Gioia, K. A., Tobin, R. M., & Schneider, W. J. (2010, January 3). Effortful control and emotion regulation in kindergartners [Poster]. National Association of School Psychologists Convention, Chicago, IL.\n\n\nObilade, M. H., Gioia, K. A., Tobin, R. M., & Schneider, W. J. (2010, January 3). Verbal ability and responsiveness to the Second Step curriculum [Poster]. National Association of School Psychologists Convention, Chicago, IL.\n\n\nSchneider, W. J., & Tobin, R. M. (2010, January 3). Previously impossible feats of interpretation and explanation with cognitive and achievement data [Poster]. National Association of School Psychologists Conference, Chicago, IL.\n\n\nTaylor, S. A., & Schneider, W. J. (2009, October 15). Implicit attitudes in folk explanations of digital piracy [Paper]. Frontiers in Service Conference, Honolulu, HI.\n\n\nBooth, B., Lederer, S., Dick, S., Linnell, J., Meyers, A., Schneider, W. J., Swerdlik, M. E., & Anweiler, J. (2009, August 15). Reintegration experiences of returning National Guard war veterans and implications for reintegration programming: Results from two states [Poster]. Annual American Psychological Association Convention, Toronto, ON.\n\n\nGadke, D. L., Tobin, R. M., & Schneider, W. J. (2009, August 15). Agreeableness and prejudice towards overweight men and women [Poster]. Annual American Psychological Association Convention, Toronto, ON.\n\n\nGioia, K. A., Tobin, R. M., & Schneider, W. J. (2009, August 15). Individual differences in children’s responsiveness to a social skills intervention [Poster]. Annual American Psychological Association Convention, Toronto, ON.\n\n\nGioia, K. A., Miller, K. A., Tobin, R. M., & Schneider, W. J. (2009, February 15). Children’s responsiveness to a social skills intervention [Poster]. National Association of School Psychologists Convention, Boston, MA.\n\n\nBaird, S. A., Schneider, W. J., & Tobin, R. M. (2008, August 15). Implicit agreeableness predicts laboratory measures of aggression [Poster]. Annual American Psychological Association Convention, Boston, MA.\n\n\nBarr, L. K., Kahn, J. H., & Schneider, W. J. (2008, March 15). Emotion expression tendencies and mood/anxiety symptoms [Poster]. International Counseling Psychology Conference, Chicago, IL.\n\n\nTobin, R. M., Schneider, W. J., & Landau, S. E. (2008, February 15). Assessing attention-deficit/hyperactivity disorder using an RTI approach [Poster]. National Association of School Psychologists Convention, New Orleans, LA.\n\n\nStagg, J. W., & Schneider, W. J. (2007, November 15). So you flunked the Stroop Test, so what? The utility of single vs. multiple indicators in predicting behavioral outcomes related to executive functioning among non-referred individuals [Poster]. Annual National Academy of Neuropsychology Conference, Scottsdale, AZ.\n\n\nSchneider, W. J. (2007, August 15). Is the TAT helpful in assessing complex aspects of attention and executive functions? An extremely labor-intensive celebration of the null hypothesis [Poster]. Annual American Psychological Association Convention, San Francisco, CA.\n\n\nSchneider, W. J., & Kasson, D. M. (2007, January 3). Do narrow abilities matter more for people with higher IQ? Implications of Spearman’s Law of Diminishing Returns for the discrepancy model of LD [Poster]. National Association of School Psychologists Convention, New York, NY.\n\n\nSchneider, W. J., & McKenna, T. L. (2006, August 15). Using the Implicit Association Test to measure early maladaptive schemas [Poster]. Annual American Psychological Association Convention, New Orleans, LA.\n\n\nKahn, J. H., Vogel, D. L., Schneider, W. J., Barr, L. K., & Henning, K. (2006, April 15). Emotional self-disclosures of college-student clients and counseling session outcome [Poster]. Great Lakes Conference, West Lafayette, IN.\n\n\nJones, G. B., & Schneider, W. J. (2005, August 15). Intelligence, human capital, and economic growth [Paper]. Econometric Society World Congress, London, UK.\n\n\nTobin, R. M., Bodner, A. C., & Schneider, W. J. (2005, August 15). Executive function and emotion regulation in children [Poster]. Annual American Psychological Association Convention, Washington, D.C..\n\n\nMarch, A., Funk, K., Schneider, W. J., & Landau, S. E. (2005, January 3). Student and teacher attributions of school shootings [Poster]. National Association of School Psychologists Convention, Atlanta, GA.\n\n\nTobin, R. M., Schneider, W. J., Graziano, W. G., & Pizzitola, K. M. (2002, February 15). Nice kids in competitive situations [Poster]. Annual Society for Personality and Social Psychology Meeting, Savannah, GA.\n\n\nSnyder, D. K., Schneider, W. J., Oxford, M. C., & Quinn, T. (2001, July 15). Secondary prevention group intervention for couples [Paper]. World Congress of Behavioral and Cognitive Therapies, Vancouver, BC.\n\n\nTobin, R. M., Pizzitola, K. M., Schneider, W. J., & Graziano, W. G. (2001, April 15). Goal structures and competitiveness in children’s games [Poster]. Biennial Meeting of the Society for Research in Child Development, Minneapolis, MN.\n\n\nSchneider, W. J., Loss, R. M., Cavell, T. A., & Oxford, M. C. (2000, November 15). Parenting and children with callous-unemotional traits: Relationship quality as a potential socialization mechanism [Poster]. Annual Convention of the Association for the Advancement Behavior Therapy, New Orleans, LA.\n\n\nSnyder, D. K., Schneider, W. J., Oxford, M. C., & Quinn, T. (2000, November 15). SUCCESS: The efficacy of a relationship enhancement group [Poster]. Annual Convention of the Association for the Advancement Behavior Therapy, New Orleans, LA.\n\n\nSchneider, W. J., Cavell, T. A., Hughes, J. N., & Oxford, M. C. (1999, August 15). The development of the Perceived Containment Questionnaire [Poster]. Annual American Psychological Association Convention, Boston, MA."
  },
  {
    "objectID": "vita.html#courses",
    "href": "vita.html#courses",
    "title": "Curriculum Vitae",
    "section": "Courses",
    "text": "Courses\n\nTemple University\n\nCPSY 5519—Group Counseling (Spring 2018–2019)\nCPSY 5694—Introduction to Assessment (Fall 2017–2019, 2023)\nEDUC 5101—Critical Understanding of Social Science Research (Fall 2018)\nEDUC 5325—Introduction to Statistics and Research (Fall 2017, 2021)\nEPSY 5529—Tests and Measurements (Spring 2019–2020, 2022, 2023)\nEPSY 8825—Advanced Data Analysis (Spring 2020, 2022)\nSPSY 9687/8—Seminar in School Psychology/Psychoeducational Clinic (Fall 2019, Spring 2020–2021, Fall 2023)\n\n\n\nIllinois State University\n\nPSY 110—Explaining Human Behavior/Fundamentals of Psychology (Fall 2002–2006, Spring 2003–2004, 2006–2007)\nPSY 138—Social Science Reasoning Using Statistics/Reasoning in Psychology Using Statistics (Spring 2004, 2008–2012, 2014; Fall 2007–2014, 2016)\nPSY 340—Statistics for the Social Sciences (Spring 2005)\nPSY 432—Psychodiagnostics I: Cognitive Assessment/ Theory and Practice of Cognitive Assessment (Fall 2004–2016)\nPSY 436—Clinical/Counseling Practicum (Spring 2004)\nPSY 442—Test Theory (Spring 2015, 2017)\nPSY 443—Regression Analysis (Spring 2016)\nPSY 444—Multivariate Analysis (Fall 2015){.smalldate}\nPSY 464—Theories and Techniques of Counseling: Adults (Spring 2005–2012, 2014–2016)\nPSY 480—Advanced Practicum in Dialectical Behavior Therapy Skills (Fall 2015–2017)\nPSY 480.31—Practicum in Dialectical Behavior Therapy Skills Training (Fall 2014, Spring 2105–2017)\nPSY 480.33—Seminar in Psychology: Supervision of a Dialectical Behavior Therapy Group (Fall 2014, Spring 2105, 2017)"
  },
  {
    "objectID": "vita.html#mentoring",
    "href": "vita.html#mentoring",
    "title": "Curriculum Vitae",
    "section": "Mentoring",
    "text": "Mentoring\n\nDoctoral Dissertation Chair\n\nTemple University\n\nAidan Campagnolio\nChristine An (co-chair)\nMegan Barone\nRandy Taylor\nBernard Dillard (2024)\nStephanie Iaccarino (2023, co-chair)\nJustin Harper (2022, co-chair)\n\n\n\nIllinois State University\n\nC. Lee Affrunti (2013)\nDaniel L. Gadke (co-chair, 2012)\nJonathan W. Stagg (2008)\n\n\n\n\nDoctoral Dissertation Committee Member\n\nTemple University\n\nEmunah Mager-Garfield (2024)\nOctavia Blount (2024)\nValerie Woxholdt (2024)\nCodie Kane (2023)\nPatrick Clancy (2023)\nShana Levi-Nielsen (2022)\nKaiyla Darmer (2022)\nKathryn DeVries (2022)\nTera Gibbs (2022)\nMariah Davis (2022)\nLinda Ruan (2020)\n\n\n\nIllinois State University\n\nJennifer Engelland-Schultz (2015)\nMandi Martinez-Dick (2015)\nThomas Mulderink (2015)\nAlyssa A. Sondalle (2015)\nRachelle Cantin (2013)\nJennifer Wallace (2013)\nSilas Dick (2013)\nTrisha Mann (2012)\nKatherine A. Gioia (2009)\nSarah Reck (2008)\nAnna C. Bodner (2005)\n\n\n\nExternal Reviewer\n\nBrianna Paul (2022) Texas Women’s University\nJake Blair Kraska (2021) Monash University\nPaul Jewsbury (2014) University of Melbourne\n\n\n\n\nMaster’s Thesis Chair\n\nIllinois State University\n\nFeng Ji (2018)\nKatrin Klieme (2016)\nZachary Roman (2016)\nKiera Dymit (2015)\nMeera Afzal (2012)\nRachelle Bauer (2010)\nSunthud Pornprasertmanit (2010)\nDavid Kasson (2006)\nKristina Taylor (2006)\nRobin Van Herrmann (2010)\nMelissa Zygmun (2006)\n\n\n\n\nMaster’s Thesis Committee Member\n\nIllinois State University\n\nRyan Willard (2017)\nRachel Workman (2017)\nHayley Love (2016)\nAnges Strojewska (2016)\nDanielle Freund (2015)\nAmanda Fisher (2015)\nDaniel Nuccio (2014)\nKevin Wallpe (2014)\nNicole Moore (2013)\nDrew Abney (2012)\nThomas Mulderink (2012)\nJames Clinton (2011)\nJamie Hansen (2010)\nYin Ying Ong (2010)\nMelanie Hewett (2010)\nKaty Adler (2008)\nPoonam Joshi (2008)\nRebecca Hoerr (2008)\nArusha Sethi (2008)\nLeah Barr (2008)\nSara Byczek (2007)"
  },
  {
    "objectID": "vita.html#university-service",
    "href": "vita.html#university-service",
    "title": "Curriculum Vitae",
    "section": "University Service",
    "text": "University Service\n\nIllinois State University\n\n\n\n\n\n\n2007–2017\n\n\nCoordinator and Supervisor, College Learning Assessment Service (CLAS), Psychological Services Center"
  },
  {
    "objectID": "vita.html#college-service",
    "href": "vita.html#college-service",
    "title": "Curriculum Vitae",
    "section": "College Service",
    "text": "College Service\n\nTemple University\n\n\n\n\n\n\n2022–2023\n\n\nMember, CEHD Transition Committee\n\n\n\n\n2022–2023\n\n\nMember, CEHD Tenure and Promotion Committee\n\n\n\n\n2020\n\n\nMember, Higher Education Non-Tenure-Track Faculty Search Committee\n\n\n\n\n2019–2022\n\n\nChair, Faculty Merit Committee\n\n\n\n\n2019\n\n\nCo-Chair, Data and Assessment Working Group\n\n\n\n\n2019\n\n\nMember, Higher Education Tenure-Track Faculty Search Committee\n\n\n\n\n2019\n\n\nTraining presenter: Make Your Data POP! How to interpret and present data visually to maximize its impact\n\n\n\n\n2018–2019\n\n\nMember, Faculty Resource and Development Committee\n\n\n\n\n2018\n\n\nMember, Special Education Faculty Search committee\n\n\n\n\n2018\n\n\nTraining presenter: Introduction to Statistical Analysis with R Workshop"
  },
  {
    "objectID": "vita.html#departmental-service",
    "href": "vita.html#departmental-service",
    "title": "Curriculum Vitae",
    "section": "Departmental Service",
    "text": "Departmental Service\n\nTemple University\n\n\n\n\n\n\n2018–\n\n\nMember, School Psychology Program Committee\n\n\n\n\n2017–2020, 2023–\n\n\nMember, Counseling Psychology Program Committee\n\n\n\n\n2020–2023\n\n\nMember, Educational Leadership Program Committee\n\n\n\n\n2018–2020\n\n\nMember, Departmental Promotion and Tenure Committee\n\n\n\n\n\n\nIllinois State University\n\n\n\n\n\n\n2004–2017\n\n\nMember, Clinical/Counseling Psychology Coordinating committee\n\n\n\n\n2004–2017\n\n\nMember, IRB Committee\n\n\n\n\n2004–2017\n\n\nMember, Psychological Services Center Administrative Team\n\n\n\n\n2004–2017\n\n\nMember, Quantitative Psychology Sequence committee\n\n\n\n\n2004–2017\n\n\nMember, Research Committee\n\n\n\n\n2015–2017\n\n\nMember, Department Faculty Status Committee (Evaluates annual merit, tenure, and promotion)\n\n\n\n\n2017\n\n\nChair, Salary Compression Review Team\n\n\n\n\n2015\n\n\nProgram Coordinator, Quantitative Sequence (Fall only)\n\n\n\n\n2014\n\n\nMember, Clinical/Counseling Faculty Search committee\n\n\n\n\n2004–2005\n\n\nMember, Clinical/Counseling Faculty Search committee"
  },
  {
    "objectID": "vita.html#consulting",
    "href": "vita.html#consulting",
    "title": "Curriculum Vitae",
    "section": "Consulting",
    "text": "Consulting\n\n\n\n\n\n\n2019–\n\n\nAcademic Consultant, Empass Learning, developing an online test of cognitive abilities based on CHC Theory, Gurgaon, India\n\n\n\n\n2014–\n\n\nExpert Consultant, providing expert evaluations of death penalty and medical cases for various legal firms, US\n\n\n\n\n2017–2020\n\n\nAcademic Consultant, PT Melintas Cakrawala, developing AJT CogTest?a comprehensive test of cognitive abilities based on CHC Theory, Jakarta, Indonesia\n\n\n\n\n2015–2017\n\n\nAcademic Consultant, Ayrton Senna Institute, developing a comprehensive assessment of 21st century skills, Sao Paulo, Brazil\n\n\n\n\n2016\n\n\nAcademic Consultant, Academic Therapy Publications, developing and reviewing cognitive and oral language assessment batteries, Novato, CA\n\n\n\n\n2007–2010\n\n\nResearch Consultant, Illinois Army National Guard, conducting research examining reintegration of service personnel"
  },
  {
    "objectID": "vita.html#media-appearances",
    "href": "vita.html#media-appearances",
    "title": "Curriculum Vitae",
    "section": "Media Appearances",
    "text": "Media Appearances\n\n\n\n\n\n\n02/03/2014\n\n\nInterviewed in Scientific American Blog Beautiful Minds by Scott Barry Kaufman\n\n\n\n\n08/05/2016\n\n\nQuoted in the Wall Street Journal about IQ tests\n\n\n\n\n10/13/2016\n\n\nQuoted in ScienceNews for Students about IQ tests\n\n\n\n\n10/11/2017\n\n\nOpined in the Guardian about the president’s IQ\n\n\n\n\n04/09/2018\n\n\nAppeared as a guest of Knowledge@Wharton Radio to discuss hunger and homelessness among college students.\n\n\n\n\n04/03/2018\n\n\nAppeared on CNN Headline News (HLN) with MichaeLA to discuss hunger and homelessness among college students.\n\n\n\n\n10/20/2019\n\n\nPresented on Writing Assessment Reports People Will Read, Understand, and Remember on the School Psyched Podcast\n\n\n\n\n03/01/2020\n\n\nQuoted in APA Monitor on writing assessment reports\n\n\n\n\n03/29/2021\n\n\nPresented on the evolution of cognitive assessment on the Testing Psychologist Podcasts"
  },
  {
    "objectID": "vita.html#editorial-positions",
    "href": "vita.html#editorial-positions",
    "title": "Curriculum Vitae",
    "section": "Editorial Positions",
    "text": "Editorial Positions\n\n\n\n\n\n\n\n\n\n\n2019⁠–⁠2020\nJournal of Psychoeducational Assessment\nAssociate Editor\n\n\n2015\nJournal of School Psychology\nGuest Action Editor"
  },
  {
    "objectID": "vita.html#editorial-boards",
    "href": "vita.html#editorial-boards",
    "title": "Curriculum Vitae",
    "section": "Editorial Boards",
    "text": "Editorial Boards\n\n\n\n\n\n\n\n\n\n\n2018⁠–⁠\nJournal of Intelligence\nEditorial Board\n\n\n2011⁠–⁠\nJournal of School Psychology\nEditorial Board\n\n\n2010⁠–⁠\nJournal of Psychoeducational Assessment\nEditorial Board\n\n\n2017⁠–⁠2021\nArchives of Scientific Psychology\nConsulting Editor\n\n\n2009⁠–⁠2018\nPsychological Assessment\nConsulting Editor\n\n\n1997⁠–⁠2000\nClinician's Research Digest\nEditorial Associate"
  },
  {
    "objectID": "vita.html#ad-hoc-reviewing",
    "href": "vita.html#ad-hoc-reviewing",
    "title": "Curriculum Vitae",
    "section": "Ad Hoc Reviewing",
    "text": "Ad Hoc Reviewing\n\nApplied Neuropsychology\nEuropean Journal of Psychological Assessment\nFrontiers in Human Neuroscience\nJournal of Family Psychology\nJournal of Psychoeducational Assessment\nNASP Communiqué\nPsychological Assessment\nPsychology in the Schools"
  },
  {
    "objectID": "vita.html#professional-memberships",
    "href": "vita.html#professional-memberships",
    "title": "Curriculum Vitae",
    "section": "Professional Memberships",
    "text": "Professional Memberships\n\nMember, American Psychological Association\nMember, National Association of School Psychologists"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html",
    "href": "tutorials/SEMGrowth/semgrowth.html",
    "title": "Latent Growth Models",
    "section": "",
    "text": "Setup Code\noptions(digits = 4, knitr.kable.NA = '')\n\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(lavaan)\nlibrary(ggdiagram)\nlibrary(gt)\nlibrary(geomtextpath)\nlibrary(sjPlot)\nlibrary(easystats)\n\nmy_font &lt;- \"Roboto Condensed\"\n\nmy_apa &lt;- function(tb) {\n  # amended from https://benediktclaus.github.io/benelib/reference/theme_gt_apa.html\n  tb %&gt;% \n    cols_align(columns = 1, align = \"left\") %&gt;% \n    opt_table_lines(extent = \"none\") %&gt;% \n    tab_options(table.border.top.style = \"solid\",\n                table.border.top.color = \"black\", \n                table.border.top.width = 1,\n                table_body.border.bottom.style = \"solid\", \n                table_body.border.bottom.color = \"black\",\n                table_body.border.bottom.width = 1, \n                column_labels.border.bottom.style = \"solid\", column_labels.border.bottom.color = \"black\", \n            column_labels.border.bottom.width = 1)\n}"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#level-1-measurement-occasion",
    "href": "tutorials/SEMGrowth/semgrowth.html#level-1-measurement-occasion",
    "title": "Latent Growth Models",
    "section": "Level 1 (Measurement Occasion)",
    "text": "Level 1 (Measurement Occasion)\n\nVocabulary_{ti} = b_{0i}+b_{1i}Time_{ti} +b_{2i}Time_{ti}^2+e_{ti}"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#level-2-person",
    "href": "tutorials/SEMGrowth/semgrowth.html#level-2-person",
    "title": "Latent Growth Models",
    "section": "Level 2 (Person)",
    "text": "Level 2 (Person)\n\n\\begin{aligned}\nb_{0i}&=b_{00} + b_{01}Intervention_{i} + u_{0i}\\\\\nb_{1i}&=b_{10} + b_{11}Intervention_{i} + u_{1i}\\\\\nb_{2i}&=b_{20} + b_{21}Intervention_{i}\n\\end{aligned}"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#combined-model",
    "href": "tutorials/SEMGrowth/semgrowth.html#combined-model",
    "title": "Latent Growth Models",
    "section": "Combined Model",
    "text": "Combined Model\n\n\\begin{aligned}\nVocabulary_{ti} &=\n\\underbrace{b_{00} + b_{01}Intervention_{i} + u_{0i}}_{b_{0i}} +\\\\\n&(\\underbrace{b_{10} + b_{11}Intervention_{i} + u_{1i}}_{b_{1i}})Time_{ti} +\\\\\n&(\\underbrace{b_{20}+ b_{21}Intervention_{i}}_{b_{2i}})Time_{ti}^2 + e_{ti}\n\\end{aligned}\n\n\n\nCode\ntibble::tribble(\n     ~Symbol,                                                            ~Meaning,\n  \"$b_{00}$\", \"Intercept (Predicted Vocabulary when Intervention and Time are 0)\",\n  \"$b_{01}$\",                                  \"Slope for Intervention at Time 0\",\n  \"$b_{10}$\",            \"Linear Slope for Time when Intervention and Time are 0\",\n  \"$b_{11}$\",                    \"Interaction of Intervention and Time at Time 0\",\n  \"$b_{20}$\",                   \"Quadratic Effect of Time when Intervention is 0\",\n  \"$b_{21}$\",          \"Interaction of Intervention and Quadratic Effect of Time\",\n  \"$b_{0i}$\",                     \"Predicted Vocabulary at Time 0 for Person *i*\",\n  \"$b_{1i}$\",                    \"Linear Effect of Time at Time 0 for Person *i*\",\n  \"$b_{2i}$\",                           \"Quadratic Effect of Time for Person *i*\",\n  \"$u_{0i}$\",                                             \"Residual for $b_{0i}$\",\n  \"$u_{1i}$\",                                             \"Residual for $b_{1i}$\",\n  \"$e_{ti}$\",                               \"Residual for Person *i* at Time *t*\"\n  ) %&gt;% \n  knitr::kable(align = \"cl\")\n\n\n\n\n\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\nb_{00}\nIntercept (Predicted Vocabulary when Intervention and Time are 0)\n\n\nb_{01}\nSlope for Intervention at Time 0\n\n\nb_{10}\nLinear Slope for Time when Intervention and Time are 0\n\n\nb_{11}\nInteraction of Intervention and Time at Time 0\n\n\nb_{20}\nQuadratic Effect of Time when Intervention is 0\n\n\nb_{21}\nInteraction of Intervention and Quadratic Effect of Time\n\n\nb_{0i}\nPredicted Vocabulary at Time 0 for Person i\n\n\nb_{1i}\nLinear Effect of Time at Time 0 for Person i\n\n\nb_{2i}\nQuadratic Effect of Time for Person i\n\n\nu_{0i}\nResidual for b_{0i}\n\n\nu_{1i}\nResidual for b_{1i}\n\n\ne_{ti}\nResidual for Person i at Time t\n\n\n\n\n\n\nTable 6: Symbols and Meanings of HLM Model"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#analysis",
    "href": "tutorials/SEMGrowth/semgrowth.html#analysis",
    "title": "Latent Growth Models",
    "section": "Analysis",
    "text": "Analysis\nI am going straight to my hypothesized model. In a real analysis, I might have gone through the series of steps we have seen elsewhere in the course. However, I want save time so that I can show how this model is the same as the latent growth model. When I walk through all the latent growth models in sequence, I will show the comparable hierarchical linear models one at a time.\n\nm &lt;- lmer(vocabulary ~ 1 + time * intervention + \n            I(time ^ 2) * intervention + \n            (1 + time | person_id), \n          data = d, \n          REML = FALSE)\n\ntab_model(m)\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n52.00\n46.36 – 57.64\n&lt;0.001\n\n\ntime\n10.50\n9.91 – 11.09\n&lt;0.001\n\n\nintervention\n-0.09\n-7.91 – 7.74\n0.983\n\n\ntime^2\n-1.06\n-1.15 – -0.96\n&lt;0.001\n\n\ntime × intervention\n9.14\n8.33 – 9.95\n&lt;0.001\n\n\nintervention × time^2\n0.09\n-0.05 – 0.22\n0.217\n\n\nRandom Effects\n\n\n\nσ2\n8.99\n\n\n\nτ00 person_id\n786.64\n\n\nτ11 person_id.time\n2.02\n\n\nρ01 person_id\n0.12\n\n\nICC\n0.99\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.381 / 0.993\n\n\n\n\nreport(m)\n\nWe fitted a linear mixed model (estimated using ML and nloptwrap optimizer) to\npredict vocabulary with time and intervention (formula: vocabulary ~ 1 + time *\nintervention + I(time^2) * intervention). The model included time as random\neffects (formula: ~1 + time | person_id). The model's total explanatory power\nis substantial (conditional R2 = 0.99) and the part related to the fixed\neffects alone (marginal R2) is of 0.38. The model's intercept, corresponding to\ntime = 0 and intervention = 0, is at 52.00 (95% CI [46.36, 57.64], t(1190) =\n18.08, p &lt; .001). Within this model:\n\n  - The effect of time is statistically significant and positive (beta = 10.50,\n95% CI [9.91, 11.09], t(1190) = 35.18, p &lt; .001; Std. beta = 0.47, 95% CI\n[0.46, 0.48])\n  - The effect of intervention is statistically non-significant and negative\n(beta = -0.09, 95% CI [-7.91, 7.74], t(1190) = -0.02, p = 0.983; Std. beta =\n0.32, 95% CI [0.21, 0.43])\n  - The effect of time^2 is statistically significant and negative (beta = -1.06,\n95% CI [-1.15, -0.96], t(1190) = -21.10, p &lt; .001; Std. beta = -0.08, 95% CI\n[-0.09, -0.07])\n  - The effect of time × intervention is statistically significant and positive\n(beta = 9.14, 95% CI [8.33, 9.95], t(1190) = 22.08, p &lt; .001; Std. beta = 0.22,\n95% CI [0.21, 0.23])\n  - The effect of intervention × time^2 is statistically non-significant and\npositive (beta = 0.09, 95% CI [-0.05, 0.22], t(1190) = 1.24, p = 0.217; Std.\nbeta = 3.40e-03, 95% CI [-2.00e-03, 8.81e-03])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\nDon’t be fooled by the non-significant effect of the intervention! It is a conditional effect at Time 0—before the intervention began.\nIt is the interaction of time and the intervention that matters.\nLet’s plot the model using sjPlot::plot_model:\n\nplot_model(m, type = \"pred\", terms = c(\"time [all]\", \"intervention\"))\n\n\n\n\n\n\n\n\nInterpretation: As hypothesized, the effect of time has diminishing returns, and the time by interaction interaction effect is evident. The two groups were comparable before the intervention, but the treatment group’s vocabulary grew faster over time. If I needed to publish a plot showing these results, it would look like Figure 10.\n\n\nSidenote: I love the sjPlot::plot_model function. With almost no effort, I can get a plot that helps me see what is going on with my fixed effects. It used to take me so much more code to arrive at where sjPlot::plot_model lands. However, if you want a publication-worthy plot, sjPlot::plot_model tends to impose too much control. Fortunately, the same person who made sjPlot is involved with another easystats package called modelbased that allows for greater flexibility but still reduces what would otherwise be onerous coding.\n\n\nCode\nm_fixed &lt;- fixef(m)\n\nmy_coefs &lt;- tibble(b00 = c(m_fixed[\"(Intercept)\"],\n                          m_fixed[\"(Intercept)\"] + m_fixed[\"intervention\"]),\n                   b10 = c(m_fixed[\"time\"],\n                           m_fixed[\"time\"] + m_fixed[\"time:intervention\"]),\n                   b20 = c(m_fixed[\"I(time^2)\"],\n                           m_fixed[\"I(time^2)\"] + m_fixed[\"intervention:I(time^2)\"])) %&gt;% \n  mutate(across(.fns = \\(x) formatC(x, 2, format = \"f\"))) %&gt;% \n  mutate(intervention = factor(c(\"Control\", \"Treatment\")),\n         eq = glue::glue('{intervention}: {b00} + {b10}Time + {b20}Time&lt;sup&gt;2&lt;/sup&gt;')\n                   )\n\n\n\n# Predicted Data\nd_pred &lt;- modelbased::estimate_relation(m, data = crossing(time = 0:5, intervention = c(0,1))) %&gt;% \n  as_tibble() %&gt;% \n  mutate(intervention = factor(intervention, levels = c(0,1), labels = c(\"Control\", \"Treatment\"))) %&gt;% \n  mutate(vocabulary = Predicted, \n         hjust = ifelse(intervention == \"Treatment\", .93, .91)) %&gt;% \n  left_join(my_coefs, by = join_by(intervention)) \n\nd %&gt;% \n  mutate(intervention = factor(intervention, levels = c(0,1), labels = c(\"Control\", \"Treatment\")),\n         person_id = factor(person_id)) %&gt;% \n  ggplot(aes(time, vocabulary, color = intervention)) +\n  geom_line(aes(group = person_id,\n                x = time + (intervention == \"Treatment\") * 0.2 - 0.1),\n            size = .1, \n            alpha = .5) + \n  ggbeeswarm::geom_quasirandom(\n    aes(x = time + (intervention == \"Treatment\") * 0.2 - 0.1), \n    width = .075, \n    pch = 16, \n    size = .75) +\n  geom_line(data = d_pred,\n            linewidth = 1) +\n  geom_labelline(aes(label = eq,\n                     hjust = hjust),\n                 data = d_pred, \n                 text_only = T,\n                 alpha = .8, size = 5,\n                 # linewidth = 4.25,\n                 linecolour = NA,\n                 vjust = -0.2,\n                 label.padding = unit(0, units = \"pt\"),\n                 family = \"Roboto Condensed\", \n                 rich = TRUE) +\n  scale_x_continuous(\"Time\", \n                     breaks = 0:5, \n                     minor_breaks = NULL) + \n  scale_y_continuous(\"Vocabulary\") +\n  theme_minimal(base_size = 16, \n                base_family = \"Roboto Condensed\") + \n  theme(legend.position = \"none\") + \n  scale_color_brewer(type = \"qual\", palette = 1)\n\n\n\n\n\n\n\n\nFigure 10: The Growth of Vocabulary by Treatment Group"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#restructure-from-long-to-wide-data",
    "href": "tutorials/SEMGrowth/semgrowth.html#restructure-from-long-to-wide-data",
    "title": "Latent Growth Models",
    "section": "Restructure from Long to Wide Data",
    "text": "Restructure from Long to Wide Data\nSEM analyses usually need wide data with each participant’s data on 1 row. We learned how to do so in a previous tutorial.\nUse the pivot_wider function to restructure your data so that the data set d is restructured to look like this:\n\n\n# A tibble: 200 × 8\n   person_id intervention voc_0 voc_1 voc_2 voc_3 voc_4 voc_5\n       &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1         1            0  31.3  38.6  51.2  54.9  50.6  55.3\n 2         2            0  94.1 100.  114.  119.  123.  125. \n 3         3            0  57.0  66.5  73.6  80.9  88.2  93.1\n 4         4            0  60.3  70.2  78.9  78.9  78.4  79.0\n 5         5            0  32.6  39.1  47.3  48.1  51.1  48.3\n 6         6            1  88.6 105.  120.  141.  160.  169. \n 7         7            0  65.6  72.4  76.8  83.3  86.3  87.0\n 8         8            0  35.9  41.7  57.4  64.8  64.1  73.1\n 9         9            1  71.7  78.9  97.4 105.  121.  126. \n10        10            1  75.9  85.0 106.  119.  132.  142. \n# ℹ 190 more rows\n\n\nSee if you can figure out how to restructure the data using pivot_wider. If it takes more than a minute or two, check out the hints.\nCall the new data frame d_wide.\n\n\n\n\n\n\nHintHint\n\n\n\n\n\nSet the values parameter equal to vocabulary.\nvalues = vocabulary\n\n\n\n\n\n\nHintHint 2\n\n\n\n\n\nThe names come from time.\nnames = time\n\n\n\n\n\n\nHintHint 3\n\n\n\n\n\nSpecify voc_ as the names prefix.\nnames_prefix = \"voc_\"\n\n\n\n\n\n\nHintHint 4\n\n\n\n\n\nHere is how I would do it:\n\nd_wide &lt;- d %&gt;% \n  pivot_wider(names_from = time, \n              values_from = vocabulary, \n              names_prefix = \"voc_\")"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-specification",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-specification",
    "title": "Latent Growth Models",
    "section": "Model Specification",
    "text": "Model Specification\nIn general, setting up a latent growth curve model requires much more typing than the corresponding hierarchical linear model. In this model, for all this extra work we get nothing that we did not already have. However, the latent growth modeling approach is extremely flexible, particularly with respect to modeling the simultaneous effects of many other variables. In more complex models, we can evaluate hypotheses that are difficult, if not impossible, in hierarchical linear models.\nNote that intercepts are symbolized with the symbol 1 when it is alone or precided by a coefficient (e.g., b_00 ~ 1)\n\nlatent_model &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Acceleration effect of time (quadratic effect)\nq =~ 0 * voc_0 + 1 * voc_1 + 4 * voc_2 + 9 * voc_3 + 16 * voc_4 + 25 * voc_5\n# Predicting intercept, slope, and quadratic effects\ni ~ b_00 * 1 + b_01 * intervention\ns ~ b_10 * 1 + b_11 * intervention\nq ~ b_20 * 1 + b_21 * intervention\n# Variances and covariances\ni ~~ tau_00 * i + tau_01 * s\ns ~~ tau_11 * s\nq ~~ 0 * q + 0 * s + 0 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n\"\n\nIn the hierarchical model, we did not make b_{2i} (the quadratic effect of time) random. Thus, we will set the \\tau_{22} variance to 0, and also the covariances involving u_{2i} residual (i.e., \\tau_{02} and \\tau_{12}).\nYou can see from the summary output that the parameters are same as before.\n\nfit_latentgrowth &lt;- growth(latent_model, data = d_wide)\n\nsummary(fit_latentgrowth, \n        fit.measures = T, \n        standardized = T)\n\nlavaan 0.6-19 ended normally after 111 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        15\n  Number of equality constraints                     5\n\n  Number of observations                           200\n\nModel Test User Model:\n                                                      \n  Test statistic                                20.473\n  Degrees of freedom                                23\n  P-value (Chi-square)                           0.613\n\nModel Test Baseline Model:\n\n  Test statistic                              4123.548\n  Degrees of freedom                                21\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.001\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3806.202\n  Loglikelihood unrestricted model (H1)      -3795.966\n                                                      \n  Akaike (AIC)                                7632.405\n  Bayesian (BIC)                              7665.388\n  Sample-size adjusted Bayesian (SABIC)       7633.707\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.051\n  P-value H_0: RMSEA &lt;= 0.050                    0.947\n  P-value H_0: RMSEA &gt;= 0.080                    0.001\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.006\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    voc_0             1.000                              28.047    0.994\n    voc_1             1.000                              28.047    0.975\n    voc_2             1.000                              28.047    0.931\n    voc_3             1.000                              28.047    0.870\n    voc_4             1.000                              28.047    0.802\n    voc_5             1.000                              28.047    0.734\n  s =~                                                                  \n    voc_0             0.000                               0.000    0.000\n    voc_1             1.000                               4.781    0.166\n    voc_2             2.000                               9.563    0.317\n    voc_3             3.000                              14.344    0.445\n    voc_4             4.000                              19.126    0.547\n    voc_5             5.000                              23.907    0.625\n  q =~                                                                  \n    voc_0             0.000                               0.000    0.000\n    voc_1             1.000                               0.043    0.001\n    voc_2             4.000                               0.171    0.006\n    voc_3             9.000                               0.386    0.012\n    voc_4            16.000                               0.686    0.020\n    voc_5            25.000                               1.072    0.028\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i ~                                                                   \n    intrvnt (b_01)   -0.086    3.988   -0.022    0.983   -0.003   -0.002\n  s ~                                                                   \n    intrvnt (b_11)    9.138    0.414   22.079    0.000    1.911    0.955\n  q ~                                                                   \n    intrvnt (b_21)    0.086    0.069    1.236    0.217    2.002    1.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .i ~~                                                                  \n   .s       (t_01)    4.602    3.175    1.449    0.147    0.115    0.115\n .s ~~                                                                  \n   .q                 0.000                                 NaN      NaN\n .i ~~                                                                  \n   .q                 0.000                                 NaN      NaN\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .i       (b_00)   52.002    2.876   18.082    0.000    1.854    1.854\n   .s       (b_10)   10.500    0.298   35.181    0.000    2.196    2.196\n   .q       (b_20)   -1.056    0.050  -21.097    0.000  -24.647  -24.647\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .i       (t_00)  786.620   79.133    9.940    0.000    1.000    1.000\n   .s       (t_11)    2.019    0.255    7.932    0.000    0.088    0.088\n   .q                 0.000                               0.000    0.000\n   .voc_0      (e)    8.987    0.449   20.000    0.000    8.987    0.011\n   .voc_1      (e)    8.987    0.449   20.000    0.000    8.987    0.011\n   .voc_2      (e)    8.987    0.449   20.000    0.000    8.987    0.010\n   .voc_3      (e)    8.987    0.449   20.000    0.000    8.987    0.009\n   .voc_4      (e)    8.987    0.449   20.000    0.000    8.987    0.007\n   .voc_5      (e)    8.987    0.449   20.000    0.000    8.987    0.006\n\n\nNotice that we went straight to the final model. We could have done a sequence of steps identical to the hierarchical linear model.\nI will show the sequence of models and compare them."
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-0-random-intercepts",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-0-random-intercepts",
    "title": "Latent Growth Models",
    "section": "Model 0: Random Intercepts",
    "text": "Model 0: Random Intercepts\n\n\nCode\npd0 &lt;- ggdiagram(font_family = my_font, font_size = 14) +\n  {v &lt;- my_observed() %&gt;%\n      ob_array(\n        6,\n        where = \"south\",\n        label = my_observed_label(paste0(\"*V*~\", 0:5, \"~\")),\n        sep = 1.5,\n        fill = class_color(cD)@lighten(seq(0.7, 1, length.out = 6))\n      )} +\n  {e &lt;- my_error(fill = v@fill, label = my_error_label(paste0(\"*e*~\", 0:5, \"~\"))) %&gt;%\n      place(v, \"right\")} +\n  ob_variance(\n    e,\n    \"right\",\n    label = ob_label(\n      \"*&tau;*~1~\",\n      color = cD,\n      label.padding = margin(t = 1)\n    ),\n    color = cD,\n    looseness = 3.15,\n    theta = degree(60)\n  ) +\n  my_connect(e, v, color = v@fill) +\n  {i &lt;- my_latent(\n      label = my_latent_label(\n        \"*i*&lt;br&gt;&lt;span style='font-size: 18pt'&gt;Intercept&lt;/span&gt;&lt;br&gt;*b*~0*i*~\",\n        lineheight = 1,\n        size = 20\n      )\n    ) %&gt;% place(midpoint(v[1], v[2]), \"left\", left)} +\n  {li &lt;- my_connect(i, v@point_at(180 + seq(0, -10, -2)))} +\n  ob_label(\n    1,\n    center = map_ob(li, \\(x) intersection(x, ls[1]@nudge(y = 1.1))),\n    label.padding = margin(1, 0, 0, 0),\n    color = cA\n  ) +\n  {u_0i &lt;- my_error(radius = 1, label = my_error_label(\"*u*~0*i*~\")) %&gt;% place(i, \"left\", left)} +\n  my_connect(u_0i, i) +\n  {t1 &lt;- ob_intercept(\n      intersection(connect(u_0i, s), connect(u_1i, i)) + ob_point(-1.85, 0),\n      width = 3,\n      fill = cD,\n      color = NA,\n      vertex_radius = .004,\n      label = my_observed_label(\"1\", vjust = .4)\n    )} +\n  {b00 &lt;- my_connect(t1, i)} +\n  {vl &lt;- ob_segment(my_connect(u_0i, i)@midpoint(.87),\n                     my_connect(u_2i, q)@midpoint(.87), alpha = 0)} +\n     ob_label(\n      \"*b*~00~\",\n      intersection(b00, vl),\n      label.padding = margin(2),\n      color = cA\n    ) +\n  ob_variance(\n    u_0i,\n    where = \"west\",\n    color = cA,\n    looseness = 1.75,\n    label = ob_label(paste0(\"*&tau;*~00~\"), color = cA))\npd0\n\n\n\n\n\n\n\n\nFigure 12: Path Diagram for Model 0\n\n\n\n\n\nAs usual, we start with a simple model with random intercepts. Time is not yet included in the model. The latent variable i has a mean of b_{00} and a variance of \\tau_{00}. The variances of error terms for each observed variable are constrained to be equal (\\tau_1). The intercepts of the observed variables are not modeled here, so they are assumbed to be 0.\n\nm_0 &lt;- '\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 \n# Variances and covariances\ni ~~ tau_00 * i \nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n'\n\nfit_0 &lt;- growth(m_0, data = d_wide)\n# The standard way to get output from lavaan\nsummary(fit_0, fit.measures = T, standardized = T)\n\nlavaan 0.6-19 ended normally after 18 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         8\n  Number of equality constraints                     5\n\n  Number of observations                           200\n\nModel Test User Model:\n                                                      \n  Test statistic                              3228.267\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                              3659.200\n  Degrees of freedom                                15\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.121\n  Tucker-Lewis Index (TLI)                       0.450\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5642.273\n  Loglikelihood unrestricted model (H1)      -4028.140\n                                                      \n  Akaike (AIC)                               11290.547\n  Bayesian (BIC)                             11300.442\n  Sample-size adjusted Bayesian (SABIC)      11290.938\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.817\n  90 Percent confidence interval - lower         0.793\n  90 Percent confidence interval - upper         0.841\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.357\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    voc_0             1.000                              29.736    0.809\n    voc_1             1.000                              29.736    0.809\n    voc_2             1.000                              29.736    0.809\n    voc_3             1.000                              29.736    0.809\n    voc_4             1.000                              29.736    0.809\n    voc_5             1.000                              29.736    0.809\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    i       (b_00)   80.811    2.193   36.844    0.000    2.718    2.718\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    i       (t_00)  884.245   96.275    9.185    0.000    1.000    1.000\n   .voc_0      (e)  467.273   20.897   22.361    0.000  467.273    0.346\n   .voc_1      (e)  467.273   20.897   22.361    0.000  467.273    0.346\n   .voc_2      (e)  467.273   20.897   22.361    0.000  467.273    0.346\n   .voc_3      (e)  467.273   20.897   22.361    0.000  467.273    0.346\n   .voc_4      (e)  467.273   20.897   22.361    0.000  467.273    0.346\n   .voc_5      (e)  467.273   20.897   22.361    0.000  467.273    0.346\n\n# The easystats way\nparameters(fit_0, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |       95% CI |      p\n-------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\n\n# Mean\n\nLink         | Coefficient |   SE |         95% CI |     z |      p\n-------------------------------------------------------------------\ni ~1  (b_00) |       80.81 | 2.19 | [76.51, 85.11] | 36.84 | &lt; .001\nvoc_0 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_1 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_2 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_3 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_4 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_5 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\n\n# Variance\n\nLink               | Coefficient |    SE |            95% CI |     z |      p\n-----------------------------------------------------------------------------\ni ~~ i (tau_00)    |      884.25 | 96.28 | [695.55, 1072.94] |  9.18 | &lt; .001\nvoc_0 ~~ voc_0 (e) |      467.27 | 20.90 | [426.32,  508.23] | 22.36 | &lt; .001\nvoc_1 ~~ voc_1 (e) |      467.27 | 20.90 | [426.32,  508.23] | 22.36 | &lt; .001\nvoc_2 ~~ voc_2 (e) |      467.27 | 20.90 | [426.32,  508.23] | 22.36 | &lt; .001\nvoc_3 ~~ voc_3 (e) |      467.27 | 20.90 | [426.32,  508.23] | 22.36 | &lt; .001\nvoc_4 ~~ voc_4 (e) |      467.27 | 20.90 | [426.32,  508.23] | 22.36 | &lt; .001\nvoc_5 ~~ voc_5 (e) |      467.27 | 20.90 | [426.32,  508.23] | 22.36 | &lt; .001\n\n# There are many fit indices. https://easystats.github.io/performance/reference/model_performance.lavaan.html\n# Here are some common indices:\nmy_metrics &lt;- c(\"CFI\", \"NNFI\", \"AGFI\", \"RMSEA\", \"AIC\", \"BIC\")\nperformance(fit_0, metrics = my_metrics)\n\n# Indices of model performance\n\nCFI   |  NNFI |  AGFI | RMSEA |       AIC |       BIC\n-----------------------------------------------------\n0.121 | 0.450 | 0.426 | 0.817 | 11290.547 | 11300.442\n\n\nNot surprisingly, the fit statistics of our null model indicate poor fit. The CFI (Comparative Fit Index), NNFI (Non-Normed Fit Index), and AGFI (Adjusted Goodness of Fit) approach 1 when the fit is good. There are no absolute standards for these fit indices, and good values can vary from model to model, but in general look for values better than .90. The RMSEA (Root Mean Squared Error of Approximation) approaches 0 when fit is good. Again, there is no absolute standard for the RMSEA but in general look for values below .10 and be pleased when values are below .05.\nThe reason that the data fit poorly can be seen clearly in Figure 13. Although vocabulary clearly increases over time, Model 0 constrains the prediction to not grow—to be the same for each person across time.\n\n\nCode\nmy_predictions &lt;- function(fit) {\n  lavPredict(fit, type = \"ov\") %&gt;%\n    as_tibble() %&gt;%\n    mutate(across(everything(), .fns = as.numeric)) %&gt;%\n    mutate(person_id = dplyr::row_number()) %&gt;%\n    pivot_longer(\n      starts_with(\"voc_\"),\n      values_to = \"vocabulary\",\n      names_to = \"time\",\n      names_prefix = \"voc_\",\n      names_transform = list(time = as.integer)\n    ) %&gt;%\n    mutate(person_id = factor(person_id) %&gt;% fct_reorder(vocabulary))\n}\n\nggplot(d, aes(time, vocabulary)) +\n  ggbeeswarm::geom_quasirandom(width = .2, size = .5, alpha = .5) +\n  geom_line(\n    data = predict(fit_0) %&gt;%\n      as_tibble() %&gt;%\n      mutate(across(everything(), .fns = as.numeric)) %&gt;%\n      mutate(person_id = dplyr::row_number()) %&gt;%\n      crossing(time = 0:5) %&gt;%\n      mutate(vocabulary = i ),\n    aes(group = person_id),\n    alpha = .25,\n  ) +\n  stat_function(\n    data = tibble(x = 0:5),\n    geom = \"line\",\n    fun = \\(x) coef(fit_0)[\"b_00\"] ,\n    inherit.aes = F,\n    color = \"dodgerblue3\",\n    linewidth = 2\n  ) +\n  theme_minimal(base_size = 20, base_family = my_font) +\n  labs(x = \"Time\", y = \"Vocabulary\") \n\n\n\n\n\n\n\n\nFigure 13: Model 0: Random Intercepts)\n\n\n\n\n\nAs seen in Table 7, the various parameters of the analogous hierarchical linear model are nearly identical—if maximum likelihood is used instead of restricted maximum likelihood (i.e., REML = FALSE)\n\nlmer(vocabulary ~ 1  + (1 | person_id), data = d, REML = FALSE) %&gt;%\n  tab_model()\n\n\n\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n80.81\n76.51 – 85.11\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n467.27\n\n\n\nτ00 person_id\n884.25\n\n\nICC\n0.65\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.000 / 0.654\n\n\n\n\n\n\nTable 7: HLM Model Analogous to Model 0"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-1-model-0-fixed-linear-effect-of-time",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-1-model-0-fixed-linear-effect-of-time",
    "title": "Latent Growth Models",
    "section": "Model 1: Model 0 + Fixed Linear Effect of Time",
    "text": "Model 1: Model 0 + Fixed Linear Effect of Time\n\n\nCode\npd1 &lt;- pd0 +\n  {s &lt;- my_latent(\n      label = my_latent_label(\n        \"*s*&lt;br&gt;&lt;span style='font-size: 18pt'&gt;Slope&lt;/span&gt;&lt;br&gt;*b*~1*i*~\",\n        lineheight = 1,\n        size = 20\n      ),\n      fill = cB\n    ) %&gt;%\n      place(midpoint(v[3], v[4]), \"left\", left)} +\n  {ls &lt;- my_connect(s, v@point_at(\"west\"), color = cB)} +\n  {b10 &lt;- my_connect(t1, s, color = cB)} +\n       ob_label(\n      \"*b*~10~\",\n      intersection(b10, vl),\n      label.padding = margin(2),\n      color = cB\n    ) +\n  ob_label(\n    0:5,\n    center = map_ob(ls, \\(x) intersection(\n      x, ob_circle(s@center, radius = s@radius + .65)\n    )),\n    label.padding = margin(1, 0, 0, 0),\n    color = cB\n  ) \npd1\n\n\n\n\n\n\n\n\nFigure 14: Path Diagram for Model 1\n\n\n\n\n\nWe add a fixed slope for time (latent variable s), setting the residual variance of s and its covariance with the intercept i to 0.\n\nm_1 &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 \ns ~ b_10 * 1\n# Variances and covariances\ni ~~ tau_00 * i \ns ~~ 0 * s + 0 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n\"\n\nfit_1 &lt;- growth(m_1, data = d_wide)\nparameters(fit_1, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |       95% CI |      p\n-------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ns =~ voc_0 |        0.00 | 0.00 | [0.00, 0.00] | &lt; .001\ns =~ voc_1 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ns =~ voc_2 |        2.00 | 0.00 | [2.00, 2.00] | &lt; .001\ns =~ voc_3 |        3.00 | 0.00 | [3.00, 3.00] | &lt; .001\ns =~ voc_4 |        4.00 | 0.00 | [4.00, 4.00] | &lt; .001\ns =~ voc_5 |        5.00 | 0.00 | [5.00, 5.00] | &lt; .001\n\n# Mean\n\nLink         | Coefficient |   SE |         95% CI |     z |      p\n-------------------------------------------------------------------\ni ~1  (b_00) |       55.33 | 2.24 | [50.95, 59.71] | 24.75 | &lt; .001\ns ~1  (b_10) |       10.19 | 0.17 | [ 9.86, 10.53] | 59.22 | &lt; .001\nvoc_0 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_1 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_2 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_3 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_4 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_5 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\n\n# Variance\n\nLink               | Coefficient |    SE |            95% CI |     z |      p\n-----------------------------------------------------------------------------\ni ~~ i (tau_00)    |      944.85 | 96.22 | [756.27, 1133.42] |  9.82 | &lt; .001\ns ~~ s             |        0.00 |  0.00 | [  0.00,    0.00] |       | &lt; .001\nvoc_0 ~~ voc_0 (e) |      103.67 |  4.64 | [ 94.59,  112.76] | 22.36 | &lt; .001\nvoc_1 ~~ voc_1 (e) |      103.67 |  4.64 | [ 94.59,  112.76] | 22.36 | &lt; .001\nvoc_2 ~~ voc_2 (e) |      103.67 |  4.64 | [ 94.59,  112.76] | 22.36 | &lt; .001\nvoc_3 ~~ voc_3 (e) |      103.67 |  4.64 | [ 94.59,  112.76] | 22.36 | &lt; .001\nvoc_4 ~~ voc_4 (e) |      103.67 |  4.64 | [ 94.59,  112.76] | 22.36 | &lt; .001\nvoc_5 ~~ voc_5 (e) |      103.67 |  4.64 | [ 94.59,  112.76] | 22.36 | &lt; .001\n\n# Correlation\n\nLink   | Coefficient |   SE |       95% CI |      p\n---------------------------------------------------\ni ~~ s |        0.00 | 0.00 | [0.00, 0.00] | &lt; .001\n\n\nLet’s see if adding the linear effect of time improved model fit:\n\n# Compare new model with the previous model\ntest_likelihoodratio(fit_0, fit_1)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nModel |   Type | df | df_diff |    Chi2 |      p\n------------------------------------------------\nfit_0 | lavaan | 24 |       1 | 3228.27 | &lt; .001\nfit_1 | lavaan | 23 |         | 1722.61 |       \n\ncompare_performance(fit_0, fit_1, metrics = my_metrics)\n\n# Comparison of Model Performance Indices\n\nName  |  Model |   CFI |  NNFI |  AGFI | RMSEA\n----------------------------------------------\nfit_0 | lavaan | 0.121 | 0.450 | 0.426 | 0.817\nfit_1 | lavaan | 0.534 | 0.696 | 0.726 | 0.608\n\n\nBecause the p-value is significant, adding the linear effect of time improved the model fit.\n\n\nCode\nggplot(d, aes(time, vocabulary)) +\n  ggbeeswarm::geom_quasirandom(width = .2, size = .5, alpha = .5) +\n  geom_line(\n    data = predict(fit_1) %&gt;%\n      as_tibble() %&gt;%\n      mutate(across(everything(), .fns = as.numeric)) %&gt;%\n      mutate(person_id = dplyr::row_number()) %&gt;%\n      crossing(time = 0:5) %&gt;%\n      mutate(vocabulary = i + s * time),\n    aes(group = person_id),\n    alpha = .25,\n  ) +\n  stat_function(\n    data = tibble(x = 0:5),\n    geom = \"line\",\n    fun = \\(x) coef(fit_1)[\"b_00\"] + coef(fit_1)[\"b_10\"] * x ,\n    inherit.aes = F,\n    color = \"dodgerblue3\",\n    linewidth = 2\n  ) +\n  theme_minimal(base_size = 20, base_family = my_font) +\n  labs(x = \"Time\", y = \"Vocabulary\") \n\n\n\n\n\n\n\n\nFigure 15: Model 1: Predicted Vocabulary Over Time (Model 0 + Linear Effect of Time)\n\n\n\n\n\nAs seen in Table 8, the various parameters of the analogous hierarchical linear model are nearly identical.\n\nlmer(vocabulary ~ 1 + time + (1 | person_id), \n               data = d, \n               REML = FALSE) %&gt;% \n  tab_model()\n\n\n\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n55.33\n50.94 – 59.72\n&lt;0.001\n\n\ntime\n10.19\n9.85 – 10.53\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n103.67\n\n\n\nτ00 person_id\n944.84\n\n\nICC\n0.90\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.224 / 0.923\n\n\n\n\n\n\nTable 8: HLM Model Analogous to Model 1"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-2-model-1-quadratic-effect-of-time",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-2-model-1-quadratic-effect-of-time",
    "title": "Latent Growth Models",
    "section": "Model 2: Model 1 + Quadratic Effect of Time",
    "text": "Model 2: Model 1 + Quadratic Effect of Time\n\n\nCode\npd2 &lt;- pd1 +\n  {q &lt;- my_latent(\n      label = my_latent_label(\n        \"*q*&lt;br&gt;&lt;span style='font-size: 18pt'&gt;Slope^2^&lt;/span&gt;&lt;br&gt;*b*~2*i*~\",\n        lineheight = 1,\n        size = 20\n      ),\n      fill = cC\n    ) %&gt;%\n      place(midpoint(v[5], v[6]), \"left\", left)} +\n  {lq &lt;- my_connect(q, v@point_at(180 + seq(10, 0, -2)), color = cC)} +\n  ob_label((0:5)^2,\n           center = map_ob(lq, \\(x) intersection(x, ls[6]@nudge(y = -1.1))),\n           label.padding = margin(1, 0, 0, 0),\n           color = cC\n  ) +\n  {b20 &lt;- my_connect(t1, q, color = cC)} +\n     ob_label(\n      \"*b*~20~\",\n      intersection(b20, vl),\n      label.padding = margin(2),\n      color = cC\n    ) \npd2\n\n\n\n\n\n\n\n\nFigure 16: Path Diagram for Model 2\n\n\n\n\n\nWe set a fixed quadratic effect for time, with the residual variance set to 0.\n\nm_2 &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Acceleration effect of time (quadratic effect)\nq =~ 0 * voc_0 + 1 * voc_1 + 4 * voc_2 + 9 * voc_3 + 16 * voc_4 + 25 * voc_5\n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 \ns ~ b_10 * 1\nq ~ b_20 * 1\n# Variances and covariances\ni ~~ tau_00 * i \ns ~~ 0 * s + 0 * i\nq ~~ 0 * q + 0 * s + 0 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n\"\n\nfit_2 &lt;- growth(m_2, data = d_wide)\nparameters(fit_2, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |         95% CI |      p\n---------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\ns =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_2 |        2.00 | 0.00 | [ 2.00,  2.00] | &lt; .001\ns =~ voc_3 |        3.00 | 0.00 | [ 3.00,  3.00] | &lt; .001\ns =~ voc_4 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\ns =~ voc_5 |        5.00 | 0.00 | [ 5.00,  5.00] | &lt; .001\nq =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\nq =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\nq =~ voc_2 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\nq =~ voc_3 |        9.00 | 0.00 | [ 9.00,  9.00] | &lt; .001\nq =~ voc_4 |       16.00 | 0.00 | [16.00, 16.00] | &lt; .001\nq =~ voc_5 |       25.00 | 0.00 | [25.00, 25.00] | &lt; .001\n\n# Mean\n\nLink         | Coefficient |   SE |         95% CI |     z |      p\n-------------------------------------------------------------------\ni ~1  (b_00) |       51.96 | 2.26 | [47.52, 56.39] | 22.95 | &lt; .001\ns ~1  (b_10) |       15.25 | 0.59 | [14.09, 16.41] | 25.82 | &lt; .001\nq ~1  (b_20) |       -1.01 | 0.11 | [-1.23, -0.79] | -8.92 | &lt; .001\nvoc_0 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_1 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_2 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_3 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_4 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\nvoc_5 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |       | &lt; .001\n\n# Variance\n\nLink               | Coefficient |    SE |            95% CI |     z |      p\n-----------------------------------------------------------------------------\ni ~~ i (tau_00)    |      946.11 | 96.21 | [757.54, 1134.69] |  9.83 | &lt; .001\ns ~~ s             |        0.00 |  0.00 | [  0.00,    0.00] |       | &lt; .001\nq ~~ q             |        0.00 |  0.00 | [  0.00,    0.00] |       | &lt; .001\nvoc_0 ~~ voc_0 (e) |       96.03 |  4.29 | [ 87.61,  104.45] | 22.36 | &lt; .001\nvoc_1 ~~ voc_1 (e) |       96.03 |  4.29 | [ 87.61,  104.45] | 22.36 | &lt; .001\nvoc_2 ~~ voc_2 (e) |       96.03 |  4.29 | [ 87.61,  104.45] | 22.36 | &lt; .001\nvoc_3 ~~ voc_3 (e) |       96.03 |  4.29 | [ 87.61,  104.45] | 22.36 | &lt; .001\nvoc_4 ~~ voc_4 (e) |       96.03 |  4.29 | [ 87.61,  104.45] | 22.36 | &lt; .001\nvoc_5 ~~ voc_5 (e) |       96.03 |  4.29 | [ 87.61,  104.45] | 22.36 | &lt; .001\n\n# Correlation\n\nLink   | Coefficient |   SE |       95% CI |      p\n---------------------------------------------------\ni ~~ s |        0.00 | 0.00 | [0.00, 0.00] | &lt; .001\ns ~~ q |        0.00 | 0.00 | [0.00, 0.00] | &lt; .001\ni ~~ q |        0.00 | 0.00 | [0.00, 0.00] | &lt; .001\n\n# Compare new model with the previous models\ntest_likelihoodratio(fit_0, fit_1, fit_2)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nModel |   Type | df | df_diff |    Chi2 |      p\n------------------------------------------------\nfit_0 | lavaan | 24 |       1 | 3228.27 | &lt; .001\nfit_1 | lavaan | 23 |       1 | 1722.61 | &lt; .001\nfit_2 | lavaan | 22 |         | 1646.01 |       \n\ncompare_performance(fit_0, fit_1, fit_2, \n                    metrics = my_metrics,\n                    verbose = F)\n\n# Comparison of Model Performance Indices\n\nName  |  Model |   CFI |  NNFI |  AGFI | RMSEA\n----------------------------------------------\nfit_0 | lavaan | 0.121 | 0.450 | 0.426 | 0.817\nfit_1 | lavaan | 0.534 | 0.696 | 0.726 | 0.608\nfit_2 | lavaan | 0.554 | 0.696 | 0.709 | 0.608\n\n\nThe quadratic effect improves fit.\n\n\nCode\nggplot(d, aes(time, vocabulary)) +\n  ggbeeswarm::geom_quasirandom(width = .2, size = .5, alpha = .5) +\n  geom_line(\n    data = predict(fit_2) %&gt;%\n      as_tibble() %&gt;%\n      mutate(across(everything(), .fns = as.numeric)) %&gt;%\n      mutate(person_id = dplyr::row_number()) %&gt;%\n      crossing(time = 0:5) %&gt;%\n      mutate(vocabulary = i + s * time + q * time^2),\n    aes(group = person_id),\n    alpha = .25,\n  ) +\n  stat_function(\n    data = tibble(x = 0:5),\n    geom = \"line\",\n    fun = \\(x) coef(fit_2)[\"b_00\"] + coef(fit_2)[\"b_10\"] * x + coef(fit_2)[\"b_20\"] * (x^2),\n    inherit.aes = F,\n    color = \"dodgerblue3\",\n    linewidth = 2\n  ) +\n  theme_minimal(base_size = 20, base_family = my_font) +\n  labs(x = \"Time\", y = \"Vocabulary\") \n\n\n\n\n\n\n\n\nFigure 17: Model 2: Predicted Vocabulary by Intervention Group Over Time (Model 1 + Quadratic Effect of Time)\n\n\n\n\n\nAgain, Table 9 shows that the same results would have been found with the analogous HLM model—and with much less typing! Why are we torturing ourselves like this? There will a payoff to latent growth modeling, just not yet.\n\nlmer(vocabulary ~ 1 + time + I(time^2) + (1 | person_id),\n     data = d,\n     REML = FALSE) %&gt;%\n  tab_model()\n\n\n\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n51.96\n47.52 – 56.40\n&lt;0.001\n\n\ntime\n15.25\n14.09 – 16.41\n&lt;0.001\n\n\ntime^2\n-1.01\n-1.23 – -0.79\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n96.03\n\n\n\nτ00 person_id\n946.12\n\n\nICC\n0.91\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.229 / 0.929\n\n\n\n\n\n\nTable 9: HLM Model Analogous to Model 2"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-3-model-2-random-linear-time-effect",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-3-model-2-random-linear-time-effect",
    "title": "Latent Growth Models",
    "section": "Model 3: Model 2 + Random Linear Time Effect",
    "text": "Model 3: Model 2 + Random Linear Time Effect\n\n\nCode\npd3 &lt;- pd2 + \n  {u_1i &lt;- my_error(radius = 1,\n                     label = my_error_label(\"*u*~1*i*~\"),\n                     fill = cB) %&gt;% place(s, \"left\", left)} +\n    my_connect(u_1i, s, color = cB) +\n  ob_variance(\n    u_1i,\n    where = \"west\",\n    color = cB,\n    looseness = 1.75,\n    label = ob_label(paste0(\"*&tau;*~11~\"), color = cB))\npd3\n\n\n\n\n\n\n\n\nFigure 18: Path Diagram for Model 3\n\n\n\n\n\n\nm_3 &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Acceleration effect of time (quadratic effect)\nq =~ 0 * voc_0 + 1 * voc_1 + 4 * voc_2 + 9 * voc_3 + 16 * voc_4 + 25 * voc_5\n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 \ns ~ b_10 * 1\nq ~ b_20 * 1\n# Variances and covariances\ni ~~ tau_00 * i \ns ~~ tau_11 * s + 0 * i\nq ~~ 0 * q + 0 * s + 0 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n\"\n\nfit_3 &lt;- growth(m_3, data = d_wide)\nparameters(fit_3, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |         95% CI |      p\n---------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\ns =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_2 |        2.00 | 0.00 | [ 2.00,  2.00] | &lt; .001\ns =~ voc_3 |        3.00 | 0.00 | [ 3.00,  3.00] | &lt; .001\ns =~ voc_4 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\ns =~ voc_5 |        5.00 | 0.00 | [ 5.00,  5.00] | &lt; .001\nq =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\nq =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\nq =~ voc_2 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\nq =~ voc_3 |        9.00 | 0.00 | [ 9.00,  9.00] | &lt; .001\nq =~ voc_4 |       16.00 | 0.00 | [16.00, 16.00] | &lt; .001\nq =~ voc_5 |       25.00 | 0.00 | [25.00, 25.00] | &lt; .001\n\n# Mean\n\nLink         | Coefficient |   SE |         95% CI |      z |      p\n--------------------------------------------------------------------\ni ~1  (b_00) |       51.96 | 1.99 | [48.05, 55.86] |  26.07 | &lt; .001\ns ~1  (b_10) |       15.25 | 0.40 | [14.47, 16.03] |  38.48 | &lt; .001\nq ~1  (b_20) |       -1.01 | 0.03 | [-1.08, -0.94] | -29.14 | &lt; .001\nvoc_0 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_1 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_2 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_3 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_4 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_5 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\n\n# Variance\n\nLink               | Coefficient |    SE |           95% CI |     z |      p\n----------------------------------------------------------------------------\ni ~~ i (tau_00)    |      787.02 | 79.17 | [631.86, 942.19] |  9.94 | &lt; .001\ns ~~ s (tau_11)    |       24.88 |  2.54 | [ 19.90,  29.85] |  9.80 | &lt; .001\nq ~~ q             |        0.00 |  0.00 | [  0.00,   0.00] |       | &lt; .001\nvoc_0 ~~ voc_0 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_1 ~~ voc_1 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_2 ~~ voc_2 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_3 ~~ voc_3 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_4 ~~ voc_4 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_5 ~~ voc_5 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\n\n# Correlation\n\nLink   | Coefficient |   SE |       95% CI |      p\n---------------------------------------------------\ni ~~ s |        0.00 | 0.00 | [0.00, 0.00] | &lt; .001\ns ~~ q |        0.00 | 0.00 | [0.00, 0.00] | &lt; .001\ni ~~ q |        0.00 | 0.00 | [0.00, 0.00] | &lt; .001\n\n# Compare new model with the previous models\ntest_likelihoodratio(fit_0, fit_1, fit_2, fit_3)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nModel |   Type | df | df_diff |    Chi2 |      p\n------------------------------------------------\nfit_0 | lavaan | 24 |       1 | 3228.27 | &lt; .001\nfit_1 | lavaan | 23 |       1 | 1722.61 | &lt; .001\nfit_2 | lavaan | 22 |       1 | 1646.01 | &lt; .001\nfit_3 | lavaan | 21 |         |   19.75 |       \n\ncompare_performance(fit_0, fit_1, fit_2, fit_3, metrics = my_metrics) \n\n# Comparison of Model Performance Indices\n\nName  |  Model |   CFI |  NNFI |  AGFI | RMSEA\n----------------------------------------------\nfit_0 | lavaan | 0.121 | 0.450 | 0.426 | 0.817\nfit_1 | lavaan | 0.534 | 0.696 | 0.726 | 0.608\nfit_2 | lavaan | 0.554 | 0.696 | 0.709 | 0.608\nfit_3 | lavaan | 1.000 | 1.000 | 0.992 | 0.000\n\n\nIncluding the random linear effect of time improves model fit.\nModel 3 plot\n\n\nCode\nggplot(d, aes(time, vocabulary)) +\n  ggbeeswarm::geom_quasirandom(width = .2, size = .5, alpha = .5) +\n  geom_line(\n    data = predict(fit_3) %&gt;%\n      as_tibble() %&gt;%\n      mutate(across(everything(), .fns = as.numeric)) %&gt;%\n      mutate(person_id = dplyr::row_number()) %&gt;%\n      crossing(time = 0:5) %&gt;%\n      mutate(vocabulary = i + s * time + q * time^2),\n    aes(group = person_id),\n    alpha = .25,\n  ) +\n  stat_function(\n    data = tibble(x = 0:5),\n    geom = \"line\",\n    fun = \\(x) coef(fit_3)[\"b_00\"] + coef(fit_3)[\"b_10\"] * x + coef(fit_3)[\"b_20\"] * (x^2),\n    inherit.aes = F,\n    color = \"dodgerblue3\",\n    linewidth = 2\n  ) +\n  theme_minimal(base_size = 20, base_family = my_font) +\n  labs(x = \"Time\", y = \"Vocabulary\") \n\n\n\n\n\n\n\n\nFigure 19: Model 3: Predicted Vocabulary by Intervention Group Over Time (Model 2 + Random Linear Effect of Time)\n\n\n\n\n\n\nlmer(vocabulary ~ 1 + time + I(time ^ 2) + (1 + time || person_id), \n     data = d, \n     REML = FALSE) %&gt;% \n  tab_model()\n\n\n\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n51.96\n48.05 – 55.87\n&lt;0.001\n\n\ntime\n15.25\n14.47 – 16.03\n&lt;0.001\n\n\ntime^2\n-1.01\n-1.08 – -0.94\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n9.00\n\n\n\nτ00 person_id\n787.02\n\n\nτ11 person_id.time\n24.88\n\n\nρ01\n \n\n\nρ01\n \n\n\nICC\n0.99\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.280 / 0.992\n\n\n\n\n\n\nTable 10: HLM Model Analogous to Model 3"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-4-model-3-covariance-between-random-intercepts-and-slopes",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-4-model-3-covariance-between-random-intercepts-and-slopes",
    "title": "Latent Growth Models",
    "section": "Model 4: Model 3 + Covariance between Random Intercepts and Slopes",
    "text": "Model 4: Model 3 + Covariance between Random Intercepts and Slopes\n\n\nCode\npd4 &lt;- pd3 +\n    ob_covariance(u_1i,\n                u_0i,\n                color = cD,\n                label = ob_label(\"*&tau;*~01~\", color = cD)) \npd4\n\n\n\n\n\n\n\n\nFigure 20: Path Diagram for Model 4\n\n\n\n\n\n\nm_4 &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Acceleration effect of time (quadratic effect)\nq =~ 0 * voc_0 + 1 * voc_1 + 4 * voc_2 + 9 * voc_3 + 16 * voc_4 + 25 * voc_5\n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 \ns ~ b_10 * 1\nq ~ b_20 * 1\n# Variances and covariances\ni ~~ tau_00 * i \ns ~~ tau_11 * s + tau_01 * i\nq ~~ 0 * q + 0 * s + 0 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n\"\n\nfit_4 &lt;- growth(m_4, data = d_wide)\nparameters(fit_4, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |         95% CI |      p\n---------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\ns =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_2 |        2.00 | 0.00 | [ 2.00,  2.00] | &lt; .001\ns =~ voc_3 |        3.00 | 0.00 | [ 3.00,  3.00] | &lt; .001\ns =~ voc_4 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\ns =~ voc_5 |        5.00 | 0.00 | [ 5.00,  5.00] | &lt; .001\nq =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\nq =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\nq =~ voc_2 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\nq =~ voc_3 |        9.00 | 0.00 | [ 9.00,  9.00] | &lt; .001\nq =~ voc_4 |       16.00 | 0.00 | [16.00, 16.00] | &lt; .001\nq =~ voc_5 |       25.00 | 0.00 | [25.00, 25.00] | &lt; .001\n\n# Mean\n\nLink         | Coefficient |   SE |         95% CI |      z |      p\n--------------------------------------------------------------------\ni ~1  (b_00) |       51.96 | 1.99 | [48.05, 55.86] |  26.08 | &lt; .001\ns ~1  (b_10) |       15.25 | 0.40 | [14.47, 16.03] |  38.49 | &lt; .001\nq ~1  (b_20) |       -1.01 | 0.03 | [-1.08, -0.94] | -29.14 | &lt; .001\nvoc_0 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_1 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_2 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_3 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_4 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_5 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\n\n# Variance\n\nLink               | Coefficient |    SE |           95% CI |     z |      p\n----------------------------------------------------------------------------\ni ~~ i (tau_00)    |      786.65 | 79.14 | [631.54, 941.75] |  9.94 | &lt; .001\ns ~~ s (tau_11)    |       24.86 |  2.54 | [ 19.89,  29.84] |  9.80 | &lt; .001\nq ~~ q             |        0.00 |  0.00 | [  0.00,   0.00] |       | &lt; .001\nvoc_0 ~~ voc_0 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_1 ~~ voc_1 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_2 ~~ voc_2 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_3 ~~ voc_3 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_4 ~~ voc_4 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\nvoc_5 ~~ voc_5 (e) |        9.00 |  0.45 | [  8.12,   9.89] | 20.00 | &lt; .001\n\n# Correlation\n\nLink            | Coefficient |    SE |          95% CI |    z |      p\n-----------------------------------------------------------------------\ni ~~ s (tau_01) |        3.71 | 10.02 | [-15.93, 23.36] | 0.37 | 0.711 \ns ~~ q          |        0.00 |  0.00 | [  0.00,  0.00] |      | &lt; .001\ni ~~ q          |        0.00 |  0.00 | [  0.00,  0.00] |      | &lt; .001\n\n# Compare new model with the previous models\ntest_likelihoodratio(fit_0, fit_1, fit_2, fit_3, fit_4)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nModel |   Type | df | df_diff |    Chi2 |      p\n------------------------------------------------\nfit_0 | lavaan | 24 |       1 | 3228.27 | &lt; .001\nfit_1 | lavaan | 23 |       1 | 1722.61 | &lt; .001\nfit_2 | lavaan | 22 |       1 | 1646.01 | &lt; .001\nfit_3 | lavaan | 21 |       1 |   19.75 |  0.711\nfit_4 | lavaan | 20 |         |   19.61 |       \n\ncompare_performance(fit_0, fit_1, fit_2, fit_3, fit_4, metrics = my_metrics)\n\n# Comparison of Model Performance Indices\n\nName  |  Model |   CFI |  NNFI |  AGFI | RMSEA\n----------------------------------------------\nfit_0 | lavaan | 0.121 | 0.450 | 0.426 | 0.817\nfit_1 | lavaan | 0.534 | 0.696 | 0.726 | 0.608\nfit_2 | lavaan | 0.554 | 0.696 | 0.709 | 0.608\nfit_3 | lavaan | 1.000 | 1.000 | 0.992 | 0.000\nfit_4 | lavaan | 1.000 | 1.000 | 0.992 | 0.000\n\n\nThe covariance is not significantly different from 0. I am going to leave it in for now because I want to test the random quadratic effect.\n\n\nCode\nggplot(d, aes(time, vocabulary)) +\n  ggbeeswarm::geom_quasirandom(width = .2, size = .5, alpha = .5) +\n  geom_line(\n    data = predict(fit_4) %&gt;%\n      as_tibble() %&gt;%\n      mutate(across(everything(), .fns = as.numeric)) %&gt;%\n      mutate(person_id = dplyr::row_number()) %&gt;%\n      crossing(time = 0:5) %&gt;%\n      mutate(vocabulary = i + s * time + q * time^2),\n    aes(group = person_id),\n    alpha = .25,\n  ) +\n  stat_function(\n    data = tibble(x = 0:5),\n    geom = \"line\",\n    fun = \\(x) coef(fit_4)[\"b_00\"] + coef(fit_4)[\"b_10\"] * x + coef(fit_4)[\"b_20\"] * (x^2),\n    inherit.aes = F,\n    color = \"dodgerblue3\",\n    linewidth = 2\n  ) +\n  theme_minimal(base_size = 20, base_family = my_font) +\n  labs(x = \"Time\", y = \"Vocabulary\") \n\n\n\n\n\n\n\n\nFigure 21: Model 4: Predicted Vocabulary Over Time (Model 3 + Correlated Slopes and Intercepts)\n\n\n\n\n\n\nlmer(vocabulary ~ 1 + time + I(time ^ 2) + (1 + time | person_id), \n     data = d, \n     REML = FALSE) %&gt;% \n  tab_model()\n\n\n\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n51.96\n48.05 – 55.87\n&lt;0.001\n\n\ntime\n15.25\n14.47 – 16.03\n&lt;0.001\n\n\ntime^2\n-1.01\n-1.08 – -0.94\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n9.00\n\n\n\nτ00 person_id\n786.64\n\n\nτ11 person_id.time\n24.86\n\n\nρ01 person_id\n0.03\n\n\nICC\n0.99\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.229 / 0.993\n\n\n\n\n\n\nTable 11: HLM Model Analogous to Model 4"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-5-model-4-random-quadratic-effect",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-5-model-4-random-quadratic-effect",
    "title": "Latent Growth Models",
    "section": "Model 5: Model 4 + Random Quadratic Effect",
    "text": "Model 5: Model 4 + Random Quadratic Effect\n\n\nCode\npd5 &lt;- pd4 +\n  {u_2i &lt;- my_error(radius = 1,\n                     label = my_error_label(\"*u*~2*i*~\"),\n                     fill = cC) %&gt;% place(q, \"left\", left)} +\n  my_connect(u_2i, q, color = cC) +\n      ob_variance(\n    u_2i,\n    where = \"west\",\n    color = cC,\n    looseness = 1.75,\n    label = ob_label(paste0(\"*&tau;*~22~\"), color = cC)\n  ) \npd5\n\n\n\n\n\n\n\n\nFigure 22: Path Diagram for Model 5\n\n\n\n\n\n\nm_5 &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Acceleration effect of time (quadratic effect)\nq =~ 0 * voc_0 + 1 * voc_1 + 4 * voc_2 + 9 * voc_3 + 16 * voc_4 + 25 * voc_5\n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 \ns ~ b_10 * 1\nq ~ b_20 * 1\n# Variances and covariances\ni ~~ tau_00 * i \ns ~~ tau_11 * s + tau_01 * i\nq ~~ tau_22 * q + 0 * s + 0 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n\"\n\nfit_5 &lt;- growth(m_5, data = d_wide)\nparameters(fit_5, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |         95% CI |      p\n---------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\ns =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_2 |        2.00 | 0.00 | [ 2.00,  2.00] | &lt; .001\ns =~ voc_3 |        3.00 | 0.00 | [ 3.00,  3.00] | &lt; .001\ns =~ voc_4 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\ns =~ voc_5 |        5.00 | 0.00 | [ 5.00,  5.00] | &lt; .001\nq =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\nq =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\nq =~ voc_2 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\nq =~ voc_3 |        9.00 | 0.00 | [ 9.00,  9.00] | &lt; .001\nq =~ voc_4 |       16.00 | 0.00 | [16.00, 16.00] | &lt; .001\nq =~ voc_5 |       25.00 | 0.00 | [25.00, 25.00] | &lt; .001\n\n# Mean\n\nLink         | Coefficient |   SE |         95% CI |      z |      p\n--------------------------------------------------------------------\ni ~1  (b_00) |       51.96 | 1.99 | [48.06, 55.86] |  26.10 | &lt; .001\ns ~1  (b_10) |       15.25 | 0.39 | [14.49, 16.01] |  39.50 | &lt; .001\nq ~1  (b_20) |       -1.01 | 0.04 | [-1.09, -0.94] | -26.85 | &lt; .001\nvoc_0 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_1 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_2 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_3 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_4 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_5 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\n\n# Variance\n\nLink               | Coefficient |    SE |           95% CI |     z |      p\n----------------------------------------------------------------------------\ni ~~ i (tau_00)    |      785.74 | 79.07 | [630.77, 940.71] |  9.94 | &lt; .001\ns ~~ s (tau_11)    |       23.68 |  2.56 | [ 18.66,  28.71] |  9.24 | &lt; .001\nq ~~ q (tau_22)    |        0.06 |  0.03 | [  0.01,   0.11] |  2.25 | 0.024 \nvoc_0 ~~ voc_0 (e) |        8.45 |  0.47 | [  7.53,   9.36] | 18.07 | &lt; .001\nvoc_1 ~~ voc_1 (e) |        8.45 |  0.47 | [  7.53,   9.36] | 18.07 | &lt; .001\nvoc_2 ~~ voc_2 (e) |        8.45 |  0.47 | [  7.53,   9.36] | 18.07 | &lt; .001\nvoc_3 ~~ voc_3 (e) |        8.45 |  0.47 | [  7.53,   9.36] | 18.07 | &lt; .001\nvoc_4 ~~ voc_4 (e) |        8.45 |  0.47 | [  7.53,   9.36] | 18.07 | &lt; .001\nvoc_5 ~~ voc_5 (e) |        8.45 |  0.47 | [  7.53,   9.36] | 18.07 | &lt; .001\n\n# Correlation\n\nLink            | Coefficient |    SE |          95% CI |    z |      p\n-----------------------------------------------------------------------\ni ~~ s (tau_01) |        4.94 | 10.01 | [-14.68, 24.56] | 0.49 | 0.622 \ns ~~ q          |        0.00 |  0.00 | [  0.00,  0.00] |      | &lt; .001\ni ~~ q          |        0.00 |  0.00 | [  0.00,  0.00] |      | &lt; .001\n\n# Compare new model with the previous models\ntest_likelihoodratio(fit_0, fit_1, fit_2, fit_3, fit_4, fit_5)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nModel |   Type | df | df_diff |    Chi2 |      p\n------------------------------------------------\nfit_0 | lavaan | 24 |       1 | 3228.27 | &lt; .001\nfit_1 | lavaan | 23 |       1 | 1722.61 | &lt; .001\nfit_2 | lavaan | 22 |       1 | 1646.01 | &lt; .001\nfit_3 | lavaan | 21 |       1 |   19.75 |  0.711\nfit_4 | lavaan | 20 |       1 |   19.61 |  0.012\nfit_5 | lavaan | 19 |         |   13.23 |       \n\ncompare_performance(fit_0, fit_1, fit_2, fit_3, fit_4, fit_5, metrics = c())\n\n# Comparison of Model Performance Indices\n\nName  |  Model |     Chi2 | Chi2_df | p (Chi2) | Baseline(15) | p (Baseline)\n----------------------------------------------------------------------------\nfit_0 | lavaan | 3228.267 |  24.000 |   &lt; .001 |     3659.200 |       &lt; .001\nfit_1 | lavaan | 1722.613 |  23.000 |   &lt; .001 |     3659.200 |       &lt; .001\nfit_2 | lavaan | 1646.015 |  22.000 |   &lt; .001 |     3659.200 |       &lt; .001\nfit_3 | lavaan |   19.752 |  21.000 |   0.537  |     3659.200 |       &lt; .001\nfit_4 | lavaan |   19.615 |  20.000 |   0.482  |     3659.200 |       &lt; .001\nfit_5 | lavaan |   13.233 |  19.000 |   0.826  |     3659.200 |       &lt; .001\n\nName  |   GFI |  AGFI | NFI |  NNFI |   CFI | RMSEA |      RMSEA  CI\n--------------------------------------------------------------------\nfit_0 | 0.489 | 0.426 |     | 0.450 | 0.121 | 0.817 | [0.793, 0.841]\nfit_1 | 0.767 | 0.726 |     | 0.696 | 0.534 | 0.608 | [0.584, 0.632]\nfit_2 | 0.763 | 0.709 |     | 0.696 | 0.554 | 0.608 | [0.583, 0.633]\nfit_3 | 0.994 | 0.992 |     | 1.000 | 1.000 | 0.000 | [0.000, 0.056]\nfit_4 | 0.994 | 0.992 |     | 1.000 | 1.000 | 0.000 | [0.000, 0.060]\nfit_5 | 0.996 | 0.994 |     | 1.001 | 1.000 | 0.000 | [0.000, 0.038]\n\nName  | p (RMSEA) |     RMR |  SRMR | RFI |  PNFI |   IFI |   RNI\n-----------------------------------------------------------------\nfit_0 |    &lt; .001 | 222.006 | 0.357 |     | 0.188 | 0.119 | 0.121\nfit_1 |    &lt; .001 | 160.723 | 0.154 |     | 0.811 | 0.533 | 0.534\nfit_2 |    &lt; .001 | 160.688 | 0.148 |     | 0.807 | 0.553 | 0.554\nfit_3 |    0.916  |  19.138 | 0.017 |     | 1.392 | 1.000 | 1.000\nfit_4 |    0.889  |   6.180 | 0.006 |     | 1.326 | 1.000 | 1.000\nfit_5 |    0.982  |   7.453 | 0.007 |     | 1.262 | 1.002 | 1.002\n\nName  | Loglikelihood |   AIC (weights) |   BIC (weights) | BIC_adjusted\n------------------------------------------------------------------------\nfit_0 |     -5642.273 | 11290.5 (&lt;.001) | 11300.4 (&lt;.001) |    11290.938\nfit_1 |     -4889.446 |  9786.9 (&lt;.001) |  9800.1 (&lt;.001) |     9787.413\nfit_2 |     -4851.147 |  9712.3 (&lt;.001) |  9728.8 (&lt;.001) |     9712.945\nfit_3 |     -4038.016 |  8088.0 (0.203) |  8107.8 (0.829) |     8088.813\nfit_4 |     -4037.947 |  8089.9 (0.080) |  8113.0 (0.063) |     8090.806\nfit_5 |     -4034.756 |  8085.5 (0.717) |  8111.9 (0.108) |     8086.554\n\n\nLet’s keep the random quadratic effect.\n\n\nCode\nggplot(d, aes(time, vocabulary)) +\n  ggbeeswarm::geom_quasirandom(width = .2, size = .5, alpha = .5) +\n  geom_line(\n    data = predict(fit_5) %&gt;%\n      as_tibble() %&gt;%\n      mutate(across(everything(), .fns = as.numeric)) %&gt;%\n      mutate(person_id = dplyr::row_number()) %&gt;%\n      crossing(time = 0:5) %&gt;%\n      mutate(vocabulary = i + s * time + q * time^2),\n    aes(group = person_id),\n    alpha = .25,\n  ) +\n  stat_function(\n    data = tibble(x = 0:5),\n    geom = \"line\",\n    fun = \\(x) coef(fit_5)[\"b_00\"] + coef(fit_5)[\"b_10\"] * x + coef(fit_5)[\"b_20\"] * (x^2),\n    inherit.aes = F,\n    color = \"dodgerblue3\",\n    linewidth = 2\n  ) +\n  theme_minimal(base_size = 20, base_family = my_font) +\n  labs(x = \"Time\", y = \"Vocabulary\")\n\n\n\n\n\n\n\n\nFigure 23: Model 5: Predicted Vocabulary Over Time (Model 4 + Random Quadratic Effect of Time)\n\n\n\n\n\n\nlmer(vocabulary ~ 1 + time + I(time ^ 2) + \n       (1 + time | person_id) + \n       (0 + I(time ^ 2) | person_id), \n     data = d, \n     REML = FALSE) %&gt;% \n  tab_model()\n\n\n\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n51.96\n48.05 – 55.86\n&lt;0.001\n\n\ntime\n15.25\n14.49 – 16.01\n&lt;0.001\n\n\ntime^2\n-1.01\n-1.09 – -0.94\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n8.45\n\n\n\nτ00 person_id\n785.73\n\n\nτ11 person_id.time\n23.68\n\n\nτ11 person_id.I(time^2)\n0.06\n\n\nρ01 person_id\n0.04\n\n\nICC\n0.99\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.230 / 0.994\n\n\n\n\n\n\nTable 12: HLM Model Analogous to Model 5"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-6-model-5-correlations-with-random-quadratic-effects",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-6-model-5-correlations-with-random-quadratic-effects",
    "title": "Latent Growth Models",
    "section": "Model 6: Model 5 + Correlations with Random Quadratic Effects",
    "text": "Model 6: Model 5 + Correlations with Random Quadratic Effects\n\n\nCode\npd6 &lt;- pd5 +\n  ob_covariance(bind(c(u_2i, u_2i)),\n                bind(c(u_0i, u_1i)),\n                color = cD,\n                label = ob_label(paste0(\"*&tau;*~\", c(0, 1), c(2, 2), \"~\"), color = cD))\npd6\n\n\n\n\n\n\n\n\nFigure 24: Path Diagram for Model 6\n\n\n\n\n\nNow lets allow the covariance between the quadratic effect and the linear and intercept to be non-zero:\n\nm_6 &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Acceleration effect of time (quadratic effect)\nq =~ 0 * voc_0 + 1 * voc_1 + 4 * voc_2 + 9 * voc_3 + 16 * voc_4 + 25 * voc_5\n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 \ns ~ b_10 * 1\nq ~ b_20 * 1\n# Variances and covariances\ni ~~ tau_00 * i \ns ~~ tau_11 * s + tau_01 * i\nq ~~ tau_22 * q + tau_12 * s + tau_02 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n\"\n\nfit_6 &lt;- growth(m_6, data = d_wide)\nparameters(fit_6, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |         95% CI |      p\n---------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\ns =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_2 |        2.00 | 0.00 | [ 2.00,  2.00] | &lt; .001\ns =~ voc_3 |        3.00 | 0.00 | [ 3.00,  3.00] | &lt; .001\ns =~ voc_4 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\ns =~ voc_5 |        5.00 | 0.00 | [ 5.00,  5.00] | &lt; .001\nq =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\nq =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\nq =~ voc_2 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\nq =~ voc_3 |        9.00 | 0.00 | [ 9.00,  9.00] | &lt; .001\nq =~ voc_4 |       16.00 | 0.00 | [16.00, 16.00] | &lt; .001\nq =~ voc_5 |       25.00 | 0.00 | [25.00, 25.00] | &lt; .001\n\n# Mean\n\nLink         | Coefficient |   SE |         95% CI |      z |      p\n--------------------------------------------------------------------\ni ~1  (b_00) |       51.96 | 1.99 | [48.06, 55.85] |  26.13 | &lt; .001\ns ~1  (b_10) |       15.25 | 0.39 | [14.48, 16.02] |  38.78 | &lt; .001\nq ~1  (b_20) |       -1.01 | 0.04 | [-1.09, -0.94] | -26.39 | &lt; .001\nvoc_0 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_1 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_2 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_3 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_4 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_5 ~1     |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\n\n# Variance\n\nLink               | Coefficient |    SE |           95% CI |     z |      p\n----------------------------------------------------------------------------\ni ~~ i (tau_00)    |      783.75 | 79.06 | [628.79, 938.70] |  9.91 | &lt; .001\ns ~~ s (tau_11)    |       24.86 |  3.11 | [ 18.76,  30.96] |  7.99 | &lt; .001\nq ~~ q (tau_22)    |        0.07 |  0.03 | [  0.01,   0.13] |  2.19 | 0.029 \nvoc_0 ~~ voc_0 (e) |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_1 ~~ voc_1 (e) |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_2 ~~ voc_2 (e) |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_3 ~~ voc_3 (e) |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_4 ~~ voc_4 (e) |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_5 ~~ voc_5 (e) |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\n\n# Correlation\n\nLink            | Coefficient |    SE |          95% CI |     z |     p\n-----------------------------------------------------------------------\ni ~~ s (tau_01) |        6.07 | 11.06 | [-15.61, 27.75] |  0.55 | 0.583\ns ~~ q (tau_12) |       -0.17 |  0.24 | [ -0.64,  0.30] | -0.71 | 0.475\ni ~~ q (tau_02) |       -0.37 |  1.08 | [ -2.48,  1.75] | -0.34 | 0.732\n\n# Compare new model with the previous models\ntest_likelihoodratio(fit_0, fit_1, fit_2, fit_3, fit_4, fit_5, fit_6)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nModel |   Type | df | df_diff |    Chi2 |      p\n------------------------------------------------\nfit_0 | lavaan | 24 |       1 | 3228.27 | &lt; .001\nfit_1 | lavaan | 23 |       1 | 1722.61 | &lt; .001\nfit_2 | lavaan | 22 |       1 | 1646.01 | &lt; .001\nfit_3 | lavaan | 21 |       1 |   19.75 |  0.711\nfit_4 | lavaan | 20 |       1 |   19.61 |  0.012\nfit_5 | lavaan | 19 |       2 |   13.23 |  0.714\nfit_6 | lavaan | 17 |         |   12.56 |       \n\ncompare_performance(fit_0, fit_1, fit_2, fit_3, fit_4, fit_5, fit_6, metrics = my_metrics)\n\n# Comparison of Model Performance Indices\n\nName  |  Model |   CFI |  NNFI |  AGFI | RMSEA\n----------------------------------------------\nfit_0 | lavaan | 0.121 | 0.450 | 0.426 | 0.817\nfit_1 | lavaan | 0.534 | 0.696 | 0.726 | 0.608\nfit_2 | lavaan | 0.554 | 0.696 | 0.709 | 0.608\nfit_3 | lavaan | 1.000 | 1.000 | 0.992 | 0.000\nfit_4 | lavaan | 1.000 | 1.000 | 0.992 | 0.000\nfit_5 | lavaan | 1.000 | 1.001 | 0.994 | 0.000\nfit_6 | lavaan | 1.000 | 1.001 | 0.993 | 0.000\n\n\nThese effects are not needed. We have the option of trimming them now, but I am going to wait a little longer.\n\n\nCode\nggplot(d, aes(time, vocabulary)) +\n  ggbeeswarm::geom_quasirandom(width = .2, size = .5, alpha = .5) +\n  geom_line(\n    data = predict(fit_6) %&gt;%\n      as_tibble() %&gt;%\n      mutate(across(everything(), .fns = as.numeric)) %&gt;%\n      mutate(person_id = dplyr::row_number()) %&gt;%\n      crossing(time = 0:5) %&gt;%\n      mutate(vocabulary = i + s * time + q * time^2),\n    aes(group = person_id),\n    alpha = .25,\n  ) +\n  stat_function(\n    data = tibble(x = 0:5),\n    geom = \"line\",\n    fun = \\(x) coef(fit_6)[\"b_00\"] + coef(fit_6)[\"b_10\"] * x + coef(fit_6)[\"b_20\"] * (x^2),\n    inherit.aes = F,\n    color = \"dodgerblue3\",\n    linewidth = 2\n  ) +\n  theme_minimal(base_size = 20, base_family = my_font) +\n  labs(x = \"Time\", y = \"Vocabulary\")\n\n\n\n\n\n\n\n\nFigure 25: Model 6: Predicted Vocabulary Over Time (Model 5 + All Random Effects Correlated)\n\n\n\n\n\n\nlmer(vocabulary ~ 1 + time + I(time ^ 2) + \n       (1 + time + I(time ^ 2) | person_id), \n     data = d, \n     REML = FALSE) %&gt;% \n  tab_model()\n\n\n\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n51.96\n48.06 – 55.86\n&lt;0.001\n\n\ntime\n15.25\n14.48 – 16.02\n&lt;0.001\n\n\ntime^2\n-1.01\n-1.09 – -0.94\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n8.35\n\n\n\nτ00 person_id\n783.68\n\n\nτ11 person_id.time\n24.87\n\n\nτ11 person_id.I(time^2)\n0.07\n\n\nρ01\n0.04\n\n\n\n-0.05\n\n\nICC\n0.99\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.229 / 0.994\n\n\n\n\n\n\nTable 13: HLM Model Analogous to Model 6"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-7-model-6-treatment-effects",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-7-model-6-treatment-effects",
    "title": "Latent Growth Models",
    "section": "Model 7: Model 6 + Treatment Effects",
    "text": "Model 7: Model 6 + Treatment Effects\n\n\nCode\npd7 &lt;- pd6 +\n  {intervention &lt;- my_observed(\n      intersection(connect(u_2i, s), connect(u_1i, q)) + ob_point(-1.85, 0),\n      a = 1.75,\n      b = 1.75,\n      fill = cD,\n      label = my_observed_label(\"Intervention\", size = 15)\n    )}  +\n  {lt1 &lt;- my_connect(intervention, bind(c(i, s, q)), color = my_colors)} +\n  ob_label(\n    paste0(\"*b*~\", 0:2, \"1~\"),\n    intersection(lt1, vl),\n    label.padding = margin(2),\n    color = my_colors\n  ) \npd7\n\n\n\n\n\n\n\n\nFigure 26: Path Diagram for Model 7\n\n\n\n\n\nI am going to add all three treatment effects at the same time. We could do them one-at-a-time, but no important theoretical question would be answered by doing so. I am mostly interested in the interactions of treatment and the time variables.\n\nm_7 &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Acceleration effect of time (quadratic effect)\nq =~ 0 * voc_0 + 1 * voc_1 + 4 * voc_2 + 9 * voc_3 + 16 * voc_4 + 25 * voc_5\n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 + b_01 * intervention\ns ~ b_10 * 1 + b_11 * intervention\nq ~ b_20 * 1 + b_21 * intervention\n# Variances and covariances\ni ~~ tau_00 * i \ns ~~ tau_11 * s + tau_01 * i\nq ~~ tau_22 * q + tau_12 * s + tau_02 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n\"\n\nfit_7 &lt;- growth(m_7, data = d_wide)\nparameters(fit_7, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |         95% CI |      p\n---------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\ns =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_2 |        2.00 | 0.00 | [ 2.00,  2.00] | &lt; .001\ns =~ voc_3 |        3.00 | 0.00 | [ 3.00,  3.00] | &lt; .001\ns =~ voc_4 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\ns =~ voc_5 |        5.00 | 0.00 | [ 5.00,  5.00] | &lt; .001\nq =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\nq =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\nq =~ voc_2 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\nq =~ voc_3 |        9.00 | 0.00 | [ 9.00,  9.00] | &lt; .001\nq =~ voc_4 |       16.00 | 0.00 | [16.00, 16.00] | &lt; .001\nq =~ voc_5 |       25.00 | 0.00 | [25.00, 25.00] | &lt; .001\n\n# Mean\n\nLink            | Coefficient |   SE |         95% CI |      z |      p\n-----------------------------------------------------------------------\ni ~1  (b_00)    |       52.00 | 2.87 | [46.38, 57.63] |  18.12 | &lt; .001\ns ~1  (b_10)    |       10.50 | 0.32 | [ 9.86, 11.14] |  32.39 | &lt; .001\nq ~1  (b_20)    |       -1.06 | 0.06 | [-1.16, -0.95] | -19.15 | &lt; .001\nvoc_0 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_1 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_2 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_3 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_4 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_5 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nintervention ~1 |        0.52 | 0.00 | [ 0.52,  0.52] |        | &lt; .001\n\n# Regression\n\nLink                    | Coefficient |   SE |         95% CI |     z |      p\n------------------------------------------------------------------------------\ni ~ intervention (b_01) |       -0.09 | 3.98 | [-7.89,  7.71] | -0.02 | 0.983 \ns ~ intervention (b_11) |        9.14 | 0.45 | [ 8.26, 10.02] | 20.33 | &lt; .001\nq ~ intervention (b_21) |        0.09 | 0.08 | [-0.06,  0.24] |  1.12 | 0.262 \n\n# Variance\n\nLink                         | Coefficient |    SE |           95% CI |     z |      p\n--------------------------------------------------------------------------------------\ni ~~ i (tau_00)              |      783.74 | 79.06 | [628.79, 938.70] |  9.91 | &lt; .001\ns ~~ s (tau_11)              |        4.02 |  1.07 | [  1.93,   6.11] |  3.77 | &lt; .001\nq ~~ q (tau_22)              |        0.07 |  0.03 | [  0.01,   0.13] |  2.14 | 0.032 \nvoc_0 ~~ voc_0 (e)           |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_1 ~~ voc_1 (e)           |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_2 ~~ voc_2 (e)           |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_3 ~~ voc_3 (e)           |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_4 ~~ voc_4 (e)           |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nvoc_5 ~~ voc_5 (e)           |        8.35 |  0.48 | [  7.40,   9.29] | 17.32 | &lt; .001\nintervention ~~ intervention |        0.25 |  0.00 | [  0.25,   0.25] |       | &lt; .001\n\n# Correlation\n\nLink            | Coefficient |   SE |         95% CI |     z |     p\n---------------------------------------------------------------------\ni ~~ s (tau_01) |        6.26 | 6.32 | [-6.13, 18.65] |  0.99 | 0.322\ns ~~ q (tau_12) |       -0.37 | 0.17 | [-0.71, -0.03] | -2.12 | 0.034\ni ~~ q (tau_02) |       -0.37 | 1.08 | [-2.48,  1.74] | -0.34 | 0.733\n\n# Compare new model with the previous models\ntest_likelihoodratio(fit_6, fit_7)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nModel |   Type | df | df_diff |  Chi2 |     p\n---------------------------------------------\nfit_6 | lavaan | 17 |         | 12.56 |      \nfit_7 | lavaan | 20 |       3 | 14.44 | 0.598\n\ncompare_performance(fit_6, fit_7, metrics = my_metrics)\n\n# Comparison of Model Performance Indices\n\nName  |  Model |   CFI |  NNFI |  AGFI | RMSEA\n----------------------------------------------\nfit_6 | lavaan | 1.000 | 1.001 | 0.993 | 0.000\nfit_7 | lavaan | 1.000 | 1.001 | 0.999 | 0.000\n\n\nSo the intervention effects improve the model fit. Specifically, the intervention predicts the slope such that the slope is steeper for the treatment group.\n\n\nCode\npred_7 &lt;- lavPredictY(\n  fit_7,\n  ynames = d_wide %&gt;% select(starts_with(\"voc_\")) %&gt;%  colnames(),\n  xnames = \"intervention\",\n  newdata = tibble(\n    intervention = c(0, 1),\n    voc_0 = 0,\n    voc_1 = 0,\n    voc_2 = 0,\n    voc_3 = 0,\n    voc_4 = 0,\n    voc_5 = 0\n  )\n) %&gt;%\n  as_tibble() %&gt;%\n  mutate(intervention = factor(c(0, 1), \n                               labels = c(\"Control\", \"Treatment\"))) %&gt;%\n  pivot_longer(\n    starts_with(\"voc_\"),\n    names_to = \"time\",\n    names_prefix = \"voc_\",\n    names_transform = list(time = as.integer),\n    values_to = \"vocabulary\"\n  )\n\n\n\nggplot(\n  data = d %&gt;% \n    mutate(intervention = factor(\n      intervention, \n      labels = c(\"Control\", \"Treatment\"))), \n  aes(time, vocabulary)) +\n  ggbeeswarm::geom_quasirandom(\n    aes(color = intervention, group = intervention),\n    width = .05,\n    data = . %&gt;% \n      mutate(time = time + (as.numeric(intervention) - 1.5) / 6),\n    size = .5,\n    alpha = .5\n  ) +\n  geom_line(\n    data = my_predictions(fit_7) %&gt;% \n      mutate(intervention = factor(\n        intervention, \n        labels = c(\"Control\", \"Treatment\")\n    )),\n    aes(group = person_id, color = intervention),\n    alpha = .3,\n    linewidth = .5\n  ) +\n  geom_line(\n    data = pred_7,\n    aes(group = intervention, color = intervention),\n    linewidth = 3\n  ) +\n  geomtextpath::geom_labelline(\n    data = pred_7,\n    aes(\n      color = intervention,\n      group = intervention,\n      label = intervention\n    ),\n    vjust = -.3,\n    size = 6,\n    family = my_font,\n    fill = \"#FFFFFFBB\",\n    text_only = T,\n    linewidth = 0,\n    label.padding = margin(1, 1, 1, 1),\n    hjust = .95\n  ) +\n  theme_minimal(base_size = 20, base_family = my_font) +\n  theme(legend.position = \"none\") +\n  scale_color_manual(values = c(\n    Control = \"dodgerblue4\",\n    Treatment = \"forestgreen\"\n  )) + \n  labs(x = \"Time\", y = \"Vocabulary\")\n\n\n\n\n\n\n\n\nFigure 27: Model 7: Predicted Vocabulary by Intervention Group Over Time (Model 6 + Effects of Intervention on Intercept, Linear Effect of Time, and Quadratic Effect of Time)\n\n\n\n\n\n\nlmer(vocabulary ~ 1 + time*intervention + I(time ^ 2) * intervention + \n       (1 + time + I(time ^ 2) | person_id), \n     data = d, \n     REML = FALSE) %&gt;% \n  tab_model()\n\n\n\n\n\n\n \nvocabulary\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n52.00\n46.37 – 57.63\n&lt;0.001\n\n\ntime\n10.50\n9.86 – 11.14\n&lt;0.001\n\n\nintervention\n-0.09\n-7.89 – 7.72\n0.983\n\n\ntime^2\n-1.06\n-1.16 – -0.95\n&lt;0.001\n\n\ntime × intervention\n9.14\n8.26 – 10.02\n&lt;0.001\n\n\nintervention × time^2\n0.09\n-0.06 – 0.24\n0.262\n\n\nRandom Effects\n\n\n\nσ2\n8.35\n\n\n\nτ00 person_id\n783.75\n\n\nτ11 person_id.time\n4.02\n\n\nτ11 person_id.I(time^2)\n0.07\n\n\nρ01\n0.11\n\n\n\n-0.05\n\n\nICC\n0.99\n\n\nN person_id\n200\n\nObservations\n1200\n\n\nMarginal R2 / Conditional R2\n0.381 / 0.994\n\n\n\n\n\n\nTable 14: HLM Model Analogous to Model 7"
  },
  {
    "objectID": "tutorials/SEMGrowth/semgrowth.html#model-8-model-7-autoregression-effect",
    "href": "tutorials/SEMGrowth/semgrowth.html#model-8-model-7-autoregression-effect",
    "title": "Latent Growth Models",
    "section": "Model 8: Model 7 + Autoregression Effect",
    "text": "Model 8: Model 7 + Autoregression Effect\n\n\nCode\npd8 &lt;- pd7 +\n    my_connect(\n    v[1:5],\n    v[2:6],\n    color = v@fill[-6],\n    label = ob_label(\n      \"*r*\",\n      angle = 0,\n      position = .4,\n      size = 14,\n      label.padding = margin()\n    ),\n    resect = 1\n  ) \n\npd8\n\n\n\n\n\n\n\n\nFigure 28: Path Diagram for Model 8\n\n\n\n\n\nSuppose that we want to add an autoregression effect to our latent growth model.\nAn autoregression allows for the residuals of earlier observed variables to have lasting effects on subsequent variables.\n\nm_8 &lt;- \"\n# Intercept\ni =~ 1 * voc_0 + 1 * voc_1 + 1 * voc_2 + 1 * voc_3 + 1 * voc_4 + 1 * voc_5 \n# Linear slope of time\ns =~ 0 * voc_0 + 1 * voc_1 + 2 * voc_2 + 3 * voc_3 + 4 * voc_4 + 5 * voc_5\n# Acceleration effect of time (quadratic effect)\nq =~ 0 * voc_0 + 1 * voc_1 + 4 * voc_2 + 9 * voc_3 + 16 * voc_4 + 25 * voc_5\n# Intercept, slope, and quadratic effects\ni ~ b_00 * 1 + b_01 * intervention\ns ~ b_10 * 1 + b_11 * intervention\nq ~ b_20 * 1 + b_21 * intervention\n# Variances and covariances\ni ~~ tau_00 * i \ns ~~ tau_11 * s + tau_01 * i\nq ~~ tau_22 * q + tau_12 * s + tau_02 * i\nvoc_0 ~~ e * voc_0\nvoc_1 ~~ e * voc_1\nvoc_2 ~~ e * voc_2\nvoc_3 ~~ e * voc_3\nvoc_4 ~~ e * voc_4\nvoc_5 ~~ e * voc_5\n# Autoregression\nvoc_1 ~ r * voc_0\nvoc_2 ~ r * voc_1\nvoc_3 ~ r * voc_2\nvoc_4 ~ r * voc_3\nvoc_5 ~ r * voc_4\n\"\n\nfit_8 &lt;- growth(m_8, data = d_wide)\n\nparameters(fit_8, component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |         95% CI |      p\n---------------------------------------------------------\ni =~ voc_0 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_2 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_3 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_4 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ni =~ voc_5 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\ns =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\ns =~ voc_2 |        2.00 | 0.00 | [ 2.00,  2.00] | &lt; .001\ns =~ voc_3 |        3.00 | 0.00 | [ 3.00,  3.00] | &lt; .001\ns =~ voc_4 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\ns =~ voc_5 |        5.00 | 0.00 | [ 5.00,  5.00] | &lt; .001\nq =~ voc_0 |        0.00 | 0.00 | [ 0.00,  0.00] | &lt; .001\nq =~ voc_1 |        1.00 | 0.00 | [ 1.00,  1.00] | &lt; .001\nq =~ voc_2 |        4.00 | 0.00 | [ 4.00,  4.00] | &lt; .001\nq =~ voc_3 |        9.00 | 0.00 | [ 9.00,  9.00] | &lt; .001\nq =~ voc_4 |       16.00 | 0.00 | [16.00, 16.00] | &lt; .001\nq =~ voc_5 |       25.00 | 0.00 | [25.00, 25.00] | &lt; .001\n\n# Mean\n\nLink            | Coefficient |   SE |         95% CI |      z |      p\n-----------------------------------------------------------------------\ni ~1  (b_00)    |       52.01 | 2.87 | [46.38, 57.64] |  18.11 | &lt; .001\ns ~1  (b_10)    |       10.54 | 0.50 | [ 9.56, 11.52] |  21.03 | &lt; .001\nq ~1  (b_20)    |       -1.06 | 0.07 | [-1.21, -0.92] | -14.39 | &lt; .001\nvoc_0 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_1 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_2 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_3 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_4 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nvoc_5 ~1        |        0.00 | 0.00 | [ 0.00,  0.00] |        | &lt; .001\nintervention ~1 |        0.52 | 0.00 | [ 0.52,  0.52] |        | &lt; .001\n\n# Regression\n\nLink                    | Coefficient |   SE |         95% CI |     z |      p\n------------------------------------------------------------------------------\ni ~ intervention (b_01) |       -0.09 | 3.98 | [-7.89,  7.71] | -0.02 | 0.982 \ns ~ intervention (b_11) |        9.14 | 0.45 | [ 8.26, 10.03] | 20.25 | &lt; .001\nq ~ intervention (b_21) |        0.09 | 0.08 | [-0.06,  0.24] |  1.13 | 0.261 \nvoc_1 ~ voc_0 (r)       |   -1.06e-03 | 0.01 | [-0.02,  0.02] | -0.10 | 0.920 \nvoc_2 ~ voc_1 (r)       |   -1.06e-03 | 0.01 | [-0.02,  0.02] | -0.10 | 0.920 \nvoc_3 ~ voc_2 (r)       |   -1.06e-03 | 0.01 | [-0.02,  0.02] | -0.10 | 0.920 \nvoc_4 ~ voc_3 (r)       |   -1.06e-03 | 0.01 | [-0.02,  0.02] | -0.10 | 0.920 \nvoc_5 ~ voc_4 (r)       |   -1.06e-03 | 0.01 | [-0.02,  0.02] | -0.10 | 0.920 \n\n# Variance\n\nLink                         | Coefficient |    SE |           95% CI |     z |      p\n--------------------------------------------------------------------------------------\ni ~~ i (tau_00)              |      784.05 | 79.15 | [628.92, 939.18] |  9.91 | &lt; .001\ns ~~ s (tau_11)              |        4.04 |  1.09 | [  1.91,   6.18] |  3.71 | &lt; .001\nq ~~ q (tau_22)              |        0.07 |  0.03 | [  0.01,   0.13] |  2.14 | 0.033 \nvoc_0 ~~ voc_0 (e)           |        8.34 |  0.49 | [  7.38,   9.30] | 17.04 | &lt; .001\nvoc_1 ~~ voc_1 (e)           |        8.34 |  0.49 | [  7.38,   9.30] | 17.04 | &lt; .001\nvoc_2 ~~ voc_2 (e)           |        8.34 |  0.49 | [  7.38,   9.30] | 17.04 | &lt; .001\nvoc_3 ~~ voc_3 (e)           |        8.34 |  0.49 | [  7.38,   9.30] | 17.04 | &lt; .001\nvoc_4 ~~ voc_4 (e)           |        8.34 |  0.49 | [  7.38,   9.30] | 17.04 | &lt; .001\nvoc_5 ~~ voc_5 (e)           |        8.34 |  0.49 | [  7.38,   9.30] | 17.04 | &lt; .001\nintervention ~~ intervention |        0.25 |  0.00 | [  0.25,   0.25] |       | &lt; .001\n\n# Correlation\n\nLink            | Coefficient |   SE |         95% CI |     z |     p\n---------------------------------------------------------------------\ni ~~ s (tau_01) |        6.74 | 7.95 | [-8.84, 22.33] |  0.85 | 0.396\ns ~~ q (tau_12) |       -0.37 | 0.18 | [-0.71, -0.03] | -2.11 | 0.034\ni ~~ q (tau_02) |       -0.44 | 1.30 | [-2.98,  2.10] | -0.34 | 0.735\n\n# Compare models\ntest_likelihoodratio(fit_7, fit_8)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison\n\nModel |   Type | df | df_diff |  Chi2 |     p\n---------------------------------------------\nfit_7 | lavaan | 20 |       1 | 14.44 | 0.920\nfit_8 | lavaan | 19 |         | 14.43 |      \n\n\nThe autoregression parameter was not statistically significant. Comparing the two models revealed no difference in model fit. Thus, there appears to be no need of the autoregression parameter, once we have accounted for the latent growth."
  },
  {
    "objectID": "tutorials/RandomSlopes/random_slopes.html",
    "href": "tutorials/RandomSlopes/random_slopes.html",
    "title": "Random Slopes",
    "section": "",
    "text": "This is a long tutorial. Don’t let that scare you. Once understood, the analyses themselves do not take that much time or effort to complete. My goal here is to give you a sense of your options when conducting analyses like these."
  },
  {
    "objectID": "tutorials/RandomSlopes/random_slopes.html#reading1-model-0-random-intercepts",
    "href": "tutorials/RandomSlopes/random_slopes.html#reading1-model-0-random-intercepts",
    "title": "Random Slopes",
    "section": "Reading1 Model 0: Random Intercepts",
    "text": "Reading1 Model 0: Random Intercepts\nFirst we start modeling reading1 with our multilevel “null” model: random intercepts. We will the model equations, then a table with a description of all symbols, then a visual representation fo the model in the form of a path diagram.\n\nModel 0 Equations\n\\[\n\\begin{aligned}\n\\textbf{Level 1:}\\\\\nReading_{ij1} &= \\beta_{0j}+e_{ij}\\\\\ne_{ij}&\\sim \\mathcal{N}(0,\\sigma_e^2)\\\\\n\\textbf{Level 2:}\\\\\n\\beta_{0j}&=\\gamma_{00}+u_{0j}\\\\\nu_{0j}&\\sim \\mathcal{N}(0,\\tau_{00})\\\\\n\\textbf{Combined:}\\\\\nReading_{ij1} &= \\underbrace{\\gamma_{00}+u_{0j}}_{\\beta_{0j}}+e_{ij}\n\\end{aligned}\n\\]\n\n\nModel 0 Fully Annotated\nTable 2 explains the meaning of the symbols in the Model 0 equations. The symbols I do not explain repeatedly are \\(\\sim\\) (“is distributed as.”) and \\(\\mathcal{N}\\) (normal disbribution). Thus, \\(X\\sim\\mathcal{N}(\\mu, \\sigma^2)\\) means \\(X\\) is a normally distributed random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\n\n\nCode\ntibble(\n  Symbol = c(\n    \"$Reading_{ij1}$\",\n    \"$\\\\beta_{0j}$\",\n    \"$e_{ij}$\",\n    \"$\\\\sigma_e^2$\",\n    \"$\\\\gamma_{00}$\",\n    \"$u_{0j}$\",\n    \"$\\\\tau_{00}$\"\n    \n  ),\n  Meaning = c(\n    \"Reading score for person $i$ in class $j$ at time 1\",\n    \"Random intercept for class $j$. In this model, it represents the classroom mean for class $j$\",\n    \"Level 1 prediction error for person $i$ in class $j$\",\n    \"Variance of $e_{ij}$\",\n    \"Fixed Intercept. In this model, it is the overall classroom mean for Time 1 Reading\",\n    \"School-level (L2) residual for the random intercept $\\\\beta_{0j}$. It is the deviation of classroom $j$, from the overall fixed intercept *&gamma;*~00~\",\n    \"Variance of $u_{0j}$\"),\n  level = paste0(\"Level \", c(1,1,1,1,2,2,2))\n) %&gt;%\n  gt(groupname_col = \"level\", row_group_as_column = T) %&gt;%\n  gt::tab_options(\n    column_labels.font.weight = \"bold\",\n    column_labels.font.size = 20,\n    footnotes.font.size = 16, table.border.bottom.color = \"white\") %&gt;% \n  gt::cols_width(Symbol ~ pct(15), level ~ pct(23)) %&gt;% \n  tab_row_group(label = md(\"$$\\\\begin{aligned}\n  \\\\textbf{Level 2}\\\\\\\\\n  \\\\beta_{0j}&=\\\\gamma_{00}+u_{0j}\\\\\\\\\n  u_{0j}&\\\\sim \\\\mathcal{N}(0,\\\\tau_{00})\n  \\\\end{aligned}$$\"), rows = level == \"Level 2\", id = \"l2\") %&gt;%  \n  tab_row_group(label = md(\"$$\\\\begin{align*}\n  \\\\textbf{Level 1}\\\\\\\\\nReading_{ij1} &= \\\\beta_{0j}+e_{ij}\\\\\\\\\ne_{ij}&\\\\sim \\\\mathcal{N}(0,\\\\sigma_e^2)\\\\end{align*}$$\"), rows = level == \"Level 1\", id = \"l1\") %&gt;% \n    fmt_markdown() %&gt;% \n  gt::cols_align(\"center\", \"level\") %&gt;% \n  gt::tab_footnote(footnote = md(\"$$\\\\begin{align*}\n\\\\textbf{Combined Model}\\\\\\\\\nReading_{ij1} &= \\\\underbrace{\\\\gamma_{00}+u_{0j}}_{\\\\beta_{0j}}+e_{ij}\n\\\\end{align*}$$\")) %&gt;% \ngt::tab_stubhead(\"Level\")\n\n\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Level\n      Symbol\n      Meaning\n    \n  \n  \n    $$\\begin{align*}\n  \\textbf{Level 1}\\\\\nReading_{ij1} &= \\beta_{0j}+e_{ij}\\\\\ne_{ij}&\\sim \\mathcal{N}(0,\\sigma_e^2)\\end{align*}$$\n\\(Reading_{ij1}\\)\nReading score for person \\(i\\) in class \\(j\\) at time 1\n    \\(\\beta_{0j}\\)\nRandom intercept for class \\(j\\). In this model, it represents the classroom mean for class \\(j\\)\n    \\(e_{ij}\\)\nLevel 1 prediction error for person \\(i\\) in class \\(j\\)\n    \\(\\sigma_e^2\\)\nVariance of \\(e_{ij}\\)\n    $$\\begin{aligned}\n  \\textbf{Level 2}\\\\\n  \\beta_{0j}&=\\gamma_{00}+u_{0j}\\\\\n  u_{0j}&\\sim \\mathcal{N}(0,\\tau_{00})\n  \\end{aligned}$$\n\\(\\gamma_{00}\\)\nFixed Intercept. In this model, it is the overall classroom mean for Time 1 Reading\n    \\(u_{0j}\\)\nSchool-level (L2) residual for the random intercept \\(\\beta_{0j}\\). It is the deviation of classroom \\(j\\), from the overall fixed intercept γ00\n    \\(\\tau_{00}\\)\nVariance of \\(u_{0j}\\)\n  \n  \n  \n    \n       $$\\begin{align*}\n\\textbf{Combined Model}\\\\\nReading_{ij1} &= \\underbrace{\\gamma_{00}+u_{0j}}_{\\beta_{0j}}+e_{ij}\n\\end{align*}$$\n    \n  \n\n\n\n\n\nTable 2: Random Intercepts Model for Reading at Time 1\n\n\n\n\n\n\nModel 0 Path Diagram\nSometimes equations can be overwhelming, especially for complex models. It can help to visualize a model as a path diagram like Figure 1.\n\n\nCode\nmyfont &lt;- \"Roboto Condensed\"\n\n# ggdiagram is an experimental package that can be installed \n# remotes::install_github(\"wjschne/ggdiagram\")\n\nmy_intercept &lt;- redefault(\n  ob_ngon,\n  radius = .7,\n  angle = 90,\n  fill = NA,\n  color = \"black\",\n  label = \"1~1~\",\n  vertex_radius = unit(0.5, \"mm\")\n)\n\nmy_path &lt;- redefault(connect, resect = 2)\n\n\nmy_error &lt;- redefault(ob_circle, radius = .5)\nmy_error_variance &lt;- redefault(ob_variance,\n    resect = 1,\n    linewidth = .4,\n    theta = 60,\n    looseness = 1.5,\n    where = \"right\"\n  )\nmy_error_variance_label &lt;- redefault(\n  ob_latex,\n  tex = r\"(\\text{\\emph{σ}}^{\\text{2}}_{\\mkern-1.5mu\\text{\\emph{e}}})\",\n  width = .3,\n  family = myfont,\n  filename = \"sigma_e_2\",\n  force_recompile = F,\n  delete_files = F\n)\n\nmy_path_label &lt;- redefault(ob_label,\n        size = 13,\n        angle = 0,\n        position = .45,\n        label.padding = margin(1, 1, 1, 1)\n      )\n\nmy_coefficient &lt;- redefault(ob_circle, radius = .4, fill = \"white\")\nmy_residual &lt;- redefault(ob_circle, radius = .3, fill = \"white\")\nmy_residual_label &lt;- redefault(ob_label, size = 13)\nmy_residual_variance &lt;- redefault(\n  ob_variance,\n  resect = 1,\n  linewidth = .4,\n  theta = 60,\n  looseness = 2,\n  where = \"right\"\n)\nmy_residual_variance_label &lt;- redefault(\n  ob_label,\n  size = 13,\n  label.padding = margin(l = 1, r = 1, t = 1, b = 1),\n  position = .5\n)\nmy_observed &lt;- redefault(ob_ellipse, m1 = 20)\nmy_observed_label &lt;- redefault(ob_label, size = 22, fill = NA)\n\nmy_observed_l2 &lt;- redefault(ob_ellipse, m1 = 20, a = .7, b = .7)\n\n\n\n\nCode\np_read1 &lt;- ggdiagram(myfont, font_size = 16) +\n  {read_1 &lt;- my_observed(label = my_observed_label(\"*Reading*~*1ij*~\"))} +\n  {i_1 &lt;- my_intercept(label = \"1~1~\") %&gt;% \n    place(read_1, \"left\", 3)} +\n  {i12read1 &lt;- my_path(i_1, read_1)} +\n  {b_0j &lt;- my_coefficient(\n    center = midpoint(i12read1),\n    label = \"*&beta;*~0*j*~\")} +\n  {i_2 &lt;- my_intercept(label = \"1~2~\") %&gt;% \n    place(b_0j, \"above\", 1.3)} +\n  {i12read1 &lt;- my_path(\n      i_2,\n      b_0j,\n      label = my_path_label(\"*&gamma;*~00~\")\n    )} +\n  {e &lt;- my_error(label = \"*e*~*ij*~\") %&gt;% \n    place(read_1, \"right\", .6)} +\n  my_path(e, read_1) +\n  {e_var &lt;- my_error_variance(e)} +\n  {u_0j &lt;- my_residual(label = my_residual_label(\"*u*~0*j*~\")) %&gt;% \n    place(b_0j, \"below\", .6)} +\n  my_residual_variance(\n    u_0j,\n    \"below\",\n    label = my_residual_variance_label(\"*&tau;*~00~\")\n  ) +\n  my_path(u_0j, b_0j) +\n  my_error_variance_label(center = e_var@midpoint())\n\np_read1\n\n\n\n\n\n\n\n\nFigure 1: Path Diagram for Reading Time 1 Random Intercepts Model\n\n\n\n\n\nPath models have these characteristics:\n\nObserved variables are squares/rectangles.\nLatent variables and residuals are circles/ellipses.\nRegression coefficients are single-headed arrows leading from predictor to outcome.\nCovariances are double-headed arrows between two variables.\nVariances are double-headed arrows leading back to the same variable.\nIntercepts/means are triangles with a “1” as a label.\nRandom coefficients are circles that lie on a path.\n\nThe idea of representing random coefficients as latent variables (circles) on paths was proposed by Curran & Bauer (2007)."
  },
  {
    "objectID": "tutorials/RandomSlopes/random_slopes.html#reading1-model-0-in-r",
    "href": "tutorials/RandomSlopes/random_slopes.html#reading1-model-0-in-r",
    "title": "Random Slopes",
    "section": "Reading1 Model 0 in R",
    "text": "Reading1 Model 0 in R\nThe lmer function from the lme4 package (Bates et al., 2015) specifies model formulas with fixed effects listed in the same way as in the lm function (ordinary least squares linear model) and random effects listed inside parentheses, with the grouping variable listed after a post symbol (|). Consider this model:\nY ~ 1 + X + (1 + X | group)\n\n\n\n\n\n\nFigure 2: Annotated formula for lmer models\n\n\n\n\nm_read_1_0 &lt;- lmer(reading_1 ~ 1 + (1 | school_id), d)\n\n\n\nThe fixed intercept is optional, but I like to include it for clarity. Omitting the fixed intercept looks like this:\n\nlmer(reading_1 ~ (1 | school_id), d)\n\nIn R, an outcome variable is placed on the left hand side of the tilde ~. The first 1 means to specify an overall fixed intercept. The parentheses indicate a random effect. The 1 inside the parentheses means that a random intercept should be specified by the grouping variable, school_id.\nNow we can use various tools to inspect the overall results\n\nCheck Assumptions\n\ncheck_model(m_read_1_0)\n\n\n\n\n\n\n\nFigure 3: Check assumptions for reading time 1 null model\n\n\n\n\n\nNothing in the diagnostic plots in Figure 3 raises concerns.\n\n\nEvaluate Model\nAs always, we can get a quick summary of the model like so:\n\nsummary(m_read_1_0)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_1 ~ 1 + (1 | school_id)\n   Data: d\n\nREML criterion at convergence: 33352\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.649 -0.673  0.005  0.668  4.035 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n school_id (Intercept) 0.655    0.809   \n Residual              0.337    0.580   \nNumber of obs: 18257, groups:  school_id, 300\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) -0.00464    0.04695    -0.1\n\n\nAn overall automated report:\n\nreport(m_read_1_0) \n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML\nand nloptwrap optimizer) to predict reading_1 (formula: reading_1 ~ 1). The\nmodel included school_id as random effect (formula: ~1 | school_id). The\nmodel's intercept is at -4.64e-03 (95% CI [-0.10, 0.09], t(18254) = -0.10, p =\n0.921).\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\nThere is not much to comment on here because this model does not attempt to explain anything. It will be a basis of comparison for the next model.\nWe can get a quick look at some performance metrics:\n\nperformance(m_read_1_0)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n-----------------------------------------------------------------------------------\n33358.049 | 33358.051 | 33381.486 |      0.660 |      0.000 | 0.660 | 0.576 | 0.580\n\n\nThe ICC for this model suggests that schools are highly segregated by reading ability. That is, variation by classroom accounts for 66% of the variance in reading1.\nThe sjPlot:tab_model function can produce a table that is very close to publication-ready:\n\ntab_model(m_read_1_0)\n\n\n\n\n\n\n \nreading 1\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-0.00\n-0.10 – 0.09\n0.921\n\n\nRandom Effects\n\n\n\nσ2\n0.34\n\n\n\nτ00 school_id\n0.65\n\n\nICC\n0.66\n\n\nN school_id\n300\n\nObservations\n18257\n\n\nMarginal R2 / Conditional R2\n0.000 / 0.660\n\n\n\n\n\n\nTable 3: Model summary for reading time 1 null model"
  },
  {
    "objectID": "tutorials/RandomSlopes/random_slopes.html#reading1-model-1-l2-covariate",
    "href": "tutorials/RandomSlopes/random_slopes.html#reading1-model-1-l2-covariate",
    "title": "Random Slopes",
    "section": "Reading1 Model 1: L2 Covariate",
    "text": "Reading1 Model 1: L2 Covariate\nThis model builds upon the previous model by adding the classroom’s average socioeconomic status (SES) variable as a covariate. Because average SES varies by classroom instead of by individual, it is a level-2 covariate. It can be thought of as a predictor of the random intercept. Because the random intercept represents the classroom mean, the level 2 equation in this model is sometimes revered to as a means-as-outcomes model.\nNote that SES’s coefficient is γ01. The subscript of coefficient, 01, has a 0 in the first position, which tells us that this is a predictor of the random intercept (β0j). The 1 in the second position in the subscript tell us that this is the first predictor of the β0j.\n\\[\n\\begin{align}\n\\textbf{Level 1:}\\\\\nReading_{ij1} &= \\beta_{0j}+e_{ij}\\\\\ne&\\sim \\mathcal{N}(0,\\sigma_e^2)\\\\\n\\textbf{Level 2:}\\\\\n\\beta_{0j}&=\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}\\\\\nu_{0j}&\\sim \\mathcal{N}(0,\\tau_{00})\\\\\n\\textbf{Combined:}\\\\\nReading_{ij1} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}}_{\\beta_{0j}}+e_{ij}\n\\end{align}\n\\]\n\n\nCode\ntibble(\n  Symbol = c(\n    \"$Reading_{ij1}$\",\n    \"$\\\\beta_{0j}$\",\n    \"$e_{ij}$\",\n    \"$\\\\sigma_e^2$\",\n    \"$\\\\gamma_{00}$\",\n    \"$\\\\gamma_{01}$\",\n    \"$u_{0j}$\",\n    \"$\\\\tau_{00}$\"\n    \n  ),\n  Meaning = c(\n    \"Reading score for person $i$ in class $j$ at time 1\",\n    \"Random intercept for class $j$. In this model, it represents the classroom mean for class $j$\",\n    \"Level 1 prediction error for person $i$ in class $j$\",\n    \"Variance of $e_{ij}$\",\n    \"Fixed Intercept. In this model, it is predicted classroom mean for Reading~1~ when SES is 0.\",\n    \"Fixed Effect for SES. It represents the predicted increase in the classroom mean for Reading~1~ when SES increases by 1.\",\n    \"School-level (L2) residual for the random intercept $\\\\beta_{0j}$. It is the deviation of classroom $j$, from the overall fixed intercept *&gamma;*~00~\",\n    \"Variance of $u_{0j}$\"),\n  level = paste0(\"Level \", c(1,1,1,1,2,2,2, 2))\n) %&gt;%\n  gt(groupname_col = \"level\", row_group_as_column = T) %&gt;%\n  gt::tab_options(\n    column_labels.font.weight = \"bold\",\n    column_labels.font.size = 20,\n    footnotes.font.size = 16, table.border.bottom.color = \"white\") %&gt;% \n  gt::cols_width(Symbol ~ pct(15), level ~ pct(23)) %&gt;% \n  tab_row_group(label = md(\"$$\\\\begin{aligned}\n  \\\\textbf{Level 2}\\\\\\\\\n  \\\\beta_{0j}&=\\\\gamma_{00}+\\\\gamma_{01}SES_j+u_{0j}\\\\\\\\\n  u_{0j}&\\\\sim \\\\mathcal{N}(0,\\\\tau_{00})\n  \\\\end{aligned}$$\"), rows = level == \"Level 2\", id = \"l2\") %&gt;%  \n  tab_row_group(label = md(\"$$\\\\begin{align*}\n  \\\\textbf{Level 1}\\\\\\\\\nReading_{ij1} &= \\\\beta_{0j}+e_{ij}\\\\\\\\\ne_{ij}&\\\\sim \\\\mathcal{N}(0,\\\\sigma_e^2)\\\\end{align*}$$\"), rows = level == \"Level 1\", id = \"l1\") %&gt;% \n    fmt_markdown() %&gt;% \n  gt::cols_align(\"center\", \"level\") %&gt;% \n  gt::tab_footnote(footnote = md(\"$$\\\\begin{align*}\n\\\\textbf{Combined Model}\\\\\\\\\nReading_{ij1} &= \\\\underbrace{\\\\gamma_{00}+\\\\gamma_{01}SES_j+u_{0j}}_{\\\\beta_{0j}}+e_{ij}\n\\\\end{align*}$$\")) %&gt;% \ngt::tab_stubhead(\"Level\")\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Level\n      Symbol\n      Meaning\n    \n  \n  \n    $$\\begin{align*}\n  \\textbf{Level 1}\\\\\nReading_{ij1} &= \\beta_{0j}+e_{ij}\\\\\ne_{ij}&\\sim \\mathcal{N}(0,\\sigma_e^2)\\end{align*}$$\n\\(Reading_{ij1}\\)\nReading score for person \\(i\\) in class \\(j\\) at time 1\n    \\(\\beta_{0j}\\)\nRandom intercept for class \\(j\\). In this model, it represents the classroom mean for class \\(j\\)\n    \\(e_{ij}\\)\nLevel 1 prediction error for person \\(i\\) in class \\(j\\)\n    \\(\\sigma_e^2\\)\nVariance of \\(e_{ij}\\)\n    $$\\begin{aligned}\n  \\textbf{Level 2}\\\\\n  \\beta_{0j}&=\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}\\\\\n  u_{0j}&\\sim \\mathcal{N}(0,\\tau_{00})\n  \\end{aligned}$$\n\\(\\gamma_{00}\\)\nFixed Intercept. In this model, it is predicted classroom mean for Reading1 when SES is 0.\n    \\(\\gamma_{01}\\)\nFixed Effect for SES. It represents the predicted increase in the classroom mean for Reading1 when SES increases by 1.\n    \\(u_{0j}\\)\nSchool-level (L2) residual for the random intercept \\(\\beta_{0j}\\). It is the deviation of classroom \\(j\\), from the overall fixed intercept γ00\n    \\(\\tau_{00}\\)\nVariance of \\(u_{0j}\\)\n  \n  \n  \n    \n       $$\\begin{align*}\n\\textbf{Combined Model}\\\\\nReading_{ij1} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}}_{\\beta_{0j}}+e_{ij}\n\\end{align*}$$\n    \n  \n\nReading Time 1 Model 1: Model 0 + Level 2 SES\n\n\n\n\nCode\np_read1 +\n  {SES &lt;- my_observed_l2(\n    center = i_2 %-|% i_1,\n    label = my_observed_label(\"*SES*~*j*~\", vjust = .6))} + \n  my_path(SES@point_at(\"southeast\"), \n          b_0j, \n          label = my_path_label(\"*&gamma;*~01~\"))\n\n\n\n\n\n\n\n\nFigure 4: Reading Time 1 Model 0 + Level 2 SES\n\n\n\n\n\n\nExploratory Plot for Reading1\nIt helps to get a rough plot to see what is happening. Let’s create and informal plot of school_ses with reading_1. We display individual reading scores as little points and the means for each school with big dots and error bars.\n\nd %&gt;%\n  ggplot(aes(school_ses, reading_1, color = school_id)) +\n  geom_point(size = 0.1, alpha = 0.3) +\n  stat_summary(fun.data = mean_se) +\n  geom_smooth(color = \"black\", method = \"lm\", formula = \"y ~ x\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x = \"School SES\", y = \"Reading at Time 1\")\n\n\n\n\n\n\n\nFigure 5: Mean SES predicting reading time 1\n\n\n\n\n\nIn Figure 5 we see that school_ses and reading_1 are correlated. Why are all the data in columns? Because school_ses is the mean SES for each school. Every child in the same school has the same school_ses.\n\n\nModel Summary\nWe can create the model from scratch:\n\nm_read_1_1 &lt;- lmer(reading_1 ~ school_ses + (1 | school_id), d)\n\nAlternately, we can update the previous model, which is usually safer:\n\nm_read_1_1 &lt;- update(m_read_1_0, . ~ . + school_ses)\n\nThe model can be summarized in many ways:\n\nsummary(m_read_1_1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_1 ~ (1 | school_id) + school_ses\n   Data: d\n\nREML criterion at convergence: 33175\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.661 -0.674  0.006  0.665  4.032 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n school_id (Intercept) 0.354    0.595   \n Residual              0.337    0.580   \nNumber of obs: 18257, groups:  school_id, 300\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) -0.00475    0.03467   -0.14\nschool_ses   0.54917    0.03472   15.82\n\nCorrelation of Fixed Effects:\n           (Intr)\nschool_ses 0.000 \n\nsjPlot::tab_model(m_read_1_1)\n\n\n\n \nreading 1\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-0.00\n-0.07 – 0.06\n0.891\n\n\nschool ses\n0.55\n0.48 – 0.62\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n0.34\n\n\n\nτ00 school_id\n0.35\n\n\nICC\n0.51\n\n\nN school_id\n300\n\nObservations\n18257\n\n\nMarginal R2 / Conditional R2\n0.307 / 0.662\n\n\n\n\nperformance(m_read_1_1)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n-----------------------------------------------------------------------------------\n33182.831 | 33182.833 | 33214.080 |      0.662 |      0.307 | 0.513 | 0.576 | 0.580\n\nparameters(m_read_1_1)\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |        95% CI | t(18253) |      p\n--------------------------------------------------------------------\n(Intercept) |   -4.75e-03 | 0.03 | [-0.07, 0.06] |    -0.14 | 0.891 \nschool ses  |        0.55 | 0.03 | [ 0.48, 0.62] |    15.82 | &lt; .001\n\n# Random Effects\n\nParameter                 | Coefficient\n---------------------------------------\nSD (Intercept: school_id) |        0.59\nSD (Residual)             |        0.58\n\nreport(m_read_1_1)\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading_1 with school_ses (formula: reading_1 ~ school_ses). The\nmodel included school_id as random effect (formula: ~1 | school_id). The\nmodel's total explanatory power is substantial (conditional R2 = 0.66) and the\npart related to the fixed effects alone (marginal R2) is of 0.31. The model's\nintercept, corresponding to school_ses = 0, is at -4.75e-03 (95% CI [-0.07,\n0.06], t(18253) = -0.14, p = 0.891). Within this model:\n\n  - The effect of school ses is statistically significant and positive (beta =\n0.55, 95% CI [0.48, 0.62], t(18253) = 15.82, p &lt; .001; Std. beta = 0.55, 95% CI\n[0.48, 0.62])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\nThe slope for school_ses is statistically significant and explains significant variance in reading_1.\n\n\nCompare model fit\nWe have many options for comparing models:\n\ncompare_models(m_read_1_0, m_read_1_1)\n\nParameter    |              m_read_1_0 |              m_read_1_1\n----------------------------------------------------------------\n(Intercept)  | -4.64e-03 (-0.10, 0.09) | -4.75e-03 (-0.07, 0.06)\nschool ses   |                         |      0.55 ( 0.48, 0.62)\n----------------------------------------------------------------\nObservations |                   18257 |                   18257\n\nsjPlot::tab_model(m_read_1_0, m_read_1_1)\n\n\n\n \nreading 1\nreading 1\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n-0.00\n-0.10 – 0.09\n0.921\n-0.00\n-0.07 – 0.06\n0.891\n\n\nschool ses\n\n\n\n0.55\n0.48 – 0.62\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n0.34\n0.34\n\n\n\nτ00\n0.65 school_id\n0.35 school_id\n\n\nICC\n0.66\n0.51\n\n\nN\n300 school_id\n300 school_id\n\nObservations\n18257\n18257\n\n\nMarginal R2 / Conditional R2\n0.000 / 0.660\n0.307 / 0.662\n\n\n\n\nanova(m_read_1_0, m_read_1_1)\n\nData: d\nModels:\nm_read_1_0: reading_1 ~ 1 + (1 | school_id)\nm_read_1_1: reading_1 ~ (1 | school_id) + school_ses\n           npar   AIC   BIC logLik -2*log(L) Chisq Df Pr(&gt;Chisq)    \nm_read_1_0    3 33354 33377 -16674     33348                        \nm_read_1_1    4 33173 33204 -16583     33165   183  1     &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncompare_performance(m_read_1_0, m_read_1_1)\n\n# Comparison of Model Performance Indices\n\nName       |   Model |   AIC (weights) |  AICc (weights) |   BIC (weights)\n--------------------------------------------------------------------------\nm_read_1_0 | lmerMod | 33353.8 (&lt;.001) | 33353.8 (&lt;.001) | 33377.2 (&lt;.001)\nm_read_1_1 | lmerMod | 33173.1 (&gt;.999) | 33173.1 (&gt;.999) | 33204.3 (&gt;.999)\n\nName       | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n------------------------------------------------------------\nm_read_1_0 |      0.660 |      0.000 | 0.660 | 0.576 | 0.580\nm_read_1_1 |      0.662 |      0.307 | 0.513 | 0.576 | 0.580\n\ntest_performance(m_read_1_0, m_read_1_1)\n\nName       |   Model |     BF | df | df_diff |   Chi2 |      p\n--------------------------------------------------------------\nm_read_1_0 | lmerMod |        |  3 |         |        |       \nm_read_1_1 | lmerMod | &gt; 1000 |  4 |    1.00 | 182.71 | &lt; .001\nModels were detected as nested (in terms of fixed parameters) and are compared in sequential order.\n\nlmerTest::ranova(m_read_1_1)\n\nANOVA-like table for random-effects: Single term deletions\n\nModel:\nreading_1 ~ school_ses + (1 | school_id)\n                npar logLik   AIC   LRT Df Pr(&gt;Chisq)    \n&lt;none&gt;             4 -16587 33183                        \n(1 | school_id)    3 -22333 44673 11492  1     &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nUsing school_ses as an L2 covariate, Model 1 fits significantly better than Model 0.\nWhen in doubt, the anova results are the most common way of comparing models. The comparison only works if you are comparing nested models with exactly the same data. The anova function will warn you if you add a predictor with missing data that causes the comparison to be misleading.\nThe AIC weights are probably your next best option, especially for comparing non-nested models. Here, our models are nested.\n\n\nNested or Non-Nested?\nIf you create one model from another by adding one or more features, the models are nested. If you create one model from another by adding some features AND removing other features, the models are non-nested.\n\nNested Models\nModel 1: Y ~ A\nModel 2: Y ~ A + B\nThese models are nested because we added predictor B to the first model.\nThese models are also nested because we are adding a random intercept:\nModel 1: Y ~ A\nModel 2: Y ~ A + (1 | id)\nNested models can differ by more than one feature. For example, the following models are nested because we add predictor B and the random slope for A:\nModel 1: Y ~ A + (1 | id)\nModel 2: Y ~ A + B + (1 + A | id)\n\n\nNon-nested Models\nModel 1: Y ~ A\nModel 2: Y ~ B\nThese models are non-nested because we add predictor B AND remove predictor A.\nModel 1: Y ~ A + (1 + A | id)\nModel 2: Y ~ A + B + (1 | id)\nThese models are non-nested because we add predictor B but remove the random slopes for A\n\n\n\nCheck Conditional intraclass correlation coefficient and variance explained\n\nperformance::icc(m_read_1_1)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.513\n  Unadjusted ICC: 0.355\n\n\nIt looks like the conditional ICC is much lower than before.\n\nperformance::r2(m_read_1_1)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.662\n     Marginal R2: 0.307\n\n\nIt looks like the school SES explained a lot of variance.\nHow much does the level-2 random intercept variance \\((\\tau_{00})\\) shrink when we compare the two models we have run so far:\n\\[\n\\Delta R_2^2 = \\frac{\\tau_{00_0}-\\tau_{00_1}}{\\tau_{00_0}}\n\\]\nWe can also see how much variance was reduced in the level-1 residuals. The level-1 variance reduction is calculated like so:\n\\[\n\\Delta R_1^2 = \\frac{\\sigma_0^2-\\sigma_1^2}{\\sigma_0^2}\n\\]\nI do not want to perform this calculation by hand, so I will make a function that automates the process:\n\n# Function to calculate variance reduction for levels 1 and 2\nvariance_reduction &lt;- function(m1, m2) {\n  tau_00_1 &lt;- get_variance_random(m1)\n  tau_00_2 &lt;- get_variance_random(m2)\n  level2_reduction &lt;- (tau_00_1 - tau_00_2) / tau_00_1\n\n  sigma_1 &lt;- get_sigma(m1)\n  sigma_2 &lt;- get_sigma(m2)\n  level1_reduction &lt;- (sigma_1 - sigma_2) / sigma_1\n\n  tibble(\n    level = c(1, 2),\n    model_1_variance = c(sigma_1, tau_00_1),\n    model_2_variance = c(sigma_2, tau_00_2),\n    variance_reduction = c(level1_reduction, level2_reduction)\n  )\n}\n\nvariance_reduction(m_read_1_0, m_read_1_1)\n\n# A tibble: 2 × 4\n  level model_1_variance model_2_variance variance_reduction\n  &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;              &lt;dbl&gt;\n1     1            0.580            0.580        -0.00000574\n2     2            0.655            0.354         0.459     \n\n\nIt looks like school SES explained a good bit of the level-2 intercept variability.\nBecause the school SES variable is a level-2 covariate, it cannot reduce level-1 residual variance."
  },
  {
    "objectID": "tutorials/RandomSlopes/random_slopes.html#formal-plot-for-reading1-model-1",
    "href": "tutorials/RandomSlopes/random_slopes.html#formal-plot-for-reading1-model-1",
    "title": "Random Slopes",
    "section": "Formal plot for Reading1 Model 1",
    "text": "Formal plot for Reading1 Model 1\nThe sjPlot::plot_model function is a quick and easy way to create models of fixed effects.\n\nsjPlot::plot_model(m_read_1_1, type = \"pred\", terms = \"school_ses\") +\n  coord_fixed(xlim = c(-2, 2),\n              ylim = c(-2, 2))"
  },
  {
    "objectID": "tutorials/RandomSlopes/random_slopes.html#overall-interpretation",
    "href": "tutorials/RandomSlopes/random_slopes.html#overall-interpretation",
    "title": "Random Slopes",
    "section": "Overall interpretation",
    "text": "Overall interpretation\nIt appears that schools vary greatly by their average reading_1 scores. Almost half of this variability is explained by the school’s average SES, which is positively associated with reading_1."
  },
  {
    "objectID": "tutorials/RandomSlopes/random_slopes.html#reading-2-model-0-random-intercepts",
    "href": "tutorials/RandomSlopes/random_slopes.html#reading-2-model-0-random-intercepts",
    "title": "Random Slopes",
    "section": "Reading 2 Model 0: Random Intercepts",
    "text": "Reading 2 Model 0: Random Intercepts\n\\[\n\\begin{align}\n\\textbf{Level 1:}\\\\\nReading_{ij2} &= \\beta_{0j}+e_{ij}\\\\\ne&\\sim \\mathcal{N}(0,\\sigma_e^2)\\\\\n\\textbf{Level 2:}\\\\\n\\beta_{0j}&=\\gamma_{00}+u_{0j}\\\\\nu_{0j}&\\sim \\mathcal{N}(0,\\tau_{00})\\\\\n\\textbf{Combined:}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+u_{0j}}_{\\beta_{0j}}+e_{ij}\n\\end{align}\n\\]\n\n\nCode\ntibble(\n  Symbol = c(\n    \"$Reading_{ij2}$\",\n    \"$\\\\beta_{0j}$\",\n    \"$e_{ij}$\",\n    \"$\\\\sigma_e^2$\",\n    \"$\\\\gamma_{00}$\",\n    \"$u_{0j}$\",\n    \"$\\\\tau_{00}$\"\n    \n  ),\n  Meaning = c(\n    \"Reading score for person $i$ in class $j$ at time 2\",\n    \"Random intercept for class $j$. In this model, it represents the reading~2~ classroom mean for class $j$\",\n    \"Level 1 prediction error for person $i$ in class $j$\",\n    \"Variance of $e_{ij}$\",\n    \"Fixed Intercept. In this model, it is the overall classroom mean for Time 2 Reading\",\n    \"School-level (L2) residual for the random intercept $\\\\beta_{0j}$. It is the deviation of classroom $j$, from the overall fixed intercept *&gamma;*~00~\",\n    \"Variance of $u_{0j}$\"),\n  level = paste0(\"Level \", c(1,1,1,1,2,2,2))\n) %&gt;%\n  gt(groupname_col = \"level\", row_group_as_column = T) %&gt;%\n  gt::tab_options(\n    column_labels.font.weight = \"bold\",\n    column_labels.font.size = 20,\n    footnotes.font.size = 16, table.border.bottom.color = \"white\") %&gt;% \n  gt::cols_width(Symbol ~ pct(15), level ~ pct(23)) %&gt;% \n  tab_row_group(label = md(\"$$\\\\begin{aligned}\n  \\\\textbf{Level 2}\\\\\\\\\n  \\\\beta_{0j}&=\\\\gamma_{00}+u_{0j}\\\\\\\\\n  u_{0j}&\\\\sim \\\\mathcal{N}(0,\\\\tau_{00})\n  \\\\end{aligned}$$\"), rows = level == \"Level 2\", id = \"l2\") %&gt;%  \n  tab_row_group(label = md(\"$$\\\\begin{align*}\n  \\\\textbf{Level 1}\\\\\\\\\nReading_{ij2} &= \\\\beta_{0j}+e_{ij}\\\\\\\\\ne_{ij}&\\\\sim \\\\mathcal{N}(0,\\\\sigma_e^2)\\\\end{align*}$$\"), rows = level == \"Level 1\", id = \"l1\") %&gt;% \n    fmt_markdown() %&gt;% \n  gt::cols_align(\"center\", \"level\") %&gt;% \n  gt::tab_footnote(footnote = md(\"$$\\\\begin{align*}\n\\\\textbf{Combined Model}\\\\\\\\\nReading_{ij2} &= \\\\underbrace{\\\\gamma_{00}+u_{0j}}_{\\\\beta_{0j}}+e_{ij}\n\\\\end{align*}$$\")) %&gt;% \ngt::tab_stubhead(\"Level\")\n\n\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Level\n      Symbol\n      Meaning\n    \n  \n  \n    $$\\begin{align*}\n  \\textbf{Level 1}\\\\\nReading_{ij2} &= \\beta_{0j}+e_{ij}\\\\\ne_{ij}&\\sim \\mathcal{N}(0,\\sigma_e^2)\\end{align*}$$\n\\(Reading_{ij2}\\)\nReading score for person \\(i\\) in class \\(j\\) at time 2\n    \\(\\beta_{0j}\\)\nRandom intercept for class \\(j\\). In this model, it represents the reading2 classroom mean for class \\(j\\)\n    \\(e_{ij}\\)\nLevel 1 prediction error for person \\(i\\) in class \\(j\\)\n    \\(\\sigma_e^2\\)\nVariance of \\(e_{ij}\\)\n    $$\\begin{aligned}\n  \\textbf{Level 2}\\\\\n  \\beta_{0j}&=\\gamma_{00}+u_{0j}\\\\\n  u_{0j}&\\sim \\mathcal{N}(0,\\tau_{00})\n  \\end{aligned}$$\n\\(\\gamma_{00}\\)\nFixed Intercept. In this model, it is the overall classroom mean for Time 2 Reading\n    \\(u_{0j}\\)\nSchool-level (L2) residual for the random intercept \\(\\beta_{0j}\\). It is the deviation of classroom \\(j\\), from the overall fixed intercept γ00\n    \\(\\tau_{00}\\)\nVariance of \\(u_{0j}\\)\n  \n  \n  \n    \n       $$\\begin{align*}\n\\textbf{Combined Model}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+u_{0j}}_{\\beta_{0j}}+e_{ij}\n\\end{align*}$$\n    \n  \n\n\n\n\n\nTable 4: Reading at Time 2 Model 0: Random Intercepts\n\n\n\n\n\n\nCode\np_read1 + \n  my_observed(label = my_observed_label(\"*Reading*~*2ij*~\"),\n              center = read_1@center, \n              fill = \"white\")\n\n\n\n\n\n\n\n\nFigure 6: Path Diagram for Reading Time 2 Random Intercepts Model\n\n\n\n\n\n\nm_read_2_0 &lt;- lmer(reading_2 ~ 1 + (1 | school_id), d)\nsummary(m_read_2_0)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_2 ~ 1 + (1 | school_id)\n   Data: d\n\nREML criterion at convergence: 22814\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-4.076 -0.644  0.003  0.642  4.417 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n school_id (Intercept) 0.800    0.895   \n Residual              0.187    0.432   \nNumber of obs: 18257, groups:  school_id, 300\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   0.9891     0.0518    19.1\n\nsjPlot::tab_model(m_read_2_0)\n\n\n\n \nreading 2\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n0.99\n0.89 – 1.09\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n0.19\n\n\n\nτ00 school_id\n0.80\n\n\nICC\n0.81\n\n\nN school_id\n300\n\nObservations\n18257\n\n\nMarginal R2 / Conditional R2\n0.000 / 0.811\n\n\n\n\nperformance(m_read_2_0)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n-----------------------------------------------------------------------------------\n22819.501 | 22819.502 | 22842.937 |      0.811 |      0.000 | 0.811 | 0.428 | 0.432\n\nparameters(m_read_2_0)\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |       95% CI | t(18254) |      p\n-------------------------------------------------------------------\n(Intercept) |        0.99 | 0.05 | [0.89, 1.09] |    19.11 | &lt; .001\n\n# Random Effects\n\nParameter                 | Coefficient\n---------------------------------------\nSD (Intercept: school_id) |        0.89\nSD (Residual)             |        0.43\n\nreport(m_read_2_0)\n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML\nand nloptwrap optimizer) to predict reading_2 (formula: reading_2 ~ 1). The\nmodel included school_id as random effect (formula: ~1 | school_id). The\nmodel's intercept is at 0.99 (95% CI [0.89, 1.09], t(18254) = 19.11, p &lt; .001).\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\n\nCheck model assumptions\n\ncheck_model(m_read_2_0)\n\n\n\n\n\n\n\n\nAll is well.\n\n\nCheck Intraclass correlation coefficient\n\nperformance::icc(m_read_2_0)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.811\n  Unadjusted ICC: 0.811\n\n\nThe ICC for this model suggests that schools are highly segregated by reading ability. Because there are no fixed effects in the random intercepts model, the adjusted and conditional ICC are the same.\n\n\nVariance Explained\n\nperformance::r2(m_read_2_0)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.811\n     Marginal R2: 0.000\n\n\nThe conditional \\(R^2\\) tells us the proportion of variance in the outcome variable explained by the grouping structure (i.e., the fact that each class has its own intercept) and the fixed effects. Because random intercepts models have no fixed effects, the conditional \\(R^2\\) and the ICC are the same.\nThe marginal \\(R^2\\) tells us the proportion of variance in the outcome variable explained by the fixed effects. For random intercepts models, the marginal \\(R^2\\) is 0 because there are no fixed effects."
  },
  {
    "objectID": "tutorials/RandomSlopes/random_slopes.html#reading-2-model-1-level-1-fixed-effects-reading1",
    "href": "tutorials/RandomSlopes/random_slopes.html#reading-2-model-1-level-1-fixed-effects-reading1",
    "title": "Random Slopes",
    "section": "Reading 2 Model 1: Level-1 Fixed Effects (Reading1)",
    "text": "Reading 2 Model 1: Level-1 Fixed Effects (Reading1)\nIn the prediction of reading_2, we have an important level-1 covariate—reading_1.\n\\[\n\\begin{align}\n\\textbf{Level 1:}\\\\\nReading_{ij2} &= \\beta_{0j}+\\beta_{1j}Reading_{ij1}+e_{ij}\\\\\ne&\\sim \\mathcal{N}(0,\\sigma_e^2)\\\\\n\\textbf{Level 2:}\\\\\n\\beta_{0j}&=\\gamma_{00}+u_{0j}\\\\\n\\beta_{1j}&=\\gamma_{10}\\\\\nu_{0j}&\\sim \\mathcal{N}(0,\\tau_{00})\\\\\n\\textbf{Combined:}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+u_{0j}}_{\\beta_{0j}}+\\underbrace{\\gamma_{10}}_{\\beta_{1j}}Reading_{ij1}+e_{ij}\\\\\n&\\text{Fixed vs. Random Effects}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{10}Reading_{ij1}}_{\\text{Fixed}}+\\underbrace{u_{0j}}_{\\text{L2 Random}}+e_{ij}\n\\end{align}\n\\]\n\n\nCode\ntibble(\n  Symbol = c(\n    \"$Reading_{ij2}$\",\n    \"$\\\\beta_{0j}$\",\n    \"$\\\\beta_{0j}$\",\n    \"$e_{ij}$\",\n    \"$\\\\sigma_e^2$\",\n    \"$\\\\gamma_{00}$\",\n    \"$\\\\gamma_{10}$\",\n    \"$u_{0j}$\",\n    \"$\\\\tau_{00}$\"\n    \n  ),\n  Meaning = c(\n    \"Reading score for person $i$ in class $j$ at time 2\",\n    \"Random intercept for class $j$. In this model, it represents the reading~2~ classroom mean for class $j$ when reading~1~ is 0.\",\n    \"Slope for reading~1~ for class $j$. In this model, however, there are no random slopes. Thus, the slope for every classroom is the same: $\\\\beta_{1j} = \\\\gamma_{10}$.\",\n    \"Level 1 prediction error for person $i$ in class $j$\",\n    \"Variance of $e_{ij}$\",\n    \"Fixed Intercept. In this model, it is the predicted Time 2 Reading when reading~1~ is zero.\",\n    \"Fixed slope for reading~1~. When reading~1~ increates by 1, reading~2~ is expected to increase by this amount.\",\n    \"School-level (L2) residual for the random intercept $\\\\beta_{0j}$. It is the deviation of classroom $j$, from the overall fixed intercept *&gamma;*~00~\",\n    \"Variance of $u_{0j}$\"),\n  level = paste0(\"Level \", c(1,1,1,1,1,2,2,2,2))\n) %&gt;%\n  gt(groupname_col = \"level\", row_group_as_column = T) %&gt;%\n  gt::tab_options(\n    column_labels.font.weight = \"bold\",\n    column_labels.font.size = 20,\n    footnotes.font.size = 16, table.border.bottom.color = \"white\") %&gt;% \n  gt::cols_width(Symbol ~ pct(15), level ~ pct(23)) %&gt;% \n  tab_row_group(label = md(\"$$\\\\begin{aligned}\n  \\\\textbf{Level 2}\\\\\\\\\n  \\\\beta_{0j}&=\\\\gamma_{00}+u_{0j}\\\\\\\\\n  \\\\beta_{1j}&=\\\\gamma_{10}\\\\\\\\\n  u_{0j}&\\\\sim \\\\mathcal{N}(0,\\\\tau_{00})\n  \\\\end{aligned}$$\"), rows = level == \"Level 2\", id = \"l2\") %&gt;%  \n  tab_row_group(label = md(\"$$\\\\begin{align*}\n  \\\\textbf{Level 1}\\\\\\\\\nReading_{ij2} &= \\\\beta_{0j}+\\\\beta_{1j}Reading_{ij1}+e_{ij}\\\\\\\\\ne_{ij}&\\\\sim \\\\mathcal{N}(0,\\\\sigma_e^2)\\\\end{align*}$$\"), rows = level == \"Level 1\", id = \"l1\") %&gt;% \n    fmt_markdown() %&gt;% \n  gt::cols_align(\"center\", \"level\") %&gt;% \n  gt::tab_footnote(footnote = md(r\"($$\\begin{align*}\n\\textbf{Combined:}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+u_{0j}}_{\\beta_{0j}}+\\underbrace{\\gamma_{10}}_{\\beta_{1j}}Reading_{ij1}+e_{ij}\\\\\n&\\text{Fixed vs. Random Effects}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{10}Reading_{ij1}}_{\\text{Fixed}}+\\underbrace{u_{0j}}_{\\text{L2 Random}}+e_{ij}\n\\end{align*}$$)\")) %&gt;% \ngt::tab_stubhead(\"Level\")\n\n\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Level\n      Symbol\n      Meaning\n    \n  \n  \n    $$\\begin{align*}\n  \\textbf{Level 1}\\\\\nReading_{ij2} &= \\beta_{0j}+\\beta_{1j}Reading_{ij1}+e_{ij}\\\\\ne_{ij}&\\sim \\mathcal{N}(0,\\sigma_e^2)\\end{align*}$$\n\\(Reading_{ij2}\\)\nReading score for person \\(i\\) in class \\(j\\) at time 2\n    \\(\\beta_{0j}\\)\nRandom intercept for class \\(j\\). In this model, it represents the reading2 classroom mean for class \\(j\\) when reading1 is 0.\n    \\(\\beta_{0j}\\)\nSlope for reading1 for class \\(j\\). In this model, however, there are no random slopes. Thus, the slope for every classroom is the same: \\(\\beta_{1j} = \\gamma_{10}\\).\n    \\(e_{ij}\\)\nLevel 1 prediction error for person \\(i\\) in class \\(j\\)\n    \\(\\sigma_e^2\\)\nVariance of \\(e_{ij}\\)\n    $$\\begin{aligned}\n  \\textbf{Level 2}\\\\\n  \\beta_{0j}&=\\gamma_{00}+u_{0j}\\\\\n  \\beta_{1j}&=\\gamma_{10}\\\\\n  u_{0j}&\\sim \\mathcal{N}(0,\\tau_{00})\n  \\end{aligned}$$\n\\(\\gamma_{00}\\)\nFixed Intercept. In this model, it is the predicted Time 2 Reading when reading1 is zero.\n    \\(\\gamma_{10}\\)\nFixed slope for reading1. When reading1 increates by 1, reading2 is expected to increase by this amount.\n    \\(u_{0j}\\)\nSchool-level (L2) residual for the random intercept \\(\\beta_{0j}\\). It is the deviation of classroom \\(j\\), from the overall fixed intercept γ00\n    \\(\\tau_{00}\\)\nVariance of \\(u_{0j}\\)\n  \n  \n  \n    \n       $$\\begin{align*}\n\\textbf{Combined:}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+u_{0j}}_{\\beta_{0j}}+\\underbrace{\\gamma_{10}}_{\\beta_{1j}}Reading_{ij1}+e_{ij}\\\\\n&\\text{Fixed vs. Random Effects}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{10}Reading_{ij1}}_{\\text{Fixed}}+\\underbrace{u_{0j}}_{\\text{L2 Random}}+e_{ij}\n\\end{align*}$$\n    \n  \n\n\n\n\n\nTable 5: Reading at Time 2 Model 1: Model 0 + L1 Covariate (Reading1)\n\n\n\n\n\n\nCode\np_read2L1 &lt;- ggdiagram(myfont, font_size = 16) +\n  {read_2 &lt;- my_observed(\n    label = my_observed_label(\"*Reading*~*2ij*~\"))} +\n  {read_1 &lt;- my_observed(\n    label = my_observed_label(\"*Reading*~*1ij*~\")) %&gt;% \n    place(read_2, \"left\", 5)} +\n  {i_1 &lt;- my_intercept(label = \"1~1~\") %&gt;% \n    place(read_1, \"above\", 2.9)} +\n  {i12read2 &lt;- my_path(i_1, read_2)} +\n  {r1r2 &lt;- my_path(read_1, read_2)} +\n  {b_0j &lt;- my_coefficient(\n      center = midpoint(i12read2, position = .25),\n      label = \"*&beta;*~0*j*~\"\n    )} +\n  {b_1j &lt;- my_coefficient(\n      center = midpoint(r1r2, position = .65),\n      label = \"*&beta;*~1*j*~\"\n    )} +\n  {i_2 &lt;- my_intercept(\n    label = \"1~2~\", \n    center = intersection(\n      rotate(i12read2@line, \n             theta = turn(-.25), \n             origin = b_0j@center),\n      ob_line(xintercept = b_1j@center@x)))} +\n  {i22b0j &lt;- my_path(\n      i_2,\n      b_0j,\n      label = my_path_label(\n        \"*&gamma;*~00~\")\n  )} +\n  {i22b1j &lt;- my_path(\n      i_2,\n      b_1j,\n      label = my_path_label(\n        \"*&gamma;*~10~\")\n    )} +\n  {e &lt;- my_error(label = \"*e*~*ij*~\", radius = .5) %&gt;% \n    place(read_2, \"right\", .6)} +\n  my_path(e, read_2) +\n  {e_var &lt;- my_error_variance(e)} +\n  {u_0j &lt;- my_residual(label = my_residual_label(\"*u*~0*j*~\")) %&gt;% \n    place(b_0j, i12read2@line@angle - turn(.25), .6)} +\n  my_residual_variance(\n    u_0j,\n    i12read2@line@angle - turn(.25),\n    label = my_residual_variance_label(\n      \"*&tau;*~00~\")\n  ) +\n  my_path(u_0j, b_0j) +\n  my_error_variance_label(center = e_var@midpoint()) \n\np_read2L1\n\n\n\n\n\n\n\n\nFigure 7: Reading Time 2 Random Intercepts Model + Level 1 Covariate\n\n\n\n\n\nNote that \\(\\beta_{1j}\\) could have been random, but it is equal to the constant \\(\\gamma_{10}\\) in this model.\n\nExploratory Plot for Reading1 predicting Reading2\nIt helps to get an exploratory plot to see what is happening. This is not the model 1 plot. I am letting the data speak.\nThe schools with the lighter (yellower) regression lines have higher SES.\n\nd %&gt;%\n  ggplot(aes(reading_1, reading_2, color = school_ses)) +\n  geom_point(size = 0.5,\n             alpha = 0.3,\n             shape = 16) +\n  geom_smooth(\n    aes(color = school_ses, group = school_id),\n    method = \"lm\",\n    se = F,\n    linewidth = 0.25,\n    alpha = 0.5\n  ) +\n  geom_smooth(color = \"black\",\n              method = \"lm\",\n              se = F) +\n  scale_color_viridis_c(\"School\\nSES\") +\n  labs(x = \"Reading (Time 1)\", y = \"Reading (Time 2)\", caption = \"\") \n\n\n\n\n\n\n\n\nInteresting…\nTo my eye, it looks like reading_1 and reading_2 are correlated, but less so in schools with high SES.\nLet’s model it formally. First, we add the L1 covariate reading_1 to our model.\n\n\nModel Summary\n\nm_read_2_1 &lt;- update(m_read_2_0, . ~ . + reading_1)\nsummary(m_read_2_1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_2 ~ (1 | school_id) + reading_1\n   Data: d\n\nREML criterion at convergence: 15765\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-4.959 -0.670  0.005  0.673  4.094 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n school_id (Intercept) 0.549    0.741   \n Residual              0.127    0.356   \nNumber of obs: 18257, groups:  school_id, 300\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.99108    0.04287    23.1\nreading_1    0.42342    0.00456    92.8\n\nCorrelation of Fixed Effects:\n          (Intr)\nreading_1 0.000 \n\nreport(m_read_2_1)\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading_2 with reading_1 (formula: reading_2 ~ reading_1). The model\nincluded school_id as random effect (formula: ~1 | school_id). The model's\ntotal explanatory power is substantial (conditional R2 = 0.85) and the part\nrelated to the fixed effects alone (marginal R2) is of 0.21. The model's\nintercept, corresponding to reading_1 = 0, is at 0.99 (95% CI [0.91, 1.08],\nt(18253) = 23.12, p &lt; .001). Within this model:\n\n  - The effect of reading 1 is statistically significant and positive (beta =\n0.42, 95% CI [0.41, 0.43], t(18253) = 92.82, p &lt; .001; Std. beta = 0.42, 95% CI\n[0.41, 0.43])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\nperformance(m_read_2_1)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n-----------------------------------------------------------------------------------\n15773.337 | 15773.339 | 15804.586 |      0.852 |      0.210 | 0.812 | 0.353 | 0.356\n\nparameters(m_read_2_1)\n\n# Fixed Effects\n\nParameter   | Coefficient |       SE |       95% CI | t(18253) |      p\n-----------------------------------------------------------------------\n(Intercept) |        0.99 |     0.04 | [0.91, 1.08] |    23.12 | &lt; .001\nreading 1   |        0.42 | 4.56e-03 | [0.41, 0.43] |    92.82 | &lt; .001\n\n# Random Effects\n\nParameter                 | Coefficient\n---------------------------------------\nSD (Intercept: school_id) |        0.74\nSD (Residual)             |        0.36\n\n\nClearly, reading_1 is a significant covariate.\n\n\nCompare model fit\n\ncompare_models(m_read_2_0, m_read_2_1)\n\nParameter    |        m_read_2_0 |        m_read_2_1\n----------------------------------------------------\n(Intercept)  | 0.99 (0.89, 1.09) | 0.99 (0.91, 1.08)\nreading 1    |                   | 0.42 (0.41, 0.43)\n----------------------------------------------------\nObservations |             18257 |             18257\n\ncompare_performance(m_read_2_0, m_read_2_1)\n\n# Comparison of Model Performance Indices\n\nName       |   Model |   AIC (weights) |  AICc (weights) |   BIC (weights)\n--------------------------------------------------------------------------\nm_read_2_0 | lmerMod | 22815.4 (&lt;.001) | 22815.4 (&lt;.001) | 22838.9 (&lt;.001)\nm_read_2_1 | lmerMod | 15759.9 (&gt;.999) | 15759.9 (&gt;.999) | 15791.2 (&gt;.999)\n\nName       | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n------------------------------------------------------------\nm_read_2_0 |      0.811 |      0.000 | 0.811 | 0.428 | 0.432\nm_read_2_1 |      0.852 |      0.210 | 0.812 | 0.353 | 0.356\n\ntest_likelihoodratio(m_read_2_0, m_read_2_1)\n\n# Likelihood-Ratio-Test (LRT) for Model Comparison (ML-estimator)\n\nName       |   Model | df | df_diff |    Chi2 |      p\n------------------------------------------------------\nm_read_2_0 | lmerMod |  3 |         |         |       \nm_read_2_1 | lmerMod |  4 |       1 | 7057.48 | &lt; .001\n\ntest_performance(m_read_2_0, m_read_2_1)\n\nName       |   Model |     BF | df | df_diff |    Chi2 |      p\n---------------------------------------------------------------\nm_read_2_0 | lmerMod |        |  3 |         |         |       \nm_read_2_1 | lmerMod | &gt; 1000 |  4 |    1.00 | 7057.48 | &lt; .001\nModels were detected as nested (in terms of fixed parameters) and are compared in sequential order.\n\n\nUsing reading_1 as an L1 covariate, the model fits significantly better.\n\n\nCheck the variance explained\n\nperformance::r2(m_read_2_1)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.852\n     Marginal R2: 0.210\n\n\nIn this model, the reading_1 covariate explained 0.21 of variance in reading_2.\nWe can also see how much variance was reduced in the levels 1 and 2 residuals.\n\nvariance_reduction(m_read_2_0, m_read_2_1)\n\n# A tibble: 2 × 4\n  level model_1_variance model_2_variance variance_reduction\n  &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;              &lt;dbl&gt;\n1     1            0.432            0.356              0.176\n2     2            0.800            0.549              0.314\n\n\nIt looks like reading_1 explained substantial amounts of variance in residuals at both levels.\n\n\nFormal plot for model 1\nQuick plot of fixed effects only:\n\nsjPlot::plot_model(m_read_2_1, type = \"pred\", terms = \"reading_1\")\n\n\n\n\n\n\n\n\nTo see all the data with the separate regression lines:\n\nbroom.mixed::augment(m_read_2_1) %&gt;%\n  ggplot(aes(reading_1, reading_2, color = school_id)) +\n  geom_point(size = 0.1, alpha = 0.3) +\n  geom_line(aes(y = .fitted), linewidth = 0.1) +\n  geom_line(aes(y = .fixed), linewidth = 1, color = \"black\") +\n  scale_color_viridis_d() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nNote that all the regression lines are parallel. Why? Because we did not allow them to differ."
  },
  {
    "objectID": "tutorials/RandomSlopes/random_slopes.html#reading-2-model-2-level-2-fixed-effects",
    "href": "tutorials/RandomSlopes/random_slopes.html#reading-2-model-2-level-2-fixed-effects",
    "title": "Random Slopes",
    "section": "Reading 2 Model 2: Level-2 Fixed Effects",
    "text": "Reading 2 Model 2: Level-2 Fixed Effects\nLet’s add school_ses as a level-2 covariate. In essence, we are trying to explain why the L2 random intercept \\((\\beta_{0j})\\) differs from school to school: \\(\\beta_{0j}=\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}\\)\nHere is the complete model:\n\\[\n\\begin{align}\n\\textbf{Level 1:}\\\\\nReading_{ij2} &= \\beta_{0j}+\\beta_{1j}Reading_{ij1}+e_{ij}\\\\\ne&\\sim \\mathcal{N}(0,\\sigma_e^2)\\\\\n\\textbf{Level 2:}\\\\\n\\beta_{0j}&=\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}\\\\\n\\beta_{1j}&=\\gamma_{10}\\\\\nu_{0j}&\\sim \\mathcal{N}(0,\\tau_{00})\\\\\n\\textbf{Combined:}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}}_{\\beta_{0j}}+\\underbrace{\\gamma_{10}}_{\\beta_{1j}}Reading_{ij1}+e_{ij}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+\\gamma_{10}Reading_{ij1}}_{\\text{Fixed}}+\\underbrace{u_{0j}}_{\\text{Random}}+e_{ij}\n\\end{align}\n\\]\n\n\nCode\ntibble(\n  Symbol = c(\n    \"$Reading_{ij2}$\",\n    \"$\\\\beta_{0j}$\",\n    \"$\\\\beta_{0j}$\",\n    \"$e_{ij}$\",\n    \"$\\\\sigma_e^2$\",\n    \"$\\\\gamma_{00}$\",\n    \"$\\\\gamma_{01}$\",\n    \"$\\\\gamma_{10}$\",\n    \"$u_{0j}$\",\n    \"$\\\\tau_{00}$\"\n    \n  ),\n  Meaning = c(\n    \"Reading score for person $i$ in class $j$ at time 2\",\n    \"Random intercept for class $j$. In this model, it represents the reading~2~ classroom mean for class $j$ when reading~1~ is 0.\",\n    \"Slope for reading~1~ for class $j$. In this model, however, there are no random slopes. Thus, the slope for every classroom is the same: $\\\\beta_{1j} = \\\\gamma_{10}$.\",\n    \"Level 1 prediction error for person $i$ in class $j$\",\n    \"Variance of $e_{ij}$\",\n    \"Fixed Intercept. In this model, it is the predicted Time 2 Reading when reading~1~ is zero.\",\n    \"Fixed slope for SES. When SES increates by 1, reading~2~ is expected to increase by this amount.\",\n    \"Fixed slope for reading~1~. When reading~1~ increates by 1, reading~2~ is expected to increase by this amount.\",\n    \"School-level (L2) residual for the random intercept $\\\\beta_{0j}$. It is the deviation of classroom $j$, from the overall fixed intercept *&gamma;*~00~ when SES is zero.\",\n    \"Variance of $u_{0j}$\"),\n  level = paste0(\"Level \", c(1,1,1,1,1,2,2,2,2,2))\n) %&gt;%\n  gt(groupname_col = \"level\", row_group_as_column = T) %&gt;%\n  gt::tab_options(\n    column_labels.font.weight = \"bold\",\n    column_labels.font.size = 20,\n    footnotes.font.size = 16, table.border.bottom.color = \"white\") %&gt;% \n  gt::cols_width(Symbol ~ pct(15), level ~ pct(23)) %&gt;% \n  tab_row_group(label = md(\"$$\\\\begin{aligned}\n  \\\\textbf{Level 2}\\\\\\\\\n  \\\\beta_{0j}&=\\\\gamma_{00}+\\\\gamma_{01}SES_j+u_{0j}\\\\\\\\\n  \\\\beta_{1j}&=\\\\gamma_{10}\\\\\\\\\n  u_{0j}&\\\\sim \\\\mathcal{N}(0,\\\\tau_{00})\n  \\\\end{aligned}$$\"), rows = level == \"Level 2\", id = \"l2\") %&gt;%  \n  tab_row_group(label = md(\"$$\\\\begin{align*}\n  \\\\textbf{Level 1}\\\\\\\\\nReading_{ij2} &= \\\\beta_{0j}+\\\\beta_{1j}Reading_{ij1}+e_{ij}\\\\\\\\\ne_{ij}&\\\\sim \\\\mathcal{N}(0,\\\\sigma_e^2)\\\\end{align*}$$\"), rows = level == \"Level 1\", id = \"l1\") %&gt;% \n    fmt_markdown() %&gt;% \n  gt::cols_align(\"center\", \"level\") %&gt;% \n  gt::tab_footnote(footnote = md(r\"($$\\begin{align*}\n\\textbf{Combined:}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}}_{\\beta_{0j}}+\\underbrace{\\gamma_{10}}_{\\beta_{1j}}Reading_{ij1}+e_{ij}\\\\\n&\\text{Fixed vs. Random Effects}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+\\gamma_{10}Reading_{ij1}}_{\\text{Fixed}}+\\underbrace{u_{0j}}_{\\text{L2 Random}}+e_{ij}\n\\end{align*}$$)\")) %&gt;% \ngt::tab_stubhead(\"Level\")\n\n\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Level\n      Symbol\n      Meaning\n    \n  \n  \n    $$\\begin{align*}\n  \\textbf{Level 1}\\\\\nReading_{ij2} &= \\beta_{0j}+\\beta_{1j}Reading_{ij1}+e_{ij}\\\\\ne_{ij}&\\sim \\mathcal{N}(0,\\sigma_e^2)\\end{align*}$$\n\\(Reading_{ij2}\\)\nReading score for person \\(i\\) in class \\(j\\) at time 2\n    \\(\\beta_{0j}\\)\nRandom intercept for class \\(j\\). In this model, it represents the reading2 classroom mean for class \\(j\\) when reading1 is 0.\n    \\(\\beta_{0j}\\)\nSlope for reading1 for class \\(j\\). In this model, however, there are no random slopes. Thus, the slope for every classroom is the same: \\(\\beta_{1j} = \\gamma_{10}\\).\n    \\(e_{ij}\\)\nLevel 1 prediction error for person \\(i\\) in class \\(j\\)\n    \\(\\sigma_e^2\\)\nVariance of \\(e_{ij}\\)\n    $$\\begin{aligned}\n  \\textbf{Level 2}\\\\\n  \\beta_{0j}&=\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}\\\\\n  \\beta_{1j}&=\\gamma_{10}\\\\\n  u_{0j}&\\sim \\mathcal{N}(0,\\tau_{00})\n  \\end{aligned}$$\n\\(\\gamma_{00}\\)\nFixed Intercept. In this model, it is the predicted Time 2 Reading when reading1 is zero.\n    \\(\\gamma_{01}\\)\nFixed slope for SES. When SES increates by 1, reading2 is expected to increase by this amount.\n    \\(\\gamma_{10}\\)\nFixed slope for reading1. When reading1 increates by 1, reading2 is expected to increase by this amount.\n    \\(u_{0j}\\)\nSchool-level (L2) residual for the random intercept \\(\\beta_{0j}\\). It is the deviation of classroom \\(j\\), from the overall fixed intercept γ00 when SES is zero.\n    \\(\\tau_{00}\\)\nVariance of \\(u_{0j}\\)\n  \n  \n  \n    \n       $$\\begin{align*}\n\\textbf{Combined:}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}}_{\\beta_{0j}}+\\underbrace{\\gamma_{10}}_{\\beta_{1j}}Reading_{ij1}+e_{ij}\\\\\n&\\text{Fixed vs. Random Effects}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+\\gamma_{10}Reading_{ij1}}_{\\text{Fixed}}+\\underbrace{u_{0j}}_{\\text{L2 Random}}+e_{ij}\n\\end{align*}$$\n    \n  \n\n\n\n\n\nTable 6: Reading at Time 2 Model 1: Model 1 + L2 Covariate (SES)\n\n\n\n\n\n\nCode\np_read2L1L2 &lt;- p_read2L1 +\n  {SES &lt;- my_observed_l2(\n    center = b_0j %-|% read_2,\n    label = my_observed_label(\"*SES*~*j*~\", vjust = .6))} + \n  my_path(SES, b_0j, label = my_path_label(\"*&gamma;*~01~\", position = .4))\n\np_read2L1L2\n\n\n\n\n\n\n\n\nFigure 8: Reading Time 2 Model with Level 1 and Level 2 Covariates\n\n\n\n\n\nCreating the entire model from scratch:\n\nm_read_2_2 &lt;- lmer(reading_2 ~ 1 + school_ses + reading_1 + (1 | school_id), d)\n\nHowever, updating from a previous model is generally safer:\n\nm_read_2_2 &lt;- update(m_read_2_1, . ~ . + school_ses)\n\n\nsummary(m_read_2_2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_2 ~ (1 | school_id) + reading_1 + school_ses\n   Data: d\n\nREML criterion at convergence: 15496\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-4.933 -0.667  0.005  0.673  4.072 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n school_id (Intercept) 0.218    0.467   \n Residual              0.127    0.356   \nNumber of obs: 18257, groups:  school_id, 300\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.99104    0.02709    36.6\nreading_1    0.41722    0.00456    91.6\nschool_ses   0.57772    0.02725    21.2\n\nCorrelation of Fixed Effects:\n           (Intr) rdng_1\nreading_1   0.001       \nschool_ses  0.000 -0.092\n\nperformance(m_read_2_2)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n-----------------------------------------------------------------------------------\n15506.071 | 15506.075 | 15545.133 |      0.888 |      0.696 | 0.632 | 0.353 | 0.356\n\nparameters(m_read_2_2)\n\n# Fixed Effects\n\nParameter   | Coefficient |       SE |       95% CI | t(18252) |      p\n-----------------------------------------------------------------------\n(Intercept) |        0.99 |     0.03 | [0.94, 1.04] |    36.58 | &lt; .001\nreading 1   |        0.42 | 4.56e-03 | [0.41, 0.43] |    91.58 | &lt; .001\nschool ses  |        0.58 |     0.03 | [0.52, 0.63] |    21.20 | &lt; .001\n\n# Random Effects\n\nParameter                 | Coefficient\n---------------------------------------\nSD (Intercept: school_id) |        0.47\nSD (Residual)             |        0.36\n\nreport(m_read_2_2)\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading_2 with reading_1 and school_ses (formula: reading_2 ~\nreading_1 + school_ses). The model included school_id as random effect\n(formula: ~1 | school_id). The model's total explanatory power is substantial\n(conditional R2 = 0.89) and the part related to the fixed effects alone\n(marginal R2) is of 0.70. The model's intercept, corresponding to reading_1 = 0\nand school_ses = 0, is at 0.99 (95% CI [0.94, 1.04], t(18252) = 36.58, p &lt;\n.001). Within this model:\n\n  - The effect of reading 1 is statistically significant and positive (beta =\n0.42, 95% CI [0.41, 0.43], t(18252) = 91.58, p &lt; .001; Std. beta = 0.42, 95% CI\n[0.41, 0.43])\n  - The effect of school ses is statistically significant and positive (beta =\n0.58, 95% CI [0.52, 0.63], t(18252) = 21.20, p &lt; .001; Std. beta = 0.58, 95% CI\n[0.53, 0.64])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\nCompare m_read_2_1 and m_read_2_2 with the anova function.\n\ncompare_models(m_read_2_1, m_read_2_2)\n\nParameter    |        m_read_2_1 |        m_read_2_2\n----------------------------------------------------\n(Intercept)  | 0.99 (0.91, 1.08) | 0.99 (0.94, 1.04)\nreading 1    | 0.42 (0.41, 0.43) | 0.42 (0.41, 0.43)\nschool ses   |                   | 0.58 (0.52, 0.63)\n----------------------------------------------------\nObservations |             18257 |             18257\n\nanova(m_read_2_1, m_read_2_2)\n\nData: d\nModels:\nm_read_2_1: reading_2 ~ (1 | school_id) + reading_1\nm_read_2_2: reading_2 ~ (1 | school_id) + reading_1 + school_ses\n           npar   AIC   BIC logLik -2*log(L) Chisq Df Pr(&gt;Chisq)    \nm_read_2_1    4 15760 15791  -7876     15752                        \nm_read_2_2    5 15486 15525  -7738     15476   276  1     &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncompare_performance(m_read_2_1, m_read_2_2)\n\n# Comparison of Model Performance Indices\n\nName       |   Model |   AIC (weights) |  AICc (weights) |   BIC (weights)\n--------------------------------------------------------------------------\nm_read_2_1 | lmerMod | 15759.9 (&lt;.001) | 15759.9 (&lt;.001) | 15791.2 (&lt;.001)\nm_read_2_2 | lmerMod | 15486.4 (&gt;.999) | 15486.4 (&gt;.999) | 15525.4 (&gt;.999)\n\nName       | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n------------------------------------------------------------\nm_read_2_1 |      0.852 |      0.210 | 0.812 | 0.353 | 0.356\nm_read_2_2 |      0.888 |      0.696 | 0.632 | 0.353 | 0.356\n\ntest_performance(m_read_2_1, m_read_2_2)\n\nName       |   Model |     BF | df | df_diff |   Chi2 |      p\n--------------------------------------------------------------\nm_read_2_1 | lmerMod |        |  4 |         |        |       \nm_read_2_2 | lmerMod | &gt; 1000 |  5 |    1.00 | 275.56 | &lt; .001\nModels were detected as nested (in terms of fixed parameters) and are compared in sequential order.\n\n\nA quick plot of the model’s fixed effects form the sjPlot package:\n\nsjPlot::plot_model(m_read_2_2, type = \"pred\", terms = c(\"reading_1\", \"school_ses\"))\n\n\n\n\n\n\n\n\nThe first term (reading_1) is on the x-axis. The second term (school_ses) is plotted at its mean and plus or minus 1 standard deviation.\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nIs school_ses a significant predictor?\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nComparing Models 1 and 2, by how much did school_ses reduce L2 variability?\n\n\n\n\n\n\n\n\nTipHint (click to see)\n\n\n\n\n\nUse the variance_reduction function defined earlier:\n\nvariance_reduction(m_read_2_1, m_read_2_2)"
  },
  {
    "objectID": "tutorials/RandomSlopes/random_slopes.html#reading-2-model-3-model-2-random-slopes-uncorrelated-with-random-intercepts",
    "href": "tutorials/RandomSlopes/random_slopes.html#reading-2-model-3-model-2-random-slopes-uncorrelated-with-random-intercepts",
    "title": "Random Slopes",
    "section": "Reading 2 Model 3 = Model 2 + Random slopes uncorrelated with random intercepts",
    "text": "Reading 2 Model 3 = Model 2 + Random slopes uncorrelated with random intercepts\n\\[\n\\begin{align}\n\\textbf{Level 1:}\\\\\nReading_{ij2} &= \\beta_{0j}+\\beta_{1j}Reading_{ij1}+e_{ij}\\\\\ne&\\sim \\mathcal{N}(0,\\sigma_e^2)\\\\\n\\textbf{Level 2:}\\\\\n\\beta_{0j}&=\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}\\\\\n\\beta_{1j}&=\\gamma_{10}+u_{1j}\\\\\n\\boldsymbol{u}&=\\begin{bmatrix}\nu_{0j}\\\\\nu_{1j}\n\\end{bmatrix}\\sim \\mathcal{N}\\left(\\begin{bmatrix}\n0\\\\\n0\n\\end{bmatrix}\n\\begin{matrix}\n\\\\\n,\n\\end{matrix}\n\\begin{bmatrix}\n\\tau_{00} &\\\\\n0 & \\tau_{11}\n\\end{bmatrix}\\right)\\\\\n\\textbf{Combined:}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}}_{\\beta_{0j}}+(\\underbrace{\\gamma_{10}+u_{1j}}_{\\beta_{1j}})Reading_{ij1}+e_{ij}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+\\gamma_{10}Reading_{ij1}}_{\\text{Fixed Effects}}+\\underbrace{u_{0j}+u_{1j}Reading_{ij1}}_{\\text{Random Effects}}+e_{ij}\n\\end{align}\n\\]\nIn the equations above, how can you tell that the slopes and intercepts are uncorrelated? Look at the tau matrix:\n\\[\n\\boldsymbol{\\tau}=\\begin{bmatrix}\\tau_{00}&\\\\ 0 & \\tau_{11}\\end{bmatrix}\n\\]\nHere we see that the lower right corner, the covariance between \\(u_{0j}\\) and \\(u_{1j}\\), \\(\\tau_{10}\\), is zero. Because the matrix is symmetric, I omitted the 0 in the upper right position.\n\n\nCode\ntibble(\n  Symbol = c(\n    \"$Reading_{ij2}$\",\n    \"$\\\\beta_{0j}$\",\n    \"$\\\\beta_{0j}$\",\n    \"$e_{ij}$\",\n    \"$\\\\sigma_e^2$\",\n    \"$\\\\gamma_{00}$\",\n    \"$\\\\gamma_{01}$\",\n    \"$\\\\gamma_{10}$\",\n    \"$\\\\boldsymbol{u}$\",\n    \"$u_{0j}$\",\n    \"$u_{1j}$\",\n    \"$\\\\tau_{00}$\",\n    \"$\\\\tau_{11}$\"\n  ),\n  Meaning = c(\n    \"Reading score for person $i$ in class $j$ at time 2\",\n    \"Random intercept for class $j$. In this model, it represents the reading~2~ classroom mean for class $j$ when Reading~1~ is 0.\",\n    \"Slope for reading~1~ for class $j$.\",\n    \"Level 1 prediction error for person $i$ in class $j$\",\n    \"Variance of $e_{ij}$\",\n    \"Fixed Intercept. In this model, it is the predicted Time 2 Reading when Reading~1~ is zero.\",\n    \"Fixed slope for SES. When SES increates by 1, Reading~2~ is expected to increase by this amount.\",\n    \"Fixed slope for Reading~1~. When Reading~1~ increates by 1, Reading~2~ is expected to increase by this amount.\",\n    \"Vector of L2 residuals\",\n    \"School-level (L2) residual for the random intercept $\\\\beta_{0j}$.\",\n    \"School-level (L2) residual for the random slope $\\\\beta_{1j}$ for Reading~1~.\",\n    \"Variance of $u_{0j}$\",\n    \"Variance of $u_{1j}$\"),\n  level = paste0(\"Level \", c(1,1,1,1,1,2,2, 2,2,2,2,2,2))\n) %&gt;%\n  gt(groupname_col = \"level\", row_group_as_column = T) %&gt;%\n  gt::tab_options(\n    column_labels.font.weight = \"bold\",\n    column_labels.font.size = 20,\n    footnotes.font.size = 16, table.border.bottom.color = \"white\") %&gt;% \n  gt::cols_width(Symbol ~ pct(15), level ~ pct(23)) %&gt;% \n  tab_row_group(label = md(r\"($$\\begin{aligned}\n  \\textbf{Level 2}\\\\\n  \\beta_{0j}&=\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}\\\\\n  \\beta_{1j}&=\\gamma_{10}\\\\\n  \\boldsymbol{u}&=\\begin{bmatrix}\nu_{0j}\\\\\nu_{1j}\n\\end{bmatrix}\\sim \\mathcal{N}\\left(\\begin{bmatrix}\n0\\\\\n0\n\\end{bmatrix}\n\\begin{matrix}\n\\\\\n,\n\\end{matrix}\n\\begin{bmatrix}\n\\tau_{00} &\\\\\n0 & \\tau_{11}\n\\end{bmatrix}\\right)\n  \\end{aligned}$$)\"), rows = level == \"Level 2\", id = \"l2\") %&gt;%  \n  tab_row_group(label = md(r\"($$\\begin{align*}\n  \\textbf{Level 1}\\\\\nReading_{ij2} &= \\beta_{0j}+\\beta_{1j}Reading_{ij1}+e_{ij}\\\\\ne_{ij}&\\sim \\mathcal{N}(0,\\sigma_e^2)\\end{align*}$$)\"), rows = level == \"Level 1\", id = \"l1\") %&gt;% \n    fmt_markdown() %&gt;% \n  gt::cols_align(\"center\", \"level\") %&gt;% \n  gt::tab_footnote(footnote = md(r\"($$\\begin{align*}\n\\textbf{Combined:}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}}_{\\beta_{0j}}+(\\underbrace{\\gamma_{10}+u_{1j}}_{\\beta_{1j}})Reading_{ij1}+e_{ij}\\\\\n&\\text{Fixed vs. Random Effects}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+\\gamma_{10}Reading_{ij1}}_{\\text{Fixed Effects}}+\\underbrace{u_{0j}+u_{1j}Reading_{ij1}}_{\\text{Random Effects}}+e_{ij}\n\\end{align*}$$)\")) %&gt;% \ngt::tab_stubhead(\"Level\")\n\n\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Level\n      Symbol\n      Meaning\n    \n  \n  \n    $$\\begin{align*}\n  \\textbf{Level 1}\\\\\nReading_{ij2} &= \\beta_{0j}+\\beta_{1j}Reading_{ij1}+e_{ij}\\\\\ne_{ij}&\\sim \\mathcal{N}(0,\\sigma_e^2)\\end{align*}$$\n\\(Reading_{ij2}\\)\nReading score for person \\(i\\) in class \\(j\\) at time 2\n    \\(\\beta_{0j}\\)\nRandom intercept for class \\(j\\). In this model, it represents the reading2 classroom mean for class \\(j\\) when Reading1 is 0.\n    \\(\\beta_{0j}\\)\nSlope for reading1 for class \\(j\\).\n    \\(e_{ij}\\)\nLevel 1 prediction error for person \\(i\\) in class \\(j\\)\n    \\(\\sigma_e^2\\)\nVariance of \\(e_{ij}\\)\n    $$\\begin{aligned}\n  \\textbf{Level 2}\\\\\n  \\beta_{0j}&=\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}\\\\\n  \\beta_{1j}&=\\gamma_{10}\\\\\n  \\boldsymbol{u}&=\\begin{bmatrix}\nu_{0j}\\\\\nu_{1j}\n\\end{bmatrix}\\sim \\mathcal{N}\\left(\\begin{bmatrix}\n0\\\\\n0\n\\end{bmatrix}\n\\begin{matrix}\n\\\\\n,\n\\end{matrix}\n\\begin{bmatrix}\n\\tau_{00} &\\\\\n0 & \\tau_{11}\n\\end{bmatrix}\\right)\n  \\end{aligned}$$\n\\(\\gamma_{00}\\)\nFixed Intercept. In this model, it is the predicted Time 2 Reading when Reading1 is zero.\n    \\(\\gamma_{01}\\)\nFixed slope for SES. When SES increates by 1, Reading2 is expected to increase by this amount.\n    \\(\\gamma_{10}\\)\nFixed slope for Reading1. When Reading1 increates by 1, Reading2 is expected to increase by this amount.\n    \\(\\boldsymbol{u}\\)\nVector of L2 residuals\n    \\(u_{0j}\\)\nSchool-level (L2) residual for the random intercept \\(\\beta_{0j}\\).\n    \\(u_{1j}\\)\nSchool-level (L2) residual for the random slope \\(\\beta_{1j}\\) for Reading1.\n    \\(\\tau_{00}\\)\nVariance of \\(u_{0j}\\)\n    \\(\\tau_{11}\\)\nVariance of \\(u_{1j}\\)\n  \n  \n  \n    \n       $$\\begin{align*}\n\\textbf{Combined:}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}}_{\\beta_{0j}}+(\\underbrace{\\gamma_{10}+u_{1j}}_{\\beta_{1j}})Reading_{ij1}+e_{ij}\\\\\n&\\text{Fixed vs. Random Effects}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+\\gamma_{10}Reading_{ij1}}_{\\text{Fixed Effects}}+\\underbrace{u_{0j}+u_{1j}Reading_{ij1}}_{\\text{Random Effects}}+e_{ij}\n\\end{align*}$$\n    \n  \n\n\n\n\n\nTable 7: Reading at Time 2 Model 1: Model 2 + Random Slopes for Reading1\n\n\n\n\n\n\nCode\np_read2L1L2uncorrelated &lt;- p_read2L1L2 +\n  {u_1j &lt;- my_residual(label = my_residual_label(\"*u*~1*j*~\")) %&gt;% \n    place(b_1j, i12read2@line@angle - turn(.25), .6)} +\n  my_residual_variance(\n    u_1j,\n    i12read2@line@angle - turn(.25),\n    label = my_residual_variance_label(\n      \"*&tau;*~11~\")) +\n  my_path(u_1j, b_1j) \n\np_read2L1L2uncorrelated\n\n\n\n\n\n\n\n\nFigure 9: Reading Time 2 Model with Level 1 and Level 2 Covariates with Uncorrelated Random Intercepts and Random Slopes\n\n\n\n\n\nTo specify a model in which reading_1 has random slopes uncorrelated with random intercepts requires some knowledge of lme4’s formula syntax. If \\(X\\) is L1, and \\(A\\) is L2, \\(X\\) has random slopes uncorrelated with random intercepts if\nY ~ 1 + X + A + (1 + X || cluster_id)\nNote the double bar (||). It means uncorrelated slopes. A single bar would be correlated slopes.\nAnother way to specify uncorrelated slopes would be to add the intercept and the slopes separately:\nY ~ 1 + X + A + (1 | cluster_id) + (0 + X | cluster_id)\nNote the use of the 0 to indicate no intercept.\nHere we specify Model 3:\n\nm_read_2_3 &lt;- lmer(reading_2 ~ 1 + school_ses + reading_1 + (1 + reading_1 || school_id), d)\n\n\nsummary(m_read_2_3)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_2 ~ 1 + school_ses + reading_1 + ((1 | school_id) + (0 +  \n    reading_1 | school_id))\n   Data: d\n\nREML criterion at convergence: 13910\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.922 -0.669  0.009  0.665  3.985 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n school_id   (Intercept) 0.1845   0.430   \n school_id.1 reading_1   0.0433   0.208   \n Residual                0.1124   0.335   \nNumber of obs: 18257, groups:  school_id, 300\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   1.0122     0.0252    40.2\nschool_ses    0.5785     0.0254    22.8\nreading_1     0.4212     0.0129    32.6\n\nCorrelation of Fixed Effects:\n           (Intr) schl_s\nschool_ses  0.000       \nreading_1   0.000 -0.034\n\nsjPlot::tab_model(m_read_2_3)\n\n\n\n \nreading 2\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n1.01\n0.96 – 1.06\n&lt;0.001\n\n\nschool ses\n0.58\n0.53 – 0.63\n&lt;0.001\n\n\nreading 1\n0.42\n0.40 – 0.45\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n0.11\n\n\n\nτ00 school_id\n0.18\n\n\nτ11 school_id.reading_1\n0.04\n\n\nρ01\n \n\n\nρ01\n \n\n\nICC\n0.62\n\n\nN school_id\n300\n\nObservations\n18257\n\n\nMarginal R2 / Conditional R2\n0.728 / 0.897\n\n\n\n\n\nLet’s compare the models:\n\nanova(m_read_2_2, m_read_2_3)\n\nData: d\nModels:\nm_read_2_2: reading_2 ~ (1 | school_id) + reading_1 + school_ses\nm_read_2_3: reading_2 ~ 1 + school_ses + reading_1 + ((1 | school_id) + (0 + reading_1 | school_id))\n           npar   AIC   BIC logLik -2*log(L) Chisq Df Pr(&gt;Chisq)    \nm_read_2_2    5 15486 15525  -7738     15476                        \nm_read_2_3    6 13905 13951  -6946     13893  1584  1     &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncompare_models(m_read_2_2, m_read_2_3)\n\nParameter    |        m_read_2_2 |        m_read_2_3\n----------------------------------------------------\n(Intercept)  | 0.99 (0.94, 1.04) | 1.01 (0.96, 1.06)\nreading 1    | 0.42 (0.41, 0.43) | 0.42 (0.40, 0.45)\nschool ses   | 0.58 (0.52, 0.63) | 0.58 (0.53, 0.63)\n----------------------------------------------------\nObservations |             18257 |             18257\n\nsjPlot::tab_model(m_read_2_2, m_read_2_3)\n\n\n\n \nreading 2\nreading 2\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n0.99\n0.94 – 1.04\n&lt;0.001\n1.01\n0.96 – 1.06\n&lt;0.001\n\n\nreading 1\n0.42\n0.41 – 0.43\n&lt;0.001\n0.42\n0.40 – 0.45\n&lt;0.001\n\n\nschool ses\n0.58\n0.52 – 0.63\n&lt;0.001\n0.58\n0.53 – 0.63\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n0.13\n0.11\n\n\n\nτ00\n0.22 school_id\n0.18 school_id\n\n\nτ11\n \n0.04 school_id.reading_1\n\n\nρ01\n \n \n\n\nρ01\n \n \n\n\nICC\n0.63\n0.62\n\n\nN\n300 school_id\n300 school_id\n\nObservations\n18257\n18257\n\n\nMarginal R2 / Conditional R2\n0.696 / 0.888\n0.728 / 0.897\n\n\n\n\ncompare_performance(m_read_2_2, m_read_2_3)\n\n# Comparison of Model Performance Indices\n\nName       |   Model |   AIC (weights) |  AICc (weights) |   BIC (weights)\n--------------------------------------------------------------------------\nm_read_2_2 | lmerMod | 15486.4 (&lt;.001) | 15486.4 (&lt;.001) | 15525.4 (&lt;.001)\nm_read_2_3 | lmerMod | 13904.5 (&gt;.999) | 13904.5 (&gt;.999) | 13951.4 (&gt;.999)\n\nName       | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n------------------------------------------------------------\nm_read_2_2 |      0.888 |      0.696 | 0.632 | 0.353 | 0.356\nm_read_2_3 |      0.897 |      0.728 | 0.621 | 0.330 | 0.335\n\ntest_performance(m_read_2_2, m_read_2_3)\n\nName       |   Model |     BF | df | df_diff |    Chi2 |      p\n---------------------------------------------------------------\nm_read_2_2 | lmerMod |        |  5 |         |         |       \nm_read_2_3 | lmerMod | &gt; 1000 |  6 |    1.00 | 1583.83 | &lt; .001\nModels were detected as nested (in terms of fixed parameters) and are compared in sequential order.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nDoes Model 3 fit better than Model 2?"
  },
  {
    "objectID": "tutorials/RandomSlopes/random_slopes.html#reading-2-model-4-model-3-random-slopes-correlated-with-random-intercepts",
    "href": "tutorials/RandomSlopes/random_slopes.html#reading-2-model-4-model-3-random-slopes-correlated-with-random-intercepts",
    "title": "Random Slopes",
    "section": "Reading 2 Model 4 = Model 3 + Random slopes correlated with random intercepts",
    "text": "Reading 2 Model 4 = Model 3 + Random slopes correlated with random intercepts\n\\[\n\\begin{align}\n\\textbf{Level 1:}\\\\\nReading_{ij2} &= \\beta_{0j}+\\beta_{1j}Reading_{ij1}+e_{ij}\\\\\ne&\\sim \\mathcal{N}(0,\\sigma_e^2)\\\\\n\\textbf{Level 2:}\\\\\n\\beta_{0j}&=\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}\\\\\n\\beta_{1j}&=\\gamma_{10}+u_{1j}\\\\\n\\boldsymbol{u}&=\\begin{bmatrix}\nu_{0j}\\\\\nu_{1j}\n\\end{bmatrix}\\sim \\mathcal{N}\\left(\\begin{bmatrix}\n0\\\\\n0\n\\end{bmatrix}\n\\begin{matrix}\n\\\\\n,\n\\end{matrix}\n\\begin{bmatrix}\n\\tau_{00} & \\\\\n\\tau_{10} & \\tau_{11}\n\\end{bmatrix}\\right)\\\\\n\\textbf{Combined:}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}}_{\\beta_{0j}}+(\\underbrace{\\gamma_{10}+u_{1j}}_{\\beta_{1j}})Reading_{ij1}+e_{ij}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+\\gamma_{10}Reading_{ij1}}_{\\text{Fixed Effects}}+\\underbrace{u_{0j}+u_{1j}Reading_{ij1}}_{\\text{Random Effects}}+e_{ij}\n\\end{align}\n\\]\n\n\nCode\ntibble(\n  Symbol = c(\n    \"$Reading_{ij2}$\",\n    \"$\\\\beta_{0j}$\",\n    \"$\\\\beta_{0j}$\",\n    \"$e_{ij}$\",\n    \"$\\\\sigma_e^2$\",\n    \"$\\\\gamma_{00}$\",\n    \"$\\\\gamma_{01}$\",\n    \"$\\\\gamma_{10}$\",\n    \"$u_{0j}$\",\n    \"$u_{1j}$\",\n    \"$\\\\tau_{00}$\",\n    \"$\\\\tau_{11}$\",\n    \"$\\\\tau_{10}$\"\n  ),\n  Meaning = c(\n    \"Reading score for person $i$ in class $j$ at time 2\",\n    \"Random intercept for class $j$. In this model, it represents the reading~2~ classroom mean for class $j$ when Reading~1~ is 0.\",\n    \"Slope for reading~1~ for class $j$.\",\n    \"Level 1 prediction error for person $i$ in class $j$\",\n    \"Variance of $e_{ij}$\",\n    \"Fixed Intercept. In this model, it is the predicted Time 2 Reading when Reading~1~ is zero.\",\n    \"Fixed slope for SES. When SES increates by 1, Reading~2~ is expected to increase by this amount.\",\n    \"Fixed slope for Reading~1~. When Reading~1~ increates by 1, Reading~2~ is expected to increase by this amount.\",\n    \"School-level (L2) residual for the random intercept $\\\\beta_{0j}$.\",\n    \"School-level (L2) residual for the random slope $\\\\beta_{1j}$ for Reading~1~.\",\n    \"Variance of $u_{0j}$\",\n    \"Variance of $u_{1j}$\",\n    \"Covariance of $u_{0j}$ and $u_{1j}$\"),\n  level = paste0(\"Level \", c(1,1,1,1,1,2,2,2,2,2,2,2, 2))\n) %&gt;%\n  gt(groupname_col = \"level\", row_group_as_column = T) %&gt;%\n  gt::tab_options(\n    column_labels.font.weight = \"bold\",\n    column_labels.font.size = 20,\n    footnotes.font.size = 16, table.border.bottom.color = \"white\") %&gt;% \n  gt::cols_width(Symbol ~ pct(15), level ~ pct(23)) %&gt;% \n  tab_row_group(label = md(r\"($$\\begin{aligned}\n  \\textbf{Level 2}\\\\\n  \\beta_{0j}&=\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}\\\\\n  \\beta_{1j}&=\\gamma_{10}+u_{1j}\\\\\n  \\boldsymbol{u}&=\\begin{bmatrix}\nu_{0j}\\\\\nu_{1j}\n\\end{bmatrix}\\sim \\mathcal{N}\\left(\\begin{bmatrix}\n0\\\\\n0\n\\end{bmatrix}\n\\begin{matrix}\n\\\\\n,\n\\end{matrix}\n\\begin{bmatrix}\n\\tau_{00} &\\\\\n\\tau_{10} & \\tau_{11}\n\\end{bmatrix}\\right)\n  \\end{aligned}$$)\"), rows = level == \"Level 2\", id = \"l2\") %&gt;%  \n  tab_row_group(label = md(r\"($$\\begin{align*}\n  \\textbf{Level 1}\\\\\nReading_{ij2} &= \\beta_{0j}+\\beta_{1j}Reading_{ij1}+e_{ij}\\\\\ne_{ij}&\\sim \\mathcal{N}(0,\\sigma_e^2)\\end{align*}$$)\"), rows = level == \"Level 1\", id = \"l1\") %&gt;% \n    fmt_markdown() %&gt;% \n  gt::cols_align(\"center\", \"level\") %&gt;% \n  gt::tab_footnote(footnote = md(r\"($$\\begin{align*}\n\\textbf{Combined:}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}}_{\\beta_{0j}}+(\\underbrace{\\gamma_{10}+u_{1j}}_{\\beta_{1j}})Reading_{ij1}+e_{ij}\\\\\n&\\text{Fixed vs. Random Effects}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+\\gamma_{10}Reading_{ij1}}_{\\text{Fixed Effects}}+\\underbrace{u_{0j}+u_{1j}Reading_{ij1}}_{\\text{Random Effects}}+e_{ij}\n\\end{align*}$$)\")) %&gt;% \ngt::tab_stubhead(\"Level\")\n\n\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Level\n      Symbol\n      Meaning\n    \n  \n  \n    $$\\begin{align*}\n  \\textbf{Level 1}\\\\\nReading_{ij2} &= \\beta_{0j}+\\beta_{1j}Reading_{ij1}+e_{ij}\\\\\ne_{ij}&\\sim \\mathcal{N}(0,\\sigma_e^2)\\end{align*}$$\n\\(Reading_{ij2}\\)\nReading score for person \\(i\\) in class \\(j\\) at time 2\n    \\(\\beta_{0j}\\)\nRandom intercept for class \\(j\\). In this model, it represents the reading2 classroom mean for class \\(j\\) when Reading1 is 0.\n    \\(\\beta_{0j}\\)\nSlope for reading1 for class \\(j\\).\n    \\(e_{ij}\\)\nLevel 1 prediction error for person \\(i\\) in class \\(j\\)\n    \\(\\sigma_e^2\\)\nVariance of \\(e_{ij}\\)\n    $$\\begin{aligned}\n  \\textbf{Level 2}\\\\\n  \\beta_{0j}&=\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}\\\\\n  \\beta_{1j}&=\\gamma_{10}+u_{1j}\\\\\n  \\boldsymbol{u}&=\\begin{bmatrix}\nu_{0j}\\\\\nu_{1j}\n\\end{bmatrix}\\sim \\mathcal{N}\\left(\\begin{bmatrix}\n0\\\\\n0\n\\end{bmatrix}\n\\begin{matrix}\n\\\\\n,\n\\end{matrix}\n\\begin{bmatrix}\n\\tau_{00} &\\\\\n\\tau_{10} & \\tau_{11}\n\\end{bmatrix}\\right)\n  \\end{aligned}$$\n\\(\\gamma_{00}\\)\nFixed Intercept. In this model, it is the predicted Time 2 Reading when Reading1 is zero.\n    \\(\\gamma_{01}\\)\nFixed slope for SES. When SES increates by 1, Reading2 is expected to increase by this amount.\n    \\(\\gamma_{10}\\)\nFixed slope for Reading1. When Reading1 increates by 1, Reading2 is expected to increase by this amount.\n    \\(u_{0j}\\)\nSchool-level (L2) residual for the random intercept \\(\\beta_{0j}\\).\n    \\(u_{1j}\\)\nSchool-level (L2) residual for the random slope \\(\\beta_{1j}\\) for Reading1.\n    \\(\\tau_{00}\\)\nVariance of \\(u_{0j}\\)\n    \\(\\tau_{11}\\)\nVariance of \\(u_{1j}\\)\n    \\(\\tau_{10}\\)\nCovariance of \\(u_{0j}\\) and \\(u_{1j}\\)\n  \n  \n  \n    \n       $$\\begin{align*}\n\\textbf{Combined:}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}}_{\\beta_{0j}}+(\\underbrace{\\gamma_{10}+u_{1j}}_{\\beta_{1j}})Reading_{ij1}+e_{ij}\\\\\n&\\text{Fixed vs. Random Effects}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+\\gamma_{10}Reading_{ij1}}_{\\text{Fixed Effects}}+\\underbrace{u_{0j}+u_{1j}Reading_{ij1}}_{\\text{Random Effects}}+e_{ij}\n\\end{align*}$$\n    \n  \n\n\n\n\n\nTable 8: Reading at Time 2 Model 1: Model 3 + correlation between random intercepts and random slopes\n\n\n\n\n\n\nCode\np_read_2L1L2correlated &lt;- p_read2L1L2uncorrelated +\n  ob_covariance(\n    u_0j@point_at(\"east\"),\n    u_1j@point_at(\"north\"),\n    label = my_path_label(\"&tau;~10~\", position = .5),\n    length_head = 5,\n    length_fins = 5,\n    size = .5, \n    looseness = 1,\n    bend = -6,\n    resect = 1\n  ) \n\np_read_2L1L2correlated\n\n\n\n\n\n\n\n\nFigure 10: Reading Time 2 Model with Level 1 and Level 2 Covariates with Correlated Random Intercepts and Random Slopes\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nMake a model in which reading_1 has random slopes correlated with random intercepts.\nCall your model m_read_2_4.\nDoes Model 4 fit better than Model 3?\n\n\n\n\n\n\n\n\nTipHint (click to see)\n\n\n\n\n\nIf X is L1 and A is L2, X has random slopes correlated with random intercepts if\nY ~ 1 + X + A + (1 + X | cluster_id )\nNote the single bar (|)."
  },
  {
    "objectID": "tutorials/RandomSlopes/random_slopes.html#reading-2-model-5-model-4-cross-level-interaction-of-reading1-and-school-ses",
    "href": "tutorials/RandomSlopes/random_slopes.html#reading-2-model-5-model-4-cross-level-interaction-of-reading1-and-school-ses",
    "title": "Random Slopes",
    "section": "Reading 2 Model 5 = Model 4 + Cross-Level Interaction of Reading1 and School SES",
    "text": "Reading 2 Model 5 = Model 4 + Cross-Level Interaction of Reading1 and School SES\nAn interaction effect (i.e., a moderator effect) is when the effect of one variable depends on the other variable. For example, impulsivity predicts a wide range of negative life outcomes, but its effect is worse when the person is violent. That is impulsive violent people have worse outcomes than non-impulsive violent people or non-violent impulsive people.\nInteractions can occur within level (e.g,. between two level-1 variables) or across levels (e.g., between a level-1 variable and a level-2 variable). Here we want to test the cross-level interaction between Reading1 and classroom SES.\nInteraction effects involve multiplicative relationships among variables. Thus the interaction between A and B would be specified like so:\nY ~ A * B\nTechnically, the * operator in a formula includes all lower-order effects among the variables multiplied (i.e., the simple slopes for A and B in addition to the interaction of A and B). Expanded, A * B becomes A + B + A:B. In this context, the : is just the multiplicative term. With three variables, A*B*C becomes A + B + C + A:B + A:C + B:C + A:B:C. If we wanted just the simple slopes and two-way interactions, we could specify A + B + C + A:B + A:C + B:C or A*B - A:B:C.\n\\[\n\\begin{align}\n\\textbf{Level 1:}\\\\\nReading_{ij2} &= \\beta_{0j}+\\beta_{1j}Reading_{ij1}+e_{ij}\\\\\ne&\\sim \\mathcal{N}(0,\\sigma_e^2)\\\\\n\\textbf{Level 2:}\\\\\n\\beta_{0j}&=\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}\\\\\n\\beta_{1j}&=\\gamma_{10}+\\gamma_{11}SES_j+u_{1j}\\\\\n\\boldsymbol{u}&=\\begin{bmatrix}\nu_{0j}\\\\\nu_{1j}\n\\end{bmatrix}\\sim \\mathcal{N}\\left(\\begin{bmatrix}\n0\\\\\n0\n\\end{bmatrix}\n\\begin{matrix}\n\\\\\n,\n\\end{matrix}\n\\begin{bmatrix}\n\\tau_{00} & \\\\\n\\tau_{10} & \\tau_{11}\n\\end{bmatrix}\\right)\\\\\n\\textbf{Combined:}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}}_{\\beta_{0j}}+(\\underbrace{\\gamma_{10}+\\gamma_{11}SES_j+u_{1j}}_{\\beta_{1j}})Reading_{ij1}+e_{ij}\\\\\n&\\text{Fixed vs. Random Effects}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+(\\gamma_{10}+\\gamma_{11}SES_j)Reading_{ij1}}_{\\text{Fixed Effects}}+\\underbrace{u_{0j}+u_{1j}Reading_{ij1}}_{\\text{L2 Random Effects}}+e_{ij}\n\\end{align}\n\\]\n\n\nCode\ntibble(\n  Symbol = c(\n    \"$Reading_{ij2}$\",\n    \"$\\\\beta_{0j}$\",\n    \"$\\\\beta_{0j}$\",\n    \"$e_{ij}$\",\n    \"$\\\\sigma_e^2$\",\n    \"$\\\\gamma_{00}$\",\n    \"$\\\\gamma_{01}$\",\n    \"$\\\\gamma_{10}$\",\n    \"$\\\\gamma_{11}$\",\n    \"$u_{0j}$\",\n    \"$u_{1j}$\",\n    \"$\\\\tau_{00}$\",\n    \"$\\\\tau_{11}$\",\n    \"$\\\\tau_{10}$\"\n  ),\n  Meaning = c(\n    \"Reading score for person $i$ in class $j$ at time 2\",\n    \"Random intercept for class $j$. In this model, it represents the reading~2~ classroom mean for class $j$ when Reading~1~ is 0.\",\n    \"Slope for reading~1~ for class $j$.\",\n    \"Level 1 prediction error for person $i$ in class $j$\",\n    \"Variance of $e_{ij}$\",\n    \"Fixed Intercept. In this model, it is the predicted Time 2 Reading when Reading~1~ is zero.\",\n    \"Fixed slope for SES. When SES increates by 1, Reading~2~ is expected to increase by this amount.\",\n    \"Fixed slope for Reading~1~. When Reading~1~ increates by 1, Reading~2~ is expected to increase by this amount.\",\n    \"Fixed cross-level interaction effect of Reading~1~ and SES. When SES increases by 1, the fixed slope for Reading~1~ increates by this amount.\",\n    \"School-level (L2) residual for the random intercept $\\\\beta_{0j}$.\",\n    \"School-level (L2) residual for the random slope $\\\\beta_{1j}$ for Reading~1~.\",\n    \"Variance of $u_{0j}$\",\n    \"Variance of $u_{1j}$\",\n    \"Covariance of $u_{0j}$ and $u_{1j}$\"),\n  level = paste0(\"Level \", c(1,1,1,1,1,2,2,2,2,2,2,2,2, 2))\n) %&gt;%\n  gt(groupname_col = \"level\", row_group_as_column = T) %&gt;%\n  gt::tab_options(\n    column_labels.font.weight = \"bold\",\n    column_labels.font.size = 20,\n    footnotes.font.size = 16, table.border.bottom.color = \"white\") %&gt;% \n  gt::cols_width(Symbol ~ pct(15), level ~ pct(23)) %&gt;% \n  tab_row_group(label = md(r\"($$\\begin{aligned}\n  \\textbf{Level 2}\\\\\n  \\beta_{0j}&=\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}\\\\\n  \\beta_{1j}&=\\gamma_{10}+\\gamma_{11}SES_j+u_{1j}\\\\\n  \\boldsymbol{u}&=\\begin{bmatrix}\nu_{0j}\\\\\nu_{1j}\n\\end{bmatrix}\\sim \\mathcal{N}\\left(\\begin{bmatrix}\n0\\\\\n0\n\\end{bmatrix}\n\\begin{matrix}\n\\\\\n,\n\\end{matrix}\n\\begin{bmatrix}\n\\tau_{00} &\\\\\n\\tau_{10} & \\tau_{11}\n\\end{bmatrix}\\right)\n  \\end{aligned}$$)\"), rows = level == \"Level 2\", id = \"l2\") %&gt;%  \n  tab_row_group(label = md(r\"($$\\begin{align*}\n  \\textbf{Level 1}\\\\\nReading_{ij2} &= \\beta_{0j}+\\beta_{1j}Reading_{ij1}+e_{ij}\\\\\ne_{ij}&\\sim \\mathcal{N}(0,\\sigma_e^2)\\end{align*}$$)\"), rows = level == \"Level 1\", id = \"l1\") %&gt;% \n    fmt_markdown() %&gt;% \n  gt::cols_align(\"center\", \"level\") %&gt;% \n  gt::tab_footnote(footnote = md(r\"($$\\begin{align*}\n\\textbf{Combined:}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}}_{\\beta_{0j}}+(\\underbrace{\\gamma_{10}+\\gamma_{11}SES_j+u_{1j}}_{\\beta_{1j}})Reading_{ij1}+e_{ij}\\\\\n&\\text{Fixed vs. Random Effects}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+(\\gamma_{10}+\\gamma_{11}SES_j)Reading_{ij1}}_{\\text{Fixed Effects}}+\\underbrace{u_{0j}+u_{1j}Reading_{ij1}}_{\\text{L2 Random Effects}}+e_{ij}\n\\end{align*}$$)\")) %&gt;% \ngt::tab_stubhead(\"Level\")\n\n\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Level\n      Symbol\n      Meaning\n    \n  \n  \n    $$\\begin{align*}\n  \\textbf{Level 1}\\\\\nReading_{ij2} &= \\beta_{0j}+\\beta_{1j}Reading_{ij1}+e_{ij}\\\\\ne_{ij}&\\sim \\mathcal{N}(0,\\sigma_e^2)\\end{align*}$$\n\\(Reading_{ij2}\\)\nReading score for person \\(i\\) in class \\(j\\) at time 2\n    \\(\\beta_{0j}\\)\nRandom intercept for class \\(j\\). In this model, it represents the reading2 classroom mean for class \\(j\\) when Reading1 is 0.\n    \\(\\beta_{0j}\\)\nSlope for reading1 for class \\(j\\).\n    \\(e_{ij}\\)\nLevel 1 prediction error for person \\(i\\) in class \\(j\\)\n    \\(\\sigma_e^2\\)\nVariance of \\(e_{ij}\\)\n    $$\\begin{aligned}\n  \\textbf{Level 2}\\\\\n  \\beta_{0j}&=\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}\\\\\n  \\beta_{1j}&=\\gamma_{10}+\\gamma_{11}SES_j+u_{1j}\\\\\n  \\boldsymbol{u}&=\\begin{bmatrix}\nu_{0j}\\\\\nu_{1j}\n\\end{bmatrix}\\sim \\mathcal{N}\\left(\\begin{bmatrix}\n0\\\\\n0\n\\end{bmatrix}\n\\begin{matrix}\n\\\\\n,\n\\end{matrix}\n\\begin{bmatrix}\n\\tau_{00} &\\\\\n\\tau_{10} & \\tau_{11}\n\\end{bmatrix}\\right)\n  \\end{aligned}$$\n\\(\\gamma_{00}\\)\nFixed Intercept. In this model, it is the predicted Time 2 Reading when Reading1 is zero.\n    \\(\\gamma_{01}\\)\nFixed slope for SES. When SES increates by 1, Reading2 is expected to increase by this amount.\n    \\(\\gamma_{10}\\)\nFixed slope for Reading1. When Reading1 increates by 1, Reading2 is expected to increase by this amount.\n    \\(\\gamma_{11}\\)\nFixed cross-level interaction effect of Reading1 and SES. When SES increases by 1, the fixed slope for Reading1 increates by this amount.\n    \\(u_{0j}\\)\nSchool-level (L2) residual for the random intercept \\(\\beta_{0j}\\).\n    \\(u_{1j}\\)\nSchool-level (L2) residual for the random slope \\(\\beta_{1j}\\) for Reading1.\n    \\(\\tau_{00}\\)\nVariance of \\(u_{0j}\\)\n    \\(\\tau_{11}\\)\nVariance of \\(u_{1j}\\)\n    \\(\\tau_{10}\\)\nCovariance of \\(u_{0j}\\) and \\(u_{1j}\\)\n  \n  \n  \n    \n       $$\\begin{align*}\n\\textbf{Combined:}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+u_{0j}}_{\\beta_{0j}}+(\\underbrace{\\gamma_{10}+\\gamma_{11}SES_j+u_{1j}}_{\\beta_{1j}})Reading_{ij1}+e_{ij}\\\\\n&\\text{Fixed vs. Random Effects}\\\\\nReading_{ij2} &= \\underbrace{\\gamma_{00}+\\gamma_{01}SES_j+(\\gamma_{10}+\\gamma_{11}SES_j)Reading_{ij1}}_{\\text{Fixed Effects}}+\\underbrace{u_{0j}+u_{1j}Reading_{ij1}}_{\\text{L2 Random Effects}}+e_{ij}\n\\end{align*}$$\n    \n  \n\n\n\n\n\nTable 9: Reading at Time 2 Model 1: Model 4 + Cross-level Interaction of Reading_1 and School SES\n\n\n\n\n\n\nCode\np_read_2L1L2correlated + \n  my_path(SES, \n          b_1j, \n          label = my_path_label(\"*&gamma;*~*11*~\"))\n\n\n\n\n\n\n\n\nFigure 11: Reading Time 2 Model with Cross-Level Interaction of SES and Reading1\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion 5\n\n\n\nSpecify a model in which reading_1 and school_ses interact.\nCall your model m_read_2_5\nDoes Model 5 fit better than Model 4?\n\n\n\n\n\n\n\n\nTipHint (click to see)\n\n\n\n\n\nIf X is L1 and A is L2, the cross-level interaction is specified like so:\nY ~ 1 + X * A + (1 + X | cluster_id )\n\n\n\n\n\n\n\n\n\nNoteQuestion 6\n\n\n\nCompared to Model 0, by how much is level-2 variance reduced by model 5?\n\n\n\n\n\n\n\n\nTipHint (click to see)\n\n\n\n\n\nFor this question and the next, use the variance_reduction function created earlier.\n\n\n\n\n\n\n\n\n\nNoteQuestion 7\n\n\n\nCompared to Model 0, by how much is level-1 variance reduced by model 5?\n\n\n\n\n\n\n\n\nNoteQuestion 8\n\n\n\nWhat is your overall interpretation of these data?\n\n\nRun this code to see some plots that might aid your interpretation:\n\n# Fixed effects only\nsjPlot::plot_model(m_read_2_5,\n  type = \"pred\",\n  terms = c(\"reading_1\", \"school_ses [-2, -1, 0, 1, 2]\")\n) +\n  scale_y_continuous(limits = c(-4, 4))\n\n# Random effects only\naugment(m_read_2_5) %&gt;%\n  ggplot(aes(reading_1, reading_2, color = school_ses)) +\n  geom_point(size = 0.2, alpha = 0.3) +\n  geom_line(aes(y = .fitted, group = school_id), size = 0.2) +\n  scale_color_viridis_c() +\n  coord_fixed()"
  },
  {
    "objectID": "tutorials/PowerAnalysis/power_analysis.html",
    "href": "tutorials/PowerAnalysis/power_analysis.html",
    "title": "Power Analysis with Multilevel Models",
    "section": "",
    "text": "Specify Model\nCreate a Data Generator Function that Creates a Single Data Set\nCreate a Model Fitting Function\nCreate a Detect Effect Function\nGenerate Many Data Sets with Varying Sample Sizes\nTest the Model on Each Data Set\nPlot the Relation Between Sample Size and the Probability of Detecting the Effect."
  },
  {
    "objectID": "tutorials/PowerAnalysis/power_analysis.html#level-1-measurement-occasion",
    "href": "tutorials/PowerAnalysis/power_analysis.html#level-1-measurement-occasion",
    "title": "Power Analysis with Multilevel Models",
    "section": "Level 1: Measurement Occasion",
    "text": "Level 1: Measurement Occasion\nAlthough the model can appear complicated, the idea here is simple: At first reading grows slowly each month. Once an assessment has occurred, the reading growth rate is faster.\n\n\\begin{align*}\nReading_{ti} &= b_{0i}+b_{1i}Month_{ti}+b_{2i}MonthsAfterAssessment_{ti}+e_{ti}\\\\\ne_{ti}&\\sim\\mathcal{N}(0, \\sigma_e^2)\n\\end{align*}\n\n\n\n\n\n\n\n\nVariable\nInterpretation\n\n\n\n\nReading_{ti}\nThe measure of Reading Comprehension for person i at month t.\n\n\nb_{0i}\nThe predicted level of Reading Comprehension for person i at month 0.\n\n\nMonth_{ti}\nThe number of months elapsed for person i at month t.\n\n\nb_{1i}\nThe pre-assessment growth rate in reading comprehension for person i at month t.\n\n\nMonthsAfterAssessment_{ti}\nThe number of months after the assessment for person i at month t.\n\n\nb_{2i}\nThe degree to which assessment increases growth rate in reading comprehension for person i at month t.\n\n\ne_{ti}\nThe deviation from expectations for Reading Comprehension at occasion t for person i. It represents the model’s failure to accurately predict Reading Comprehension at each measurement occasion.\n\n\n\\sigma_e^2\nThe variance of e_{ti}"
  },
  {
    "objectID": "tutorials/PowerAnalysis/power_analysis.html#level-2-student",
    "href": "tutorials/PowerAnalysis/power_analysis.html#level-2-student",
    "title": "Power Analysis with Multilevel Models",
    "section": "Level 2: Student",
    "text": "Level 2: Student\nFor the sake of simplicity, the model does not include any student-level predictors.\n\n\\begin{align*}\n   b_{0i} &= b_{00}+e_{0i}\\\\\n   b_{1i} &= b_{10}+e_{1i}\\\\\n   b_{2i} &= b_{20}\\\\\n   \\boldsymbol{e}_2 &= \\begin{bmatrix}e_{0i}\\\\e_{1i}\\end{bmatrix}\\\\\n   \\boldsymbol{e}_2&\\sim\\mathcal{N}\\left(\\boldsymbol{0},\\boldsymbol{\\tau}\\right)\\\\\n   \\boldsymbol{\\tau} &= \\begin{bmatrix}\\tau_{00} & \\tau_{10} \\\\ \\tau_{10} & \\tau_{11}\\end{bmatrix}\n\\end{align*}\n\n\n\n\n\n\n\n\nVariable\nInterpretation\n\n\n\n\nb_{0i}\nThe predicted level of Reading Comprehension for person i at month 0.\n\n\nb_{00}\nThe overall predicted value of Reading Comprehension at month 0.\n\n\ne_{0i}\nPerson i’s deviation from expectations for the random intercept. It represents the model’s failure to predict each person’s random intercept.\n\n\nb_{1i}\nThe pre-assessment growth rate in reading comprehension for person i at month t.\n\n\nb_{10}\nThe rate at which Reading Comprehension increases for each month if no assessment has occurred (i.e., when MonthsAfterAssessment_{ti} = 0).\n\n\ne_{1i}\nPerson i’s the deviation from expectations for the random slope for month. It represents the model’s failure to predict each person’s rate of increase in Reading Comprehension over time (after controlling for months after assessment).\n\n\nb_{2i}\nThe degree to which assessment increases growth rate in reading comprehension for person i at month t. Because no random component is included, it equals b_{20}.\n\n\nb_{20}\nThe overall degree to which assessment increases growth rate in reading comprehension.\n\n\n\\boldsymbol{e}_2\nThe 1-column matrix of Level-2 random effects (i.e., e_{0i} and e_{1i})\n\n\n\\boldsymbol{\\tau}\nThe covariance matrix of of Level-2 random effects\n\n\n\\tau_{00}\nThe variance of e_{0i}\n\n\n\\tau_{11}\nThe variance of e_{1i}\n\n\n\\tau_{10}\nThe covariance of e_{0i} and e_{1i}"
  },
  {
    "objectID": "tutorials/PowerAnalysis/power_analysis.html#hypothesized-model-as-a-lmer-formula",
    "href": "tutorials/PowerAnalysis/power_analysis.html#hypothesized-model-as-a-lmer-formula",
    "title": "Power Analysis with Multilevel Models",
    "section": "Hypothesized Model as a lmer formula",
    "text": "Hypothesized Model as a lmer formula\nreading ~ 1 + month + months_after_assessment + (1 + month | student_id)"
  },
  {
    "objectID": "tutorials/PowerAnalysis/power_analysis.html#an-alternate-tidy-method",
    "href": "tutorials/PowerAnalysis/power_analysis.html#an-alternate-tidy-method",
    "title": "Power Analysis with Multilevel Models",
    "section": "An Alternate “Tidy” Method",
    "text": "An Alternate “Tidy” Method\nHere we create a tibble with 1000 rows, each with a different sample size between 10 and 200. We create a new variable called Power that will contain the output of a function that makes data and tells us whether the effect was detected. The map_dbl is part of the map family. It takes the input of our sample size column n and performs our custom function for every row. The Power variable is just a column of 1s and 0s, telling whether the effect was detected or not.\nI then plot the results using the geom_smooth function. I specify a binomial glm, which is a logistic regression—regression for binary output. The result is a plot showing the relationship between sample size and power.\n\ntibble(n = 10:200) %&gt;%\n  mutate(Power = map(n, \\(n) {\n    make_data(\n      n = n,\n      b_00 = b_00,\n      b_10 = b_10,\n      b_20 = b_20,\n      sigma_e = 2,\n      tau_00 = tau_00,\n      tau_11 = tau_11,\n      r_01 = r_01,\n      k_months_total = 12,\n      k_months_before = 4,\n      k_months_delay = 2\n    ) %&gt;%\n      fit_model() %&gt;%\n      detect_effect()\n  })) %&gt;%\n  unnest(Power) %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(term = snakecase::to_title_case(term)) %&gt;%\n  ggplot(aes(n, significant)) +\n  geom_smooth(method = \"glm\",\n              method.args = list(family = \"binomial\"), \n              se = FALSE) + \n  geom_hline(yintercept = .9) +\n  facet_wrap(vars(term)) + \n  scale_x_continuous(\"Sample Size (n)\", limits = c(0,200)) +\n  scale_y_continuous(\"Proportion Significant\",\n                     limits = c(0,1), breaks = seq(0,1,.2)) +\n  theme_light()\n\n\n\n\n\n\n\n\nGiven the assumptions I set up, 90% power for the month variable is present in even small sample sizes. For the months_after_assessment variable, 90% power is present at a sample size of about 75.\nSuppose that I am not so sure about the size of assessment’s effect. Therefore, I specify a variety of effect sizes for b_{20}. The crossing function creates tibble with all combinations of n and b_20. I then set repeats to 2, so that I can see the effect of sample size and effect size on power. The uncount function creates a new row for each row in the tibble, repeating the rows by the number of times specified in repeats.\n\ncrossing(n = c(seq(10, 90, 5), seq(100, 1000, 25)),\n              b_20 = c(\n                Smaller = .1,\n                Medium = .2,\n                Larger = .3\n              )) %&gt;% \n  mutate(repeats = 2,\n         `Effect Size` = paste0(names(b_20),\n                                \" (\",\n                                b_20, \n                                \")\")) %&gt;%\n  uncount(repeats) %&gt;% \n  mutate(Power = map2(n, b_20, \\(nn, bb_20) {\n   make_data(\n      n = nn,\n      b_00 = b_00,\n      b_10 = b_10,\n      b_20 = bb_20,\n      sigma_e = 2,\n      tau_00 = tau_00,\n      tau_11 = tau_11,\n      r_01 = r_01,\n      k_months_total = 12,\n      k_months_before = 4,\n      k_months_delay = 2\n    ) %&gt;%\n      fit_model() %&gt;%\n      detect_effect()\n  })) %&gt;%\n  unnest(Power) %&gt;% \n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(term = snakecase::to_title_case(term)) %&gt;%\n  ggplot(aes(n, significant)) +\n  geom_smooth(\n    method = \"glm\",\n    se = F,\n    method.args = list(family = \"binomial\"),\n    aes(color = `Effect Size`)\n  ) +\n  scale_x_log10(\"Sample Size (log)\", \n                breaks = c(10,20, 50, 100, 200, 500, 1000),\n                minor_breaks = c(seq(10,90, 10), seq(100, 1000, 100))) +\n  geom_hline(yintercept = .9) +\n  theme(legend.position.inside = c(1, 0),\n        legend.justification = c(1, 0)) +\n  scale_color_viridis_d(end = .9, begin = .1) +\n  scale_y_continuous(\"Proportion Significant\", \n                     breaks = seq(0, 1, 0.2)) +\n  facet_wrap(vars(term)) +\n  theme_light()\n\n\n\n\n\n\n\nFigure 1: Power Analysis by Sample Size and Effect Size\n\n\n\n\n\nThus, it appears that under the assumptions my model has, I can detect my effect with around 100 or fewer when the effect of assessment is greater than 0.2. If the effect is 0.1, I would need about 500 participants to detect the effect with reasonable power.\nIn the same way that we varied the sample size and effect size, we can vary any other parameter we want (e.g., number of measurements, length of time before the assessment) and plot the how these factors influence power."
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html",
    "href": "tutorials/Longitudinal/longitudinal.html",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "",
    "text": "Longitudinal data analysis refers to the examination of how variables change over time, typically within individuals. Multilevel modeling allows us to study “unbalanced” longitudinal data in which not everyone has the same number of measurements.\nIn clustered data, the lowest level is typically a person, with higher levels referring to groups of people. With longitudinal data, the person is typically level 2, and level 1 is a measurement occasion associated with a specific instance in time.\nWith multilevel approaches to longitudinal data, data for each person fits on multiple rows with each measurement instance on its own row as seen in Table 1.\nSpecifying when the time variable equals 0 becomes increasingly important with complex longitudinal model because it influences the interpretation of other coefficients in the model. If there are interaction effects or polynomial effects, the lower-order effects refer to the effect of the variable when all other predictors are 0. Thus, we try to mark time such that T0 (when time = 0) is meaningful. For example, T0 is often simply the first measurement occasion. T0 could be the point at which an important event occurs, such as the onset of an intervention. In such cases, measurements before the intervention are marked as negative (Time = −3, −2, −1, 0, 1, 2, …).\nTime does not have to be marked as integers, not everyone needs to be measured at the same times, and measurements do not necessarily need to occur at exactly T0. For person 1, the measurements could be at Time = −2.1 minutes, +5.3 minutes, and +44.2 minutes. For person 2, the measurements could be at Time = −8.8 minutes, −6.0 minutes, +10.1 minutes, and +10.5 minutes."
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#level-1-measurement-occasion",
    "href": "tutorials/Longitudinal/longitudinal.html#level-1-measurement-occasion",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Level 1: Measurement Occasion",
    "text": "Level 1: Measurement Occasion\nEvery time a person is measured, we will call this a measurement occasion. In longitudinal studies, each measurement occasion is placed in its own data row.\n\n\\begin{align*}\nY_{ti}&=b_{0i}+b_{1i}T_{ti}+b_{2i}RD_{ti}+e_{ti}\\\\\ne_{ti}&\\sim\\mathcal{N}(0, \\tau_1^2)\n\\end{align*}\n\n\n\n\n\n\n\n\nVariable\nInterpretation\n\n\n\n\nY_{ti}\nThe measure of Reading Comprehension for person i at occasion (time) t.\n\n\nRD_{ti}\nThe measure of Reading Decoding at occasion t for person i.\n\n\nT_{ti}\nHow much time has elapsed at occasion t for person i.\n\n\nb_{0i}\nThe random intercept: The predicted Reading Comprehension for person i at time 0 (T0) if Reading Decoding = 0.\n\n\nb_{1i}\nThe random slope of Time: The rate of increase in Reading Comprehension for person i if Reading Decoding is held constant at 0.\n\n\nb_{2i}\nThe size of Reading Decoding’s association with Reading Comprehension for person i.\n\n\ne_{ti}\nThe deviation from expectations for Reading Comprehension at occasion t for person i. It represents the model’s failure to accurately predict Reading Comprehension at each measurement occasion.\n\n\n\\tau_1\nThe variance of e_{ti}"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#level-2-person",
    "href": "tutorials/Longitudinal/longitudinal.html#level-2-person",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Level 2: Person",
    "text": "Level 2: Person\n\nPredicting the Random Intercept (b_{0i}):  What Determines the Conditions at T0?\nIf Time and Reading Decoding = 0, what determines our prediction of Reading Comprehension?\n\nb_{0i}=b_{00}+b_{01}WM_i+e_{0i}\n\n\n\n\n\n\n\n\nVariable\nInterpretation\n\n\n\n\nb_{0i}\nThe random intercept: The predicted Reading Comprehension for person i at time 0 (T0) if Reading Decoding = 0.\n\n\nWM_i\nThe measurement of Working Memory Capacity for person i.\n\n\nb_{00}\nThe predicted value of Reading Comprehension when Time, Reading Decoding, and Working Memory = 0.\n\n\nb_{01}\nWorking Memory’s association with Reading Comprehension at T0 when Reading Decoding is 0.\n\n\ne_{0i}\nPerson i’s deviation from expectations for the random intercept. It represents the model’s failure to predict each person’s random intercept.\n\n\n\n\n\nPredicting Time’s Random Slope:  What determines the effect of time (Grade)?\nSuppose we believe that Reading Comprehension will, on average, increase over time for everyone, but the rate of increase will be faster for children with greater working memory capacity.\n\nb_{1i}=b_{10}+b_{11}WM_i+e_{1i}\n\n\n\n\n\n\n\n\nVariable\nInterpretation\n\n\n\n\nb_{1i}\nThe random slope of Time: The rate of increase in Reading Comprehension for person i if Reading Decoding is held constant at 0.\n\n\nb_{10}\nThe rate at which Reading Comprehension increases for each unit of time if both Reading Decoding and Working Memory are held constant at 0.\n\n\nb_{11}\nWorking Memory’s effect on the rate of Reading Comprehension’s change over time, after controlling for Reading Decoding (i.e., if Reading Decoding were held constant at 0). If positive, students with higher working memory should improve their Reading Comprehension ability faster than students with lower working memory.\n\n\ne_{1i}\nPerson i’s the deviation from expectations for the random slope for time. It represents the model’s failure to predict each person’s rate of increase in Reading Comprehension over time (after controlling for Reading Decoding).\n\n\n\n\n\nPredicting Reading Decoding’s Random Slope:  What determines the effect of Reading Decoding?\nSuppose that the effect of Reading Decoding on Reading Comprehension differs from person to person. Obviously, to comprehend text a person needs at least some ability to decode words. However, there is evidence that working memory helps a person make inferences in reading comprehension such that one can work around unfamiliar words and still derive meaning from the text. Thus, we might predict that higher Working Memory Capacity makes Reading Decoding a less important predictor of Reading Comprehension.\n\nb_{2i}=b_{20}+b_{21}WM_i+e_{2i}\n\n\n\n\n\n\n\n\nVariable\nInterpretation\n\n\n\n\nb_{2i}\nThe size of Reading Decoding’s association with Reading Comprehension for person i.\n\n\nb_{20}\nThe size of the association of Reading Decoding and Reading Comprehension when Working Memory is 0.\n\n\nb_{21}\nThe interaction effect of Working Memory and Reading Decoding. It represents how much the effect of Reading Decoding on Reading Comprehension changes for each unit of increase in Working Memory.\n\n\ne_{2i}\nPerson i’s the deviation from expectations for the random slope for Reading Decoding. It represents the model’s failure to predict the person-specific relationship between Reading Decoding and Reading Comprehension.\n\n\n\n\n\nLevel-2 Random Effects\nThe error terms at level 2 (e_{0i}, e_{1i}, \\&~e_{2i}) have variances, and possibly covariances.\n\n\\begin{align*}\n\\boldsymbol{e}_2&=\n\\begin{bmatrix}\ne_{0i}\\\\\ne_{1i}\\\\\ne_{2i}\n\\end{bmatrix}\\\\\n\\boldsymbol{e}_2&\\sim\\mathcal{N}\\left(\\boldsymbol{0},\\boldsymbol{\\tau}_2\\right)\\\\\n\\boldsymbol{\\tau}_2&=\n\\begin{bmatrix}\n\\tau_{2.00}\\\\\n\\tau_{2.10} & \\tau_{2.11}\\\\\n\\tau_{2.20} & \\tau_{2.21} & \\tau_{2.22}\n\\end{bmatrix}\n\\end{align*}"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#combined-equations",
    "href": "tutorials/Longitudinal/longitudinal.html#combined-equations",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Combined Equations",
    "text": "Combined Equations\n\n\\begin{align}\nY_{ti}&=\n\\underbrace{b_{00}+b_{01}WM_i+u_{0i}}_{b_{0i}~\\text{(Random Intercept)}}\\\\\n&+(\\underbrace{b_{10}+b_{11}WM_i+u_{1i}}_{b_{1i}~\\text{(Random Slope for Time)}})T_{ti}\\\\\n&+(\\underbrace{b_{20}+b_{21}WM_i+u_{2i}}_{b_{2i}~\\text{(Random Slope for RD)}})RD_{ti}\\\\\n&+e_{ti}\n\\end{align}\n\nAn equation aligned to the lmer formula:\n\n\\begin{align}\nY_{ti}&=b_{00}\\\\\n&+\\underbrace{b_{01}WM_i+b_{10}T_{ti}+b_{20}RD_{ti}}_{\\text{Simple Slopes (when other predictors = 0)}}\\\\\n&+\\underbrace{b_{11}T_{ti}WM_i+b_{21}RD_{ti}WM_i}_{\\text{Interactions}}\\\\\n&+\\underbrace{e_{0i}+e_{1i}T_{ti}+e_{2i}RD_{ti}}_{\\text{L2 Random Effects}}\\\\\n&+e_{ti}\n\\end{align}\n\nHere is the model as a lmer formula:\nreading_comprehension~1+\nworking_memory+time+reading_decoding+\ntime:working_memory+reading_decoding:working_memory+\n(1+time+reading_decoding | person_id)\nA simplified version would look like this:\nreading_comprehension~\nworking_memory * time + \nworking_memory * reading_decoding +\n(time+reading_decoding | person_id)"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#simulate-longitudinal-data",
    "href": "tutorials/Longitudinal/longitudinal.html#simulate-longitudinal-data",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Simulate longitudinal data",
    "text": "Simulate longitudinal data\nI am going to simulate a longitudinal observational study of these three variables.\nTo make things simple, suppose that Reading Decoding also grows over time like so:\n\nRD_{ti}=6+2T_{ti}+.5WM_{ti}+e_{ti}\\\\\ne\\sim\\mathcal{N}(0, 1^2)\n\n\nSimulate Level-2 Data (Person Level)\nWe begin at level 2 by setting the sample size, the fixed effects, the level-2 random effect covariance matrix (\\boldsymbol{\\tau}), and\nWe can simulate Working Memory Capacity as a standard normal variable.\n\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(sjPlot)\nset.seed(123456)\n# Sample size\nn_person &lt;- 2000\n\n# Number of occasions\nn_occasions &lt;- 6\n\n# Fixed effects\nb_00 &lt;- 60 # Fixed intercept\nb_01 &lt;- 10 # simple main effect of working memory\nb_10 &lt;- 50 # simple main effect of time (Grade)\nb_11 &lt;- 5 # time * working memory interaction\nb_20 &lt;- 8 # simple main effect of reading decoding\nb_21 &lt;- -0.9 # reading decoding * working memory interaction\n\n# Standard deviations of level-2 random effects\ne_sd &lt;- c(e_0i = 50, # sd of random intercept\n          e_1i = 1.7, # sd of random slope for time\n          e_2i = 1.8) # sd of random slope for reading decoding\n\n# Correlations among level-2 random effects\ne_cor &lt;- c(1,\n           0, 1,\n           0, 0, 1) %&gt;% \n  lavaan::lav_matrix_lower2full(.)\n\n# level-2 random effects covariance matrix\ntau_2 &lt;- lavaan::cor2cov(R = e_cor, \n                       sds = e_sd, \n                       names = names(e_sd))\n\n# simulate level-2 random effects\ne_2 &lt;- mvtnorm::rmvnorm(n_person, \n                      mean = c(e_0i = 0, e_1i = 0, e_2i = 0), \n                      sigma = tau_2) %&gt;% \n  as_tibble()\n\n# Simulate level-2 data and coefficients\nd_level2 &lt;- tibble(person_id = factor(1:n_person),\n                   n_occasions = n_occasions,\n               working_memory = rnorm(n_person)) %&gt;% \n  bind_cols(e_2) %&gt;% \n  mutate(b_0i = b_00 + b_01 * working_memory + e_0i,\n         b_1i = b_10 + b_11 * working_memory + e_1i,\n         b_2i = b_20 + b_21 * working_memory + e_2i)\n\nd_level2\n\n# A tibble: 2,000 × 9\n   person_id n_occasions working_memory   e_0i    e_1i    e_2i  b_0i  b_1i  b_2i\n   &lt;fct&gt;           &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1                   6        -1.22    41.7  -0.469  -0.639   89.5  43.4  8.46\n 2 2                   6         1.20     4.37  3.83    1.50    76.3  59.8  8.43\n 3 3                   6        -0.956   65.6   4.25    2.10   116.   49.5 11.0 \n 4 4                   6        -0.746  -21.3  -1.69   -2.01    31.2  44.6  6.67\n 5 5                   6        -1.64    -2.79  2.00    1.90    40.9  43.8 11.4 \n 6 6                   6        -0.0452   2.88 -1.25    1.67    62.4  48.5  9.72\n 7 7                   6        -0.933   83.4   0.951  -1.36   134.   46.3  7.48\n 8 8                   6        -0.291   62.8   0.0654  0.341  120.   48.6  8.60\n 9 9                   6         1.36    23.1  -0.727   0.0299  96.7  56.1  6.81\n10 10                  6         0.311   35.2   1.65   -1.12    98.4  53.2  6.60\n# ℹ 1,990 more rows\n\n\n\n\nSimulate Level-1 Data (Occasion Level)\nLet’s say that our measurement of our level-1 variables occurs at the beginning of each Grade starting in Kindergarten (i.e., Grade 0) and continues every year until fifth grade. Our measurement of working memory capacity occurs once—at the beginning of Kindergarten.\n\n# level-1 sd\ntau_1 &lt;- 15^2\n\nd_level1 &lt;- d_level2 %&gt;%\n  uncount(n_occasions) %&gt;%\n  mutate(Grade = row_number() - 1, .by = person_id) %&gt;%\n  mutate(\n    reading_decoding = 6 + 2 * Grade + .5 * working_memory + rnorm(n()),\n    e_ti = rnorm(n(), mean = 0, sd = sqrt(tau_1)),\n    reading_comprehension = b_0i + b_1i * Grade + b_2i * reading_decoding + e_ti\n  ) %&gt;%\n  arrange(Grade, reading_comprehension) %&gt;%\n  mutate(person_id = fct_inorder(person_id)) %&gt;%\n  arrange(person_id, Grade)\n\nd_level1\n\n# A tibble: 12,000 × 12\n   person_id working_memory  e_0i  e_1i   e_2i   b_0i  b_1i  b_2i Grade\n   &lt;fct&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 12                 0.557 -137. -1.92 -1.55   -71.9  50.9  5.95     0\n 2 12                 0.557 -137. -1.92 -1.55   -71.9  50.9  5.95     1\n 3 12                 0.557 -137. -1.92 -1.55   -71.9  50.9  5.95     2\n 4 12                 0.557 -137. -1.92 -1.55   -71.9  50.9  5.95     3\n 5 12                 0.557 -137. -1.92 -1.55   -71.9  50.9  5.95     4\n 6 12                 0.557 -137. -1.92 -1.55   -71.9  50.9  5.95     5\n 7 918               -1.87  -151.  2.37 -0.284 -109.   43.0  9.40     0\n 8 918               -1.87  -151.  2.37 -0.284 -109.   43.0  9.40     1\n 9 918               -1.87  -151.  2.37 -0.284 -109.   43.0  9.40     2\n10 918               -1.87  -151.  2.37 -0.284 -109.   43.0  9.40     3\n# ℹ 11,990 more rows\n# ℹ 3 more variables: reading_decoding &lt;dbl&gt;, e_ti &lt;dbl&gt;,\n#   reading_comprehension &lt;dbl&gt;\n\n\nNotes on the code from above:\n\n\n\n\n\n\n\nCode\nInterpretation\n\n\n\n\nuncount(n_occasions)\nMake copies of each row in d_level2 according to what value is in n_occasions, which in this case is 6 in every row. Thus, the 6 copies of each row d_level2 are made.\n\n\nmutate(Grade = row_number() - 1), .by = person_id\nCreate a variable called Grade by taking the row number of each group defined by person_id and subtract 1. Because there are 6 rows per person, the values within each person will be 0, 1, 2, 3, 4, 5\n\n\n\nIf the data were real, and I wanted to test the hypothesized model, I would run this code:\n\nlibrary(lme4)\nfit &lt;- lmer(reading_comprehension ~ 1 + Grade * working_memory  + reading_decoding*working_memory + (1 + Grade + reading_decoding || person_id), data = d_level1 ) \nsummary(fit)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: \nreading_comprehension ~ 1 + Grade * working_memory + reading_decoding *  \n    working_memory + ((1 | person_id) + (0 + Grade | person_id) +  \n    (0 + reading_decoding | person_id))\n   Data: d_level1\n\nREML criterion at convergence: 109220.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3829 -0.5871 -0.0044  0.5850  3.0921 \n\nRandom effects:\n Groups      Name             Variance Std.Dev.\n person_id   (Intercept)      2460.172 49.600  \n person_id.1 Grade               2.241  1.497  \n person_id.2 reading_decoding    3.517  1.875  \n Residual                      222.953 14.932  \nNumber of obs: 12000, groups:  person_id, 2000\n\nFixed effects:\n                                Estimate Std. Error t value\n(Intercept)                      60.7239     1.4854  40.880\nGrade                            50.1376     0.3272 153.240\nworking_memory                   10.7375     1.4716   7.296\nreading_decoding                  7.9683     0.1640  48.599\nGrade:working_memory              5.5824     0.3233  17.265\nworking_memory:reading_decoding  -1.0912     0.1620  -6.734\n\nCorrelation of Fixed Effects:\n            (Intr) Grade  wrkng_ rdng_d Grd:w_\nGrade        0.587                            \nworkng_mmry  0.045  0.043                     \nredng_dcdng -0.621 -0.932 -0.039              \nGrd:wrkng_m  0.042 -0.010  0.584  0.012       \nwrkng_mmr:_ -0.039  0.012 -0.619 -0.018 -0.931\n\n\nFigure 1 plots the observed data, in which we can see that both reading decoding and reading comprehension increase across grade and that high working memory is associated with higher levels of the reading variables.\n\nggplot(d_level1, aes(reading_decoding, reading_comprehension, color = working_memory)) + \n  geom_point(pch = 16, size = .75) + \n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  facet_grid(rows = vars(Grade), as.table = F, labeller = label_both) +\n  scale_color_gradient2(\"Working\\nMemory\\nCapacity\", mid = \"gray80\", low = \"royalblue\", high = \"firebrick\") + \n  labs(x = \"Reading Decoding\", y = \"Reading Comprehension\")\n\n\n\n\n\n\n\nFigure 1: Reading Decoding Predicting\n\n\n\n\n\nThese effects might be easier to see in an animated plot like Figure 2.\n\nlibrary(gganimate)\nggplot(d_level1 %&gt;% mutate(Grade = factor(Grade)), aes(reading_decoding, reading_comprehension, color = working_memory)) + \n  geom_point(pch = 16, size = .75) + \n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  # facet_grid(rows = vars(Grade), as.table = F, labeller = label_both) +\n  scale_color_gradient2(\"Working\\nMemory\\nCapacity\", mid = \"gray80\", low = \"royalblue\", high = \"firebrick\") + \n  scale_x_continuous(\"Reading Decoding\", limits = c(0,20)) +\n  scale_y_continuous(\"Reading Comprehension\") + \n  transition_states(Grade) + \n  labs(title = 'Grade: {closest_state}') +\n  coord_fixed(1 / 40)\n\n\n\n\n\n\n\nFigure 2: Reading Decoding Predicting\n\n\n\n\n\nIf you need to plot interaction effects, and you want to make your life much, much easier, use the plot_model function from the sjPlot package.\nIn the terms parameter of the plot_model function, the first term is what goes on the x-axis. The second term is the moderator variable that shows up as lines with different colors. By default, it plots values at plus or minus 1 standard deviation, but you can specify any values you wish.\nIn Figure 3, it can be seen that reading decoding is related to reading comprehension but the strength of that relation depends on working memory. The effects of reading decoding are increasingly strong as working memory decreases.\n\nlibrary(sjPlot)\n# Simple slopes of reading decoding at different levels of working memory (collapsing across grade)\nplot_model(fit, \n           type = \"pred\", \n           terms = c(\"reading_decoding\", \"working_memory [-1, 0, 1]\")) \n\n\n\n\n\n\n\nFigure 3: The Effects of Reading Decoding Are Moderated by Working Memory\n\n\n\n\n\nAnother way to think about the interaction is to say that for poor decoders working memory strongly predicts reading comprehension. By contrast, for strong decoders, working memory is a weak predictor. In Figure 4, we are viewing the same interaction seen Figure 3 but we have flipped the x-axis and the color-facet. There is no new information here, but it helps to see the finding from different vantage points.\n\nlibrary(sjPlot)\n# Simple slopes of working memory at different levels of reading decoding (collapsing across grade)\nplot_model(fit, \n           type = \"pred\", \n           terms = c(\"working_memory\", \"reading_decoding [0, 10, 20]\")) \n\n\n\n\n\n\n\nFigure 4: The Effects of Working Memory Depend on Reading Decoding\n\n\n\n\n\nIn Figure 5, we can view the simple slopes of reading decoding at different levels of grade (collapsing across working memory). Because the simple slopes are parallel, it appears that the effect of reading decoding is constant across grades.\n\nplot_model(fit, \n           type = \"pred\", \n           terms = c(\"reading_decoding\", \"Grade\"))\n\n\n\n\n\n\n\nFigure 5: The Effects of Reading Decoding at Different Grades\n\n\n\n\n\nIn Figure 6, we see that reading comprehension grows faster for students with higher working memory than for students with lower working memory.\n\nplot_model(fit, \n           type = \"pred\", \n           terms = c(\"Grade\", \"working_memory [-1, 0, 1]\"))\n\n\n\n\n\n\n\nFigure 6: The Growth of Reading Comprehension Across Grade at Different Levels of Working Memory\n\n\n\n\n\nIn Figure 7, we see the effects of reading decoding, working memory, and grade simultaneously. It appears that reading decoding’s effect is stronger for students with lower working memory but that the effect of working memory becomes increasingly strong across grades.\n\n# Show simple slopes of reading decoding at different levels of grade and working memory\nplot_model(fit, \n           type = \"pred\", \n           terms = c(\"reading_decoding\", \"working_memory [-1, 0, 1]\", \"Grade [0:5]\"))\n\n\n\n\n\n\n\nFigure 7: The Effect of Reading Decoding on Reading Comprehensionat Different Levels of Working Memory and Grade\n\n\n\n\n\nI can’t tell you how much trouble functions like plot_model will save you. I don’t care to admit how many hours of my life have been spent making interaction plots. Of course, ggplot2 made my coding experience much simpler than it used to to be. I mainly use sjPlot to make quick plots. If I needed to make a publication-ready plot, I would probably use code like Figure 8.\n\n# Make predicted values at each unique combination of predictor variables\nd_predicted &lt;- crossing(Grade = 0:5,\n                        working_memory = c(-1, 0, 1),\n                        reading_decoding = c(0, 20)) %&gt;%\n  mutate(reading_comprehension = predict(fit, newdata = ., re.form = NA),\n         working_memory = factor(working_memory,\n                                 levels = c(1, 0,-1),\n                                 labels = c(\"High (+1 SD)\",\n                                            \"Mean\",\n                                            \"Low (-1 SD)\")))\n\nggplot(d_predicted,\n       aes(x = reading_decoding, \n           y = reading_comprehension, \n           color = working_memory)) +\n  geom_line(aes(group = working_memory), size = 1) +\n  facet_grid(cols = vars(Grade), \n             labeller = label_bquote(cols = Grade ~ .(Grade))) +\n  scale_color_manual(\"Working Memory\", \n                     values = c(\"firebrick4\", \"gray30\", \"royalblue4\")) +\n  theme(legend.position = c(1, 0),\n        legend.justification = c(1, 0)) +\n  labs(x = \"Reading Decoding\", \n       y = \"Reading Comprehension\") + \n  ggtitle(label = \"Predicted Values of Reading Comprehension\")\n\n\n\n\n\n\n\nFigure 8: The Effects of Reading Decoding on Reading Comprehension by Working Memory Capacity Across Grades\n\n\n\n\n\nAlso from the sjPlot package, an attractive table:\n\nsjPlot::tab_model(fit)\n\n\n\n\n\n\n \nreading comprehension\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n60.72\n57.81 – 63.64\n&lt;0.001\n\n\nGrade\n50.14\n49.50 – 50.78\n&lt;0.001\n\n\nworking memory\n10.74\n7.85 – 13.62\n&lt;0.001\n\n\nreading decoding\n7.97\n7.65 – 8.29\n&lt;0.001\n\n\nGrade × working memory\n5.58\n4.95 – 6.22\n&lt;0.001\n\n\nworking memory × readingdecoding\n-1.09\n-1.41 – -0.77\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n222.95\n\n\n\nτ00 person_id\n2460.17\n\n\nτ11 person_id.Grade\n2.24\n\n\nτ11 person_id.reading_decoding\n3.52\n\n\nρ01\n \n\n\nρ01\n \n\n\nICC\n0.92\n\n\nN person_id\n2000\n\nObservations\n12000\n\n\nMarginal R2 / Conditional R2\n0.831 / 0.986\n\n\n\n\n\n\nTable 2: Model Results"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#model-building-plan",
    "href": "tutorials/Longitudinal/longitudinal.html#model-building-plan",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Model Building Plan",
    "text": "Model Building Plan\nThe sequence of model building is the same but at every step, start with the time variable (Grade in this case). This sequence is a suggestion only.\n\nNull Model (Random Intercepts)\nLevel 1 Fixed Effects\n\nTime (as many polynomial terms as you hypothesized you would need)\n\nLinear (Time)\nQuadratic (Time + Time ^ 2)\nCubic (Time + Time ^ 2 + Time ^ 3)\nQuartic (Time + Time ^ 2 + Time ^ 3 + Time ^ 4)\nYou can keep going but consider alternatives to polynomials like Generalized Additive Mixed Models.\n\nOther level-1 time-varying covariates\nInteractions of time-varying covariates with Time\nInteractions of time-varying covariates with each other\n\nLevel-2 Fixed Effects\n\nMain effects of level-2 covariates\nInteractions among level-2 covariates\n\nLevel 1 Random Slopes\n\nTime’s random slope\n\nUncorrelated with random intercepts\nCorrelated with random intercepts\n\nOther level-1 random slopes\n\nUncorrelated with random intercepts\nCorrelated with random intercepts\n\nInteractions of random slopes\n\nThese models are pretty unlikely to converge, but it is up to you.\n\n\nCross-Level Interactions\n\nInteraction of level-2 covariates with Time\nInteraction of level-2 covariates with level-1 time-varying covariates"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#load-packages",
    "href": "tutorials/Longitudinal/longitudinal.html#load-packages",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Load Packages",
    "text": "Load Packages\n\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(broom.mixed)\nlibrary(performance)\nlibrary(report)\nlibrary(sjPlot)"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#null-model",
    "href": "tutorials/Longitudinal/longitudinal.html#null-model",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Null Model",
    "text": "Null Model\nLet’s start where we always start, with no predictors except the random intercept variable. In this case it refers to the person-level intercept.\n\nfit_null &lt;- lmer(reading_comprehension ~ 1 + (1 | person_id), data = d_level1)\n\nsummary(fit_null)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_comprehension ~ 1 + (1 | person_id)\n   Data: d_level1\n\nREML criterion at convergence: 150425.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.3787 -0.7944 -0.0162  0.7785  2.8396 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n person_id (Intercept)   630.6   25.11  \n Residual              15707.2  125.33  \nNumber of obs: 12000, groups:  person_id, 2000\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  273.644      1.274   214.7\n\nsjPlot::tab_model(fit_null)\n\n\n\n \nreading comprehension\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n273.64\n271.15 – 276.14\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n15707.18\n\n\n\nτ00 person_id\n630.63\n\n\nICC\n0.04\n\n\nN person_id\n2000\n\nObservations\n12000\n\n\nMarginal R2 / Conditional R2\n0.000 / 0.039\n\n\n\n\nperformance::model_performance(fit_null)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |    RMSE |   Sigma\n---------------------------------------------------------------------------------------\n1.504e+05 | 1.504e+05 | 1.505e+05 |      0.039 |      0.000 | 0.039 | 123.280 | 125.328\n\n\nHere we see that the R2-conditional is 0.04, which is the proportion of variance explained by the entire model. Because the model has only the random intercept as a predictor, it represents the proportion of variance explained by the random intercept (i.e., between-persons variance)."
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#linear-effect-of-time-grade",
    "href": "tutorials/Longitudinal/longitudinal.html#linear-effect-of-time-grade",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Linear Effect of Time (Grade)",
    "text": "Linear Effect of Time (Grade)\n\nfit_time &lt;- lmer(reading_comprehension ~ 1 + Grade + (1 | person_id), data = d_level1)\nsummary(fit_time)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_comprehension ~ 1 + Grade + (1 | person_id)\n   Data: d_level1\n\nREML criterion at convergence: 113425.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.3829 -0.6085 -0.0069  0.6017  3.6388 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n person_id (Intercept) 3183.8   56.42   \n Residual               388.3   19.70   \nNumber of obs: 12000, groups:  person_id, 2000\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 108.2498     1.3014   83.18\nGrade        66.1577     0.1053  628.12\n\nCorrelation of Fixed Effects:\n      (Intr)\nGrade -0.202\n\nsjPlot::tab_model(fit_time)\n\n\n\n \nreading comprehension\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n108.25\n105.70 – 110.80\n&lt;0.001\n\n\nGrade\n66.16\n65.95 – 66.36\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n388.28\n\n\n\nτ00 person_id\n3183.78\n\n\nICC\n0.89\n\n\nN person_id\n2000\n\nObservations\n12000\n\n\nMarginal R2 / Conditional R2\n0.781 / 0.976\n\n\n\n\nperformance::model_performance(fit_time)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n-------------------------------------------------------------------------------------\n1.134e+05 | 1.134e+05 | 1.135e+05 |      0.976 |      0.781 | 0.891 | 18.023 | 19.705\n\nreport::report(fit_time)\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading_comprehension with Grade (formula: reading_comprehension ~ 1\n+ Grade). The model included person_id as random effect (formula: ~1 |\nperson_id). The model's total explanatory power is substantial (conditional R2\n= 0.98) and the part related to the fixed effects alone (marginal R2) is of\n0.78. The model's intercept, corresponding to Grade = 0, is at 108.25 (95% CI\n[105.70, 110.80], t(11996) = 83.18, p &lt; .001). Within this model:\n\n  - The effect of Grade is statistically significant and positive (beta = 66.16,\n95% CI [65.95, 66.36], t(11996) = 628.12, p &lt; .001; Std. beta = 0.88, 95% CI\n[0.88, 0.89])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\nThe effect of time (Grade) is very large. The R2_marginal is 0, which is the proportion of variance explained by all the fixed effects, which in this case is Grade.\nIt is easy to predict that fit_time will be a better model than fit_null, but let’s run the code anyway:\n\nanova(fit_null, fit_time)\n\nData: d_level1\nModels:\nfit_null: reading_comprehension ~ 1 + (1 | person_id)\nfit_time: reading_comprehension ~ 1 + Grade + (1 | person_id)\n         npar    AIC    BIC logLik deviance Chisq Df Pr(&gt;Chisq)    \nfit_null    3 150434 150456 -75214   150428                        \nfit_time    4 113433 113463 -56713   113425 37002  1  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nperformance::compare_performance(fit_null, fit_time, rank = T)\n\n# Comparison of Model Performance Indices\n\nName     |   Model | R2 (cond.) | R2 (marg.) |   ICC |    RMSE |   Sigma\n------------------------------------------------------------------------\nfit_time | lmerMod |      0.976 |      0.781 | 0.891 |  18.023 |  19.705\nfit_null | lmerMod |      0.039 |      0.000 | 0.039 | 123.280 | 125.328\n\nName     | AIC weights | AICc weights | BIC weights | Performance-Score\n-----------------------------------------------------------------------\nfit_time |        1.00 |         1.00 |        1.00 |           100.00%\nfit_null |    0.00e+00 |     0.00e+00 |    0.00e+00 |             0.00%\n\n\nAs expected, the fit_time model fits better than the fit_null model."
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#quadratic-effect-of-time",
    "href": "tutorials/Longitudinal/longitudinal.html#quadratic-effect-of-time",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Quadratic effect of Time",
    "text": "Quadratic effect of Time\nWe did not have a hypothesis about whether time (Grade) would have a linear effect. Let’s see if we need a quadratic effect. We can construct a quadratic effect by using the poly function with a degree of 2. By default, it creates orthogonal polynomials (i.e., uncorrelated). Often raw polynomials are so highly correlated that they cannot be in the same regression formula. If you need raw polynomials (i.e., Grade and Grade squared), then specify poly(Grade, 2, raw = T).\n\nfit_time_quadratic &lt;- lmer(reading_comprehension ~ 1 + poly(Grade, 2) + (1 | person_id), data = d_level1)\nsummary(fit_time_quadratic)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_comprehension ~ 1 + poly(Grade, 2) + (1 | person_id)\n   Data: d_level1\n\nREML criterion at convergence: 113407.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.3812 -0.6088 -0.0071  0.6023  3.6401 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n person_id (Intercept) 3183.8   56.42   \n Residual               388.3   19.71   \nNumber of obs: 12000, groups:  person_id, 2000\n\nFixed effects:\n                 Estimate Std. Error t value\n(Intercept)       273.644      1.274 214.714\npoly(Grade, 2)1 12376.970     19.706 628.087\npoly(Grade, 2)2    -2.301     19.706  -0.117\n\nCorrelation of Fixed Effects:\n            (Intr) p(G,2)1\nply(Grd,2)1 0.000         \nply(Grd,2)2 0.000  0.000  \n\nsjPlot::tab_model(fit_time_quadratic)\n\n\n\n \nreading comprehension\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n273.64\n271.15 – 276.14\n&lt;0.001\n\n\nGrade [1st degree]\n12376.97\n12338.34 – 12415.60\n&lt;0.001\n\n\nGrade [2nd degree]\n-2.30\n-40.93 – 36.33\n0.907\n\n\nRandom Effects\n\n\n\nσ2\n388.32\n\n\n\nτ00 person_id\n3183.77\n\n\nICC\n0.89\n\n\nN person_id\n2000\n\nObservations\n12000\n\n\nMarginal R2 / Conditional R2\n0.781 / 0.976\n\n\n\n\nperformance::model_performance(fit_time_quadratic)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n-------------------------------------------------------------------------------------\n1.134e+05 | 1.134e+05 | 1.135e+05 |      0.976 |      0.781 | 0.891 | 18.023 | 19.706\n\nreport::report(fit_time_quadratic)\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading_comprehension with Grade (formula: reading_comprehension ~ 1\n+ poly(Grade, 2)). The model included person_id as random effect (formula: ~1 |\nperson_id). The model's total explanatory power is substantial (conditional R2\n= 0.98) and the part related to the fixed effects alone (marginal R2) is of\n0.78. The model's intercept, corresponding to Grade = 0, is at 273.64 (95% CI\n[271.15, 276.14], t(11995) = 214.71, p &lt; .001). Within this model:\n\n  - The effect of Grade [1st degree] is statistically significant and positive\n(beta = 12376.97, 95% CI [12338.34, 12415.60], t(11995) = 628.09, p &lt; .001;\nStd. beta = 96.83, 95% CI [96.53, 97.13])\n  - The effect of Grade [2nd degree] is statistically non-significant and\nnegative (beta = -2.30, 95% CI [-40.93, 36.33], t(11995) = -0.12, p = 0.907;\nStd. beta = -0.02, 95% CI [-0.32, 0.28])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n# Compare linear and quadratic models\nanova(fit_time, fit_time_quadratic)\n\nData: d_level1\nModels:\nfit_time: reading_comprehension ~ 1 + Grade + (1 | person_id)\nfit_time_quadratic: reading_comprehension ~ 1 + poly(Grade, 2) + (1 | person_id)\n                   npar    AIC    BIC logLik deviance  Chisq Df Pr(&gt;Chisq)\nfit_time              4 113433 113463 -56713   113425                     \nfit_time_quadratic    5 113435 113472 -56713   113425 0.0136  1      0.907\n\n\nNo effect! We could add a third-order polynomial (i.e., the effect of Grade to the third power), but we did not hypothesize such an effect. Let’s stick with the linear effect of time.\nIf we plot our data, we get this:\n\nggplot(d_level1, aes(Grade, reading_comprehension, color = person_id)) +\n  geom_line(aes(group = person_id), alpha = 0.2) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 9: Linear Effects of Time on Reading Comprehension\n\n\n\n\n\nIt looks like the effect of time really is linear."
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#fixed-effect-of-reading-decoding-level-1-time-varying-covariate",
    "href": "tutorials/Longitudinal/longitudinal.html#fixed-effect-of-reading-decoding-level-1-time-varying-covariate",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Fixed effect of reading decoding (level 1 time-varying covariate)",
    "text": "Fixed effect of reading decoding (level 1 time-varying covariate)\nUnsurprisingly, reading decoding is a predictor of reading comprehension:\n\nfit_decoding &lt;- lmer(reading_comprehension ~ 1 + Grade + reading_decoding + (1 | person_id), data = d_level1)\nsummary(fit_decoding)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_comprehension ~ 1 + Grade + reading_decoding + (1 | person_id)\n   Data: d_level1\n\nREML criterion at convergence: 111582.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.3167 -0.6144 -0.0032  0.6033  3.5148 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n person_id (Intercept) 3044     55.18   \n Residual               326     18.05   \nNumber of obs: 12000, groups:  person_id, 2000\n\nFixed effects:\n                 Estimate Std. Error t value\n(Intercept)       59.7447     1.6669   35.84\nGrade             50.0036     0.3731  134.03\nreading_decoding   8.0665     0.1800   44.83\n\nCorrelation of Fixed Effects:\n            (Intr) Grade \nGrade        0.590       \nredng_dcdng -0.649 -0.966\n\nsjPlot::tab_model(fit_decoding)\n\n\n\n \nreading comprehension\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n59.74\n56.48 – 63.01\n&lt;0.001\n\n\nGrade\n50.00\n49.27 – 50.73\n&lt;0.001\n\n\nreading decoding\n8.07\n7.71 – 8.42\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n325.96\n\n\n\nτ00 person_id\n3044.37\n\n\nICC\n0.90\n\n\nN person_id\n2000\n\nObservations\n12000\n\n\nMarginal R2 / Conditional R2\n0.792 / 0.980\n\n\n\n\nperformance::model_performance(fit_decoding)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n-------------------------------------------------------------------------------------\n1.116e+05 | 1.116e+05 | 1.116e+05 |      0.980 |      0.792 | 0.903 | 16.509 | 18.054\n\nreport::report(fit_decoding)\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading_comprehension with Grade and reading_decoding (formula:\nreading_comprehension ~ 1 + Grade + reading_decoding). The model included\nperson_id as random effect (formula: ~1 | person_id). The model's total\nexplanatory power is substantial (conditional R2 = 0.98) and the part related\nto the fixed effects alone (marginal R2) is of 0.79. The model's intercept,\ncorresponding to Grade = 0 and reading_decoding = 0, is at 59.74 (95% CI\n[56.48, 63.01], t(11995) = 35.84, p &lt; .001). Within this model:\n\n  - The effect of Grade is statistically significant and positive (beta = 50.00,\n95% CI [49.27, 50.73], t(11995) = 134.03, p &lt; .001; Std. beta = 0.67, 95% CI\n[0.66, 0.68])\n  - The effect of reading decoding is statistically significant and positive\n(beta = 8.07, 95% CI [7.71, 8.42], t(11995) = 44.82, p &lt; .001; Std. beta =\n0.23, 95% CI [0.22, 0.24])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n# Compare with linear effect of time\nanova(fit_time, fit_decoding)\n\nData: d_level1\nModels:\nfit_time: reading_comprehension ~ 1 + Grade + (1 | person_id)\nfit_decoding: reading_comprehension ~ 1 + Grade + reading_decoding + (1 | person_id)\n             npar    AIC    BIC logLik deviance  Chisq Df Pr(&gt;Chisq)    \nfit_time        4 113433 113463 -56713   113425                         \nfit_decoding    5 111590 111627 -55790   111580 1844.9  1  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nperformance::compare_performance(fit_time, fit_decoding, rank = T)\n\n# Comparison of Model Performance Indices\n\nName         |   Model | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n--------------------------------------------------------------------------\nfit_decoding | lmerMod |      0.980 |      0.792 | 0.903 | 16.509 | 18.054\nfit_time     | lmerMod |      0.976 |      0.781 | 0.891 | 18.023 | 19.705\n\nName         | AIC weights | AICc weights | BIC weights | Performance-Score\n---------------------------------------------------------------------------\nfit_decoding |        1.00 |         1.00 |        1.00 |           100.00%\nfit_time     |    0.00e+00 |     0.00e+00 |    0.00e+00 |             0.00%\n\n\nThe t value of the fixed effect of reading_decoding is huge. It is significant. The model comparisons with fit_time suggests that fit_decoding is clearly preferred.\n\nreport::report(fit_decoding)\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading_comprehension with Grade and reading_decoding (formula:\nreading_comprehension ~ 1 + Grade + reading_decoding). The model included\nperson_id as random effect (formula: ~1 | person_id). The model's total\nexplanatory power is substantial (conditional R2 = 0.98) and the part related\nto the fixed effects alone (marginal R2) is of 0.79. The model's intercept,\ncorresponding to Grade = 0 and reading_decoding = 0, is at 59.74 (95% CI\n[56.48, 63.01], t(11995) = 35.84, p &lt; .001). Within this model:\n\n  - The effect of Grade is statistically significant and positive (beta = 50.00,\n95% CI [49.27, 50.73], t(11995) = 134.03, p &lt; .001; Std. beta = 0.67, 95% CI\n[0.66, 0.68])\n  - The effect of reading decoding is statistically significant and positive\n(beta = 8.07, 95% CI [7.71, 8.42], t(11995) = 44.82, p &lt; .001; Std. beta =\n0.23, 95% CI [0.22, 0.24])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\n\nplot_model(fit_decoding, \n           type = \"pred\", \n           terms = c(\"reading_decoding\", \"Grade [0:5]\"),\n           title = \"The Effect of Reading Decoding on Reading Comprehension\\n at Different Grades\")"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#level-1-interaction-effects",
    "href": "tutorials/Longitudinal/longitudinal.html#level-1-interaction-effects",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Level-1 interaction effects",
    "text": "Level-1 interaction effects\nLet’s see if the effect of reading decoding depends on Grade.\n\nfit_decoding_x_Grade &lt;- lmer(reading_comprehension ~ 1 + Grade * reading_decoding + (1 | person_id), data = d_level1)\nsummary(fit_decoding_x_Grade)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_comprehension ~ 1 + Grade * reading_decoding + (1 | person_id)\n   Data: d_level1\n\nREML criterion at convergence: 111552.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.3295 -0.6159  0.0036  0.6114  3.4131 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n person_id (Intercept) 3044.3   55.18   \n Residual               324.9   18.02   \nNumber of obs: 12000, groups:  person_id, 2000\n\nFixed effects:\n                       Estimate Std. Error t value\n(Intercept)            63.70831    1.79477  35.497\nGrade                  47.97522    0.50605  94.803\nreading_decoding        7.61071    0.19540  38.950\nGrade:reading_decoding  0.18358    0.03099   5.924\n\nCorrelation of Fixed Effects:\n            (Intr) Grade  rdng_d\nGrade        0.150              \nredng_dcdng -0.700 -0.387       \nGrd:rdng_dc  0.372 -0.677 -0.393\n\nsjPlot::tab_model(fit_decoding_x_Grade)\n\n\n\n \nreading comprehension\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n63.71\n60.19 – 67.23\n&lt;0.001\n\n\nGrade\n47.98\n46.98 – 48.97\n&lt;0.001\n\n\nreading decoding\n7.61\n7.23 – 7.99\n&lt;0.001\n\n\nGrade × reading decoding\n0.18\n0.12 – 0.24\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n324.86\n\n\n\nτ00 person_id\n3044.29\n\n\nICC\n0.90\n\n\nN person_id\n2000\n\nObservations\n12000\n\n\nMarginal R2 / Conditional R2\n0.792 / 0.980\n\n\n\n\nperformance::model_performance(fit_decoding_x_Grade)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n-------------------------------------------------------------------------------------\n1.116e+05 | 1.116e+05 | 1.116e+05 |      0.980 |      0.792 | 0.904 | 16.480 | 18.024\n\nreport::report(fit_decoding_x_Grade)\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading_comprehension with Grade and reading_decoding (formula:\nreading_comprehension ~ 1 + Grade * reading_decoding). The model included\nperson_id as random effect (formula: ~1 | person_id). The model's total\nexplanatory power is substantial (conditional R2 = 0.98) and the part related\nto the fixed effects alone (marginal R2) is of 0.79. The model's intercept,\ncorresponding to Grade = 0 and reading_decoding = 0, is at 63.71 (95% CI\n[60.19, 67.23], t(11994) = 35.50, p &lt; .001). Within this model:\n\n  - The effect of Grade is statistically significant and positive (beta = 47.98,\n95% CI [46.98, 48.97], t(11994) = 94.80, p &lt; .001; Std. beta = 0.67, 95% CI\n[0.66, 0.68])\n  - The effect of reading decoding is statistically significant and positive\n(beta = 7.61, 95% CI [7.23, 7.99], t(11994) = 38.95, p &lt; .001; Std. beta =\n0.23, 95% CI [0.22, 0.24])\n  - The effect of Grade × reading decoding is statistically significant and\npositive (beta = 0.18, 95% CI [0.12, 0.24], t(11994) = 5.92, p &lt; .001; Std.\nbeta = 8.83e-03, 95% CI [5.91e-03, 0.01])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n# compare with main effects model\nanova(fit_decoding, fit_decoding_x_Grade)\n\nData: d_level1\nModels:\nfit_decoding: reading_comprehension ~ 1 + Grade + reading_decoding + (1 | person_id)\nfit_decoding_x_Grade: reading_comprehension ~ 1 + Grade * reading_decoding + (1 | person_id)\n                     npar    AIC    BIC logLik deviance  Chisq Df Pr(&gt;Chisq)\nfit_decoding            5 111590 111627 -55790   111580                     \nfit_decoding_x_Grade    6 111557 111602 -55773   111545 35.044  1  3.224e-09\n                        \nfit_decoding            \nfit_decoding_x_Grade ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nperformance::compare_performance(fit_decoding, fit_decoding_x_Grade, rank = T)\n\n# Comparison of Model Performance Indices\n\nName                 |   Model | R2 (cond.) | R2 (marg.) |   ICC |   RMSE\n-------------------------------------------------------------------------\nfit_decoding_x_Grade | lmerMod |      0.980 |      0.792 | 0.904 | 16.480\nfit_decoding         | lmerMod |      0.980 |      0.792 | 0.903 | 16.509\n\nName                 |  Sigma | AIC weights | AICc weights | BIC weights | Performance-Score\n--------------------------------------------------------------------------------------------\nfit_decoding_x_Grade | 18.024 |       1.000 |        1.000 |       1.000 |           100.00%\nfit_decoding         | 18.054 |    6.68e-08 |     6.68e-08 |    2.69e-06 |             0.00%\n\n\n\nplot_model(fit_decoding_x_Grade, \n           type = \"pred\", \n           terms = c(\"Grade [0:5]\", \"reading_decoding\"))\n\n\n\n\n\n\n\n\nSo the interaction effect is present, but tiny."
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#level-2-covariate-working-memory",
    "href": "tutorials/Longitudinal/longitudinal.html#level-2-covariate-working-memory",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Level-2 covariate: working memory",
    "text": "Level-2 covariate: working memory\nAs seen below, the effect of working memory is statistically significant, meaning that students with higher working memory tend to have better reading comprehension.\n\nfit_working &lt;- lmer(reading_comprehension ~ 1 + Grade * reading_decoding + working_memory + (1 | person_id), data = d_level1)\nsummary(fit_working)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: \nreading_comprehension ~ 1 + Grade * reading_decoding + working_memory +  \n    (1 | person_id)\n   Data: d_level1\n\nREML criterion at convergence: 111443\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.3005 -0.6165  0.0010  0.6123  3.4503 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n person_id (Intercept) 2884.8   53.71   \n Residual               324.8   18.02   \nNumber of obs: 12000, groups:  person_id, 2000\n\nFixed effects:\n                       Estimate Std. Error t value\n(Intercept)            64.19123    1.77295  36.206\nGrade                  48.24408    0.50670  95.212\nreading_decoding        7.47852    0.19581  38.193\nworking_memory         12.67919    1.20755  10.500\nGrade:reading_decoding  0.18332    0.03099   5.916\n\nCorrelation of Fixed Effects:\n            (Intr) Grade  rdng_d wrkng_\nGrade        0.153                     \nredng_dcdng -0.709 -0.389              \nworkng_mmry  0.029  0.054 -0.068       \nGrd:rdng_dc  0.377 -0.676 -0.392 -0.001\n\nsjPlot::tab_model(fit_working)\n\n\n\n \nreading comprehension\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n64.19\n60.72 – 67.67\n&lt;0.001\n\n\nGrade\n48.24\n47.25 – 49.24\n&lt;0.001\n\n\nreading decoding\n7.48\n7.09 – 7.86\n&lt;0.001\n\n\nworking memory\n12.68\n10.31 – 15.05\n&lt;0.001\n\n\nGrade × reading decoding\n0.18\n0.12 – 0.24\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n324.84\n\n\n\nτ00 person_id\n2884.79\n\n\nICC\n0.90\n\n\nN person_id\n2000\n\nObservations\n12000\n\n\nMarginal R2 / Conditional R2\n0.803 / 0.980\n\n\n\n\nperformance::model_performance(fit_working)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n-------------------------------------------------------------------------------------\n1.115e+05 | 1.115e+05 | 1.115e+05 |      0.980 |      0.803 | 0.899 | 16.481 | 18.023\n\nreport::report(fit_working)\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading_comprehension with Grade, reading_decoding and\nworking_memory (formula: reading_comprehension ~ 1 + Grade * reading_decoding +\nworking_memory). The model included person_id as random effect (formula: ~1 |\nperson_id). The model's total explanatory power is substantial (conditional R2\n= 0.98) and the part related to the fixed effects alone (marginal R2) is of\n0.80. The model's intercept, corresponding to Grade = 0, reading_decoding = 0\nand working_memory = 0, is at 64.19 (95% CI [60.72, 67.67], t(11993) = 36.21, p\n&lt; .001). Within this model:\n\n  - The effect of Grade is statistically significant and positive (beta = 48.24,\n95% CI [47.25, 49.24], t(11993) = 95.21, p &lt; .001; Std. beta = 0.67, 95% CI\n[0.66, 0.68])\n  - The effect of reading decoding is statistically significant and positive\n(beta = 7.48, 95% CI [7.09, 7.86], t(11993) = 38.19, p &lt; .001; Std. beta =\n0.22, 95% CI [0.21, 0.23])\n  - The effect of working memory is statistically significant and positive (beta\n= 12.68, 95% CI [10.31, 15.05], t(11993) = 10.50, p &lt; .001; Std. beta = 0.10,\n95% CI [0.08, 0.12])\n  - The effect of Grade × reading decoding is statistically significant and\npositive (beta = 0.18, 95% CI [0.12, 0.24], t(11993) = 5.92, p &lt; .001; Std.\nbeta = 8.81e-03, 95% CI [5.89e-03, 0.01])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n# compare with main effects model\nanova(fit_decoding_x_Grade, fit_working)\n\nData: d_level1\nModels:\nfit_decoding_x_Grade: reading_comprehension ~ 1 + Grade * reading_decoding + (1 | person_id)\nfit_working: reading_comprehension ~ 1 + Grade * reading_decoding + working_memory + (1 | person_id)\n                     npar    AIC    BIC logLik deviance  Chisq Df Pr(&gt;Chisq)\nfit_decoding_x_Grade    6 111557 111602 -55773   111545                     \nfit_working             7 111452 111504 -55719   111438 107.46  1  &lt; 2.2e-16\n                        \nfit_decoding_x_Grade    \nfit_working          ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nperformance::compare_performance(fit_decoding_x_Grade, fit_working, rank = T)\n\n# Comparison of Model Performance Indices\n\nName                 |   Model | R2 (cond.) | R2 (marg.) |   ICC |   RMSE\n-------------------------------------------------------------------------\nfit_working          | lmerMod |      0.980 |      0.803 | 0.899 | 16.481\nfit_decoding_x_Grade | lmerMod |      0.980 |      0.792 | 0.904 | 16.480\n\nName                 |  Sigma | AIC weights | AICc weights | BIC weights | Performance-Score\n--------------------------------------------------------------------------------------------\nfit_working          | 18.023 |        1.00 |         1.00 |        1.00 |            75.00%\nfit_decoding_x_Grade | 18.024 |    1.26e-23 |     1.26e-23 |    5.06e-22 |            25.00%\n\nplot_model(fit_working, \n           type = \"pred\", \n           terms = c(\"working_memory [-2, 2]\", \"reading_decoding\", \"Grade [0:5]\"))"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#level-1-random-slope-grade",
    "href": "tutorials/Longitudinal/longitudinal.html#level-1-random-slope-grade",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Level-1 random slope: Grade",
    "text": "Level-1 random slope: Grade\nAs seen below, letting the slope of Grade be random improves the fit of the model:\n\nfit_grade_random_uncorrelated &lt;- lmer(reading_comprehension ~ 1 + Grade * reading_decoding + working_memory + (1 + Grade || person_id), data = d_level1)\nsummary(fit_grade_random_uncorrelated)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: \nreading_comprehension ~ 1 + Grade * reading_decoding + working_memory +  \n    ((1 | person_id) + (0 + Grade | person_id))\n   Data: d_level1\n\nREML criterion at convergence: 110049.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2898 -0.5710 -0.0043  0.5689  3.2100 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n person_id   (Intercept) 2647.26  51.452  \n person_id.1 Grade         28.63   5.351  \n Residual                 226.98  15.066  \nNumber of obs: 12000, groups:  person_id, 2000\n\nFixed effects:\n                       Estimate Std. Error t value\n(Intercept)            61.32490    1.62690  37.694\nGrade                  49.44598    0.46061 107.350\nreading_decoding        7.84505    0.17440  44.983\nworking_memory          6.84561    1.16575   5.872\nGrade:reading_decoding  0.06248    0.02639   2.368\n\nCorrelation of Fixed Effects:\n            (Intr) Grade  rdng_d wrkng_\nGrade        0.179                     \nredng_dcdng -0.683 -0.418              \nworkng_mmry  0.029  0.036 -0.070       \nGrd:rdng_dc  0.346 -0.637 -0.370  0.018\n\nsjPlot::tab_model(fit_grade_random_uncorrelated)\n\n\n\n \nreading comprehension\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n61.32\n58.14 – 64.51\n&lt;0.001\n\n\nGrade\n49.45\n48.54 – 50.35\n&lt;0.001\n\n\nreading decoding\n7.85\n7.50 – 8.19\n&lt;0.001\n\n\nworking memory\n6.85\n4.56 – 9.13\n&lt;0.001\n\n\nGrade × reading decoding\n0.06\n0.01 – 0.11\n0.018\n\n\nRandom Effects\n\n\n\nσ2\n226.98\n\n\n\nτ00 person_id\n2647.26\n\n\nτ11 person_id.Grade\n28.63\n\n\nρ01\n \n\n\nρ01\n \n\n\nICC\n0.92\n\n\nN person_id\n2000\n\nObservations\n12000\n\n\nMarginal R2 / Conditional R2\n0.818 / 0.986\n\n\n\n\nperformance::model_performance(fit_grade_random_uncorrelated)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n-------------------------------------------------------------------------------------\n1.101e+05 | 1.101e+05 | 1.101e+05 |      0.986 |      0.818 | 0.921 | 12.811 | 15.066\n\nreport::report(fit_grade_random_uncorrelated)\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading_comprehension with Grade, reading_decoding and\nworking_memory (formula: reading_comprehension ~ 1 + Grade * reading_decoding +\nworking_memory). The model included person_id as random effects (formula:\nlist(~1 | person_id, ~0 + Grade | person_id)). The model's total explanatory\npower is substantial (conditional R2 = 0.99) and the part related to the fixed\neffects alone (marginal R2) is of 0.82. The model's intercept, corresponding to\nGrade = 0, reading_decoding = 0 and working_memory = 0, is at 61.32 (95% CI\n[58.14, 64.51], t(11992) = 37.69, p &lt; .001). Within this model:\n\n  - The effect of Grade is statistically significant and positive (beta = 49.45,\n95% CI [48.54, 50.35], t(11992) = 107.35, p &lt; .001; Std. beta = 0.67, 95% CI\n[0.66, 0.68])\n  - The effect of reading decoding is statistically significant and positive\n(beta = 7.85, 95% CI [7.50, 8.19], t(11992) = 44.98, p &lt; .001; Std. beta =\n0.23, 95% CI [0.22, 0.23])\n  - The effect of working memory is statistically significant and positive (beta\n= 6.85, 95% CI [4.56, 9.13], t(11992) = 5.87, p &lt; .001; Std. beta = 0.10, 95%\nCI [0.08, 0.12])\n  - The effect of Grade × reading decoding is statistically significant and\npositive (beta = 0.06, 95% CI [0.01, 0.11], t(11992) = 2.37, p = 0.018; Std.\nbeta = 3.13e-03, 95% CI [6.33e-04, 5.62e-03])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n# compare with main effects model (note refit = F because we are comparing random effects)\nanova(fit_working, fit_grade_random_uncorrelated, refit = F)\n\nData: d_level1\nModels:\nfit_working: reading_comprehension ~ 1 + Grade * reading_decoding + working_memory + (1 | person_id)\nfit_grade_random_uncorrelated: reading_comprehension ~ 1 + Grade * reading_decoding + working_memory + ((1 | person_id) + (0 + Grade | person_id))\n                              npar    AIC    BIC logLik deviance  Chisq Df\nfit_working                      7 111457 111509 -55721   111443          \nfit_grade_random_uncorrelated    8 110066 110125 -55025   110050 1393.2  1\n                              Pr(&gt;Chisq)    \nfit_working                                 \nfit_grade_random_uncorrelated  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nperformance::compare_performance(fit_working, fit_grade_random_uncorrelated, rank = T)\n\n# Comparison of Model Performance Indices\n\nName                          |   Model | R2 (cond.) | R2 (marg.) |   ICC\n-------------------------------------------------------------------------\nfit_grade_random_uncorrelated | lmerMod |      0.986 |      0.818 | 0.921\nfit_working                   | lmerMod |      0.980 |      0.803 | 0.899\n\nName                          |   RMSE |  Sigma | AIC weights | AICc weights\n----------------------------------------------------------------------------\nfit_grade_random_uncorrelated | 12.811 | 15.066 |        1.00 |         1.00\nfit_working                   | 16.481 | 18.023 |   8.74e-303 |    8.75e-303\n\nName                          | BIC weights | Performance-Score\n---------------------------------------------------------------\nfit_grade_random_uncorrelated |        1.00 |           100.00%\nfit_working                   |   3.52e-301 |             0.00%"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#level-1-random-slope-reading_decoding",
    "href": "tutorials/Longitudinal/longitudinal.html#level-1-random-slope-reading_decoding",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Level-1 random slope: reading_decoding",
    "text": "Level-1 random slope: reading_decoding\nWe run into a model convergence problem when we add the random slope for reading decoding.\n\nfit_decoding_random_uncorrelated &lt;- lmer(reading_comprehension ~ 1 + Grade * reading_decoding + working_memory + (1 + Grade + reading_decoding || person_id), data = d_level1)\n\nWe can try a different optimizer to see if that helps. The lmer function’s previous default optimizer (Nelder_Mead) sometimes finds a good solution when the current default optimizer (bobyqa) does not.\n\nfit_decoding_random_uncorrelated &lt;- lmer(reading_comprehension ~ 1 + Grade * reading_decoding + working_memory + (1 + Grade + reading_decoding || person_id), data = d_level1, control = lmerControl(optimizer=\"Nelder_Mead\"))\n\nIt worked!\n\nsummary(fit_decoding_random_uncorrelated)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: \nreading_comprehension ~ 1 + Grade * reading_decoding + working_memory +  \n    ((1 | person_id) + (0 + Grade | person_id) + (0 + reading_decoding |  \n        person_id))\n   Data: d_level1\nControl: lmerControl(optimizer = \"Nelder_Mead\")\n\nREML criterion at convergence: 110004.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2379 -0.5687 -0.0066  0.5675  3.1103 \n\nRandom effects:\n Groups      Name             Variance Std.Dev.\n person_id   (Intercept)      2495.324 49.953  \n person_id.1 Grade              17.901  4.231  \n person_id.2 reading_decoding    2.558  1.599  \n Residual                      225.299 15.010  \nNumber of obs: 12000, groups:  person_id, 2000\n\nFixed effects:\n                       Estimate Std. Error t value\n(Intercept)            61.16775    1.60592  38.089\nGrade                  49.37052    0.45458 108.607\nreading_decoding        7.85081    0.17819  44.059\nworking_memory          4.80887    1.15773   4.154\nGrade:reading_decoding  0.06668    0.02645   2.521\n\nCorrelation of Fixed Effects:\n            (Intr) Grade  rdng_d wrkng_\nGrade        0.182                     \nredng_dcdng -0.680 -0.413              \nworkng_mmry  0.034  0.032 -0.071       \nGrd:rdng_dc  0.352 -0.647 -0.363  0.025\n\nsjPlot::tab_model(fit_decoding_random_uncorrelated)\n\n\n\n \nreading comprehension\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n61.17\n58.02 – 64.32\n&lt;0.001\n\n\nGrade\n49.37\n48.48 – 50.26\n&lt;0.001\n\n\nreading decoding\n7.85\n7.50 – 8.20\n&lt;0.001\n\n\nworking memory\n4.81\n2.54 – 7.08\n&lt;0.001\n\n\nGrade × reading decoding\n0.07\n0.01 – 0.12\n0.012\n\n\nRandom Effects\n\n\n\nσ2\n225.30\n\n\n\nτ00 person_id\n2495.32\n\n\nτ11 person_id.Grade\n17.90\n\n\nτ11 person_id.reading_decoding\n2.56\n\n\nρ01\n \n\n\nρ01\n \n\n\nICC\n0.92\n\n\nN person_id\n2000\n\nObservations\n12000\n\n\nMarginal R2 / Conditional R2\n0.826 / 0.986\n\n\n\n\nperformance::model_performance(fit_decoding_random_uncorrelated)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n-------------------------------------------------------------------------------------\n1.100e+05 | 1.100e+05 | 1.101e+05 |      0.986 |      0.826 | 0.917 | 12.747 | 15.010\n\nreport::report(fit_decoding_random_uncorrelated)\n\nWe fitted a linear mixed model (estimated using REML and Nelder-Mead optimizer)\nto predict reading_comprehension with Grade, reading_decoding and\nworking_memory (formula: reading_comprehension ~ 1 + Grade * reading_decoding +\nworking_memory). The model included person_id as random effects (formula:\nlist(~1 | person_id, ~0 + Grade | person_id, ~0 + reading_decoding |\nperson_id)). The model's total explanatory power is substantial (conditional R2\n= 0.99) and the part related to the fixed effects alone (marginal R2) is of\n0.83. The model's intercept, corresponding to Grade = 0, reading_decoding = 0\nand working_memory = 0, is at 61.17 (95% CI [58.02, 64.32], t(11991) = 38.09, p\n&lt; .001). Within this model:\n\n  - The effect of Grade is statistically significant and positive (beta = 49.37,\n95% CI [48.48, 50.26], t(11991) = 108.61, p &lt; .001; Std. beta = 0.67, 95% CI\n[0.66, 0.68])\n  - The effect of reading decoding is statistically significant and positive\n(beta = 7.85, 95% CI [7.50, 8.20], t(11991) = 44.06, p &lt; .001; Std. beta =\n0.23, 95% CI [0.22, 0.23])\n  - The effect of working memory is statistically significant and positive (beta\n= 4.81, 95% CI [2.54, 7.08], t(11991) = 4.15, p &lt; .001; Std. beta = 0.10, 95%\nCI [0.08, 0.12])\n  - The effect of Grade × reading decoding is statistically significant and\npositive (beta = 0.07, 95% CI [0.01, 0.12], t(11991) = 2.52, p = 0.012; Std.\nbeta = 3.28e-03, 95% CI [7.88e-04, 5.78e-03])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n# compare with main effects model (note refit = F because we are comparing random effects)\nanova(fit_grade_random_uncorrelated, fit_decoding_random_uncorrelated, refit = F)\n\nData: d_level1\nModels:\nfit_grade_random_uncorrelated: reading_comprehension ~ 1 + Grade * reading_decoding + working_memory + ((1 | person_id) + (0 + Grade | person_id))\nfit_decoding_random_uncorrelated: reading_comprehension ~ 1 + Grade * reading_decoding + working_memory + ((1 | person_id) + (0 + Grade | person_id) + (0 + reading_decoding | person_id))\n                                 npar    AIC    BIC logLik deviance  Chisq Df\nfit_grade_random_uncorrelated       8 110066 110125 -55025   110050          \nfit_decoding_random_uncorrelated    9 110023 110089 -55002   110005 44.976  1\n                                 Pr(&gt;Chisq)    \nfit_grade_random_uncorrelated                  \nfit_decoding_random_uncorrelated  1.995e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nperformance::compare_performance(fit_grade_random_uncorrelated, fit_decoding_random_uncorrelated, rank = T)\n\n# Comparison of Model Performance Indices\n\nName                             |   Model | R2 (cond.) | R2 (marg.) |   ICC\n----------------------------------------------------------------------------\nfit_decoding_random_uncorrelated | lmerMod |      0.986 |      0.826 | 0.917\nfit_grade_random_uncorrelated    | lmerMod |      0.986 |      0.818 | 0.921\n\nName                             |   RMSE |  Sigma | AIC weights | AICc weights\n-------------------------------------------------------------------------------\nfit_decoding_random_uncorrelated | 12.747 | 15.010 |       1.000 |        1.000\nfit_grade_random_uncorrelated    | 12.811 | 15.066 |    4.66e-10 |     4.67e-10\n\nName                             | BIC weights | Performance-Score\n------------------------------------------------------------------\nfit_decoding_random_uncorrelated |       1.000 |            75.00%\nfit_grade_random_uncorrelated    |    1.88e-08 |            25.00%"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#level-1-random-intercept-slope-correlations",
    "href": "tutorials/Longitudinal/longitudinal.html#level-1-random-intercept-slope-correlations",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Level-1 random intercept-slope correlations",
    "text": "Level-1 random intercept-slope correlations\nDo the intercepts and slopes correlate?\n\nfit_correlated_intercept_slopes &lt;- lmer(reading_comprehension ~ 1 + Grade * reading_decoding + working_memory + (1 + Grade + reading_decoding | person_id), data = d_level1 %&gt;% mutate_at(vars(reading_decoding, working_memory), scale, center = T, scale = F), control = lmerControl(optimizer=\"Nelder_Mead\"))\nsummary(fit_correlated_intercept_slopes)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: \nreading_comprehension ~ 1 + Grade * reading_decoding + working_memory +  \n    (1 + Grade + reading_decoding | person_id)\n   Data: d_level1 %&gt;% mutate_at(vars(reading_decoding, working_memory),  \n    scale, center = T, scale = F)\nControl: lmerControl(optimizer = \"Nelder_Mead\")\n\nREML criterion at convergence: 109996.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2902 -0.5690 -0.0060  0.5652  3.1303 \n\nRandom effects:\n Groups    Name             Variance Std.Dev. Corr       \n person_id (Intercept)      2979.649 54.586              \n           Grade              32.179  5.673   -0.13      \n           reading_decoding    4.533  2.129    0.43 -0.47\n Residual                    223.467 14.949              \nNumber of obs: 12000, groups:  person_id, 2000\n\nFixed effects:\n                        Estimate Std. Error t value\n(Intercept)            147.91594    1.49422   98.99\nGrade                   50.09898    0.35818  139.87\nreading_decoding         7.85703    0.18135   43.33\nworking_memory           4.07157    1.15659    3.52\nGrade:reading_decoding   0.06527    0.02642    2.47\n\nCorrelation of Fixed Effects:\n            (Intr) Grade  rdng_d wrkng_\nGrade       -0.562                     \nredng_dcdng  0.615 -0.855              \nworkng_mmry -0.036  0.062 -0.071       \nGrd:rdng_dc -0.099 -0.007 -0.357  0.028\n\n\nThe model did not converge. I checked various solutions (different optimizers, more iterations, centering variables), but none worked for me. Do the slopes and intercepts correlate? From various attempts, the answer seems to be that they do not, but it is hard to be sure. The random slopes for reading decoding and Grade might be negatively correlated, but I am reluctant to trust the results of a non-convergent model, especially when I have no theoretical reason to expect such a correlation. Therefore, I am going to go with the uncorrelated slopes and intercepts model."
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#cross-level-interaction-working-memory-grade",
    "href": "tutorials/Longitudinal/longitudinal.html#cross-level-interaction-working-memory-grade",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Cross-level interaction: Working Memory × Grade",
    "text": "Cross-level interaction: Working Memory × Grade\nAs seen below, working memory has a positive interaction with Grade, meaning that the growth of reading comprehension (i.e., the effect of Grade) is faster with students with higher working memory capacity.\nNote that the cross-level interaction is significant—and the level-1 interaction between Reading Decoding and Grade is no longer significant. What happened? Usually we would have to be pretty clever to figure out the reason, but we happen to know how the data were simulated: Working Memory and Grade really do interact, and Reading Decoding and Grade really do not. However, Working Memory and Grade are correlated (i.e., Working Memory increases over time). When Grade was not allowed to interact with Working Memory, Reading Decoding interacted with Grade, a correlate of Working Memory. Now that the true interaction has been included in the model, the false interaction is no longer significant. We can leave it in for now, but we can get rid of it at the end.\n\nfit_wm_x_grade &lt;- lmer(reading_comprehension ~ 1 + working_memory * Grade + Grade * reading_decoding +  (1 + Grade + reading_decoding || person_id), data = d_level1)\n\nsummary(fit_wm_x_grade)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_comprehension ~ 1 + working_memory * Grade + Grade *  \n    reading_decoding + ((1 | person_id) + (0 + Grade | person_id) +  \n    (0 + reading_decoding | person_id))\n   Data: d_level1\n\nREML criterion at convergence: 109269.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4081 -0.5853 -0.0036  0.5802  3.0518 \n\nRandom effects:\n Groups      Name             Variance Std.Dev.\n person_id   (Intercept)      2459.903 49.597  \n person_id.1 Grade               1.746  1.321  \n person_id.2 reading_decoding    3.629  1.905  \n Residual                      224.121 14.971  \nNumber of obs: 12000, groups:  person_id, 2000\n\nFixed effects:\n                        Estimate Std. Error t value\n(Intercept)            60.470968   1.589967  38.033\nworking_memory          4.623729   1.157350   3.995\nGrade                  50.095459   0.440333 113.767\nreading_decoding        7.932988   0.176734  44.887\nworking_memory:Grade    3.554948   0.118362  30.035\nGrade:reading_decoding  0.006277   0.026503   0.237\n\nCorrelation of Fixed Effects:\n            (Intr) wrkng_ Grade  rdng_d wrk_:G\nworkng_mmry  0.034                            \nGrade        0.171  0.030                     \nredng_dcdng -0.672 -0.069 -0.400              \nwrkng_mmr:G -0.024  0.028  0.078  0.030       \nGrd:rdng_dc  0.356  0.026 -0.669 -0.367 -0.112\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00396576 (tol = 0.002, component 1)\n\nsjPlot::tab_model(fit_wm_x_grade)\n\n\n\n \nreading comprehension\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n60.47\n57.35 – 63.59\n&lt;0.001\n\n\nworking memory\n4.62\n2.36 – 6.89\n&lt;0.001\n\n\nGrade\n50.10\n49.23 – 50.96\n&lt;0.001\n\n\nreading decoding\n7.93\n7.59 – 8.28\n&lt;0.001\n\n\nworking memory × Grade\n3.55\n3.32 – 3.79\n&lt;0.001\n\n\nGrade × reading decoding\n0.01\n-0.05 – 0.06\n0.813\n\n\nRandom Effects\n\n\n\nσ2\n224.12\n\n\n\nτ00 person_id\n2459.90\n\n\nτ11 person_id.Grade\n1.75\n\n\nτ11 person_id.reading_decoding\n3.63\n\n\nρ01\n \n\n\nρ01\n \n\n\nICC\n0.92\n\n\nN person_id\n2000\n\nObservations\n12000\n\n\nMarginal R2 / Conditional R2\n0.831 / 0.986\n\n\n\n\nperformance::model_performance(fit_wm_x_grade)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n-------------------------------------------------------------------------------------\n1.093e+05 | 1.093e+05 | 1.094e+05 |      0.986 |      0.831 | 0.916 | 12.932 | 14.971\n\nreport::report(fit_wm_x_grade)\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading_comprehension with working_memory, Grade and\nreading_decoding (formula: reading_comprehension ~ 1 + working_memory * Grade +\nGrade * reading_decoding). The model included person_id as random effects\n(formula: list(~1 | person_id, ~0 + Grade | person_id, ~0 + reading_decoding |\nperson_id)). The model's total explanatory power is substantial (conditional R2\n= 0.99) and the part related to the fixed effects alone (marginal R2) is of\n0.83. The model's intercept, corresponding to working_memory = 0, Grade = 0 and\nreading_decoding = 0, is at 60.47 (95% CI [57.35, 63.59], t(11990) = 38.03, p &lt;\n.001). Within this model:\n\n  - The effect of working memory is statistically significant and positive (beta\n= 4.62, 95% CI [2.36, 6.89], t(11990) = 4.00, p &lt; .001; Std. beta = 0.10, 95%\nCI [0.08, 0.12])\n  - The effect of Grade is statistically significant and positive (beta = 50.10,\n95% CI [49.23, 50.96], t(11990) = 113.77, p &lt; .001; Std. beta = 0.67, 95% CI\n[0.66, 0.68])\n  - The effect of reading decoding is statistically significant and positive\n(beta = 7.93, 95% CI [7.59, 8.28], t(11990) = 44.89, p &lt; .001; Std. beta =\n0.22, 95% CI [0.21, 0.23])\n  - The effect of working memory × Grade is statistically significant and\npositive (beta = 3.55, 95% CI [3.32, 3.79], t(11990) = 30.03, p &lt; .001; Std.\nbeta = 0.05, 95% CI [0.04, 0.05])\n  - The effect of Grade × reading decoding is statistically non-significant and\npositive (beta = 6.28e-03, 95% CI [-0.05, 0.06], t(11990) = 0.24, p = 0.813;\nStd. beta = 4.20e-04, 95% CI [-2.08e-03, 2.92e-03])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n# compare with main effects model (note refit = F because we are comparing random effects)\nanova(fit_grade_random_uncorrelated, fit_wm_x_grade, refit = F)\n\nData: d_level1\nModels:\nfit_grade_random_uncorrelated: reading_comprehension ~ 1 + Grade * reading_decoding + working_memory + ((1 | person_id) + (0 + Grade | person_id))\nfit_wm_x_grade: reading_comprehension ~ 1 + working_memory * Grade + Grade * reading_decoding + ((1 | person_id) + (0 + Grade | person_id) + (0 + reading_decoding | person_id))\n                              npar    AIC    BIC logLik deviance Chisq Df\nfit_grade_random_uncorrelated    8 110066 110125 -55025   110050         \nfit_wm_x_grade                  10 109289 109363 -54635   109269 780.7  2\n                              Pr(&gt;Chisq)    \nfit_grade_random_uncorrelated               \nfit_wm_x_grade                 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nperformance::compare_performance(fit_grade_random_uncorrelated, fit_wm_x_grade, rank = T)\n\n# Comparison of Model Performance Indices\n\nName                          |   Model | R2 (cond.) | R2 (marg.) |   ICC\n-------------------------------------------------------------------------\nfit_wm_x_grade                | lmerMod |      0.986 |      0.831 | 0.916\nfit_grade_random_uncorrelated | lmerMod |      0.986 |      0.818 | 0.921\n\nName                          |   RMSE |  Sigma | AIC weights | AICc weights\n----------------------------------------------------------------------------\nfit_wm_x_grade                | 12.932 | 14.971 |        1.00 |         1.00\nfit_grade_random_uncorrelated | 12.811 | 15.066 |   5.33e-170 |    5.35e-170\n\nName                          | BIC weights | Performance-Score\n---------------------------------------------------------------\nfit_wm_x_grade                |        1.00 |            75.00%\nfit_grade_random_uncorrelated |   8.66e-167 |            25.00%"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#cross-level-interaction-working-memory-reading-decoding",
    "href": "tutorials/Longitudinal/longitudinal.html#cross-level-interaction-working-memory-reading-decoding",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Cross-level interaction: Working Memory × Reading Decoding",
    "text": "Cross-level interaction: Working Memory × Reading Decoding\nThe interaction of working memory and reading decoding is negative, meaning that reading decoding is a weaker predictor of reading comprehension among students with higher working memory.\n\nfit_wm_x_decoding &lt;- lmer(reading_comprehension ~ 1 + working_memory * reading_decoding + working_memory * Grade  +  reading_decoding*Grade + (1 + Grade + reading_decoding || person_id), data = d_level1)\nsummary(fit_wm_x_decoding)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_comprehension ~ 1 + working_memory * reading_decoding +  \n    working_memory * Grade + reading_decoding * Grade + ((1 |  \n    person_id) + (0 + Grade | person_id) + (0 + reading_decoding |  \n    person_id))\n   Data: d_level1\n\nREML criterion at convergence: 109225.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3824 -0.5867 -0.0049  0.5853  3.0929 \n\nRandom effects:\n Groups      Name             Variance Std.Dev.\n person_id   (Intercept)      2460.171 49.600  \n person_id.1 Grade               2.239  1.496  \n person_id.2 reading_decoding    3.517  1.875  \n Residual                      222.981 14.933  \nNumber of obs: 12000, groups:  person_id, 2000\n\nFixed effects:\n                                 Estimate Std. Error t value\n(Intercept)                     60.789650   1.588845  38.260\nworking_memory                  10.738982   1.471720   7.297\nreading_decoding                 7.960728   0.176274  45.161\nGrade                           50.103391   0.439645 113.963\nworking_memory:reading_decoding -1.090817   0.162071  -6.730\nworking_memory:Grade             5.580178   0.323878  17.229\nreading_decoding:Grade           0.003082   0.026438   0.117\n\nCorrelation of Fixed Effects:\n            (Intr) wrkng_ rdng_d Grade  wrk_:_ wrk_:G\nworkng_mmry  0.045                                   \nredng_dcdng -0.670 -0.040                            \nGrade        0.171  0.026 -0.400                     \nwrkng_mmr:_ -0.030 -0.618 -0.023 -0.003              \nwrkng_mmr:G  0.019  0.583  0.032  0.031 -0.931       \nrdng_dcdn:G  0.355  0.009 -0.367 -0.668  0.018 -0.058\n\nsjPlot::tab_model(fit_wm_x_decoding)\n\n\n\n \nreading comprehension\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n60.79\n57.68 – 63.90\n&lt;0.001\n\n\nworking memory\n10.74\n7.85 – 13.62\n&lt;0.001\n\n\nreading decoding\n7.96\n7.62 – 8.31\n&lt;0.001\n\n\nGrade\n50.10\n49.24 – 50.97\n&lt;0.001\n\n\nworking memory × readingdecoding\n-1.09\n-1.41 – -0.77\n&lt;0.001\n\n\nworking memory × Grade\n5.58\n4.95 – 6.22\n&lt;0.001\n\n\nreading decoding × Grade\n0.00\n-0.05 – 0.05\n0.907\n\n\nRandom Effects\n\n\n\nσ2\n222.98\n\n\n\nτ00 person_id\n2460.17\n\n\nτ11 person_id.Grade\n2.24\n\n\nτ11 person_id.reading_decoding\n3.52\n\n\nρ01\n \n\n\nρ01\n \n\n\nICC\n0.92\n\n\nN person_id\n2000\n\nObservations\n12000\n\n\nMarginal R2 / Conditional R2\n0.831 / 0.986\n\n\n\n\nperformance::model_performance(fit_wm_x_decoding)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n-------------------------------------------------------------------------------------\n1.092e+05 | 1.092e+05 | 1.093e+05 |      0.986 |      0.831 | 0.917 | 12.892 | 14.933\n\nreport::report(fit_wm_x_decoding)\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading_comprehension with working_memory, reading_decoding and\nGrade (formula: reading_comprehension ~ 1 + working_memory * reading_decoding +\nworking_memory * Grade + reading_decoding * Grade). The model included\nperson_id as random effects (formula: list(~1 | person_id, ~0 + Grade |\nperson_id, ~0 + reading_decoding | person_id)). The model's total explanatory\npower is substantial (conditional R2 = 0.99) and the part related to the fixed\neffects alone (marginal R2) is of 0.83. The model's intercept, corresponding to\nworking_memory = 0, reading_decoding = 0 and Grade = 0, is at 60.79 (95% CI\n[57.68, 63.90], t(11989) = 38.26, p &lt; .001). Within this model:\n\n  - The effect of working memory is statistically significant and positive (beta\n= 10.74, 95% CI [7.85, 13.62], t(11989) = 7.30, p &lt; .001; Std. beta = 0.10, 95%\nCI [0.08, 0.12])\n  - The effect of reading decoding is statistically significant and positive\n(beta = 7.96, 95% CI [7.62, 8.31], t(11989) = 45.16, p &lt; .001; Std. beta =\n0.22, 95% CI [0.21, 0.23])\n  - The effect of Grade is statistically significant and positive (beta = 50.10,\n95% CI [49.24, 50.97], t(11989) = 113.96, p &lt; .001; Std. beta = 0.67, 95% CI\n[0.66, 0.68])\n  - The effect of working memory × reading decoding is statistically significant\nand negative (beta = -1.09, 95% CI [-1.41, -0.77], t(11989) = -6.73, p &lt; .001;\nStd. beta = -0.03, 95% CI [-0.04, -0.02])\n  - The effect of working memory × Grade is statistically significant and\npositive (beta = 5.58, 95% CI [4.95, 6.22], t(11989) = 17.23, p &lt; .001; Std.\nbeta = 0.08, 95% CI [0.07, 0.08])\n  - The effect of reading decoding × Grade is statistically non-significant and\npositive (beta = 3.08e-03, 95% CI [-0.05, 0.05], t(11989) = 0.12, p = 0.907;\nStd. beta = 2.63e-04, 95% CI [-2.23e-03, 2.76e-03])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n# compare with main effects model (note refit = F because we are comparing random effects)\nanova(fit_wm_x_grade, fit_wm_x_decoding, refit = F)\n\nData: d_level1\nModels:\nfit_wm_x_grade: reading_comprehension ~ 1 + working_memory * Grade + Grade * reading_decoding + ((1 | person_id) + (0 + Grade | person_id) + (0 + reading_decoding | person_id))\nfit_wm_x_decoding: reading_comprehension ~ 1 + working_memory * reading_decoding + working_memory * Grade + reading_decoding * Grade + ((1 | person_id) + (0 + Grade | person_id) + (0 + reading_decoding | person_id))\n                  npar    AIC    BIC logLik deviance  Chisq Df Pr(&gt;Chisq)    \nfit_wm_x_grade      10 109289 109363 -54635   109269                         \nfit_wm_x_decoding   11 109248 109329 -54613   109226 43.292  1  4.714e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nperformance::compare_performance(fit_wm_x_grade, fit_wm_x_decoding, rank = T)\n\n# Comparison of Model Performance Indices\n\nName              |   Model | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n-------------------------------------------------------------------------------\nfit_wm_x_decoding | lmerMod |      0.986 |      0.831 | 0.917 | 12.892 | 14.933\nfit_wm_x_grade    | lmerMod |      0.986 |      0.831 | 0.916 | 12.932 | 14.971\n\nName              | AIC weights | AICc weights | BIC weights | Performance-Score\n--------------------------------------------------------------------------------\nfit_wm_x_decoding |       1.000 |        1.000 |       1.000 |            87.50%\nfit_wm_x_grade    |    4.36e-10 |     4.37e-10 |    1.76e-08 |            12.50%"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#final-model",
    "href": "tutorials/Longitudinal/longitudinal.html#final-model",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Final model",
    "text": "Final model\nLet’s remove the now non-significant level-1 interaction, which brings us to the true model we simulated.\n\nfit_final &lt;- lmer(reading_comprehension ~ 1 + working_memory * reading_decoding + working_memory * Grade +  (1 + Grade + reading_decoding || person_id), data = d_level1)\nsummary(fit_final)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_comprehension ~ 1 + working_memory * reading_decoding +  \n    working_memory * Grade + ((1 | person_id) + (0 + Grade |  \n    person_id) + (0 + reading_decoding | person_id))\n   Data: d_level1\n\nREML criterion at convergence: 109220.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3828 -0.5871 -0.0044  0.5850  3.0921 \n\nRandom effects:\n Groups      Name             Variance Std.Dev.\n person_id   (Intercept)      2460.165 49.600  \n person_id.1 Grade               2.241  1.497  \n person_id.2 reading_decoding    3.517  1.875  \n Residual                      222.953 14.932  \nNumber of obs: 12000, groups:  person_id, 2000\n\nFixed effects:\n                                Estimate Std. Error t value\n(Intercept)                      60.7239     1.4854  40.880\nworking_memory                   10.7375     1.4716   7.296\nreading_decoding                  7.9683     0.1640  48.599\nGrade                            50.1376     0.3272 153.240\nworking_memory:reading_decoding  -1.0912     0.1620  -6.734\nworking_memory:Grade              5.5824     0.3233  17.265\n\nCorrelation of Fixed Effects:\n            (Intr) wrkng_ rdng_d Grade  wrk_:_\nworkng_mmry  0.045                            \nredng_dcdng -0.621 -0.039                     \nGrade        0.587  0.043 -0.932              \nwrkng_mmr:_ -0.039 -0.619 -0.018  0.012       \nwrkng_mmr:G  0.042  0.584  0.012 -0.010 -0.931\n\nsjPlot::tab_model(fit_final)\n\n\n\n \nreading comprehension\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n60.72\n57.81 – 63.64\n&lt;0.001\n\n\nworking memory\n10.74\n7.85 – 13.62\n&lt;0.001\n\n\nreading decoding\n7.97\n7.65 – 8.29\n&lt;0.001\n\n\nGrade\n50.14\n49.50 – 50.78\n&lt;0.001\n\n\nworking memory × readingdecoding\n-1.09\n-1.41 – -0.77\n&lt;0.001\n\n\nworking memory × Grade\n5.58\n4.95 – 6.22\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n222.95\n\n\n\nτ00 person_id\n2460.16\n\n\nτ11 person_id.Grade\n2.24\n\n\nτ11 person_id.reading_decoding\n3.52\n\n\nρ01\n \n\n\nρ01\n \n\n\nICC\n0.92\n\n\nN person_id\n2000\n\nObservations\n12000\n\n\nMarginal R2 / Conditional R2\n0.831 / 0.986\n\n\n\n\nperformance::model_performance(fit_final)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n-------------------------------------------------------------------------------------\n1.092e+05 | 1.092e+05 | 1.093e+05 |      0.986 |      0.831 | 0.917 | 12.892 | 14.932\n\nreport::report(fit_final)\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading_comprehension with working_memory, reading_decoding and\nGrade (formula: reading_comprehension ~ 1 + working_memory * reading_decoding +\nworking_memory * Grade). The model included person_id as random effects\n(formula: list(~1 | person_id, ~0 + Grade | person_id, ~0 + reading_decoding |\nperson_id)). The model's total explanatory power is substantial (conditional R2\n= 0.99) and the part related to the fixed effects alone (marginal R2) is of\n0.83. The model's intercept, corresponding to working_memory = 0,\nreading_decoding = 0 and Grade = 0, is at 60.72 (95% CI [57.81, 63.64],\nt(11990) = 40.88, p &lt; .001). Within this model:\n\n  - The effect of working memory is statistically significant and positive (beta\n= 10.74, 95% CI [7.85, 13.62], t(11990) = 7.30, p &lt; .001; Std. beta = 0.10, 95%\nCI [0.08, 0.12])\n  - The effect of reading decoding is statistically significant and positive\n(beta = 7.97, 95% CI [7.65, 8.29], t(11990) = 48.60, p &lt; .001; Std. beta =\n0.22, 95% CI [0.21, 0.23])\n  - The effect of Grade is statistically significant and positive (beta = 50.14,\n95% CI [49.50, 50.78], t(11990) = 153.24, p &lt; .001; Std. beta = 0.67, 95% CI\n[0.66, 0.68])\n  - The effect of working memory × reading decoding is statistically significant\nand negative (beta = -1.09, 95% CI [-1.41, -0.77], t(11990) = -6.73, p &lt; .001;\nStd. beta = -0.03, 95% CI [-0.04, -0.02])\n  - The effect of working memory × Grade is statistically significant and\npositive (beta = 5.58, 95% CI [4.95, 6.22], t(11990) = 17.27, p &lt; .001; Std.\nbeta = 0.08, 95% CI [0.07, 0.08])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n# compare with main effects model (note refit = F because we are comparing random effects)\nanova(fit_wm_x_decoding, fit_final, refit = F)\n\nData: d_level1\nModels:\nfit_final: reading_comprehension ~ 1 + working_memory * reading_decoding + working_memory * Grade + ((1 | person_id) + (0 + Grade | person_id) + (0 + reading_decoding | person_id))\nfit_wm_x_decoding: reading_comprehension ~ 1 + working_memory * reading_decoding + working_memory * Grade + reading_decoding * Grade + ((1 | person_id) + (0 + Grade | person_id) + (0 + reading_decoding | person_id))\n                  npar    AIC    BIC logLik deviance Chisq Df Pr(&gt;Chisq)\nfit_final           10 109240 109314 -54610   109220                    \nfit_wm_x_decoding   11 109248 109329 -54613   109226     0  1          1\n\nperformance::compare_performance(fit_wm_x_decoding, fit_final, rank = T)\n\n# Comparison of Model Performance Indices\n\nName              |   Model | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n-------------------------------------------------------------------------------\nfit_final         | lmerMod |      0.986 |      0.831 | 0.917 | 12.892 | 14.932\nfit_wm_x_decoding | lmerMod |      0.986 |      0.831 | 0.917 | 12.892 | 14.933\n\nName              | AIC weights | AICc weights | BIC weights | Performance-Score\n--------------------------------------------------------------------------------\nfit_final         |       0.730 |        0.730 |       0.991 |           100.00%\nfit_wm_x_decoding |       0.270 |        0.270 |       0.009 |             0.00%\n\n\nIn this plot, we can see that Reading Decoding’s effect is steeper for individuals with lower working memory:\n\nplot_model(fit_final, \n           type = \"pred\", \n           terms = c(\"reading_decoding\", \"working_memory [-1, 0, 1]\", \"Grade [0:5]\"),\n           title = \"The Effect of Reading Decoding on Reading Comprehension\\n at Different Levels of Working Memory and Grade\")\n\n\n\n\n\n\n\n\nAn alternate view of the same model suggests that the effect of working memory on reading comprehension is minimal in Kindergarten (Grade 0) and 1st grade when the focus is more on reading letters, single words, and very short sentences. However as the focus moves to longer, more complex sentences and paragraphs, the effect of Working Memory on Reading Comprehension increases.\n\nplot_model(fit_final, \n           type = \"pred\", \n           terms = c(\"working_memory [-2, 2]\", \"reading_decoding [5, 10, 15]\", \"Grade [0:5]\"),\n           title = \"The Effect of Working Memory on Reading Comprehension\\n at Different Levels of Reading Decoding and Grade\") + \n  theme_light(base_size = 16, base_family = \"Roboto Condensed\") +\n  theme(legend.position = c(0,1), legend.justification = c(0,1)) + \n  scale_color_viridis_d(\"Reading\\nDecoding\", begin = .2, end = .8) + \n  labs(x = \"Working Memory\", y = \"Reading Comprehension\")\n\n\n\n\n\n\n\nFigure 10\n\n\n\n\n\nAnother view of the same model suggests that students with higher Working Memory have faster Reading Comprehension growth rates, even after controlling for Reading Decoding.\n\nplot_model(fit_final, \n           type = \"pred\", \n           terms = c(\"Grade [0:5]\", \"working_memory [-1, 0, 1]\", \"reading_decoding [5, 10, 15]\"),\n           title = \"The Effect of Working Memory on Reading Comprehension\\n at Different Levels of Reading Decoding and Grade\") + \n  theme_light(base_size = 16, base_family = \"Roboto Condensed\") +\n  theme(legend.position = c(0,1), legend.justification = c(0,1)) + \n  scale_color_viridis_d(\"Working\\nMemory\", begin = .2, end = .8) + \n  labs(x = \"Grade\", y = \"Reading Comprehension\")\n\n\n\n\n\n\n\nFigure 11\n\n\n\n\n\n\n\n\n\n\n\nNoteReflect\n\n\n\nHow this method could be used in your own research?"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#load-data-and-packages",
    "href": "tutorials/Longitudinal/longitudinal.html#load-data-and-packages",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Load Data and Packages",
    "text": "Load Data and Packages\n\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(easystats)\nlibrary(sjPlot)\n\nd &lt;- read_csv(\"https://github.com/wjschne/EDUC5529/raw/master/hw7_meditation.csv\", \n              col_types = cols(\n                person_id = col_factor(),\n                Week = col_integer(),\n                Anxiety = col_double(),\n                meditation_hours = col_integer(),\n                therapist_support = col_double()))"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#fit-random-intercept-model",
    "href": "tutorials/Longitudinal/longitudinal.html#fit-random-intercept-model",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Fit random intercept model",
    "text": "Fit random intercept model\nTry setting up the code yourself. Call the model fit_0\nRun the summary function and the model_performance function on the model.\n\n\n\n\n\n\nTipHint (Click)\n\n\n\n\n\n\nfit_0 &lt;- lmer(Anxiety ~ 1 + (1 | person_id), data = d)\nsummary(fit_0)\nperformance::model_performance(fit_0)\n\n\n\n\n\nBetween-Person Proportion of Variance\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nWhere would you look in the output for summary(fit_0) to find the average level of Anxiety across the entire data set?\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nIn model fit_0, what proportion of variance in Anxiety is between persons?"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#lets-make-a-nice-table",
    "href": "tutorials/Longitudinal/longitudinal.html#lets-make-a-nice-table",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Let’s make a nice table",
    "text": "Let’s make a nice table\n\nsjPlot::tab_model(fit_0)\n\n\n\n\n\n\n \nAnxiety\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n65.00\n64.62 – 65.38\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n69.71\n\n\n\nτ00 person_id\n30.32\n\n\nICC\n0.30\n\n\nN person_id\n1000\n\nObservations\n11000\n\n\nMarginal R2 / Conditional R2\n0.000 / 0.303\n\n\n\n\n\n\nTable 3: Null Model Results"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#fit-the-linear-effect-of-time-i.e.-week",
    "href": "tutorials/Longitudinal/longitudinal.html#fit-the-linear-effect-of-time-i.e.-week",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Fit the linear effect of time (i.e., Week)",
    "text": "Fit the linear effect of time (i.e., Week)\nEstimate the linear effect of Week. Try setting up the code yourself. Call the model fit_week\nRun summary, model_performance, and report on the model.\n\n\n\n\n\n\nTipClick to see code\n\n\n\n\n\n\nfit_week &lt;- lmer(Anxiety ~ 1 + Week + (1 | person_id), data = d)\nsummary(fit_week)\nperformance::model_performance(fit_week)\nreport::report(fit_week)\n\n\n\n\nLet’s compare the fit of both models:\n\nanova(fit_0, fit_week)\n\nData: d\nModels:\nfit_0: Anxiety ~ 1 + (1 | person_id)\nfit_week: Anxiety ~ 1 + Week + (1 | person_id)\n         npar   AIC   BIC logLik deviance  Chisq Df Pr(&gt;Chisq)    \nfit_0       3 79664 79686 -39829    79658                         \nfit_week    4 77765 77794 -38878    77757 1901.6  1  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncompare_performance(fit_0, fit_week)\n\n# Comparison of Model Performance Indices\n\nName     |   Model |   AIC (weights) |  AICc (weights) |   BIC (weights)\n------------------------------------------------------------------------\nfit_0    | lmerMod | 79664.4 (&lt;.001) | 79664.4 (&lt;.001) | 79686.3 (&lt;.001)\nfit_week | lmerMod | 77764.8 (&gt;.999) | 77764.8 (&gt;.999) | 77794.0 (&gt;.999)\n\nName     | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n----------------------------------------------------------\nfit_0    |      0.303 |      0.000 | 0.303 | 8.029 | 8.349\nfit_week |      0.424 |      0.110 | 0.353 | 7.290 | 7.592\n\n\nBy every criterion, fit_week fits better than fit_0. Thus, there is a significant association with Week and Anxiety. Because the slope of Week is negative, people’s anxiety decreased over time by about -1.05 points per week.\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nIn model fit_week, what proportion of variance in Anxiety is explained by Week?\n\n\n\n\n\n\n\n\nTipHint (Click)\n\n\n\n\n\nLook at the R2_marginal column in the performance::model_performance(fit_week) output.\n\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nIn the output of summary(fit_week), where would you look to see how much anxiety is changing from week to week, on average?\n\n\n\nPlot the Model and Make a Nice Table\n\nplot_model(fit_week, type = \"pred\", terms = \"Week\")\n\n\n\n\n\n\n\nFigure 12: Predicted Effects of Week\n\n\n\n\n\n\ntab_model(fit_0, fit_week, dv.labels = c(\"Null Model\", \"Week\"), show.ci = F)\n\n\n\n\n\n\n \nNull Model\nWeek\n\n\nPredictors\nEstimates\np\nEstimates\np\n\n\n(Intercept)\n65.00\n&lt;0.001\n70.24\n&lt;0.001\n\n\nWeek\n\n\n-1.05\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n69.71\n57.64\n\n\n\nτ00\n30.32 person_id\n31.42 person_id\n\n\nICC\n0.30\n0.35\n\n\nN\n1000 person_id\n1000 person_id\n\nObservations\n11000\n11000\n\n\nMarginal R2 / Conditional R2\n0.000 / 0.303\n0.110 / 0.424\n\n\n\n\n\n\nTable 4: The Linear Effect of Week"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#add-the-quadratic-effect-of-week.",
    "href": "tutorials/Longitudinal/longitudinal.html#add-the-quadratic-effect-of-week.",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Add the quadratic effect of Week.",
    "text": "Add the quadratic effect of Week.\nWe did not predict non-linear effects of time, but it is good idea to check. Usually you include enough polynomial degrees to be consistent with your models. If you have no prediction, add them one at a time, but be conservative in doing so.\n\nfit_week_2 &lt;- lmer(Anxiety ~ 1 + poly(Week, 2) + (1 | person_id), data = d)\nsummary(fit_week_2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Anxiety ~ 1 + poly(Week, 2) + (1 | person_id)\n   Data: d\n\nREML criterion at convergence: 77745.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.9612 -0.4497  0.1040  0.6023  2.8793 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n person_id (Intercept) 31.42    5.605   \n Residual              57.64    7.592   \nNumber of obs: 11000, groups:  person_id, 1000\n\nFixed effects:\n                Estimate Std. Error t value\n(Intercept)      65.0000     0.1915 339.502\npoly(Week, 2)1 -347.4464     7.5924 -45.763\npoly(Week, 2)2   -6.9694     7.5924  -0.918\n\nCorrelation of Fixed Effects:\n            (Intr) p(W,2)1\npoly(Wk,2)1 0.000         \npoly(Wk,2)2 0.000  0.000  \n\nanova(fit_week, fit_week_2)\n\nData: d\nModels:\nfit_week: Anxiety ~ 1 + Week + (1 | person_id)\nfit_week_2: Anxiety ~ 1 + poly(Week, 2) + (1 | person_id)\n           npar   AIC   BIC logLik deviance  Chisq Df Pr(&gt;Chisq)\nfit_week      4 77765 77794 -38878    77757                     \nfit_week_2    5 77766 77802 -38878    77756 0.8428  1     0.3586\n\n\nHere we see that the quadratic effect of Week is not significant, so we need not continue adding polynomial degrees. If we did, it would look like this:\n\nfit_week_3 &lt;- lmer(Anxiety ~ 1 + poly(Week, 3) + (1 | person_id), data = d)\nsummary(fit_week_3)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Anxiety ~ 1 + poly(Week, 3) + (1 | person_id)\n   Data: d\n\nREML criterion at convergence: 77737.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.9654 -0.4497  0.1032  0.6000  2.8946 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n person_id (Intercept) 31.42    5.605   \n Residual              57.64    7.592   \nNumber of obs: 11000, groups:  person_id, 1000\n\nFixed effects:\n                Estimate Std. Error t value\n(Intercept)      65.0000     0.1915 339.502\npoly(Week, 3)1 -347.4464     7.5920 -45.765\npoly(Week, 3)2   -6.9694     7.5920  -0.918\npoly(Week, 3)3  -10.3752     7.5920  -1.367\n\nCorrelation of Fixed Effects:\n            (Intr) p(W,3)1 p(W,3)2\npoly(Wk,3)1 0.000                 \npoly(Wk,3)2 0.000  0.000          \npoly(Wk,3)3 0.000  0.000   0.000  \n\nanova(fit_week, fit_week_2, fit_week_3)\n\nData: d\nModels:\nfit_week: Anxiety ~ 1 + Week + (1 | person_id)\nfit_week_2: Anxiety ~ 1 + poly(Week, 2) + (1 | person_id)\nfit_week_3: Anxiety ~ 1 + poly(Week, 3) + (1 | person_id)\n           npar   AIC   BIC logLik deviance  Chisq Df Pr(&gt;Chisq)\nfit_week      4 77765 77794 -38878    77757                     \nfit_week_2    5 77766 77802 -38878    77756 0.8428  1     0.3586\nfit_week_3    6 77766 77810 -38877    77754 1.8679  1     0.1717\n\n\nGiven the lack of polynomial effects, we will stick with just the linear effect in fit_week."
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#add-the-l1-time-covarying-predictor-meditation_hours-to-the-model",
    "href": "tutorials/Longitudinal/longitudinal.html#add-the-l1-time-covarying-predictor-meditation_hours-to-the-model",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Add the L1 time-covarying predictor, meditation_hours to the model",
    "text": "Add the L1 time-covarying predictor, meditation_hours to the model\nTry setting up the code yourself. Call the model fit_meditation\nRun summary, model_performance, and report on the model. Compare fit_meditation with the previous model, fit_week.\n\n\n\n\n\n\nTipHint (Click)\n\n\n\n\n\n\nfit_meditation &lt;- lmer(Anxiety ~ 1 + Week + meditation_hours + (1 | person_id), data = d)\nsummary(fit_meditation)\nperformance::model_performance(fit_meditation)\nreport::report(fit_meditation)\n\nanova(fit_week, fit_meditation)\ncompare_performance(fit_week, fit_meditation)\n\n\n\n\nWhen you the output, you will see that the effect of meditation_hours is significant. From the report function, the standardized beta (the effect if all variables were standardized) for meditation_hours is larger than the effect of Week. Theoretically, this result makes sense. Anxiety should decrease because of the intervention, not just because of time passing. Still, the effect of Week is significant, perhaps because of therapist contact and/or regression to the mean.\nNice plot and table:\n\nplot_model(fit_meditation, \n           type = \"pred\", \n           terms = c(\"Week\", \"meditation_hours [0, 5, 10]\"))\n\n\n\n\n\n\n\nFigure 13: The Predicted Effects of Week by Meditation Hours\n\n\n\n\n\n\ntab_model(fit_meditation)\n\n\n\n\n\n\n \nAnxiety\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n76.96\n76.57 – 77.34\n&lt;0.001\n\n\nWeek\n-1.07\n-1.09 – -1.04\n&lt;0.001\n\n\nmeditation hours\n-1.11\n-1.12 – -1.09\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n20.60\n\n\n\nτ00 person_id\n29.20\n\n\nICC\n0.59\n\n\nN person_id\n1000\n\nObservations\n11000\n\n\nMarginal R2 / Conditional R2\n0.497 / 0.792\n\n\n\n\n\n\nTable 5: Model for the Combined Effects of Week and Meditation"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#add-the-predicted-interaction-of-week-and-meditation_hours.",
    "href": "tutorials/Longitudinal/longitudinal.html#add-the-predicted-interaction-of-week-and-meditation_hours.",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Add the predicted interaction of Week and meditation_hours.",
    "text": "Add the predicted interaction of Week and meditation_hours.\nCall the model fit_week_x_meditation\n\nfit_week_x_meditation &lt;- lmer(Anxiety ~ 1 + Week * meditation_hours + (1 | person_id), data = d)\nsummary(fit_week_x_meditation)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Anxiety ~ 1 + Week * meditation_hours + (1 | person_id)\n   Data: d\n\nREML criterion at convergence: 61358.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.0746 -0.6355  0.0015  0.6369  4.4524 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n person_id (Intercept) 29.50    5.431   \n Residual              11.36    3.370   \nNumber of obs: 11000, groups:  person_id, 1000\n\nFixed effects:\n                       Estimate Std. Error t value\n(Intercept)           71.866534   0.194086 370.281\nWeek                  -0.041797   0.015228  -2.745\nmeditation_hours      -0.267135   0.011141 -23.978\nWeek:meditation_hours -0.170020   0.001884 -90.265\n\nCorrelation of Fixed Effects:\n            (Intr) Week   mdttn_\nWeek        -0.392              \nmedittn_hrs -0.348  0.626       \nWk:mdttn_hr  0.290 -0.745 -0.836\n\nperformance::model_performance(fit_week_x_meditation)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n-----------------------------------------------------------------------------------\n61370.252 | 61370.260 | 61414.086 |      0.885 |      0.588 | 0.722 | 3.219 | 3.370\n\nreport::report(fit_week_x_meditation)\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict Anxiety with Week and meditation_hours (formula: Anxiety ~ 1 + Week\n* meditation_hours). The model included person_id as random effect (formula: ~1\n| person_id). The model's total explanatory power is substantial (conditional\nR2 = 0.89) and the part related to the fixed effects alone (marginal R2) is of\n0.59. The model's intercept, corresponding to Week = 0 and meditation_hours =\n0, is at 71.87 (95% CI [71.49, 72.25], t(10994) = 370.28, p &lt; .001). Within\nthis model:\n\n  - The effect of Week is statistically significant and negative (beta = -0.04,\n95% CI [-0.07, -0.01], t(10994) = -2.74, p = 0.006; Std. beta = -0.33, 95% CI\n[-0.34, -0.33])\n  - The effect of meditation hours is statistically significant and negative\n(beta = -0.27, 95% CI [-0.29, -0.25], t(10994) = -23.98, p &lt; .001; Std. beta =\n-0.62, 95% CI [-0.63, -0.62])\n  - The effect of Week × meditation hours is statistically significant and\nnegative (beta = -0.17, 95% CI [-0.17, -0.17], t(10994) = -90.26, p &lt; .001;\nStd. beta = -0.30, 95% CI [-0.31, -0.29])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\nanova(fit_meditation, fit_week_x_meditation)\n\nData: d\nModels:\nfit_meditation: Anxiety ~ 1 + Week + meditation_hours + (1 | person_id)\nfit_week_x_meditation: Anxiety ~ 1 + Week * meditation_hours + (1 | person_id)\n                      npar   AIC   BIC logLik deviance  Chisq Df Pr(&gt;Chisq)    \nfit_meditation           5 67309 67345 -33649    67299                         \nfit_week_x_meditation    6 61342 61386 -30665    61330 5968.7  1  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncompare_performance(fit_meditation, fit_week_x_meditation)\n\n# Comparison of Model Performance Indices\n\nName                  |   Model |   AIC (weights) |  AICc (weights)\n-------------------------------------------------------------------\nfit_meditation        | lmerMod | 67308.9 (&lt;.001) | 67308.9 (&lt;.001)\nfit_week_x_meditation | lmerMod | 61342.2 (&gt;.999) | 61342.2 (&gt;.999)\n\nName                  |   BIC (weights) | R2 (cond.) | R2 (marg.) |   ICC\n-------------------------------------------------------------------------\nfit_meditation        | 67345.4 (&lt;.001) |      0.792 |      0.497 | 0.586\nfit_week_x_meditation | 61386.0 (&gt;.999) |      0.885 |      0.588 | 0.722\n\nName                  |  RMSE | Sigma\n-------------------------------------\nfit_meditation        | 4.340 | 4.538\nfit_week_x_meditation | 3.219 | 3.370\n\nplot_model(fit_week_x_meditation, type = \"pred\", terms = c(\"Week\", \"meditation_hours [0, 5, 10]\"))\n\n\n\n\n\n\n\ntab_model(fit_week_x_meditation)\n\n\n\n \nAnxiety\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n71.87\n71.49 – 72.25\n&lt;0.001\n\n\nWeek\n-0.04\n-0.07 – -0.01\n0.006\n\n\nmeditation hours\n-0.27\n-0.29 – -0.25\n&lt;0.001\n\n\nWeek × meditation hours\n-0.17\n-0.17 – -0.17\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n11.36\n\n\n\nτ00 person_id\n29.50\n\n\nICC\n0.72\n\n\nN person_id\n1000\n\nObservations\n11000\n\n\nMarginal R2 / Conditional R2\n0.588 / 0.885\n\n\n\n\n\nWe see that the hypothesized interaction is significant, meaning that the effect of Week depends on meditation_hours. In the plot, you can see that the reduction of Anxiety over time is larger for people who meditation more.\nNote that in the output the fixed effects of Week and meditation_hours are simple slopes. They tell us what the slope of the variable is when the other predictors in the interaction are 0. For statistical significance of simple slopes, you can look in the output of report.\n\n\n\n\n\n\nNoteQuestion 5\n\n\n\nIn model fit_week_x_meditation, when meditation_hours is 0, is the slope of Week statistically significant?"
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#add-the-l2-predictor",
    "href": "tutorials/Longitudinal/longitudinal.html#add-the-l2-predictor",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Add the L2 predictor",
    "text": "Add the L2 predictor\nAdd therapist_support to the previous model. Call the model fit_support. Run summary, model_performance, and report on the model. Compare fit_support with the previous model, fit_meditation.\n\nSet up the Code\nTry setting up the code yourself.\n\n\n\n\n\n\nTipHint (Click)\n\n\n\n\n\n\nfit_support &lt;- lmer(Anxiety ~ Week * meditation_hours + therapist_support + (1 | person_id), data = d)\nsummary(fit_support)\nperformance::model_performance(fit_support)\nreport::report(fit_support)\n\nanova(fit_meditation, fit_support)\ncompare_performance(fit_meditation, fit_support, rank = T)\n\n\n\n\nAfter running the code, you can see that the therapist support is associated with lower levels of anxiety."
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#add-random-slopes-for-level-1-predictors.",
    "href": "tutorials/Longitudinal/longitudinal.html#add-random-slopes-for-level-1-predictors.",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Add random slopes for Level 1 predictors.",
    "text": "Add random slopes for Level 1 predictors.\nIn general, be conservative about adding random slopes to your model. They often cause convergence problems and long computation times.\nLet’s see if we need Week to have a random slope.\n\nfit_week_random &lt;- lmer(Anxiety ~ Week * meditation_hours + therapist_support + (1 + Week || person_id), data = d)\nsummary(fit_week_random)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Anxiety ~ Week * meditation_hours + therapist_support + ((1 |  \n    person_id) + (0 + Week | person_id))\n   Data: d\n\nREML criterion at convergence: 61325.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.0529 -0.6342  0.0016  0.6357  4.4314 \n\nRandom effects:\n Groups      Name        Variance  Std.Dev.\n person_id   (Intercept) 28.414902 5.33056 \n person_id.1 Week         0.005105 0.07145 \n Residual                11.303273 3.36203 \nNumber of obs: 11000, groups:  person_id, 1000\n\nFixed effects:\n                       Estimate Std. Error t value\n(Intercept)           71.847112   0.191278 375.616\nWeek                  -0.041067   0.015396  -2.667\nmeditation_hours      -0.265533   0.011158 -23.797\ntherapist_support     -1.011291   0.173201  -5.839\nWeek:meditation_hours -0.170138   0.001888 -90.134\n\nCorrelation of Fixed Effects:\n            (Intr) Week   mdttn_ thrps_\nWeek        -0.394                     \nmedittn_hrs -0.354  0.621              \nthrpst_sppr  0.016 -0.004 -0.022       \nWk:mdttn_hr  0.295 -0.738 -0.836  0.004\n\nperformance::model_performance(fit_week_random)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n-----------------------------------------------------------------------------------\n61341.222 | 61341.235 | 61399.668 |      0.888 |      0.607 | 0.715 | 3.203 | 3.362\n\nanova(fit_support, fit_week_random, refit = FALSE)\n\nData: d\nModels:\nfit_support: Anxiety ~ Week * meditation_hours + therapist_support + (1 | person_id)\nfit_week_random: Anxiety ~ Week * meditation_hours + therapist_support + ((1 | person_id) + (0 + Week | person_id))\n                npar   AIC   BIC logLik deviance  Chisq Df Pr(&gt;Chisq)\nfit_support        7 61340 61392 -30663    61326                     \nfit_week_random    8 61341 61400 -30663    61325 1.2712  1     0.2595\n\n\nThere is not a significant effect here. Note the use of refit = FALSE in the anova function. This is good practice when comparing models that differ only in their random effects.\n\nfit_meditation_random &lt;- lmer(Anxiety ~ Week * meditation_hours + therapist_support + (1 + meditation_hours || person_id), data = d)\nsummary(fit_meditation_random)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Anxiety ~ Week * meditation_hours + therapist_support + ((1 |  \n    person_id) + (0 + meditation_hours | person_id))\n   Data: d\n\nREML criterion at convergence: 61323.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.0732 -0.6349  0.0008  0.6351  4.4356 \n\nRandom effects:\n Groups      Name             Variance  Std.Dev.\n person_id   (Intercept)      28.580662 5.34609 \n person_id.1 meditation_hours  0.002205 0.04696 \n Residual                     11.287908 3.35975 \nNumber of obs: 11000, groups:  person_id, 1000\n\nFixed effects:\n                       Estimate Std. Error t value\n(Intercept)           71.847721   0.191808 374.581\nWeek                  -0.041972   0.015251  -2.752\nmeditation_hours      -0.265514   0.011376 -23.339\ntherapist_support     -0.999116   0.173555  -5.757\nWeek:meditation_hours -0.169966   0.001904 -89.250\n\nCorrelation of Fixed Effects:\n            (Intr) Week   mdttn_ thrps_\nWeek        -0.398                     \nmedittn_hrs -0.350  0.621              \nthrpst_sppr  0.016 -0.002 -0.019       \nWk:mdttn_hr  0.295 -0.746 -0.829  0.002\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00610441 (tol = 0.002, component 1)\n\nperformance::model_performance(fit_meditation_random)\n\n# Indices of model performance\n\nAIC       |      AICc |       BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n-----------------------------------------------------------------------------------\n61339.774 | 61339.787 | 61398.220 |      0.888 |      0.605 | 0.717 | 3.200 | 3.360\n\nanova(fit_support, fit_meditation_random, refit = FALSE)\n\nData: d\nModels:\nfit_support: Anxiety ~ Week * meditation_hours + therapist_support + (1 | person_id)\nfit_meditation_random: Anxiety ~ Week * meditation_hours + therapist_support + ((1 | person_id) + (0 + meditation_hours | person_id))\n                      npar   AIC   BIC logLik deviance  Chisq Df Pr(&gt;Chisq)  \nfit_support              7 61340 61392 -30663    61326                       \nfit_meditation_random    8 61340 61398 -30662    61324 2.7192  1    0.09915 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAgain, no significant effect here. We are not really sure, however, because the model did not converge. However, we did not predict a random slope, so to be conservative, we revert back to fit_support as our comparison model."
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#add-hypothesized-interaction-terms",
    "href": "tutorials/Longitudinal/longitudinal.html#add-hypothesized-interaction-terms",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Add hypothesized interaction terms",
    "text": "Add hypothesized interaction terms\nWe hypothesized a 3-way interaction of Week, meditation_hours, and therapist_support. Add it to the fit_support model.\nTry setting up the code yourself. Call the model fit_3_way_interaction.\nPretend that the warning (boundary (singular) fit: see ?isSingular) is not there.\nRun summary, model_performance, and report on the model.\n\n\n\n\n\n\nTipHint (Click)\n\n\n\n\n\n\nfit_3_way_interaction &lt;- lmer(Anxiety ~ Week * meditation_hours * therapist_support + (1 | person_id), data = d)\nsummary(fit_3_way_interaction)\nperformance::model_performance(fit_3_way_interaction)\nreport::report(fit_3_way_interaction)\n\n\n\n\nLet’s compare the fit of both models:\n\nanova(fit_support, fit_3_way_interaction)\n\nData: d\nModels:\nfit_support: Anxiety ~ Week * meditation_hours + therapist_support + (1 | person_id)\nfit_3_way_interaction: Anxiety ~ Week * meditation_hours * therapist_support + (1 | person_id)\n                      npar   AIC   BIC logLik deviance  Chisq Df Pr(&gt;Chisq)    \nfit_support              7 61311 61362 -30648    61297                         \nfit_3_way_interaction   10 61296 61369 -30638    61276 20.502  3  0.0001336 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncompare_performance(fit_support, fit_3_way_interaction, rank = T)\n\n# Comparison of Model Performance Indices\n\nName                  |   Model | R2 (cond.) | R2 (marg.) |   ICC |  RMSE\n-------------------------------------------------------------------------\nfit_3_way_interaction | lmerMod |      0.888 |      0.606 | 0.715 | 3.216\nfit_support           | lmerMod |      0.888 |      0.605 | 0.715 | 3.219\n\nName                  | Sigma | AIC weights | AICc weights | BIC weights | Performance-Score\n--------------------------------------------------------------------------------------------\nfit_3_way_interaction | 3.368 |       0.999 |        0.999 |       0.024 |            87.50%\nfit_support           | 3.370 |    7.09e-04 |     7.12e-04 |       0.976 |            12.50%\n\n\nThe results of the anova model comparison is significant, but we can see that the three-way interaction is not. Let’s plot the model and see what is going on.\n\nplot_model(fit_3_way_interaction, \n           type = \"pred\", \n           terms = c(\"Week\", \"meditation_hours [0, 5, 10]\", \"therapist_support [-1, 0, 1]\")) + \n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nFigure 14: Interaction of Week, Meditation, and Therapist Support\n\n\n\n\n\nFrom the plot and the summary output, the significant effect appears to be in the 2-way interaction of meditation_hours and therapist_support. You cannot be sure about that until you run the model."
  },
  {
    "objectID": "tutorials/Longitudinal/longitudinal.html#add-2-way-interaction",
    "href": "tutorials/Longitudinal/longitudinal.html#add-2-way-interaction",
    "title": "Multilevel Modeling of Longitudinal Data in R",
    "section": "Add 2-way interaction",
    "text": "Add 2-way interaction\nAdd the interaction of therapist_support and meditation_hoursto the fit_support model\nTry setting up the code yourself. Call the model fit_support_x_meditation.\nRun summary, model_performance, and report on the model.\n\n\n\n\n\n\nTipHint (Click)\n\n\n\n\n\n\nfit_support_x_meditation &lt;- lmer(Anxiety ~ Week * meditation_hours + meditation_hours * therapist_support + (1 | person_id), data = d)\n\nsummary(fit_support_x_meditation)\n\nperformance::model_performance(fit_support_x_meditation)\nreport::report(fit_support_x_meditation)\n\n\n\n\n\n\n\n\n\n\nNoteQuestion 6\n\n\n\nIn the fit_support_x_meditation model, is the interaction of meditation_hours and therapist_support significant?"
  },
  {
    "objectID": "tutorials/GeneralizedMixed/generalized_linear_mixed_models.html",
    "href": "tutorials/GeneralizedMixed/generalized_linear_mixed_models.html",
    "title": "Generalized Linear Mixed Models",
    "section": "",
    "text": "Some outcome variables are incompatible with the general linear model’s normality assumption. Binary outcomes cannot possibly have normally distributed error terms. Generalized linear models allow us to predict outcomes that have non-normal errors. If you know how to analyze linear mixed models, and you understand generalized linear models, generalized linear mixed models require very little additional learning to get started.\nA generalized linear model has one or more random response (outcome) variables \\((\\boldsymbol{Y})\\), one or predictor variables \\(X\\) that are combined optimally using a set of coefficients \\(B\\), and a link function, \\(g\\), that specifies the relationship between the predictors and the expected value of the response variables conditioned on the predictor variables:\n\\[\ng\\left(\\mathcal{E}\\left(\\boldsymbol{Y}|\\boldsymbol{XB}\\right)\\right)=\\boldsymbol{XB}\n\\] The inverse of the link function \\(g\\) (i.e., \\(g^{-1}\\)) is called the mean function because it transforms the linear combination of the predictors to the expected value (i.e., the mean) of the response variable.\n\\[\n\\mathcal{E}\\left(\\boldsymbol{Y}|\\boldsymbol{XB}\\right)=g^{-1}\\left(\\boldsymbol{XB}\\right)\n\\]\nThe generalized linear mixed model is very similar to the generalized linear model, except that it adds a random component:\n\\[\n\\mathcal{E}\\left(\\boldsymbol{Y}|\\boldsymbol{XB}\\right)=g^{-1}\\left(\\boldsymbol{XB}+\\boldsymbol{Zv\\left(u\\right)}\\right)\n\\]\nWhere \\(Z\\) is the model matrix for random effects, \\(u\\) is a random component following a distribution conjugate to a generalized linear model family of distributions with parameters \\(\\lambda\\), and \\(v()\\) is a monotone function.\nLet’s imagine we are predicting student dropout rates using student absences and student GPA as predictors. Suppose in 200 schools we conduct an intervention study to see if a school-wide intervention prevents dropout. Schools were randomly assigned to a treatment or control condition. We want to know if our intervention has succeeded, controlling for student absence and GPA.\n\nImport data\n\nd &lt;- read_csv(\"https://github.com/wjschne/EDUC5529/raw/master/hw7_dropout.csv\")\n\n\n\nExploratory Analyses\nThe skim function from the skimr package gives a great overview of what the data looks like. It is fantastic for making sure the data look like they are supposed to. Unfortunately, it does not print well to rmarkdown. The describe function from the psych package is also great.\n\nskimr::skim(d)\n\n\n\n\n\nName\nd\n\n\nNumber of rows\n6133\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nData summaryVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nIntervention\n0\n1\n7\n9\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nschool_id\n0\n1\n100.68\n57.49\n1.00\n52.00\n101.00\n151.00\n200\n▇▇▇▇▇\n\n\nGPA\n0\n1\n2.51\n0.94\n0.04\n1.83\n2.61\n3.28\n4\n▂▃▆▇▇\n\n\nAbsences\n0\n1\n2.72\n1.86\n0.00\n1.00\n2.00\n4.00\n12\n▇▅▂▁▁\n\n\nDropout\n0\n1\n0.05\n0.21\n0.00\n0.00\n0.00\n0.00\n1\n▇▁▁▁▁\n\n\n\n\n\n\npsych::describe(d)\n\n              vars    n   mean    sd median trimmed   mad  min max  range  skew\nschool_id        1 6133 100.68 57.49 101.00  100.70 72.65 1.00 200 199.00  0.00\nIntervention*    2 6133   1.57  0.49   2.00    1.59  0.00 1.00   2   1.00 -0.29\nGPA              3 6133   2.51  0.94   2.61    2.57  1.06 0.04   4   3.96 -0.42\nAbsences         4 6133   2.72  1.86   2.00    2.58  1.48 0.00  12  12.00  0.80\nDropout          5 6133   0.05  0.21   0.00    0.00  0.00 0.00   1   1.00  4.29\n              kurtosis   se\nschool_id        -1.19 0.73\nIntervention*    -1.92 0.01\nGPA              -0.71 0.01\nAbsences          0.72 0.02\nDropout          16.41 0.00\n\n\nWhat is the distribution of GPA?\n\nd %&gt;% \n  ggplot(aes(GPA)) + \n  geom_density(fill = \"dodgerblue\", color = NA, alpha = 0.5)\n\n\n\n\n\n\n\n\nWhat is the distribution of Absences?\n\nd %&gt;% \n  count(Absences) %&gt;% \n  ggplot(aes(Absences, n)) + \n  geom_col(fill = \"dodgerblue\", color = NA) +\n  geom_text(aes(label = n), vjust = -.1)\n\n\n\n\n\n\n\n\nDropout Rates Across Schools\n\nd %&gt;% \n  group_by(school_id, Intervention) %&gt;% \n  summarise(Dropout = mean(Dropout)) %&gt;% \n  ggplot(aes(Dropout)) +\n  geom_density(aes(fill = Intervention), color = NA, alpha = 0.5) +\n  labs(x = \"School Dropout Rate\")\n\n\n\n\n\n\n\n\nLet’s use the “glm” method in geom_smooth to get a rough sense of what a proper analysis is likely to show.\n\nggplot(d, aes(Absences, Dropout)) + \n  geom_jitter(aes(color = Intervention), \n              size = 0.3, \n              width = 0.2, \n              height = .015, \n              alpha = 0.5, \n              pch = 16) + \n  geom_smooth(aes(color = Intervention), \n              method = \"glm\", \n              method.args = list(family = \"binomial\"))\n\n\n\n\n\n\n\n\nTo me, that looks like a successful intervention! However, we did not do a proper analysis that accounted for the multilevel nature of the data.\n\n\nNull model\nThe glmer function works exactly like lmer except that we must tell it which kind of generalized model we want. With binary outcomes, specify the binomial family. By default, it will do a logistic regression. You can specify probit regression with binomial(link = \"probit\"), if needed.\n\nm0 &lt;- glmer(Dropout ~ 1 + (1 | school_id), d, family = binomial())\nsummary(m0)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Dropout ~ 1 + (1 | school_id)\n   Data: d\n\n     AIC      BIC   logLik deviance df.resid \n    1827     1841     -912     1823     6131 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-1.375 -0.167 -0.070 -0.068  7.498 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n school_id (Intercept) 4.46     2.11    \nNumber of obs: 6133, groups:  school_id, 200\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -4.670      0.272   -17.2   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nperformance(m0)\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE\n------------------------------------------------------------------------\n1827.186 | 1827.188 | 1840.629 |      0.576 |      0.000 | 0.576 | 0.185\n\nAIC      | Sigma | Log_loss | Score_log | Score_spherical\n---------------------------------------------------------\n1827.186 | 1.000 |    0.121 |   -14.611 |           0.010\n\ntab_model(m0)\n\n\n\n \nDropout\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.01\n0.01 – 0.02\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 school_id\n4.46\n\n\nICC\n0.58\n\n\nN school_id\n200\n\nObservations\n6133\n\n\nMarginal R2 / Conditional R2\n0.000 / 0.576\n\n\n\n\n# If you want the raw coefficients (log-odds) instead of odds ratios\ntab_model(m0, transform = NULL)\n\n\n\n \nDropout\n\n\nPredictors\nLog-Odds\nCI\np\n\n\n(Intercept)\n-4.67\n-5.20 – -4.14\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 school_id\n4.46\n\n\nICC\n0.58\n\n\nN school_id\n200\n\nObservations\n6133\n\n\nMarginal R2 / Conditional R2\n0.000 / 0.576\n\n\n\n\n\nWe can see from the ICC (or conditional R2) that 57.6% of the variance in dropout rate is explained by the grouping variable, school_id.\n\n\nFirst L1 predictor: Absences\nLet’s add to the null model using the update function.\n\nm1 &lt;- update(m0, . ~ . + Absences)\n# Does this model fit better than the null model?\nanova(m0,m1)\n\nData: d\nModels:\nm0: Dropout ~ 1 + (1 | school_id)\nm1: Dropout ~ (1 | school_id) + Absences\n   npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)    \nm0    2 1827 1841   -912     1823                        \nm1    3 1371 1391   -682     1365   459  1     &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Overall numbers\nsummary(m1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Dropout ~ (1 | school_id) + Absences\n   Data: d\n\n     AIC      BIC   logLik deviance df.resid \n    1370     1391     -682     1364     6130 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-6.212 -0.097 -0.033 -0.015 17.443 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n school_id (Intercept) 9.59     3.1     \nNumber of obs: 6133, groups:  school_id, 200\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -9.0916     0.5889   -15.4   &lt;2e-16 ***\nAbsences      0.8777     0.0537    16.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n         (Intr)\nAbsences -0.687\n\n# Performance metrics\nperformance(m1)\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE\n------------------------------------------------------------------------\n1370.510 | 1370.514 | 1390.674 |      0.788 |      0.171 | 0.745 | 0.152\n\nAIC      | Sigma | Log_loss | Score_log | Score_spherical\n---------------------------------------------------------\n1370.510 | 1.000 |    0.080 |      -Inf |           0.010\n\n# Automated report\nreport(m1)\n\nWe fitted a logistic mixed model (estimated using ML and Nelder-Mead optimizer)\nto predict Dropout with Absences (formula: Dropout ~ Absences). The model\nincluded school_id as random effect (formula: ~1 | school_id). The model's\ntotal explanatory power is substantial (conditional R2 = 0.79) and the part\nrelated to the fixed effects alone (marginal R2) is of 0.17. The model's\nintercept, corresponding to Absences = 0, is at -9.09 (95% CI [-10.25, -7.94],\np &lt; .001). Within this model:\n\n  - The effect of Absences is statistically significant and positive (beta =\n0.88, 95% CI [0.77, 0.98], p &lt; .001; Std. beta = 1.63, 95% CI [1.44, 1.83])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald z-distribution approximation.\n\n# Nice table\ntab_model(m1)\n\n\n\n \nDropout\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.00\n0.00 – 0.00\n&lt;0.001\n\n\nAbsences\n2.41\n2.17 – 2.67\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\n\nτ00 school_id\n9.59\n\n\nICC\n0.74\n\n\nN school_id\n200\n\nObservations\n6133\n\n\nMarginal R2 / Conditional R2\n0.171 / 0.788\n\n\n\n\n# Fixed effect plot\nsjPlot::plot_model(m1, type = \"pred\", terms = \"Absences\")\n\n\n\n\n\n\n\n\n\n\nSecond L1 predictor: GPA\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nAdd the GPA variable as a predictor to a model called m2. Conduct and interpret your analyses, documenting whether the GPA is a significant predictor of dropout rate.\n\n\nThis plot might aid interpretation.\n\nsjPlot::plot_model(m2, \n                   type = \"pred\", \n                   terms = c(\"Absences\", \n                             \"GPA [0,2,4]\"))\n\n\n\nL2 predictor: Intervention\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nAdd the Intervention variable as a predictor to a model called m3. Conduct and interpret your analyses, documenting whether the intervention succeeded in reducing the dropout rate.\n\n\nThis plot might aid interpretation.\n\nsjPlot::plot_model(m3, \n                   type = \"pred\", \n                   terms = c(\"Absences\", \n                             \"GPA [0,2,4]\", \n                             \"Intervention\"))\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nWhat additional analyses might you conduct with these data? Try testing at least one more model that makes sense to you and write your conclusions."
  },
  {
    "objectID": "tutorials/DataWrangling/data_wrangling.html",
    "href": "tutorials/DataWrangling/data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "In programs like SPSS, the data seem to live in a spreadsheet that is always open and available to you. In R, you can see the data whenever you want, but usually it sits unseen inside a variable. It can be disconcerting at first, but think of your data as living in a file somewhere on your hard drive (or online!), and then the data just come for a short visit in R.\nThe major benefit of working with data in R is that all of the changes, transformations, and restructuring happens in code—which can be recreated at any time. There is usually no need to “save” the data after you have transformed it. The next time you work with the data, you just run your code and all the calculations will transform the data exactly the same way as before.\nWhy does this matter? If you feel the need to save your data all the time, you end up having multiple copies of it: data.sav, data_restructured.sav, data_new.sav, data_new_final.sav, fixed_data_new_final.sav, restructued_final_with_missing_cases_removed.sav, and so forth and so on. It can be hard to figure out where to start the next time you work with your data. You might not remember which version has errors and which version has what you need.\nIn general, start with a completely raw data file (one that is exactly the way you started). Resist the temptation to make any changes to it directly. If you must, save a pristine copy somewhere and only then change it. Import your data and make all changes to it with code. One benefit of doing so is that the code documents any changes you would otherwise need to record in your lab notebook."
  },
  {
    "objectID": "tutorials/DataWrangling/data_wrangling.html#restructuring-from-wide-to-long-format-with-pivot_longer",
    "href": "tutorials/DataWrangling/data_wrangling.html#restructuring-from-wide-to-long-format-with-pivot_longer",
    "title": "Data Wrangling",
    "section": "Restructuring from wide to long format with pivot_longer",
    "text": "Restructuring from wide to long format with pivot_longer\nUse the pivot_longer function to pivot the three time variables to long format:\n\nd_small %&gt;% \n  pivot_longer(cols = time_1:time_3)\n\n# A tibble: 6 × 5\n     id baseline_sessions_n intervention_sessions_n name   value\n  &lt;dbl&gt;               &lt;dbl&gt;                   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     1                  22                      20 time_1 11.5 \n2     1                  22                      20 time_2 14.4 \n3     1                  22                      20 time_3  6.15\n4     2                  18                      19 time_1  9.11\n5     2                  18                      19 time_2  8.59\n6     2                  18                      19 time_3 11.0 \n\n\nTo avoid the hassle of renaming our columns, we can specify what the name and value columns should be called:\n\nd_small %&gt;%\n  pivot_longer(cols = time_1:time_3,\n               names_to = \"time\",\n               values_to = \"on_task\")\n\n# A tibble: 6 × 5\n     id baseline_sessions_n intervention_sessions_n time   on_task\n  &lt;dbl&gt;               &lt;dbl&gt;                   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1     1                  22                      20 time_1   11.5 \n2     1                  22                      20 time_2   14.4 \n3     1                  22                      20 time_3    6.15\n4     2                  18                      19 time_1    9.11\n5     2                  18                      19 time_2    8.59\n6     2                  18                      19 time_3   11.0 \n\n\nNotice that the time variable is text, not a number, like we want. There are many ways to get rid of the prefix. The simplest is to tell pivot_longer to strip away the prefix “time_”.\n\nd_small %&gt;%\n  pivot_longer(cols = time_1:time_3,\n               names_to = \"time\",\n               values_to = \"on_task\",\n               names_prefix = \"time_\")\n\n# A tibble: 6 × 5\n     id baseline_sessions_n intervention_sessions_n time  on_task\n  &lt;dbl&gt;               &lt;dbl&gt;                   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1                  22                      20 1       11.5 \n2     1                  22                      20 2       14.4 \n3     1                  22                      20 3        6.15\n4     2                  18                      19 1        9.11\n5     2                  18                      19 2        8.59\n6     2                  18                      19 3       11.0 \n\n\nUnfortunately, R thinks that time is a text variable. We want it to be an integer, so we transform it using the as.integer function:\n\nd_small %&gt;%\n  pivot_longer(cols = time_1:time_3,\n               names_to = \"time\",\n               values_to = \"on_task\",\n               names_prefix = \"time_\", \n               names_transform = list(time = as.integer)) \n\n# A tibble: 6 × 5\n     id baseline_sessions_n intervention_sessions_n  time on_task\n  &lt;dbl&gt;               &lt;dbl&gt;                   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1     1                  22                      20     1   11.5 \n2     1                  22                      20     2   14.4 \n3     1                  22                      20     3    6.15\n4     2                  18                      19     1    9.11\n5     2                  18                      19     2    8.59\n6     2                  18                      19     3   11.0 \n\n\nAlternatively, we can do all these transformations after pivoting:\n\nd_small %&gt;%\n  pivot_longer(cols = time_1:time_3,\n               names_to = \"time\",\n               values_to = \"on_task\") %&gt;% \n  mutate(time = str_remove(time, \"time_\") %&gt;% \n           as.integer())\n\n# A tibble: 6 × 5\n     id baseline_sessions_n intervention_sessions_n  time on_task\n  &lt;dbl&gt;               &lt;dbl&gt;                   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1     1                  22                      20     1   11.5 \n2     1                  22                      20     2   14.4 \n3     1                  22                      20     3    6.15\n4     2                  18                      19     1    9.11\n5     2                  18                      19     2    8.59\n6     2                  18                      19     3   11.0"
  },
  {
    "objectID": "tutorials/DataWrangling/data_wrangling.html#restructuring-from-long-to-wide-format-with-pivot_wider",
    "href": "tutorials/DataWrangling/data_wrangling.html#restructuring-from-long-to-wide-format-with-pivot_wider",
    "title": "Data Wrangling",
    "section": "Restructuring from long to wide format with pivot_wider",
    "text": "Restructuring from long to wide format with pivot_wider\nLet’s pivot the data from long format to wide. First let’s assign the restructured d_small data as d_small_longer:\n\nd_small_longer &lt;- d_small %&gt;%\n  pivot_longer(cols = time_1:time_3,\n               names_to = \"time\",\n               values_to = \"on_task\",\n               names_prefix = \"time_\",\n               names_transform = list(time = as.integer))\n\nNow let’s move it back to where it was with pivot_wider:\n\nd_small_longer %&gt;% \n  pivot_wider(names_from = time, \n              values_from = on_task, \n              names_prefix = \"time_\")\n\n# A tibble: 2 × 6\n     id baseline_sessions_n intervention_sessions_n time_1 time_2 time_3\n  &lt;dbl&gt;               &lt;dbl&gt;                   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1                  22                      20  11.5   14.4    6.15\n2     2                  18                      19   9.11   8.59  11.0 \n\n\nPerfect! Now lets move back to the the original large tibble d."
  },
  {
    "objectID": "tutorials/BivariateChange/bivariatechange.html",
    "href": "tutorials/BivariateChange/bivariatechange.html",
    "title": "Bivariate Change Models",
    "section": "",
    "text": "To investigate causal relations, experiments with manipulated variables are the most straightforward and persuasive forms of evidence we have. Unfortunately, there are a lot of interesting variables that are hard to manipulate experimentally. To investigate what they cause and what causes such variables, longitudinal studies are among the most useful forms of evidence we have.\nFor example, reading comprehension and vocabulary have a reciprocally causal relationship (Killingly et al., 2025; Quinn et al., 2015). That is, if you have a good vocabulary, it is easier for you to comprehend text. If it is easy for you to comprehend text, you will likely expand your vocabulary.\nUnfortunately, many of our initial methods of understanding longitudinal relationships can generate misleading findings."
  },
  {
    "objectID": "tutorials/BivariateChange/bivariatechange.html#automating-repetitive-lavaan-models",
    "href": "tutorials/BivariateChange/bivariatechange.html#automating-repetitive-lavaan-models",
    "title": "Bivariate Change Models",
    "section": "Automating repetitive lavaan models",
    "text": "Automating repetitive lavaan models\nUnfortunately, lavaan models can become quite large. In most lavaan models, we do not need to name the parameters. However, if we are specifying parameters are constrained to equal each other, we need to name our parameters. Any parameter with the same name is constrained to be equal.\nTo specify the model with lavaan syntax, one could do it by hand like so:\n\nm &lt;- '\n# Covariances\nx1 ~~ tau_x1 * x1\ny1 ~~ tau_y1 * y1 + tau_x1y1 * x1\nx2 ~~ tau_xx * x2 + tau_xy * y2 \nx3 ~~ tau_xx * x3 + tau_xy * y3\nx4 ~~ tau_xx * x4 + tau_xy * y4\ny2 ~~ tau_yy * y2 \ny3 ~~ tau_yy * y3\ny4 ~~ tau_yy * y4\n# Regressions with intercepts\nx1 ~ b_x01 * 1\nx2 ~ b_x02 * 1 + b_xx * x1 + b_yx * y1\nx3 ~ b_x03 * 1 + b_xx * x2 + b_yx * y2\nx4 ~ b_x04 * 1 + b_xx * x3 + b_yx * y3\ny1 ~ b_y01 * 1\ny2 ~ b_y02 * 1 + b_yy * y1 + b_xy * x1\ny3 ~ b_y03 * 1 + b_yy * y2 + b_xy * x2\ny4 ~ b_y04 * 1 + b_yy * y3 + b_xy * x3\n'\n\nThe benefits of this approach are that the code is easier to create and to understand. With only 4 waves of data, this model does not seem so difficult to specify by hand.\nThe liabilities of this approach are that it is error prone and increasingly tedious for large models. We are better off using paste0 or glue from the glue package to automate model production."
  },
  {
    "objectID": "tutorials/BivariateChange/bivariatechange.html#pasting-text-together-with-paste0",
    "href": "tutorials/BivariateChange/bivariatechange.html#pasting-text-together-with-paste0",
    "title": "Bivariate Change Models",
    "section": "Pasting text together with paste0",
    "text": "Pasting text together with paste0\n\n\nThe paste0 function is a special variant of the paste function that has 0 separation between the pasted elements. By default, paste separates the elements with spaces:\n\npaste(\"A\", \"B\", \"C\")\n\n[1] \"A B C\"\n\n\nYou can put anything between the elements with the sep parameter:\n\npaste(\"A\", \"B\", \"C\", sep = \",\")\n\n[1] \"A,B,C\"\n\n\nThe paste0 function pastes text together. For example,\n\npaste0(\"A\", \"B\", \"C\")\n\n[1] \"ABC\"\n\n\nIf you use a vector, paste0 will paste multiple strings:\n\npaste0(\"A\", 1:3)\n\n[1] \"A1\" \"A2\" \"A3\"\n\n\nWe can “collapse” the vector strings into a single string separated by the newline symbol \\n:\n\npaste0(\"A\", 1:3, collapse = \"\\n\") %&gt;% \n  cat()\n\nA1\nA2\nA3"
  },
  {
    "objectID": "tutorials/BivariateChange/bivariatechange.html#pasting-repetitive-code",
    "href": "tutorials/BivariateChange/bivariatechange.html#pasting-repetitive-code",
    "title": "Bivariate Change Models",
    "section": "Pasting repetitive code",
    "text": "Pasting repetitive code\nSuppose we have 8 waves of data. We need to set the variances of the x series.\nThis is what we need:\nx1 ~~ tau_x1 * x1 \nx2 ~~ tau_xx * x2 \nx3 ~~ tau_xx * x3 \nx4 ~~ tau_xx * x4 \nx5 ~~ tau_xx * x5 \nx6 ~~ tau_xx * x6\nx7 ~~ tau_xx * x7 \nx8 ~~ tau_xx * x8 \n \nWe see that each line is very similar. We need to create vectors for the parts that change: the numbers 1 to 8 and the variance parameters (tau_x1 and tau_xx). The numbers 1 to 8 are easy to make with the : operator.\n\n1:8\n\n[1] 1 2 3 4 5 6 7 8\n\n\nAlternately, we could use the seq function to make a sequence:\n\nseq(1, 8)\n\n[1] 1 2 3 4 5 6 7 8\n\n\nHowever, we do not want our code to be tied specifically to the number 8. We want any number of waves. We define a variable k for the number of waves and make a sequence\n\nk &lt;- 8\n1:k\n\n[1] 1 2 3 4 5 6 7 8\n\n# or\nseq(1, k)\n\n[1] 1 2 3 4 5 6 7 8\n\n# or omit the start value\nseq(k)\n\n[1] 1 2 3 4 5 6 7 8\n\n\nThe variance parameters are all the same except for x1. So we need a vector that is one value for the first element and repeated k - 1 times for the remaining elements. The rep function repeats elements as many times as you specify:\n\nrep(\"tau_xx\", k - 1)\n\n[1] \"tau_xx\" \"tau_xx\" \"tau_xx\" \"tau_xx\" \"tau_xx\" \"tau_xx\" \"tau_xx\"\n\n\nPutting the first and remaining elements together:\n\nv_x &lt;- c(\"tau_x1\", rep(\"tau_xx\", k - 1))\nv_x\n\n[1] \"tau_x1\" \"tau_xx\" \"tau_xx\" \"tau_xx\" \"tau_xx\" \"tau_xx\" \"tau_xx\" \"tau_xx\"\n\n\nNow we are ready to paste the entire line of code:\n\n# Make string\nvar_x &lt;- paste0(\"x\", 1:k, \" ~~ \", v_x, \" * x\", 1:k, collapse = \"\\n\") \n\n# Output string \ncat(var_x)\n\nx1 ~~ tau_x1 * x1\nx2 ~~ tau_xx * x2\nx3 ~~ tau_xx * x3\nx4 ~~ tau_xx * x4\nx5 ~~ tau_xx * x5\nx6 ~~ tau_xx * x6\nx7 ~~ tau_xx * x7\nx8 ~~ tau_xx * x8"
  },
  {
    "objectID": "tutorials/BivariateChange/bivariatechange.html#specifying-the-clpm-model",
    "href": "tutorials/BivariateChange/bivariatechange.html#specifying-the-clpm-model",
    "title": "Bivariate Change Models",
    "section": "Specifying the CLPM Model",
    "text": "Specifying the CLPM Model\nTo specify Figure 1 with automated lavaan syntax\n\nlibrary(glue)\n# Number of waves\nk &lt;- 4\n# Sequence from 2 to k\ns2k &lt;- seq(2, k)\n# Sequence from 1 to k - 1\ns1k1 &lt;- seq(1, k - 1)\n\n# variance symbols for x\nv_x &lt;- c(\"tau_x1\", rep(\"tau_xx\", k - 1))\n# variance symbols for y\nv_y &lt;- c(\"tau_y1\", rep(\"tau_yy\", k - 1))\n# covariance symbols xy\ncov_xy &lt;- c(\"tau_x1y1\", rep(\"tau_xy\", k - 1))\n\n\n# Model with comments added\nm_clpm &lt;- c(\n  \"# Regressions\",\n  glue(\"x{s2k} ~ b_xx * x{s1k1} + b_yx * y{s1k1}\"),\n  glue(\"y{s2k} ~ b_yy * y{s1k1} + b_xy * x{s1k1}\") ,\n  \"# Variances\",\n  glue(\"x{1:k} ~~ {v_x} * x{1:k}\"),\n  glue(\"y{1:k} ~~ {v_y} * y{1:k}\"),\n  \"# Covariances\",\n  glue(\"x{1:k} ~~ {cov_xy} * y{1:k}\"),\n  \"# Intercepts\",\n  glue(\"x{1:k} ~ b_x0{1:k} * 1\"),\n  glue(\"y{1:k} ~ b_y0{1:k} * 1\")\n) %&gt;%\n  glue_collapse(\"\\n\")\n\nm_clpm\n\n# Regressions\nx2 ~ b_xx * x1 + b_yx * y1\nx3 ~ b_xx * x2 + b_yx * y2\nx4 ~ b_xx * x3 + b_yx * y3\ny2 ~ b_yy * y1 + b_xy * x1\ny3 ~ b_yy * y2 + b_xy * x2\ny4 ~ b_yy * y3 + b_xy * x3\n# Variances\nx1 ~~ tau_x1 * x1\nx2 ~~ tau_xx * x2\nx3 ~~ tau_xx * x3\nx4 ~~ tau_xx * x4\ny1 ~~ tau_y1 * y1\ny2 ~~ tau_yy * y2\ny3 ~~ tau_yy * y3\ny4 ~~ tau_yy * y4\n# Covariances\nx1 ~~ tau_x1y1 * y1\nx2 ~~ tau_xy * y2\nx3 ~~ tau_xy * y3\nx4 ~~ tau_xy * y4\n# Intercepts\nx1 ~ b_x01 * 1\nx2 ~ b_x02 * 1\nx3 ~ b_x03 * 1\nx4 ~ b_x04 * 1\ny1 ~ b_y01 * 1\ny2 ~ b_y02 * 1\ny3 ~ b_y03 * 1\ny4 ~ b_y04 * 1"
  },
  {
    "objectID": "tutorials/BivariateChange/bivariatechange.html#structured-residuals",
    "href": "tutorials/BivariateChange/bivariatechange.html#structured-residuals",
    "title": "Bivariate Change Models",
    "section": "Structured Residuals",
    "text": "Structured Residuals\nIn Figure 3, the residuals for X2 to X4 are structured residuals because they have residuals themselves. Technically, structured residuals are not really residuals, but latent variables with loadings of 1.\n\nglue(\"e_x{1:k} =~ 1 * x{1:k}\") \n\ne_x1 =~ 1 * x1\ne_x2 =~ 1 * x2\ne_x3 =~ 1 * x3\ne_x4 =~ 1 * x4\n\nglue(\"e_y{1:k} =~ 1 * y{1:k}\") \n\ne_y1 =~ 1 * y1\ne_y2 =~ 1 * y2\ne_y3 =~ 1 * y3\ne_y4 =~ 1 * y4\n\n\nTo make the structured residuals behave like residuals, we need to set the actual residuals of the observed variables to have 0 variance:\n\\{r\\} #| label:\\1 glue(\"x{1:k} ~~ 0 * x{1:k}\")  glue(\"y{1:k} ~~ 0 * y{1:k}\")\nWe also need to label the latent variance and covariance parameters such that the first one is different from the others, which are set equal:\n\nvar_ex &lt;- c(\"tau_ex1\", rep(\"tau_exx\", k - 1))\nvar_ey &lt;- c(\"tau_ey1\", rep(\"tau_eyy\", k - 1))\ncov_exy &lt;- c(\"tau_exy1\", rep(\"tau_exy\", k - 1))\nglue(\"e_x{1:k} ~~ {var_ex} * e_x{1:k}\") \n\ne_x1 ~~ tau_ex1 * e_x1\ne_x2 ~~ tau_exx * e_x2\ne_x3 ~~ tau_exx * e_x3\ne_x4 ~~ tau_exx * e_x4\n\nglue(\"e_y{1:k} ~~ {var_ey} * e_y{1:k}\") \n\ne_y1 ~~ tau_ey1 * e_y1\ne_y2 ~~ tau_eyy * e_y2\ne_y3 ~~ tau_eyy * e_y3\ne_y4 ~~ tau_eyy * e_y4\n\nglue(\"e_x{1:k} ~~ {cov_exy} * e_y{1:k}\") \n\ne_x1 ~~ tau_exy1 * e_y1\ne_x2 ~~ tau_exy * e_y2\ne_x3 ~~ tau_exy * e_y3\ne_x4 ~~ tau_exy * e_y4\n\n\nThe regression syntax is similar to the CLPM syntax except that the predictors and outcomes are between the structured residuals instead of the observed variables.\n\nglue(\"e_x{s2k} ~ b_xx * x{s1k1} + b_yx * e_y{s1k1}\") \n\ne_x2 ~ b_xx * x1 + b_yx * e_y1\ne_x3 ~ b_xx * x2 + b_yx * e_y2\ne_x4 ~ b_xx * x3 + b_yx * e_y3\n\nglue(\"e_y{s2k} ~ b_yy * y{s1k1} + b_xy * e_x{s1k1}\") \n\ne_y2 ~ b_yy * y1 + b_xy * e_x1\ne_y3 ~ b_yy * y2 + b_xy * e_x2\ne_y4 ~ b_yy * y3 + b_xy * e_x3\n\n\nThe latent random intercepts need intercepts (weird but true):\ni_x ~ b_ix_0 * 1\ni_y ~ b_iy_0 * 1\nFinally, the latent random intercepts need variances and a covariance:\ni_x ~~ tau_ix * i_x + tau_ixiy * i_y\ni_y ~~ tau_iy * i_y \nPutting it all together for the RI-CLPM:\n\n# Number of waves\nk &lt;- 4\n# Sequence from 2 to k\ns2k &lt;- seq(2, k)\n# Sequence from 1 to k - 1\ns1k1 &lt;- seq(1, k - 1)\n# Residual variances and covariances\nvar_ex &lt;- c(\"tau_ex1\", rep(\"tau_exx\", k - 1))\nvar_ey &lt;- c(\"tau_ey1\", rep(\"tau_eyy\", k - 1))\ncov_exy &lt;- c(\"tau_exy1\", rep(\"tau_exy\", k - 1))\n\n\nm_riclpm &lt;- c(\"# Latent intercepts\",\n  make_latent(\"i_x\", \"1 * x{1:k}\"),\n  make_latent(\"i_y\", \"1 * y{1:k}\"),\n  \"# Structured Residuals\",\n  glue(\"e_x{1:k} =~ 1 * x{1:k}\"), \n  glue(\"e_y{1:k} =~ 1 * y{1:k}\"),\n  \"# Set observed residuals to 0\",\n  glue(\"x{1:k} ~~ 0 * x{1:k}\"), \n  glue(\"y{1:k} ~~ 0 * y{1:k}\"),\n  \"# Structured Residual Variances and Covariances\",\n  glue(\"e_x{1:k} ~~ {var_ex} * e_x{1:k}\"),\n  glue(\"e_y{1:k} ~~ {var_ey} * e_y{1:k}\"), \n  glue(\"e_x{1:k} ~~ {cov_exy} * e_y{1:k}\"),\n  \"# Structured Residual Regressions\",\n  glue(\"e_x{s2k} ~ b_xx * e_x{s1k1} + b_yx * e_y{s1k1}\"),\n  glue(\"e_y{s2k} ~ b_yy * e_y{s1k1} + b_xy * e_x{s1k1}\"),\n  \"# Observed Intercepts set to 0\",\n  glue(\"x{1:k} ~ 0 * 1\"), \n  glue(\"y{1:k} ~ 0 * 1\"), \n  \"# Random Intercept Intercepts\",\n  \"i_x ~ b_ix_0 * 1\",\n  \"i_y ~ b_iy_0 * 1\",\n  \"# Random Intecept Variances and Covariance\",\n  \"i_x ~~ tau_ix * i_x + tau_ixiy * i_y\",\n  \"i_y ~~ tau_iy * i_y\"\n  ) %&gt;% \n  glue_collapse(\"\\n\") \n\nm_riclpm\n\n# Latent intercepts\ni_x =~ 1 * x1 + 1 * x2 + 1 * x3 + 1 * x4\ni_y =~ 1 * y1 + 1 * y2 + 1 * y3 + 1 * y4\n# Structured Residuals\ne_x1 =~ 1 * x1\ne_x2 =~ 1 * x2\ne_x3 =~ 1 * x3\ne_x4 =~ 1 * x4\ne_y1 =~ 1 * y1\ne_y2 =~ 1 * y2\ne_y3 =~ 1 * y3\ne_y4 =~ 1 * y4\n# Set observed residuals to 0\nx1 ~~ 0 * x1\nx2 ~~ 0 * x2\nx3 ~~ 0 * x3\nx4 ~~ 0 * x4\ny1 ~~ 0 * y1\ny2 ~~ 0 * y2\ny3 ~~ 0 * y3\ny4 ~~ 0 * y4\n# Structured Residual Variances and Covariances\ne_x1 ~~ tau_ex1 * e_x1\ne_x2 ~~ tau_exx * e_x2\ne_x3 ~~ tau_exx * e_x3\ne_x4 ~~ tau_exx * e_x4\ne_y1 ~~ tau_ey1 * e_y1\ne_y2 ~~ tau_eyy * e_y2\ne_y3 ~~ tau_eyy * e_y3\ne_y4 ~~ tau_eyy * e_y4\ne_x1 ~~ tau_exy1 * e_y1\ne_x2 ~~ tau_exy * e_y2\ne_x3 ~~ tau_exy * e_y3\ne_x4 ~~ tau_exy * e_y4\n# Structured Residual Regressions\ne_x2 ~ b_xx * e_x1 + b_yx * e_y1\ne_x3 ~ b_xx * e_x2 + b_yx * e_y2\ne_x4 ~ b_xx * e_x3 + b_yx * e_y3\ne_y2 ~ b_yy * e_y1 + b_xy * e_x1\ne_y3 ~ b_yy * e_y2 + b_xy * e_x2\ne_y4 ~ b_yy * e_y3 + b_xy * e_x3\n# Observed Intercepts set to 0\nx1 ~ 0 * 1\nx2 ~ 0 * 1\nx3 ~ 0 * 1\nx4 ~ 0 * 1\ny1 ~ 0 * 1\ny2 ~ 0 * 1\ny3 ~ 0 * 1\ny4 ~ 0 * 1\n# Random Intercept Intercepts\ni_x ~ b_ix_0 * 1\ni_y ~ b_iy_0 * 1\n# Random Intecept Variances and Covariance\ni_x ~~ tau_ix * i_x + tau_ixiy * i_y\ni_y ~~ tau_iy * i_y\n\n\nHere we set the parameters for data simulation:\n\nparams_riclpm &lt;- c(\n  tau_ex1 = 1,\n  tau_ey1 = 1,\n  tau_exy1 = .4,\n  tau_exx = .4,\n  tau_eyy = .5,\n  tau_exy = .2,\n  tau_ix = 0.75,\n  tau_iy = 0.75,\n  tau_ixiy = .25,\n  b_xx = .5,\n  b_yy = .4,\n  b_xy = .3,\n  b_yx = 0,\n  b_ix_0 = 0,\n  b_iy_0 = 0\n) \n\nd_riclpm &lt;- replace_parameters(model = m_riclpm, \n                   params = params_riclpm) %&gt;% \n  glue_collapse(\"\\n\") %&gt;% \n  simulateData(sample.nobs = 1000) \n\nIn Figure 4, we can see the correlations among the variables created with the RI-CLPM.\n\n\nCode\nplot_cor(d_riclpm)\n\n\n\n\n\n\n\n\nFigure 4: Correlation of X and Y in RI-CLPM\n\n\n\n\n\nIn Figure 5, we see that the RI-CLPM creates data that stays at the same overall level across time. There is no growth.\n\n\nCode\nlibrary(gganimate)\nd_riclpm %&gt;%\n  tibble::rowid_to_column(\"id\") %&gt;%\n  pivot_longer(-id) %&gt;%\n  mutate(v = str_sub(name, 1, 1),\n         time = str_sub(name, 2, 2) %&gt;% as.integer()) %&gt;%\n  select(-name) %&gt;%\n  pivot_wider(names_from = v) %&gt;%\n  mutate(id = fct_reorder(factor(id), x, .fun = mean)) %&gt;%\n  ggplot(aes(x, y)) +\n  geom_point(aes(color = id), pch = 16) +\n  transition_states(time, .75, 3) +\n  ease_aes('cubic-in-out') +\n  shadow_wake(wake_length = 0.15) +\n  theme_minimal(base_size = 24, base_family = \"Roboto Condensed\") +\n  theme(legend.position = \"none\",\n        axis.title.y = element_text(\n          angle = 0,\n          hjust = .5,\n          vjust = .5\n        )) +\n  scale_color_viridis_d() +\n  labs(title = 'Time {closest_state}', x = 'X', y = 'Y')\n\n\n\n\n\n\n\n\nFigure 5: Changes in X and Y over Time in RI-CLPM\n\n\n\n\n\nFigure 6 show the distributions over time in the RI-CLPM. It is evident that the model does not allow for change over time (by design). If we expect growth, we need a model that allows for growth.\n\n\nCode\nd_riclpm %&gt;%\n  tibble::rowid_to_column(\"id\") %&gt;%\n  pivot_longer(-id) %&gt;%\n  mutate(v = str_sub(name, 1, 1),\n         time = str_sub(name, 2, 2) %&gt;% as.integer() %&gt;% factor()) %&gt;%\n  mutate(id = fct_reorder(factor(id), value, .fun = mean)) %&gt;%\n  select(-name) %&gt;%\n  \n  ggplot(aes(time, value)) +\n  geom_violin(color = NA, fill = \"gray95\") +\n  geom_line(aes(group = id, color = id), linewidth = .2, alpha = .1) +\n  facet_grid(cols = vars(v)) +\n  scale_color_viridis_d() +\n  theme_light(base_size = 24, base_family = \"Roboto Condensed\") +\n  theme(legend.position = \"none\",\n        axis.title.y = element_text(\n          angle = 0,\n          hjust = .5,\n          vjust = .5\n        )) +\n  labs(y = NULL, x = \"Time\")\n\n\n\n\n\n\n\n\nFigure 6: Distribution of X and Y over Time in RI-CLPM\n\n\n\n\n\nIf we did not know how the data were generated, we could evaluate the model with the RI-CLPM.\n\nsem(model = m_riclpm, data = d_riclpm, auto.cov.lv.x = F) %&gt;% \n  model_parameters(component = \"all\")\n\n# Loading\n\nLink       | Coefficient |   SE |       95% CI |      p\n-------------------------------------------------------\ni_x =~ x1  |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni_x =~ x2  |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni_x =~ x3  |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni_x =~ x4  |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni_y =~ y1  |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni_y =~ y2  |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni_y =~ y3  |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ni_y =~ y4  |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ne_x1 =~ x1 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ne_x2 =~ x2 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ne_x3 =~ x3 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ne_x4 =~ x4 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ne_y1 =~ y1 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ne_y2 =~ y2 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ne_y3 =~ y3 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\ne_y4 =~ y4 |        1.00 | 0.00 | [1.00, 1.00] | &lt; .001\n\n# Variance\n\nLink                   | Coefficient |   SE |       95% CI |     z |      p\n---------------------------------------------------------------------------\nx1 ~~ x1               |        0.00 | 0.00 | [0.00, 0.00] |       | &lt; .001\nx2 ~~ x2               |        0.00 | 0.00 | [0.00, 0.00] |       | &lt; .001\nx3 ~~ x3               |        0.00 | 0.00 | [0.00, 0.00] |       | &lt; .001\nx4 ~~ x4               |        0.00 | 0.00 | [0.00, 0.00] |       | &lt; .001\ny1 ~~ y1               |        0.00 | 0.00 | [0.00, 0.00] |       | &lt; .001\ny2 ~~ y2               |        0.00 | 0.00 | [0.00, 0.00] |       | &lt; .001\ny3 ~~ y3               |        0.00 | 0.00 | [0.00, 0.00] |       | &lt; .001\ny4 ~~ y4               |        0.00 | 0.00 | [0.00, 0.00] |       | &lt; .001\ne_x1 ~~ e_x1 (tau_ex1) |        1.05 | 0.07 | [0.91, 1.20] | 14.43 | &lt; .001\ne_x2 ~~ e_x2 (tau_exx) |        0.39 | 0.02 | [0.36, 0.42] | 25.35 | &lt; .001\ne_x3 ~~ e_x3 (tau_exx) |        0.39 | 0.02 | [0.36, 0.42] | 25.35 | &lt; .001\ne_x4 ~~ e_x4 (tau_exx) |        0.39 | 0.02 | [0.36, 0.42] | 25.35 | &lt; .001\ne_y1 ~~ e_y1 (tau_ey1) |        1.07 | 0.08 | [0.91, 1.23] | 13.09 | &lt; .001\ne_y2 ~~ e_y2 (tau_eyy) |        0.51 | 0.02 | [0.47, 0.55] | 26.52 | &lt; .001\ne_y3 ~~ e_y3 (tau_eyy) |        0.51 | 0.02 | [0.47, 0.55] | 26.52 | &lt; .001\ne_y4 ~~ e_y4 (tau_eyy) |        0.51 | 0.02 | [0.47, 0.55] | 26.52 | &lt; .001\ni_x ~~ i_x (tau_ix)    |        0.84 | 0.06 | [0.72, 0.97] | 13.17 | &lt; .001\ni_y ~~ i_y (tau_iy)    |        0.62 | 0.08 | [0.47, 0.77] |  8.08 | &lt; .001\n\n# Correlation\n\nLink                    | Coefficient |   SE |       95% CI |     z |      p\n----------------------------------------------------------------------------\ne_x1 ~~ e_y1 (tau_exy1) |        0.44 | 0.06 | [0.32, 0.56] |  7.06 | &lt; .001\ne_x2 ~~ e_y2 (tau_exy)  |        0.20 | 0.01 | [0.17, 0.22] | 14.95 | &lt; .001\ne_x3 ~~ e_y3 (tau_exy)  |        0.20 | 0.01 | [0.17, 0.22] | 14.95 | &lt; .001\ne_x4 ~~ e_y4 (tau_exy)  |        0.20 | 0.01 | [0.17, 0.22] | 14.95 | &lt; .001\ni_x ~~ i_y (tau_ixiy)   |        0.24 | 0.06 | [0.12, 0.35] |  4.11 | &lt; .001\n\n# Regression\n\nLink               | Coefficient |   SE |        95% CI |     z |      p\n------------------------------------------------------------------------\ne_x2 ~ e_x1 (b_xx) |        0.48 | 0.03 | [ 0.42, 0.54] | 15.48 | &lt; .001\ne_x2 ~ e_y1 (b_yx) |    5.75e-03 | 0.03 | [-0.04, 0.06] |  0.23 | 0.819 \ne_x3 ~ e_x2 (b_xx) |        0.48 | 0.03 | [ 0.42, 0.54] | 15.48 | &lt; .001\ne_x3 ~ e_y2 (b_yx) |    5.75e-03 | 0.03 | [-0.04, 0.06] |  0.23 | 0.819 \ne_x4 ~ e_x3 (b_xx) |        0.48 | 0.03 | [ 0.42, 0.54] | 15.48 | &lt; .001\ne_x4 ~ e_y3 (b_yx) |    5.75e-03 | 0.03 | [-0.04, 0.06] |  0.23 | 0.819 \ne_y2 ~ e_y1 (b_yy) |        0.40 | 0.03 | [ 0.33, 0.47] | 11.71 | &lt; .001\ne_y2 ~ e_x1 (b_xy) |        0.33 | 0.03 | [ 0.28, 0.39] | 11.63 | &lt; .001\ne_y3 ~ e_y2 (b_yy) |        0.40 | 0.03 | [ 0.33, 0.47] | 11.71 | &lt; .001\ne_y3 ~ e_x2 (b_xy) |        0.33 | 0.03 | [ 0.28, 0.39] | 11.63 | &lt; .001\ne_y4 ~ e_y3 (b_yy) |        0.40 | 0.03 | [ 0.33, 0.47] | 11.71 | &lt; .001\ne_y4 ~ e_x3 (b_xy) |        0.33 | 0.03 | [ 0.28, 0.39] | 11.63 | &lt; .001\n\n# Mean\n\nLink             | Coefficient |   SE |        95% CI |    z |      p\n---------------------------------------------------------------------\nx1 ~1            |        0.00 | 0.00 | [ 0.00, 0.00] |      | &lt; .001\nx2 ~1            |        0.00 | 0.00 | [ 0.00, 0.00] |      | &lt; .001\nx3 ~1            |        0.00 | 0.00 | [ 0.00, 0.00] |      | &lt; .001\nx4 ~1            |        0.00 | 0.00 | [ 0.00, 0.00] |      | &lt; .001\ny1 ~1            |        0.00 | 0.00 | [ 0.00, 0.00] |      | &lt; .001\ny2 ~1            |        0.00 | 0.00 | [ 0.00, 0.00] |      | &lt; .001\ny3 ~1            |        0.00 | 0.00 | [ 0.00, 0.00] |      | &lt; .001\ny4 ~1            |        0.00 | 0.00 | [ 0.00, 0.00] |      | &lt; .001\ni_x ~1  (b_ix_0) |        0.02 | 0.03 | [-0.05, 0.09] | 0.54 | 0.586 \ni_y ~1  (b_iy_0) |        0.02 | 0.03 | [-0.04, 0.09] | 0.71 | 0.476 \ne_x1 ~1          |        0.00 | 0.00 | [ 0.00, 0.00] |      | &lt; .001\ne_x2 ~1          |        0.00 | 0.00 | [ 0.00, 0.00] |      | &lt; .001\ne_x3 ~1          |        0.00 | 0.00 | [ 0.00, 0.00] |      | &lt; .001\ne_x4 ~1          |        0.00 | 0.00 | [ 0.00, 0.00] |      | &lt; .001\ne_y1 ~1          |        0.00 | 0.00 | [ 0.00, 0.00] |      | &lt; .001\ne_y2 ~1          |        0.00 | 0.00 | [ 0.00, 0.00] |      | &lt; .001\ne_y3 ~1          |        0.00 | 0.00 | [ 0.00, 0.00] |      | &lt; .001\ne_y4 ~1          |        0.00 | 0.00 | [ 0.00, 0.00] |      | &lt; .001"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Facilitate writing documents in APA Style\n\n\n\n\n\n\n\n\n\nAn object-oriented approach to making diagrams via ggplot2\n\n\n\n\nComplex diagrams can take a long time to get right. The ggdiagram package can take away much of the burden of tedious calculation.\n\n\n\nA regression path model\n\n\n\n\n\nLatent variable path diagram with all variables scaled to their variance sizes.\n\n\n\n\n\nA hierarchical model of abilities\n\n\n\n\n\n\n\nR package for simulating data using standardized coefficients\nTutorial\n\n\n\n\nIn the model below, the path coefficients are standardized. You would like to simulate the variables in the model, but you do not know the disturbance and residual variances. The simstandard package can help.\n\n\n\nA standardized latent variable model\n\n\n\n\n\n\n\nAn R package for detecting unusual scores in a test profile\nTutorial\n\n\n\n\nThis package estimates how unusual a multivariate normal profile is.\n\n\n\n\n\n\nA ggplot2 extension package for creating normal violin plots\n\n\n\n\nI needed to show confidence intervals and conditional normal distributions with specific means and standard deviations. I wrote the ggnormalviolin package to make this happen.\nIt makes plots like this:\n\n\n\nExample plot made with ggnormalviolin\n\n\n\n\n\n\n\nFunctions useful for psychological evaluations\nThis package is still in a preliminary state, just like Individual Psychometrics, the book it accompanies.\n\n\n\n\n\n\n\nMulivariate Confindence Intervals\n\n\n\n\n\n\n\nA set of functions I find convenient to have readily available to me\n\n\n\n\n\n\n\nWhoa! How did you place those labels so perfectly?\n\n\n\n\n\n\n\nAn R package for making digital spirographs\nTutorial\nMy Gallery\n\n\n\n\nMaking digital spirographs is fun! I made an R package called spiro that can make animated spirographs like this one:\n\n\n\n\n\n\nR package for making a custom arrowheads for ggplot2 using ggarrow\nTutorial\n\n\n\n\nThe arrowheadr package allows one to create custom arrowheads that can be used with the ggarrow package."
  },
  {
    "objectID": "software.html#apa7",
    "href": "software.html#apa7",
    "title": "Software",
    "section": "",
    "text": "Facilitate writing documents in APA Style"
  },
  {
    "objectID": "software.html#ggdiagram",
    "href": "software.html#ggdiagram",
    "title": "Software",
    "section": "",
    "text": "An object-oriented approach to making diagrams via ggplot2\n\n\n\n\nComplex diagrams can take a long time to get right. The ggdiagram package can take away much of the burden of tedious calculation.\n\n\n\nA regression path model\n\n\n\n\n\nLatent variable path diagram with all variables scaled to their variance sizes.\n\n\n\n\n\nA hierarchical model of abilities"
  },
  {
    "objectID": "software.html#simstandard",
    "href": "software.html#simstandard",
    "title": "Software",
    "section": "",
    "text": "R package for simulating data using standardized coefficients\nTutorial\n\n\n\n\nIn the model below, the path coefficients are standardized. You would like to simulate the variables in the model, but you do not know the disturbance and residual variances. The simstandard package can help.\n\n\n\nA standardized latent variable model"
  },
  {
    "objectID": "software.html#unusualprofile",
    "href": "software.html#unusualprofile",
    "title": "Software",
    "section": "",
    "text": "An R package for detecting unusual scores in a test profile\nTutorial\n\n\n\n\nThis package estimates how unusual a multivariate normal profile is."
  },
  {
    "objectID": "software.html#ggnormalviolin",
    "href": "software.html#ggnormalviolin",
    "title": "Software",
    "section": "",
    "text": "A ggplot2 extension package for creating normal violin plots\n\n\n\n\nI needed to show confidence intervals and conditional normal distributions with specific means and standard deviations. I wrote the ggnormalviolin package to make this happen.\nIt makes plots like this:\n\n\n\nExample plot made with ggnormalviolin"
  },
  {
    "objectID": "software.html#psycheval",
    "href": "software.html#psycheval",
    "title": "Software",
    "section": "",
    "text": "Functions useful for psychological evaluations\nThis package is still in a preliminary state, just like Individual Psychometrics, the book it accompanies.\n\n\n\n\n\n\n\nMulivariate Confindence Intervals"
  },
  {
    "objectID": "software.html#wjsmisc",
    "href": "software.html#wjsmisc",
    "title": "Software",
    "section": "",
    "text": "A set of functions I find convenient to have readily available to me\n\n\n\n\n\n\n\nWhoa! How did you place those labels so perfectly?"
  },
  {
    "objectID": "software.html#spiro",
    "href": "software.html#spiro",
    "title": "Software",
    "section": "",
    "text": "An R package for making digital spirographs\nTutorial\nMy Gallery\n\n\n\n\nMaking digital spirographs is fun! I made an R package called spiro that can make animated spirographs like this one:"
  },
  {
    "objectID": "software.html#arrowheadr",
    "href": "software.html#arrowheadr",
    "title": "Software",
    "section": "",
    "text": "R package for making a custom arrowheads for ggplot2 using ggarrow\nTutorial\n\n\n\n\nThe arrowheadr package allows one to create custom arrowheads that can be used with the ggarrow package."
  },
  {
    "objectID": "software.html#apaquarto",
    "href": "software.html#apaquarto",
    "title": "Software",
    "section": "apaquarto",
    "text": "apaquarto\nA Quarto Extension for Creating APA 7 Style Documents\nThis is a quarto article template that creates APA Style 7th Edition documents in .docx, .html. and .pdf. I made this extension for my own workflow. If it helps you, too, I am happy. The output of the template is displayed below:\n\n\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Nominal, Ordinal, Interval, and Ratio scales\n\n\n\n\n\n\n\n\nHow to visualize which values are most frequent in a distribution\n\n\n\n\n\n\n\n\nFunctions that tell us which values are more likely to occur\n\n\n\n\n\n\n\n\nAn introduction to expected value and how it is related to the concept of the mean of a variable.\n\n\n\n\n\n\n\n\nHow variance is calculated\n\n\n\n\n\n\n\n\nSum of the Reasons Variables Are Normally Normal\n\n\n\n\n\n\n\n\nAn introduction to skewness and its relationship to variability.\n\n\n\n\n\n\n\n\nAn introduction to kurtosis and how it is not just a measure of peakedness.\n\n\n\n\n\n\n\n\nAn introduction to z-scores, stanines, stens, scaled scores, T scores, and index scores\n\n\n\n\n\n\n\n\nCovariance is rarely useful by itself but is indispensable for many statistics."
  },
  {
    "objectID": "presentations.html#psychometrics-from-the-ground-up",
    "href": "presentations.html#psychometrics-from-the-ground-up",
    "title": "Presentations",
    "section": "",
    "text": "Nominal, Ordinal, Interval, and Ratio scales\n\n\n\n\n\n\n\n\nHow to visualize which values are most frequent in a distribution\n\n\n\n\n\n\n\n\nFunctions that tell us which values are more likely to occur\n\n\n\n\n\n\n\n\nAn introduction to expected value and how it is related to the concept of the mean of a variable.\n\n\n\n\n\n\n\n\nHow variance is calculated\n\n\n\n\n\n\n\n\nSum of the Reasons Variables Are Normally Normal\n\n\n\n\n\n\n\n\nAn introduction to skewness and its relationship to variability.\n\n\n\n\n\n\n\n\nAn introduction to kurtosis and how it is not just a measure of peakedness.\n\n\n\n\n\n\n\n\nAn introduction to z-scores, stanines, stens, scaled scores, T scores, and index scores\n\n\n\n\n\n\n\n\nCovariance is rarely useful by itself but is indispensable for many statistics."
  },
  {
    "objectID": "presentations.html#other-assessment-videos",
    "href": "presentations.html#other-assessment-videos",
    "title": "Presentations",
    "section": "Other Assessment Videos",
    "text": "Other Assessment Videos\n\nEmotional Intelligence and CHC Theory\nHow might emotional intelligence relate to psychometric models of intelligence?\n\n\n\n\n\n\n\nMisunderstanding Regression to the Mean\nWhat regression to the mean is and how it is often misunderstood. Examples are provided as to how it is applied to IQ and the death penalty.\n\n\n\n\n\n\n\nTaking latent variable models seriously\nApplying latent score estimates to individuals\n\n\n\n\n\n\n\nSpecific cognitive processing weaknesses are rarely full explanations for academic deficits\nIn which I confess to having had a longstanding misunderstanding about diagnosing learning disorders.\n\n\n\n\n\n\n\nTwo kinds of cognitive ability hierarchies\nSome latent variables represent “hierarchical abstractions.”\n\n\n\n\n\n\n\nA Taxonomy of Influences on Ability Tests\nSome influences are test-specific whereas others are shared across tests. Some influences are transient whereas others are stable. Some influences are relevant to the construct of interest whereas others are irrelevant.\n\n\n\n\n\n\n\nWithin-composite differences: Why measures of the same ability differ?\nThere are a number of lesser-known reasons that two tests that are intended to measure the same ability might differ substantially.\n\n\n\n\n\n\n\nWithin-composite differences: Do large subtest score differences invalidate composite scores?\nIt is often asserted that composite scores should not be interpreted when the scores that make up that composite are discrepant. I show here why this is not typically true, depending on what the composite score is used for.\n\n\n\n\n\n\n\nWriting Assessment Reports People Will Read, Understand, and Remember\nMy appearance on the School Psyched Podcast on October 20, 2019"
  },
  {
    "objectID": "presentations.html#audio-presentations",
    "href": "presentations.html#audio-presentations",
    "title": "Presentations",
    "section": "Audio Presentations",
    "text": "Audio Presentations\n\nThe evolution of cognitive assessment\nMy appearance on the Testing Psychologist Podcast on March 29, 2021"
  },
  {
    "objectID": "posts/point-labels-perpendicular-to-a-curve-in-ggplot2/index.html",
    "href": "posts/point-labels-perpendicular-to-a-curve-in-ggplot2/index.html",
    "title": "Point labels perpendicular to a curve in ggplot2",
    "section": "",
    "text": "I would like to label points on a sine function so that the labels are always legible. In a sine wave plot in which θ ranges from 0 to 2π, sin(θ) ranges from −1 to +1. Thus, the plot’s xy ratio is\n\\text{plot ratio}= \\frac{2\\pi-0}{1- (-1)}=\\pi\nThe first derivative of the sine function is the cosine function. In my plot, the slope of the tangent line at each point is\n\n\\text{tangent slope} = \\text{plot ratio}\\times \\cos(\\theta)\n The angle of the tangent line’s slope is the arctan of the slope. I would like to place the label perpendicular to the tangent line so I will add 90 degrees (i.e., π/2 radians).\n\n\\text{text angle}=\\tan^{-1}(\\text{tangent slope})+\\pi/2\n\nNow I need a pair of functions that will convert this angle into the right values for ggplot2’s hjust and vjust arguments. I have added the angle2vjust and angle2hjust functions to the WJSmisc package, but I have defined them here as well:\n\nangle2vjust &lt;- function(theta, multiplier = 1, as_degrees = FALSE) {\n  if (as_degrees) theta &lt;- theta * pi / 180\n  (((sin(theta + pi) + 1) / 2) - 0.5) * multiplier + 0.5\n}\n\nangle2hjust &lt;- function(theta, multiplier = 1, as_degrees = FALSE) {\n  if (as_degrees) theta &lt;- theta * pi / 180\n  (((cos(theta + pi) + 1) / 2) - 0.5) * multiplier + 0.5\n}\n\nNow we plot the sine function with labels. I have used geom_richtext from the ggtext package because it allows me to set a white background along with padding and margins.\n\nlibrary(tidyverse)\nlibrary(ggtext)\n\nplot_ratio &lt;- pi\n\ntibble(theta = seq(0, 2 * pi, length.out = 13),\n            y =  sin(theta),\n            tangent_slope = cos(theta) * plot_ratio,\n            text_angle = atan(tangent_slope) + pi / 2) %&gt;%\n  ggplot(aes(theta, y)) +\n  geom_richtext(aes(label = formatC(y, digits = 2, format = \"f\"),\n                    vjust = angle2vjust(text_angle),\n                    hjust = angle2hjust(text_angle)),\n                label.color = NA,\n                label.padding = unit(1, \"pt\"),\n                label.margin = unit(5, \"pt\"),\n                size = 4) +\n  geom_point() +\n  stat_function(fun = sin) +\n  scale_x_continuous(\"&theta;\",\n                     breaks = seq(0, 2 * pi, \n                                  length.out = 13),\n                     minor_breaks = NULL,\n                     labels = function(x) round(x * 180 / pi)) +\n  scale_y_continuous(\"sin(&theta;)\") +\n  coord_fixed(ratio = plot_ratio, clip = \"off\") +\n  theme_minimal(base_size = 16) +\n  theme(axis.title.x = element_markdown(),\n        axis.title.y = element_markdown())\n\n\n\n\n\n\n\n\nIf you do not mind turning your head to one side or the other, a somewhat easier method is to set the label’s vjust to a negative value and rotate the labels by the angle of the tangent line:\n\ntibble(theta = seq(0, 2 * pi, length.out = 13),\n            y = sin(theta),\n            tangent_slope = cos(theta) * plot_ratio,\n            text_angle = atan(tangent_slope)) %&gt;%\n  ggplot(aes(theta, y)) +\n  geom_richtext(aes(label = round(y, 2),\n                    angle = text_angle * 180 / pi),\n                vjust = 0,\n                label.color = NA,\n                label.padding = unit(1, \"pt\"),\n                label.margin = unit(2, \"pt\"),\n                size = 4) +\n  geom_point() +\n  stat_function(fun = sin) +\n  scale_x_continuous(\"&theta;\",\n                     breaks = seq(0, 2 * pi, \n                                  length.out = 13),\n                     minor_breaks = NULL,\n                     labels = function(x) round(x * 180 / pi)) +\n  scale_y_continuous(\"sin(&theta;)\") +\n  coord_fixed(ratio = plot_ratio, clip = \"off\") +\n  theme_minimal(base_size = 16) +\n  theme(axis.title.x = element_markdown(),\n        axis.title.y = element_markdown())\n\n\n\n\n\n\n\n\nWhat if you do not know the function’s first derivative? You can approximate the slope of the tangent line by comparing a function’s output of each point with the output of a slightly deviated point. Here is a plot of the normal cumulative distribution function.\n\n# Small change in x\ndx &lt;- .00000001\nplot_ratio &lt;- 6\n\ntibble(x = seq(-3,3,.5),\n       y = pnorm(x),\n       tangent_slope = plot_ratio * (pnorm(x + dx) - y) / dx,\n       text_angle = atan(tangent_slope) + pi / 2,\n       degrees = text_angle * 180 / pi) %&gt;% \n  ggplot(aes(x,y)) + \n  geom_point() +\n  geom_richtext(aes(label = formatC(y, 2, format = \"f\"),\n                    vjust = angle2vjust(text_angle),\n                    hjust = angle2hjust(text_angle)),\n                label.color = NA,\n                label.padding = unit(1, \"pt\"),\n                label.margin = unit(5, \"pt\"),\n                size = 4) +\n  stat_function(fun = pnorm) + \n  theme_minimal(base_size = 16) + \n  theme(axis.title.x = element_markdown(),\n        axis.title.y = element_markdown()) +\n  coord_fixed(ratio = plot_ratio) + \n  scale_x_continuous(\"*x*\", breaks = -3:3) +\n  scale_y_continuous(\"&Phi;(*x*)\")\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2021,\n  author = {Schneider, W. Joel},\n  title = {Point Labels Perpendicular to a Curve in Ggplot2},\n  date = {2021-07-27},\n  url = {https://wjschne.github.io/posts/point-labels-perpendicular-to-a-curve-in-ggplot2/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2021, July 27). Point labels perpendicular to a curve\nin ggplot2. Schneirographs. https://wjschne.github.io/posts/point-labels-perpendicular-to-a-curve-in-ggplot2/"
  },
  {
    "objectID": "posts/insert-text-from-one-markdown-document-into-another/index.html",
    "href": "posts/insert-text-from-one-markdown-document-into-another/index.html",
    "title": "Insert Text from One Markdown Document into Another",
    "section": "",
    "text": "While assembling a reply letter for a revise-and-resubmit document, I was documenting for reviewers how we addressed their concerns. The reply letter quoted numerous sentences and paragraphs that had been changed. Unfortunately, each time my co-authors and I edited the paper, I lost track of which sentences and paragraphs I had previously copied into the reply letter. I did not want to misquote my own paper.\nCopying-and-pasting with each edit was getting tedious, and it was an error-prone process. I decided to write a function that would retrieve a named div or span from the paper and print it as a quote in my reply letter. Here it is:\nget_quote &lt;- function(id, file, blockquote = TRUE) {\n  blocktext &lt;- ifelse(blockquote, \"\\n&gt; \", \"\")\n\n  refreplace &lt;- function(x) {\n    # List of sequence types\n    crossrefs &lt;- c(`@fig-` = \"Figure\",\n                   `@fig-` = \"Table\",\n                   `@eq-` = \"Equation\",\n                   `@thm-` = \"Lemma\",\n                   `@lem-` = \"Corollary\",\n                   `@cor-` = \"Proposition\",\n                   `@prp-` = \"Conjecture\",\n                   `@cnj-` = \"Definition\",\n                   `@def-` = \"Example\",\n                   `@exm-` = \"Example\",\n                   `@exr-` = \"Exercise\",\n                   `apafg-` = \"Figure\",\n                   `apatb-` = \"Figure\"\n    )\n\n    # Find all crossreference types and number them\n    make_replacements &lt;- function(x, reftype, prefix) {\n      regstring &lt;- ifelse(stringr::str_starts(reftype, \"\\\\@\"),\n                          paste0(\"\\\\\", reftype, \"(.*?)(?=[\\\\.\\\\?\\\\!\\\\]\\\\}\\\\s,])\"),\n                          paste0(\"\\\\{\", reftype, \"(.*?)\\\\}\"))\n      patterns &lt;- stringr::str_extract_all(string = x, pattern = regstring) |&gt;\n        unlist() |&gt;\n        unique() |&gt;\n        stringr::str_replace(\"\\\\{\", \"\\\\\\\\{\") |&gt;\n        stringr::str_replace(\"\\\\}\", \"\\\\\\\\}\")\n\n      if (all(is.na(patterns))) return(NULL)\n      replacements &lt;- paste(prefix, seq_along(patterns))\n      names(replacements) &lt;- patterns\n      replacements\n    }\n    allreplacements &lt;- purrr::map2(\n      names(crossrefs),\n      crossrefs,\n      \\(rt, pf) make_replacements(\n        x = x,\n        reftype = rt,\n        prefix = pf)) |&gt;\n      unlist()\n\n    stringr::str_replace_all(x, allreplacements)\n\n  }\n\n  filetext &lt;- readLines(file) |&gt;\n    refreplace()\n\n  idcount &lt;- sum(stringr::str_count(filetext, paste0(\"#\", id)))\n  if (idcount &gt; 1)\n    stop(paste0(\n      \"The id (\",\n      id ,\n      \") is not unique. There are \",\n      idcount,\n      \" instances of id = \",\n      id,\n      \".\"\n    ))\n\n  s &lt;- filetext |&gt;\n    paste0(collapse = \"\\n\") |&gt;\n    stringr::str_match(pattern = paste0(\"(?&lt;=\\\\[).+(?=\\\\]\\\\{\\\\#\",\n                               id,\n                               \"\\\\})\"))  |&gt;\n    getElement(1)\n\n  if (is.na(s)) {\n    s &lt;- filetext |&gt;\n      paste0(collapse = \"|||\") |&gt;\n      stringr::str_match(pattern = paste0(\":::\\\\{\\\\#\",\n                                 id,\n                                 \"\\\\}(.*?):::\"))  |&gt;\n      getElement(2) |&gt;\n      stringr::str_replace_all(\"\\\\|\\\\|\\\\|\", blocktext)\n  } else {\n    s &lt;- paste0(blocktext, s)\n  }\n\n  if (is.na(s)) stop(\"Could not find a div or span with id = \", id)\n\n  s\n}\nFor your convenience and mine, I have added this function into the WJSmisc package."
  },
  {
    "objectID": "posts/insert-text-from-one-markdown-document-into-another/index.html#remove-blockquote-formatting",
    "href": "posts/insert-text-from-one-markdown-document-into-another/index.html#remove-blockquote-formatting",
    "title": "Insert Text from One Markdown Document into Another",
    "section": "Remove blockquote formatting",
    "text": "Remove blockquote formatting\nBy default, the function adds a &gt; before each line in the quoted markdown so that it appears as a block quote. You can turn off the block quote formatting like so:\n`r get_quote(\"id1\", \"index.qmd\", blockquote = FALSE)`\nText in a span"
  },
  {
    "objectID": "posts/insert-text-from-one-markdown-document-into-another/index.html#curly-braces-with-additional-information",
    "href": "posts/insert-text-from-one-markdown-document-into-another/index.html#curly-braces-with-additional-information",
    "title": "Insert Text from One Markdown Document into Another",
    "section": "Curly braces with additional information",
    "text": "Curly braces with additional information\nWhat if you need to put additional information in the curly braces of the span or div (e.g., a css class)?\n[Here is more text with extra stuff in the curly braces.]{#id3 .myclass}\nYou can trick the function into thinking that the extra stuff is part of the id like so:\n`r get_quote(\"id3 .myclass\", \"index.qmd\")`\n\nHere is more text with extra stuff in the curly braces."
  },
  {
    "objectID": "posts/concatenating-vectors-unless-a-vector-is-empty/index.html",
    "href": "posts/concatenating-vectors-unless-a-vector-is-empty/index.html",
    "title": "Concatenating vectors unless a vector is empty",
    "section": "",
    "text": "I often need to add a prefix or suffix to a character vector:\n\nlibrary(tidyverse)\nx &lt;- c(\"A\", \"B\", \"C\")\npaste0(x, \"_1\")\n\n[1] \"A_1\" \"B_1\" \"C_1\"\n\n\nHowever, if the vector is empty, I do not want bare prefixes or suffixes like this:\n\nx &lt;- character(0)\npaste0(x, \"_1\")\n\n[1] \"_1\"\n\n\nI used to write a lot of tedious if statements like this:\n\nif (length(x) == 0) {\n  y &lt;- character(0)\n} else {\n  y &lt;- paste0(x, \"_1\")\n}\n\nFor R 4.0.1 and later versions, the paste and paste0 functions acquired the recycle0 argument. Setting recycle0 to TRUE returns an empty vector if at least one string is empty:\n\npaste0(x, \"_1\", recycle0 = TRUE)\n\ncharacter(0)\n\n\nIf you need to work with an earlier version of R, you can use the sprintf function instead:\n\n# Non-empty vector\nx &lt;- c(\"A\", \"B\", \"C\")\nsprintf(\"%s_1\", x)\n\n[1] \"A_1\" \"B_1\" \"C_1\"\n\n# Empty vector\nx &lt;- character(0)\nsprintf(\"%s_1\", x)\n\ncharacter(0)\n\n\nThe glue function from the glue package also works nicely:\n\n# Non-empty vector\nx &lt;- c(\"A\", \"B\", \"C\")\nglue::glue(\"{x}_1\")\n\nA_1\nB_1\nC_1\n\n# Empty vector\nx &lt;- character(0)\nglue::glue(\"{x}_1\")\n\n\n\n\nCitationBibTeX citation:@misc{schneider2021,\n  author = {Schneider, W. Joel},\n  title = {Concatenating Vectors Unless a Vector Is Empty},\n  date = {2021-05-13},\n  url = {https://wjschne.github.io/posts/concatenating-vectors-unless-a-vector-is-empty/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2021, May 13). Concatenating vectors unless a vector\nis empty. Schneirographs. https://wjschne.github.io/posts/concatenating-vectors-unless-a-vector-is-empty/"
  },
  {
    "objectID": "posts/bar-chart-labels-on-smooth-paths-in-ggplot2/index.html",
    "href": "posts/bar-chart-labels-on-smooth-paths-in-ggplot2/index.html",
    "title": "Bar chart labels on smooth paths in ggplot2",
    "section": "",
    "text": "Setup\nThis trick takes a lot of work. I only use it when I need a plot to look its best.\nSuppose I have a Likert questionnaire item with responses ranging from Strongly Disagree to Strongly Agree. Let’s say I have many groups in my study (e.g., college majors), and I want to compare their responses to the item.\nFirst I will create fake data for 26 groups, A–Z.\n\nlibrary(extrafont)\nlibrary(tidyverse)\nmyfont &lt;- \"Roboto Condensed\"\n\nd &lt;- tibble(Group = LETTERS[1:26],\n            mu = rnorm(26, 0, .5),\n            sigma = 1) %&gt;%\n  mutate(x = pmap(list(mean = mu, sd = sigma), rnorm, n = 500)) %&gt;%\n  unnest(x) %&gt;%\n  mutate(Agree = cut_number(x, 6) %&gt;%\n           factor(\n             labels = c(\n               \"Strongly\\nDisagree\",\n               \"Disagree\",\n               \"Slightly\\nDisagree\",\n               \"Slightly\\nAgree\",\n               \"Agree\",\n               \"Strongly\\nAgree\"\n             )\n           )) %&gt;%\n  group_by(Group, Agree) %&gt;%\n  summarise(n = n(), .groups = \"drop\") %&gt;%\n  group_by(Group) %&gt;%\n  arrange(Group, Agree) %&gt;%\n  mutate(p = n / sum(n)) %&gt;%\n  ungroup()\n\n\n\nFirst Attempts at Plotting\n\nd %&gt;%\n  ggplot(mapping = aes(p, Group)) +\n  geom_col(aes(fill = fct_rev(Agree))) +\n  scale_fill_viridis_d(NULL,\n                       begin = .15,\n                       end = 0.8,\n                       direction = -1) +\n  scale_x_continuous(\"Percent\", expand = expansion()) +\n  scale_y_discrete(\"Group\", expand = expansion()) +\n  coord_fixed(1 / 20.5, clip = \"off\") +\n  geom_text(\n    aes(x = p, label = round(100 * p, 0)),\n    color = \"white\",\n    family = myfont,\n    position = position_stack(vjust = .5)\n  )\n\n\n\n\n\n\n\n\nNot bad, but it would look better if we sorted the groups. We have many sorting options. One is that we can convert the Likert scale to numeric and then sort by the group with the highest mean.\n\nd %&gt;%\n  mutate(Group = fct_reorder(Group, .x = as.numeric(Agree) * p, \n                             .fun = mean)) %&gt;%\n  ggplot(mapping = aes(p, Group)) +\n  geom_col(aes(fill = fct_rev(Agree))) +\n  scale_fill_viridis_d(NULL,\n                       begin = .15,\n                       end = 0.8,\n                       direction = -1) +\n  scale_x_continuous(\"Percent\", expand = expansion()) +\n  scale_y_discrete(\"Group\", expand = expansion()) +\n  coord_fixed(1 / 20.5, clip = \"off\") +\n  geom_text(\n    aes(x = p, label = round(100 * p, 0)),\n    color = \"white\",\n    family = myfont,\n    position = position_stack(vjust = .5)\n  )\n\n\n\n\n\n\n\n\nIf I wanted to sort by the “Strongly Agree” category (or any other category):\n\nd %&gt;%\n  mutate(Group = fct_reorder(\n    Group,\n    .x = (Agree == \"Strongly\\nAgree\") * p,\n    .fun = mean\n  )) %&gt;%\n  ggplot(mapping = aes(p, Group)) +\n  geom_col(aes(fill = fct_rev(Agree))) +\n  scale_fill_viridis_d(NULL,\n                       begin = .15,\n                       end = 0.8,\n                       direction = -1) +\n  scale_x_continuous(\"Percent\", expand = expansion()) +\n  scale_y_discrete(\"Group\", expand = expansion()) +\n  coord_fixed(1 / 20.5, clip = \"off\") +\n  geom_text(\n    aes(x = p, label = round(100 * p, 0)),\n    color = \"white\",\n    family = myfont,\n    position = position_stack(vjust = .5)\n  )\n\n\n\n\n\n\n\n\n\n\nFinal Plot\nSo far, these plots look pretty good. However, I wish that the percentage labels were placed in a more aesthetically pleasing way. I am going to group by each of the response categories and then create a loess regression equation to smooth out the placement. It will mean that the labels are no longer centered in the stacked bars, but I think the sacrifice is worth it. I think that the percentage values are much easier to compare because the eye can follow the smooth line of labels.\n\nd &lt;- tibble(Group = LETTERS[1:26],\n            mu = rnorm(26, 0, .5),\n            sigma = 1) %&gt;%\n  mutate(x = pmap(list(mean = mu, sd = sigma), rnorm, n = 500)) %&gt;%\n  unnest(x) %&gt;%\n  mutate(Agree = cut_number(x, 6) %&gt;%\n           factor(\n             labels = c(\n               \"Strongly\\nDisagree\",\n               \"Disagree\",\n               \"Slightly\\nDisagree\",\n               \"Slightly\\nAgree\",\n               \"Agree\",\n               \"Strongly\\nAgree\"\n             )\n           )) %&gt;%\n  mutate(Group = fct_reorder(Group, as.numeric(Agree), .fun = mean)) %&gt;%\n  group_by(Group, Agree) %&gt;%\n  summarise(n = n(), .groups = \"drop\") %&gt;%\n  mutate(Group_position = as.numeric(Group)) %&gt;%\n  group_by(Group) %&gt;%\n  arrange(Group, Agree) %&gt;%\n  mutate(p = n / sum(n),\n         # proportion in each response category by group\n         cp = cumsum(p),\n         # cumulative proportion\n         xpos = cp - p / 2 # center of each stacked bar\n         ) %&gt;%\n         group_by(Agree) %&gt;%\n           nest() %&gt;%\n           mutate(\n             fit = map(data,\n                       loess,\n                       formula = \"xpos ~ Group_position\",\n                       span = 0.45),\n             # loess regression for each response category\n             xhat = map(fit, predict) # x-axis position on smooth line\n           ) %&gt;%\n           select(-fit) %&gt;%\n           unnest(c(data, xhat)) %&gt;%\n           mutate(Group = factor(Group, labels = rev(LETTERS[1:26])))\n         \n         d %&gt;%\n           ggplot(mapping = aes(p, Group)) +\n           geom_col(aes(fill = fct_rev(Agree))) +\n           geom_text(aes(x = xhat,\n                         label = ifelse(p &gt; .01, # No labels on small bars\n                                        round(100 * p, 0),\n                                        \"\")),\n                     color = \"white\",\n                     family = myfont) +\n           scale_fill_viridis_d(begin = .15,\n                                end = 0.8,\n                                direction = -1) +\n           scale_x_continuous(\"Percent\", expand = expansion()) +\n           scale_y_discrete(\"Group\", expand = expansion()) +\n           coord_fixed(1 / 20.5, clip = \"off\") +\n           theme_minimal(base_size = 13, base_family = myfont) +\n           theme(\n             legend.position = \"top\",\n             legend.box.spacing = unit(0.5, \"mm\"),\n             legend.text = element_text(\n               color = \"white\",\n               size = 13,\n               margin = margin(b = -40),\n               # Lower legend text into box\n               vjust = 0.5\n             ),\n             legend.spacing.x = unit(0, \"mm\"),\n             axis.text.y = element_text(hjust = 0.5)\n           ) +\n           guides(\n             fill = guide_legend(\n               title = NULL,\n               nrow = 1,\n               # Put legend on a single row\n               reverse = T,\n               # Reverse order of legend\n               label.position = \"top\",\n               # Put text atop keys\n               keyheight = unit(15, \"mm\"),\n               # Size of legend rectangles\n               keywidth = unit(22.56, \"mm\")\n             )\n           ) \n\n\n\n\n\n\n\n\nThis feels right to me.\n\n\n\n\nCitationBibTeX citation:@misc{schneider2021,\n  author = {Schneider, W. Joel},\n  title = {Bar Chart Labels on Smooth Paths in Ggplot2},\n  date = {2021-07-31},\n  url = {https://wjschne.github.io/posts/bar-chart-labels-on-smooth-paths-in-ggplot2/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2021, July 31). Bar chart labels on smooth paths in\nggplot2. Schneirographs. https://wjschne.github.io/posts/bar-chart-labels-on-smooth-paths-in-ggplot2/"
  },
  {
    "objectID": "posts/apatables/apa723.html",
    "href": "posts/apatables/apa723.html",
    "title": "Recreating APA Manual Table 7.23 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 23 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nUse of hanging_indent\nUse of flextable::merge_v to remove redundant content\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(lme4)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.23\n\n\n\n\n\n\n\nMaking this table was relatively straightforward, except that I needed to play with the column widths and hanging indents until they fit in a table that is 6.5 inches wide.\nThe Reason for interest column needed vertical merging to remove redundant content.\nThe frequency column was aligned on the decimal with align_chr.\n\n```{r}\n#| label: tbl-723\n#| tbl-cap: \"Reasons Why Individuals Chose to Watch\n#|           the Royal Wedding (*N* = 45)\"\n\ntibble(\n   `Reason for interest` = c(\n     \"Royal family and its history\",\n     \"Royal family and its history\",\n     \"Fashion and pop culture\",\n     \"Fashion and pop culture\",\n     \"Fairy tales and love stories\",\n     \"Fairy tales and love stories\",\n     \"To pass time/it was on TV\",\n     \"To pass time/it was on TV\"),\n   `Example quote` = c(\n     paste(\n       \"\\\"I love all things British. I studied abroad in\",\n       \"the U.K. I also watched the weddings of Charles\",\n       \"& Diana and Andrew & Fergie. I watched Diana's\",\n       \"funeral. Watching William & Kate get married\",\n       \"seemed like the natural thing to do.\\\"\"),\n     paste(\n       \"\\\"I find the royal family and their practices\",\n       \"and traditions fascinating. I'm a big fan of\",\n       \"tradition in any capacity (graduation ceremonies,\",\n       \"weddings, etc.) and enjoy watching traditions\",\n       \"older than our country (the U.S.).\\\"\"),\n     paste(\n       \"\\\"When big pop culture things happen, I tend to\",\n       \"want to watch so I'm 'in on it.' Also, when I was\",\n       \"little my mom made us get up to watch Princess\",\n       \"Diana get married, so it felt a little like\",\n       \"tradition.\\\"\"),\n     paste(\n       \"\\\"I was curious. Wanted to see her dress and how\",\n       \"the other people who attended dressed. Like pomp\",\n       \"and ceremony.\\\"\"),\n     paste(\n       \"\\\"I watched his mom and dad get married, watched\",\n       \"him grow up. Plus I love a fairy tale that comes\",\n       \"true. I believe in love and romance.\\\"\"),\n     paste(\n       \"\\\"I am a romantic and I think this is a great\",\n       \"love story.\\\"\"),\n     paste(\n       \"\\\"I was at the airport and it was broadcasting on\",\n       \"TV when I was waiting for my flight.\\\"\"),\n     paste(\"\\\"It was on CNN when I got up.\\\"\")),\n  `Frequency, *n* (%)` = c(\n    \"16 (35.6)\",NA,\n    \"13 (28.9)\",NA,\n    \"11 (24.4)\",NA,\n    \"5 (11.1)\",NA)\n) |&gt; \n  mutate(`Reason for interest` = hanging_indent(`Reason for interest`, width = 16),\n         `Example quote` = hanging_indent(`Example quote`, width = 50),\n         `Frequency, *n* (%)` = align_chr(`Frequency, *n* (%)`)) |&gt; \n  apa_flextable(line_spacing = 1.5) |&gt; \n  align(j = 1:2) |&gt; \n  width(width = c(1.4, 3.8, 1.3)) |&gt; \n  merge_v(j = 1)\n```\n\n\n\n\nTable 1\n\n\nReasons Why Individuals Chose to Watch the Royal Wedding (N = 45)\n\n\n\n\nReason for interestExample quoteFrequency, n (%)Royal family and    its history“I love all things British. I studied abroad in    the U.K. I also watched the weddings of Charles    & Diana and Andrew & Fergie. I watched Diana’s    funeral. Watching William & Kate get married    seemed like the natural thing to do.”16 (35.6)“I find the royal family and their practices and    traditions fascinating. I’m a big fan of tradition    in any capacity (graduation ceremonies, weddings,    etc.) and enjoy watching traditions older than our    country (the U.S.).”Fashion and pop    culture“When big pop culture things happen, I tend to    want to watch so I’m ‘in on it.’ Also, when I was    little my mom made us get up to watch Princess    Diana get married, so it felt a little like    tradition.”13 (28.9)“I was curious. Wanted to see her dress and how    the other people who attended dressed. Like pomp    and ceremony.”Fairy tales and    love stories“I watched his mom and dad get married, watched    him grow up. Plus I love a fairy tale that comes    true. I believe in love and romance.”11 (24.4)“I am a romantic and I think this is a great love    story.”To pass time/it    was on TV“I was at the airport and it was broadcasting on    TV when I was waiting for my flight.” 5 (11.1)“It was on CNN when I got up.”\n\n\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.23 in {R} with Apa7},\n  date = {2025-10-03},\n  url = {https://wjschne.github.io/posts/apatables/apa723.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, October 3). Recreating APA Manual Table 7.23 in\nR with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa723.html"
  },
  {
    "objectID": "posts/apatables/apa721.html",
    "href": "posts/apatables/apa721.html",
    "title": "Recreating APA Manual Table 7.21 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 21 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nUse of add_star_column\nUse of thin spaces &thinsp; for footnote superscripts.\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(lme4)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.21\n\n\n\n\n\n\n\nI did not have enough information to simulate the three models, so I input the data directly. This made it comparatively easy to recreate. Adding stars to the \\chi^2 column was made easier by creating a temporary p-value column with values less than .001 and then applying add_star_column.\nThe APA Publication Manual recommends following the footnote symbols with a superscripted space. In the apa-note, the superscript symbols a, b, and c are followed by a thin space within the superscript. This gives a pleasing amount of separation.\n\n```{r}\n#| label: tbl-721\n#| tbl-cap: \"Results of Confirmatory Factor Analysis for the \n#|   Relationships Among Three Types of Intelligence\"\n#| apa-note: \n#|   - \"Structural equation modeling was used for the analysis. \n#|     NFI&nbsp;=&nbsp;normed fit index; \n#|     CFI&nbsp;=&nbsp;comparative fit index; \n#|     RMSEA&nbsp;=&nbsp;root-mean-square error of \n#|     approximation.\"\n#|   - \"^a&thinsp;^In Model A, all 57 items of social \n#|    intelligence, emotional intelligence, and cultural \n#|    intelligence were loaded onto one factor. ^b&thinsp;^In\n#|    Model B, the 21 items of social intelligence were \n#|    loaded onto one factor, and the 16 items of emotional \n#|    intelligence and the 20 items of cultural intelligence \n#|    were loaded onto another factor. ^c&thinsp;^In Model C, \n#|    the 21 items of social intelligence were loaded onto one \n#|    factor, the 16 items of emotional intelligence were \n#|    loaded onto a second factor, and the 20 items of \n#|    cultural intelligence were loaded onto a third factor.\"\n#|   - \"^\\\\*\\\\*\\\\*^*p* &lt; .001.\"\ntibble(\n  Model = c(\n    \"A: One-intelligence model^&thinsp;a^\",\n    \"B: Two-intelligences mode^&thinsp;b^\",\n    \"C: Three-intelligences model^&thinsp;c^\"),\n  Chi2 = c(10994.664, 10091.236, 8640.066),\n  df = c(1539L, 1538L, 1536L),\n  NFI = c(0.296, 0.354, 0.447),\n  CFI = c(0.326, 0.39, 0.494),\n  RMSEA = c(0.115, 0.109, 0.1),\n  p = .0001\n) |&gt; \n  add_star_column(Chi2, merge = TRUE) |&gt; \n  select(-p) |&gt; \n  apa_flextable(column_formats = column_formats(\n    accuracy = 0.001),\n    layout = \"fixed\") |&gt;\n  width(width = c(2.15, 1.15, rep(3.2 / 4, 4)))\n```\n\n\n\n\nTable 1\n\n\nResults of Confirmatory Factor Analysis for the Relationships Among Three Types of Intelligence\n\n\n\n\nModelχ2dfNFICFIRMSEAA: One-intelligence model a   10994.664***1,539.296.326.115B: Two-intelligences mode b   10091.236***1,538.354.390.109C: Three-intelligences model c    8640.066***1,536.447.494.100\n\n\n\n\n\n\nNote. Structural equation modeling was used for the analysis. NFI = normed fit index; CFI = comparative fit index; RMSEA = root-mean-square error of approximation.\n\n\na In Model A, all 57 items of social intelligence, emotional intelligence, and cultural intelligence were loaded onto one factor. b In Model B, the 21 items of social intelligence were loaded onto one factor, and the 16 items of emotional intelligence and the 20 items of cultural intelligence were loaded onto another factor. c In Model C, the 21 items of social intelligence were loaded onto one factor, the 16 items of emotional intelligence were loaded onto a second factor, and the 20 items of cultural intelligence were loaded onto a third factor.\n\n\n***p &lt; .001.\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.21 in {R} with Apa7},\n  date = {2025-10-01},\n  url = {https://wjschne.github.io/posts/apatables/apa721.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, October 1). Recreating APA Manual Table 7.21 in\nR with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa721.html"
  },
  {
    "objectID": "posts/apatables/apa719.html",
    "href": "posts/apatables/apa719.html",
    "title": "Recreating APA Manual Table 7.19 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 19 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nPresenting a sequence of regression models\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nlibrary(easystats)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.19\n\n\n\n\n\n\n\nIn the recreated table, the hanging indent for Modified latent change was omitted because the text is shorter than Simplex contemporaneous, which does not have a hanging indent.\n\n```{r}\n#| label: tbl-719\n#| tbl-cap: \"Comparison of Fit Indices in Models Fitted to Simulated Data Across Longitudinal Mediation Model Types\"\n#| apa-note: \"AIC and BIC differences are relative to the simplex \n#|   lagged model. RMSEA&nbsp;=&nbsp;root-mean-square error of \n#|   approximation; CI&nbsp;=&nbsp;confidence interval; \n#|   AIC&nbsp;=&nbsp;Akaike information criterion; \n#|   BIC&nbsp;=&nbsp;Bayesian information criterion.\"\n\n# Make data\nd &lt;- tibble(\n             Model = c(\"Simplex lagged\",\n                       \"Simplex contemporaneous\",\n                       \"Latent growth\",\n                       \"Modified latent change\"),\n  `&chi;^2^_Value` = c(63.3, 58, 65, 26.2),\n     `&chi;^2^_df` = c(28L, 29L, 33L, 33L),\n      `&chi;^2^_p` = c(1e-04, 0.001, 1e-04, 0.79),\n       RMSEA_Value = c(0.044, 0.04, 0.039, 0),\n    `RMSEA_95% CI` = c(\"[.030, .059]\", \"[.024, .054]\",\n                       \"[.025, .053]\", \"[.000, .020]\"),\n           RMSEA_p = c(0.72, 0.87, 0.9, 0.999),\n               AIC = c(13479, 13472, 13471, 13432),\n               BIC = c(13658, 13646, 13627, 13588),\n      `deltaAIC` = c(NA, -7L, -8L, -47L),\n      `deltaBIC` = c(NA, -12L, -31L, -70L)\n) \n\n# Format data\nd_formatted &lt;- d |&gt; \n  mutate(RMSEA_Value = align_chr(\n    RMSEA_Value, \n    accuracy = .001, \n    trim_leading_zeros = TRUE))\n\n# Make table\nd_formatted |&gt; \n  apa_flextable(font_size = 11) |&gt; \n  width(width = c(1.80, .45, .35, .45, .05, .40, \n                   .85, .35, .50, .50, .40, .40)) \n```\n\n\n\n\nTable 1\n\n\nComparison of Fit Indices in Models Fitted to Simulated Data Across Longitudinal Mediation Model Types\n\n\n\n\nModelχ2RMSEAAICBICΔAICΔBICValuedfpValue95% CIpSimplex lagged63.3028&lt;.001.044[.030, .059] .7213,47913,658Simplex contemporaneous58.0029 .001.040[.024, .054] .8713,47213,646 −7−12Latent growth65.0033&lt;.001.039[.025, .053] .9013,47113,627 −8−31Modified latent change26.2033 .79 .000[.000, .020]&gt;.9913,43213,588−47−70\n\n\n\n\n\n\nNote. AIC and BIC differences are relative to the simplex lagged model. RMSEA = root-mean-square error of approximation; CI = confidence interval; AIC = Akaike information criterion; BIC = Bayesian information criterion.\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.19 in {R} with Apa7},\n  date = {2025-09-29},\n  url = {https://wjschne.github.io/posts/apatables/apa719.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 29). Recreating APA Manual Table 7.19\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa719.html"
  },
  {
    "objectID": "posts/apatables/apa717.html",
    "href": "posts/apatables/apa717.html",
    "title": "Recreating APA Manual Table 7.17 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 17 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nConfidence intervals in two columns\nalign_chr with drop0trailing = TRUE for intentionally inconsistent rounding\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.17\n\n\n\n\n\n\n\nNote that the Estimate column needed inconsistent rounding. Using align_chr with drop0trailing = TRUE and accuracy = .0001, only cells with values in the fourth decimal point will be rounded to .0001.\n\n```{r}\n#| label: tbl-717\n#| tbl-cap: \"Moderator Analysis: Types of Measurement and Study Year\"\n#| apa-note: \n#|   - \"Number of studies&nbsp;=&nbsp;120, number of\n#|     effects&nbsp;=&nbsp;782, total \n#|     *N*&nbsp;=&nbsp;52,578. CI&nbsp;=&nbsp;confidence \n#|     interval; *LL*&nbsp;=&nbsp;lower limit; \n#|     *UL*&nbsp;=&nbsp;upper limit.\"\n#|   - \"^a&nbsp;^0&nbsp;=&nbsp;self-report, \n#|     1&nbsp;=&nbsp;test. ^b&nbsp;^ 0&nbsp;= test, \n#|     1&nbsp;=&nbsp;grade point average. ^c&nbsp;^Study \n#|     year was grand centered. ^d&nbsp;^0&nbsp;=&nbsp;other, \n#|     1&nbsp;=&nbsp;yes. ^e&nbsp;^0&nbsp;= no, \n#|     1&nbsp;=&nbsp;yes.\"\n\n# Make data\nd &lt;- tibble(\n  effect_type = c(\n    \"Fixed effects\",\n    \"Fixed effects\",\n    \"Fixed effects\",\n    \"Fixed effects\",\n    \"Fixed effects\",\n    \"Fixed effects\",\n    \"Random effects\",\n    \"Random effects\"\n  ),\n  Effect = c(\n    \"Intercept\",\n    \"Creativity measurement^&thinsp;a^\",\n    \"Academic achievement measurement^&thinsp;b^\",\n    \"Study year^&thinsp;c^\",\n    \"Goal^&thinsp;d^\",\n    \"Published^&thinsp;e^\",\n    \"Within-study variance\",\n    \"Between-study variance\"\n  ),\n  Estimate = c(0.119, 0.097, -0.039, 2e-04, \n               -0.003, 0.054, 0.009, 0.018),\n  SE = c(0.04, 0.028, 0.018, 0.001, \n         0.029, 0.03, 0.001, 0.003),\n  `95% CI_*LL*` = c(0.041, 0.042, -0.074, -0.001, \n                    -0.06, -0.005, 0.008, 0.012),\n  `95% CI_*UL*` = c(0.198, 0.153, -0.004, 0.002, \n                    0.054, 0.114, 0.011, 0.023),\n  p = c(0.003, 0.001, 0.03, 0.76, \n        0.91, 0.07, 9e-04, 9e-04)\n) \n\n# Format data\nd_formatted &lt;- d |&gt; \n  mutate(Estimate = align_chr(\n    Estimate, \n    accuracy = .0001, \n    drop0trailing = TRUE, \n    trim_leading_zeros = TRUE)) |&gt; \n  mutate(across(c(Estimate:`95% CI_*UL*`), \\(x) {\n    align_chr(x, \n              accuracy = .001, \n              trim_leading_zeros = TRUE)\n  })) |&gt;\n  mutate(Effect = hanging_indent(Effect, width = 35)) \n\n# Make table\nd_formatted |&gt; \n  apa_flextable(row_title_column = effect_type) |&gt; \n  align(j = 1) |&gt;\n  padding(j = 1, \n          padding.left = c(0, \n                           15, \n                           rep(30, 5), \n                           0, \n                           rep(15, 2))) |&gt; \n  width(width = c(2.5, rep(.8, 5)))\n```\n\n\n\n\nTable 1\n\n\nModerator Analysis: Types of Measurement and Study Year\n\n\n\n\nEffectEstimateSE95% CIpLLULFixed effectsIntercept .119 .040 .041 .198 .003Creativity measurement a .097 .028 .042 .153 .001Academic achievement    measurement b−.039 .018−.074−.004 .03 Study year c .0002.001−.001 .002 .76 Goal d−.003 .029−.060 .054 .91 Published e .054 .030−.005 .114 .07 Random effectsWithin-study variance .009 .001 .008 .011&lt;.001Between-study variance .018 .003 .012 .023&lt;.001\n\n\n\n\n\n\nNote. Number of studies = 120, number of effects = 782, total N = 52,578. CI = confidence interval; LL = lower limit; UL = upper limit.\n\n\na 0 = self-report, 1 = test. b  0 = test, 1 = grade point average. c Study year was grand centered. d 0 = other, 1 = yes. e 0 = no, 1 = yes.\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.17 in {R} with Apa7},\n  date = {2025-09-27},\n  url = {https://wjschne.github.io/posts/apatables/apa717.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 27). Recreating APA Manual Table 7.17\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa717.html"
  },
  {
    "objectID": "posts/apatables/apa715.html",
    "href": "posts/apatables/apa715.html",
    "title": "Recreating APA Manual Table 7.15 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 15 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nAutomatic formatting of lm (linear model) analyses with apa_performance and apa_compare_performance.\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nlibrary(easystats)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.15\n\n\n\n\n\n\n\nI love the easystats ecosystem. It provides a coherent framework for data-analysts to process and display statistical analyses. It makes so many previously tedious things simple. However, it was not specifically designed for the final display of APA tables. The apa7 package has a few functions and features that help process the output of easystats functions to make them table-ready.\nFirst, lets simulate some data that resembles the data underlying the models in Figure 1, and create regression models for Models 1 and 2.\n\nset.seed(1)\n# sample size\nn &lt;- 88\n\n# simulated data\nd &lt;- tibble(\n  leader_gender = sample(0:1, n, replace = TRUE),\n  leader_sleep_condition = sample(0:1, 88, replace = TRUE),\n  charisma = 2.76 - .09 * leader_gender - \n    .36 * leader_sleep_condition + \n    rnorm(n, sd = 0.5))\n\n# Models 1 and 2\nm1 &lt;- lm(charisma ~ leader_gender, data = d)\nm2 &lt;- lm(charisma ~ leader_gender + leader_sleep_condition, data = d)\n\nThe parameters::parameters function from the easystats ecosystem displays information about regression predictors.\n\nparameters(m2) \n\nParameter              | Coefficient |   SE |         95% CI | t(85) |      p\n-----------------------------------------------------------------------------\n(Intercept)            |        2.79 | 0.08 | [ 2.62,  2.95] | 33.17 | &lt; .001\nleader gender          |       -0.20 | 0.10 | [-0.40,  0.00] | -1.96 | 0.054 \nleader sleep condition |       -0.34 | 0.10 | [-0.55, -0.14] | -3.37 | 0.001 \n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nThat is a really nice display. However, if we convert it directly with flextable, we get something far less polished.\n\nparameters(m2) |&gt; \n  flextable()\n\nParameterCoefficientSECICI_lowCI_hightdf_errorp(Intercept)2.790.080.952.622.9533.17850.00leader_gender-0.200.100.95-0.400.00-1.96850.05leader_sleep_condition-0.340.100.95-0.55-0.14-3.37850.00\n\n\nAs it turns out, the output of the parameters function undergoes a fair bit of processing before it prints. The output is a data.frame with a parameters_model class and many attributes that can be used by the easystats formatting functions. If we want to see the underlying data, we can convert the output to a data.frame or tibble.\n\nparameters(m2) |&gt; \n  as_tibble()\n\n# A tibble: 3 × 9\n  Parameter     Coefficient     SE    CI CI_low  CI_high     t df_error        p\n  &lt;chr&gt;               &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;\n1 (Intercept)         2.79  0.0840  0.95  2.62   2.95    33.2        85 2.05e-50\n2 leader_gender      -0.200 0.102   0.95 -0.402  0.00312 -1.96       85 5.36e- 2\n3 leader_sleep…      -0.344 0.102   0.95 -0.547 -0.141   -3.37       85 1.13e- 3\n\n\nIf we really want just the output of parameters as it displays, we use the insight::format_table function.\n\nparameters(m2, ci = .95) |&gt; \n  insight::format_table() \n\n               Parameter Coefficient   SE         95% CI t(85)      p\n1            (Intercept)        2.79 0.08 [ 2.62,  2.95] 33.17 &lt; .001\n2          leader gender       -0.20 0.10 [-0.40,  0.00] -1.96 0.054 \n3 leader sleep condition       -0.34 0.10 [-0.55, -0.14] -3.37 0.001 \n\n\nThis gets us closer to APA style, but not quite all way there. We would like things to be nearly automatic and yet be able to fine-tune the output. Behold!\n\napa_parameters(m2) \n\n# A tibble: 3 × 6\n  Variable               `*B*` `*SE*` `&beta;` `*t*(85)` `*p*`\n  &lt;chr&gt;                  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;\n1 Constant                2.79 0.08    .00     33.17     &lt;.001\n2 Leader Gender          −0.20 0.10   −.20     −1.96      .05 \n3 Leader Sleep Condition −0.34 0.10   −.34     −3.37      .001\n\n\nThe apa_parameters function is a wrapper around the parameters function with defaults that produce APA style output via markdown syntax. The columns are decimal aligned, statistics are italicized, the intercept is called a “Constant”, and the variable names are printed in title case.\nApplying apa_flextable to the output gives us perfect APA style:\n\napa_parameters(m2) |&gt; \n  apa_flextable() \n\nVariableBSEβt(85)pConstant 2.790.08 .0033.17&lt;.001Leader Gender−0.200.10−.20−1.96 .05 Leader Sleep Condition−0.340.10−.34−3.37 .001\n\n\nThe apa_format_columns function and a few others like apa_parameters, apa_performance, and apa_compare_performance can process the output of the lm function.\nGetting the output of apa_parameters and apa_compare_performance into the same table requires a rather complex process. Eventually I’d like to automate some of these steps.\n\n# pf &lt;- column_formats()\n# pf$SE@header &lt;- \"*SE*\"\n\nm &lt;- list(`Model 1` = m1, \n          `Model 2` = m2) \n\nd_parameters &lt;- apa_parameters(\n  m, \n  predictor_parameters = c(\n    \"Coefficient\", \n    \"Std_Coefficient\", \n    \"SE\"))\n\nd_performance &lt;- apa_performance_comparison(\n  m,\n  metrics = c(\"R2\", \"deltaR2\"),\n  starred = \"deltaR2\") |&gt; \n  pivot_longer(-Model, names_to = \"Variable\") |&gt; \n  mutate(Model = paste0(Model, \"_&beta;\")) |&gt; \n  pivot_wider(names_from = Model)\n\nd_parameters |&gt; \n  pivot_wider_name_first(\n    id_cols = Variable, \n    names_from = Model, \n    values_from = 3:5) |&gt; \n  mutate(Variable = str_to_sentence(Variable)) |&gt;\n  add_row(d_performance) |&gt;\n  mutate(across(-Variable, \\(x) align_chr(\n    x,\n    center = \"\\\\.\", \n    side = \"left\",\n    NA_value = \"\") |&gt; \n      star_balance())) |&gt; \n  apa_flextable(no_format_columns = Variable) |&gt; \n  align(j = 1, align = \"left\") |&gt; \n  flextable::add_footer_lines(values = as_paragraph_md(\"*Note*. *N*&nbsp;=&nbsp;88. We examined the impact of leader sleep condition (control vs. sleep deprived) on ratings of charismatic leadership. In Models 1 and 2, we entered the control variable of leader gender to predict leader charisma.\")) |&gt; \n  footnote(i = 2, \n           j = 1, \n           value = as_paragraph(\"Male = 1, female = 2. \"), \n           ref_symbols = \"a \") |&gt;\n    footnote(i = 3, \n             j = 1, \n             value = as_paragraph(\n               \"Control condition = 0, sleep-deprived condition = 1\"), \n             ref_symbols = \"b \", \n             inline = T, \n             sep = \".\") |&gt; \n  add_footer_lines(\n    values = as_paragraph_md(\"\\\\* *p* &lt; .05. \\\\*\\\\* *p* &lt; .01.\")) |&gt; \n  align(align = \"left\", part = \"footer\")\n\n\n\n\nTable 1\n\n\nRegression Coefficients of Leader Sleep on Charismatic Leadership\n\n\n\n\nVariableModel 1Model 2BβSEBβSEConstant 2.65 .000.08 2.79 .000.08Leader gendera −0.24−.230.11−0.20−.200.10Leader sleep conditionb −0.34−.340.10R2 .06 .17ΔR2   .11**Note. N = 88. We examined the impact of leader sleep condition (control vs. sleep deprived) on ratings of charismatic leadership. In Models 1 and 2, we entered the control variable of leader gender to predict leader charisma.a Male = 1, female = 2. b Control condition = 0, sleep-deprived condition = 1.* p &lt; .05. ** p &lt; .01.\n\n\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.15 in {R} with Apa7},\n  date = {2025-09-25},\n  url = {https://wjschne.github.io/posts/apatables/apa715.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 25). Recreating APA Manual Table 7.15\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa715.html"
  },
  {
    "objectID": "posts/apatables/apa713.html",
    "href": "posts/apatables/apa713.html",
    "title": "Recreating APA Manual Table 7.13 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 13 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nCreate column spanners with column_spanner_label\nUsing add_star_column with merge = TRUE to create a starred column.\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.13\n\n\n\n\n\n\n\nCopying and pasting data into a table is not what happens in data analyses. To make things more realistic, I simulated data based on the means and standard deviations, and ran ANOVAs on the simulated data. I extracted the descriptives and the test statistics and formatted the resulting table.\nI used the add_star_column function to apply stars to the F ratio column from the p column. By default, add_star_column adds a special column to the right of the selected column. The selected column is then right-aligned, and the star column is left-aligned so that the column appears to be merged. Sometimes this looks great, sometimes it does not. In this case, I opted to set merge = TRUE in the add_star_column, which actually merges the selected column and its star column, then applies the star_balance function, which pads the column with blanks spaces on the left so that the columns center properly.\n\nset.seed(6)\n\n# Make data\nd_simulated &lt;- \"\nvariable                Time    Group   M       SD  \nPsychological strain    1.00    SMT     0.24    0.30\nPsychological strain    2.00    SMT     0.16    0.27\nPsychological strain    3.00    SMT     0.16    0.26\nPsychological strain    1.00    Control 0.22    0.29\nPsychological strain    2.00    Control 0.27    0.32\nPsychological strain    3.00    Control 0.26    0.31\nEmotional exhaustion    1.00    SMT     2.82    1.47\nEmotional exhaustion    2.00    SMT     2.55    1.31\nEmotional exhaustion    3.00    SMT     2.36    1.39\nEmotional exhaustion    1.00    Control 2.50    1.25\nEmotional exhaustion    2.00    Control 2.47    1.28\nEmotional exhaustion    3.00    Control 2.43    1.16\nDepersonalization       1.00    SMT     1.20    1.09\nDepersonalization       2.00    SMT     1.13    1.07\nDepersonalization       3.00    SMT     1.00    0.93\nDepersonalization       1.00    Control 1.12    1.05\nDepersonalization       2.00    Control 1.25    1.16\nDepersonalization       3.00    Control 1.24    0.93\" %&gt;%\n    readr::read_tsv() |&gt; \n  suppressMessages() |&gt; \n  mutate(n = ifelse(Group == \"SMT\", 75, 78)) |&gt; \n  mutate(x = pmap(list(mean = M, \n                          sd = SD,\n                          n = n),\n                     rnorm)) |&gt; \n  select(-c(M, SD, n)) |&gt; \n  unnest(x) |&gt; \n  mutate(id = factor(row_number()), .by = c(variable, Time)) |&gt; \n  nest(data = -variable) \n\n# Analyze data\nd_analysis &lt;- d_simulated |&gt; \n  mutate(descriptives = map(data, \\(d) {\n    d |&gt; \n      summarise(M = mean(x),\n                SD = sd(x), \n                .by = c(Group, Time)) |&gt; \n      pivot_wider_name_first(\n        names_from = Group,\n        values_from = c(M, SD)) |&gt; \n      mutate(Time = paste(\"Time\", Time))\n  }),\n         fit = map(data, \n                   \\(d) {\n                     aov(x ~ Group * Time + Error(id), \n                         data = d)\n                     }),\n         eta = map(fit, effectsize::eta_squared) |&gt; \n           map(\\(d) dplyr::select(\n             d, Parameter, Eta2_partial) |&gt; \n                 as_tibble()),\n         mp = map(fit, parameters::model_parameters) |&gt; \n           map(\\(d) dplyr::select(\n             d, Group, Parameter, `F`, df, p) |&gt; \n                 as_tibble() |&gt; \n                 mutate(df2 = sum(\n                   ifelse(Parameter == \"Residuals\", df, 0)), \n                        .by = Group) |&gt; \n                 dplyr::filter(Parameter != \"Residuals\") |&gt; \n                 unite(df, c(df, df2), sep = \", \") |&gt; \n           dplyr::select(-Group)),\n         Ftest = map2(mp, eta, \\(m, e) {\n           left_join(m, e, by = join_by(Parameter)) |&gt;\n             mutate(Parameter = factor(\n               Parameter,\n               levels = c(\"Group\", \"Time\", \"Group:Time\"),\n               labels = c(\"G\", \"T\", \"G \\u00D7 T\"))) |&gt;\n             rename(Effect = Parameter)\n         })\n) \n\n# Format data\nd_formatted &lt;- d_analysis |&gt; \n  dplyr::select(variable, descriptives, Ftest) |&gt; \n  unnest(c(descriptives, Ftest)) |&gt; \n  rename(Variable = Time) |&gt; \n  mutate(`F` = align_chr(`F`)) |&gt; \n  add_star_column(`F`, merge = TRUE) |&gt; \n  rename(`*F* ratio` = `F`) |&gt; \n  select(-p) |&gt; \n  column_spanner_label(label = \"ANOVA\", Effect:Eta2_partial)\n\n\n# Make table\nd_formatted |&gt; \n  apa_flextable(row_title_column = variable, \n                line_spacing = 1.5) \n\n\n\n\nTable 1\n\n\nMeans, Standard Deviations, and Two-Way ANOVA for Study Variables\n\n\n\n\nVariableSMTControlANOVAMSDMSDEffectF ratiodfη2Psychological strainTime 10.230.290.200.29G  8.26**1, 151.05Time 20.150.270.270.31T0.261, 304.00Time 30.140.250.250.32G × T 4.63*1, 304.02Emotional exhaustionTime 12.681.322.621.18G0.001, 151.00Time 22.591.272.401.36T 5.49*1, 304.02Time 32.161.602.391.19G × T0.861, 304.00DepersonalizationTime 11.401.191.020.94G0.031, 151.00Time 20.931.001.191.21T0.141, 304.00Time 31.170.901.331.00G × T 5.13*1, 304.02\n\n\n\n\n\n\nNote. N = 153. ANOVA = analysis of variance; SMT = stress management training group; Control = wait-list control group; G = group; T = time.\n\n\n* p &lt; .05. ** p &lt; .01.\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.13 in {R} with Apa7},\n  date = {2025-09-23},\n  url = {https://wjschne.github.io/posts/apatables/apa713.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 23). Recreating APA Manual Table 7.13\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa713.html"
  },
  {
    "objectID": "posts/apatables/apa711.html",
    "href": "posts/apatables/apa711.html",
    "title": "Recreating APA Manual Table 7.11 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 11 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nThe separate_star_column function to align numbers with significance stars\nThe add_list_column function to make an ordered list.\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.11\n\n\n\n\n\n\n\nThis correlation matrix is different above and below the diagonal. A correctable imperfection of the Figure 1 is that the correlations do not align on their decimals due to the minus signs and stars. This can be corrected by separating the correlations from the significance stars with the separate_star_column function. This function removes any stars from each selected column and places them in an adjacent column. The apa_flextable function (via apa_style) then automatically right aligns the selected columns and left aligns the star columns, and removes any padding between them, giving the illusion that they are in the same column with decimal alignment.\nThe add_list_column function can make ordered lists (numeric, alphabetical, or Roman numeral). By default, the list column is a separate column that is specially formatted by apa_flextable or apa_style. Setting merge = TRUE joins the list and text columns into a single column.\n\n# Make data\nd &lt;- \"\nVariable                1       2       3       4\nGrade point average     —       .49**   .35**   −.05\nAcademic self-concept   .35**   —       .36**   .02\nTeacher trust           .49**   .35**   —       .20**\nAge                     .10     .21*    −.15    —\" |&gt;\n  readr::read_tsv() |&gt;\n  suppressMessages()\n\n# Format data\nd_formatted &lt;- d |&gt; \n  separate_star_column(-Variable) |&gt;\n  add_list_column(Variable, merge = TRUE)\n\n\n# Make Table\nd_formatted |&gt;\n  apa_flextable(no_format_columns = Variable) |&gt;\n  width(width = c(2.1, rep(c(.7, .4), 4)))\n\n\n\n\nTable 1\n\n\nIntercorrelations for Study Variables Disaggregated by Gender\n\n\n\n\nVariable12341. Grade point average—.49**.35**−.052. Academic self-concept.35**—.36**.023. Teacher trust.49**.35**—.20**4. Age.10.21*−.15—\n\n\n\n\n\n\nNote. The results for the female sample (n = 199) are shown above the diagonal. The results for the male sample (n = 120) are shown below the diagonal.\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.11 in {R} with Apa7},\n  date = {2025-09-21},\n  url = {https://wjschne.github.io/posts/apatables/apa711.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 21). Recreating APA Manual Table 7.11\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa711.html"
  },
  {
    "objectID": "posts/apatables/apa709.html",
    "href": "posts/apatables/apa709.html",
    "title": "Recreating APA Manual Table 7.9 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 9 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nUse custom column_formats\nUse footnote\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.9\n\n\n\n\n\n\n\nSetting up the data was harder than creating the table. Once the data were ready, I needed to set the rounding of the variables and alter the column header of the F column.\nThe column_formats function creates a modifiable copy of the default column formatters. After creating a column_formats object, I set the rounding accuracy of column p to .001 using [purrr::partial], which creates a copy of a function with new defaults.\nI also set the header of the F column to *F* ratio.\n\n```{r}\n#| label: tbl-79\n#| tbl-cap: \"Analyses for the Interaction of Professor \n#|           Type and Timing of Response on Perceptions \n#|           of Professor Traits\"\n#| apa-note: \"Means with different subscripts differ at \n#|            the *p*&nbsp;=&nbsp;.05 level by Duncan’s \n#|            new multiple range test.\"\n\n# Descriptives\nd_mean &lt;- \"\nProfessor trait     Semester    Type        Mean    Subscript\nDedicated           End         Typical     4.706   b        \nEasy to understand  End         Typical     3.059   c        \nFair                End         Typical     4.000   b        \nManipulative        End         Typical     1.471   a        \nInsensitive         End         Typical     2.059   b        \nDedicated           End         Effective   4.789   b        \nEasy to understand  End         Effective   4.895   a        \nFair                End         Effective   4.263   b        \nManipulative        End         Effective   1.632   a        \nInsensitive         End         Effective   1.526   c        \nDedicated           Start       Typical     4.154   c        \nEasy to understand  Start       Typical     3.231   c        \nFair                Start       Typical     3.731   c        \nManipulative        Start       Typical     1.731   a        \nInsensitive         Start       Typical     2.538   a        \nDedicated           Start       Effective   5.000   a        \nEasy to understand  Start       Effective   4.429   b        \nFair                Start       Effective   4.667   a        \nManipulative        Start       Effective   1.238   a        \nInsensitive         Start       Effective   1.143   c\" |&gt;\n    readr::read_tsv() |&gt; \n  suppressMessages()\n\n# Statistics\nd_test &lt;- \"\nProfessor trait     F       p       eta2\nDedicated           19.26   0.001   0.15\nEasy to understand  5.01    0.028   0.03\nFair                5.75    0.019   0.06\nManipulative        3.92    0.051   0.05\nInsensitive         8.12    0.006   0.06\" |&gt;\n  readr::read_tsv() |&gt; \n  suppressMessages() \n\n# Format data\nd_formatted &lt;- d_mean |&gt;\n  mutate(Semester = paste0(\n    Semester, \n    \" of semester\\\\\\nprofessor type\")) |&gt;\n  mutate(Mean = scales::number(Mean, accuracy = .001),\n         Subscript = paste0(\"~\", Subscript, \"~\")) |&gt;\n  unite(Mean, c(Mean, Subscript), sep = \"\") |&gt;\n  unite(Semester, c(Semester, Type)) |&gt;\n  pivot_wider(names_from = Semester, \n              values_from = Mean) |&gt;\n  left_join(d_test, by = join_by(`Professor trait`)) \n\n\n# custom formatter\nmy_formatter &lt;-  column_formats()\n\n## change p column's accuracy to 0.001\nmy_formatter$p@formatter &lt;- my_formatter$p@formatter |&gt;\n  purrr::partial(accuracy = .001)\n\n## set F column header to \"*F* ratio\"\nmy_formatter$`F`@header &lt;- \"*F* ratio\"\n\n# Make table\napa_flextable(d_formatted, column_formats = my_formatter) \n```\n\n\n\n\nTable 1\n\n\nAnalyses for the Interaction of Professor Type and Timing of Response on Perceptions of Professor Traits\n\n\n\n\nProfessor traitEnd of semesterprofessor typeStart of semesterprofessor typeF ratiopη2TypicalEffectiveTypicalEffectiveDedicated4.706b4.789b4.154c5.000a19.26.001.15Easy to understand3.059c4.895a3.231c4.429b 5.01.028.03Fair4.000b4.263b3.731c4.667a 5.75.019.06Manipulative1.471a1.632a1.731a1.238a 3.92.051.05Insensitive2.059b1.526c2.538a1.143c 8.12.006.06\n\n\n\n\n\n\nNote. Means with different subscripts differ at the p = .05 level by Duncan’s new multiple range test.\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.9 in {R} with Apa7},\n  date = {2025-09-19},\n  url = {https://wjschne.github.io/posts/apatables/apa709.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 19). Recreating APA Manual Table 7.9\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa709.html"
  },
  {
    "objectID": "posts/apatables/apa707.html",
    "href": "posts/apatables/apa707.html",
    "title": "Recreating APA Manual Table 7.7 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 7 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nConvert p-values to stars and append them to chi-square statistic with add_star_column\nAdd p-value note with flextable::add_footer_lines\nHanging indent with hanging_indent\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.7\n\n\n\n\n\n\n\nI could have just copied the table directly, but I decided to illustrate how one might calculate percentages and chi-square tests results from the sample sizes.\n\n```{r}\n#| label: tbl-77\n#| tbl-cap: \"Frequencies and Chi-Square Results for \n#|           Belief Perseverance in Attitudes Toward \n#|           Celebrities (*N*&nbsp;=&nbsp;201)\"\n# Make data\nd &lt;- \"\nSource                          Do not believe  Unsure  Believe   \nMedia reports                   17              140     44\nFamily reports                  47              106     48\nFriends' reports                42              112     47\nCaught by media                 19              82      100\nCelebrity display of behavior   12              61      128\" |&gt;\n  readr::read_tsv(show_col_types = FALSE)\n\n# Format data\nd_formatted &lt;- d |&gt;\n  pivot_longer(-c(Source), values_to = \"n\") |&gt;\n  mutate(name = fct_inorder(name)) |&gt;\n  mutate(`%` = scales::number(100 * `n` / sum(`n`), accuracy = .01),\n         .by = Source) |&gt;\n  nest(data = -Source, .by = Source) |&gt;\n  mutate(\n    fit = map(data, \\(dd) {\n      chisq.test(dd$n)\n    }),\n    chisq = map_dbl(fit, \"statistic\"),\n    p = map_dbl(fit, \"p.value\")\n  ) |&gt;\n  select(-fit) |&gt;\n  unnest(data) |&gt;\n  pivot_wider_name_first(values_from = c(`n`, `%`)) |&gt;\n  relocate(chisq, .after = dplyr::last_col()) |&gt;\n  mutate(across(-Source, align_chr)) |&gt;\n  mutate(Source = hanging_indent(Source, width = 18)) |&gt;\n  add_star_column(chisq, alpha = .001, merge = TRUE) |&gt;\n  select(-p) |&gt; \n  rename(`&chi;^2^(2)` = chisq)\n\n# Make table\nd_formatted |&gt; \n  apa_flextable(table_width = 1, layout = \"fixed\") |&gt;\n  width(width = c(1.6, rep(c(.65,.65, .05), 3)[-9], .9)) |&gt; \n  add_footer_lines(values = as_paragraph_md(\"^\\\\*^*p* &lt; .001\"))\n```\n\n\n\n\nTable 1\n\n\nFrequencies and Chi-Square Results for Belief Perseverance in Attitudes Toward Celebrities (N = 201)\n\n\n\n\nSourceDo not believeUnsureBelieveχ2(2)n%n%n%Media reports17 8.4614069.65 4421.89 124.75*Family reports4723.3810652.74 4823.88  34.06*Friends’ reports4220.9011255.72 4723.38  45.52*Caught by media19 9.45 8240.8010049.75  54.00*Celebrity display    of behavior12 5.97 6130.3512863.68 101.22**p &lt; .001\n\n\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.7 in {R} with Apa7},\n  date = {2025-09-17},\n  url = {https://wjschne.github.io/posts/apatables/apa707.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 17). Recreating APA Manual Table 7.7\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa707.html"
  },
  {
    "objectID": "posts/apatables/apa705.html",
    "href": "posts/apatables/apa705.html",
    "title": "Recreating APA Manual Table 7.5 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 5 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nUsing width to make column widths consistent.\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.5\n\n\n\n\n\n\n\nReproducing APA Manual Table 7.5 shown in Figure 1 is mostly straightforward. The default column widths (provided by flextable’s dim_pretty) look good, but providing custom widths can give the table a sharper, more consistent appearance.\n\n```{r}\n#| label: tbl-75\n#| tbl-cap: Summary of Designs of Experiments 1–4\n#| apa-note: \"A, X, Y, and B&nbsp;=&nbsp;tone, clicker, \n#|           steady light, and flashing light, respectively \n#|           (counterbalanced), with the constraint that \n#|           A and B are drawn from one modality and X and \n#|           Y from another (counterbalanced); plus\n#|           sign&nbsp;(+)&nbsp;=&nbsp;shock to floor of rat\n#|           chamber; minus sign&nbsp;(–)&nbsp;=&nbsp;absence\n#|           of shock.\"\n# Make data\nd &lt;- tibble(\n     Experiment = paste(\"Experiment\", rep(1:4, c(2,3,2,3))),\n          Group = c(\"Compound\",\"Compound novel\",\n                    \"Compound A\",\"Compound X\",\"Compound novel\",\n                    \"Compound\",\"Element\",\n                    \"Control\",\"Element A\",\"Element Y\"),\n     `Preexposure 1` = c(rep(\"A−X−Y−\", 7), \n                         NA, \n                         rep(\"A−X−Y−\", 2)), \n     `Preexposure 2` = c(rep(\"AX−BY−\", 5), \n                         rep(\"AX−Y−\", 2), \n                         NA, \n                         rep(\"A−X−Y−\", 2)), \n   Conditioning = c(\"X+\",\"Y+\",\n                    \"A+\",\"X+\",\"Y+\",\n                    \"X+\",\"Y+\",\n                    \"A+/Y+\",\"A+\",\"Y+\"),\n           Test = c(\"X−\",\"Y−\",\n                    \"A−\",\"X−\",\"Y−\",\n                    \"X−\",\"Y−\",\n                    \"A−Y−\",\"A−\",\"Y−\")\n) \n\n# Make table\nd |&gt; \n  apa_flextable(row_title_column = Experiment, \n                line_spacing = 1.5) |&gt; \n  align(j = 1, align = \"left\") |&gt; \n  width(width = c(1.7, rep(1.2, 4)))\n```\n\n\n\n\nTable 1\n\n\nSummary of Designs of Experiments 1–4\n\n\n\n\nGroupPreexposure 1Preexposure 2ConditioningTestExperiment 1CompoundA−X−Y−AX−BY−X+X−Compound novelA−X−Y−AX−BY−Y+Y−Experiment 2Compound AA−X−Y−AX−BY−A+A−Compound XA−X−Y−AX−BY−X+X−Compound novelA−X−Y−AX−BY−Y+Y−Experiment 3CompoundA−X−Y−AX−Y−X+X−ElementA−X−Y−AX−Y−Y+Y−Experiment 4ControlA+/Y+A−Y−Element AA−X−Y−A−X−Y−A+A−Element YA−X−Y−A−X−Y−Y+Y−\n\n\n\n\n\n\nNote. A, X, Y, and B = tone, clicker, steady light, and flashing light, respectively (counterbalanced), with the constraint that A and B are drawn from one modality and X and Y from another (counterbalanced); plus sign (+) = shock to floor of rat chamber; minus sign (–) = absence of shock.\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.5 in {R} with Apa7},\n  date = {2025-09-15},\n  url = {https://wjschne.github.io/posts/apatables/apa705.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 15). Recreating APA Manual Table 7.5\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa705.html"
  },
  {
    "objectID": "posts/apatables/apa703.html",
    "href": "posts/apatables/apa703.html",
    "title": "Recreating APA Manual Table 7.3 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 3 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nTrim leading zeros, decimail alignment, and align on en-dash (–) with align_chr\nInternal lines with surround\nIndentation with padding\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.3\n\n\n\n\n\n\n\nAPA Manual Table 7.3 in Figure 1 was easier to reproduce than the first two tables in this series. Because only simple indentation was required, it was easy to indent subscale rows in Table 1 with the padding function.\nThe surround function provides internal lines. I make the internal lines a lighter gray than the header and bottom lines.\nWhereas the mean and standard deviation columns needed decimal alignment (provided automatically with apa_flextable), the Range column looked better if aligned on the en-dash character (–). The align_chr function makes use of the signs function, which not only gives negative numbers a true minus sign, but also can trim leading zeroes for probabilities, correlations, and reliability coefficients, which happens automatically for the cronbach column.\n\n```{r}\n#| label: tbl-73\n#| tbl-cap: Psychometric Properties for DLOPFQ Scales and Subscales\n#| apa-note: \"The *Diagnostic and Statistical Manual of Mental Disorders* \n#|           (5th ed.) Levels of Personality Functioning Questionnaire \n#|           (DLOPFQ) we developed had four scales (Identity, Self-\n#|           Directedness, Empathy, and Intimacy), each with subscales \n#|           for the work and social domains.\"\n\n# Make data\nd &lt;- tibble::tribble(\n                           ~Scale,    ~M,  ~SD,   ~Range, ~`cronbach`,\n           \"Identity total score\",  86.6,   28, \"28–155\",                0.94,\n                  \"Work Identity\",  41.6, 13.3,  \"16–76\",                0.88,\n                \"Social Identity\",    45, 15.7,  \"14–84\",                0.91,\n  \"Self-Directedness total score\",  91.2, 26.5, \"34–151\",                0.92,\n         \"Work Self-Directedness\",  44.9, 13.5,  \"16–76\",                0.85,\n       \"Social Self-Directedness\",  46.3, 14.3,  \"17–80\",                0.86,\n            \"Empathy total score\", 101.8, 15.8, \"48–139\",                0.84,\n                   \"Work Empathy\",  49.9,  8.2,  \"20–72\",                0.72,\n                 \"Social Empathy\",  51.9,  8.6,  \"28–76\",                0.77,\n           \"Intimacy total score\", 122.9, 28.6, \"56–189\",                0.91,\n                  \"Work Intimacy\",  61.7, 14.3,  \"28–94\",                0.82,\n                \"Social Intimacy\",  61.2, 15.4,  \"24–96\",                0.86\n  ) \n\n# Format data\nd_formatted &lt;- d |&gt; \n  mutate(Range = align_chr(Range, center = \"–\")) \n\n# Make table\nd_formatted |&gt;\n  apa_flextable(layout = \"fixed\") |&gt; \n  padding(j = 1, \n          i = ~!stringr::str_ends(Scale, \"total score\"), \n          padding.left = 16) |&gt; \n  width(width = 1.1) |&gt; \n  width(j = 1, 2.1) |&gt; \n  surround(i = c(4,7,10), \n           border.top = fp_border_default(color = \"gray60\"))\n```\n\n\n\n\nTable 1\n\n\nPsychometric Properties for DLOPFQ Scales and Subscales\n\n\n\n\nScaleMSDRangeCronbach’s αIdentity total score 86.6028.0028–155.94Work Identity 41.6013.3016–76 .88Social Identity 45.0015.7014–84 .91Self-Directedness total score 91.2026.5034–151.92Work Self-Directedness 44.9013.5016–76 .85Social Self-Directedness 46.3014.3017–80 .86Empathy total score101.8015.8048–139.84Work Empathy 49.90 8.2020–72 .72Social Empathy 51.90 8.6028–76 .77Intimacy total score122.9028.6056–189.91Work Intimacy 61.7014.3028–94 .82Social Intimacy 61.2015.4024–96 .86\n\n\n\n\n\n\nNote. The Diagnostic and Statistical Manual of Mental Disorders (5th ed.) Levels of Personality Functioning Questionnaire (DLOPFQ) we developed had four scales (Identity, Self- Directedness, Empathy, and Intimacy), each with subscales for the work and social domains.\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.3 in {R} with Apa7},\n  date = {2025-09-13},\n  url = {https://wjschne.github.io/posts/apatables/apa703.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 13). Recreating APA Manual Table 7.3\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa703.html"
  },
  {
    "objectID": "posts/apatables/apa701.html",
    "href": "posts/apatables/apa701.html",
    "title": "Recreating APA Manual Table 7.1 in R with apa7",
    "section": "",
    "text": "In this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nRow titles and styling with apa_flextable\nConditional placement of cell borders with flextable::surround\nGeneral notes with flextable::add_footer_lines or apa-note chunk option\nFootnotes with flextable::footnote\n\n\n\nWhen I write a research manuscript in APA style, I have twin goals that are sometimes at odds: 1) I want the document to be fully reproducible, including the tables. 2) I want the formatting to be not only correct but aesthetically pleasing.\nThe final frontier of reproducibility in APA style, for me, has been getting the tables right. At long last, I am getting close to where I want to be. My process usually goes like this:\n\nWrangle data with tidyverse functions.\nPerform analyses, and format analysis tables with easystats functions.\nPerform table-specific data shaping with apa7 functions.\nCreate a table with apa7::apa_flextable.\nAdd specific formatting and finishing touches with flextable functions."
  },
  {
    "objectID": "posts/apatables/apa701.html#tables-notes",
    "href": "posts/apatables/apa701.html#tables-notes",
    "title": "Recreating APA Manual Table 7.1 in R with apa7",
    "section": "Tables Notes",
    "text": "Tables Notes\nAPA style distinguishes between three types of table notes, each of which need to be in separate paragraphs:\n\nGeneral Note. Contains definitions, abbreviations, and other information needed to understand the table, as well as copyright information.\nSpecific Note. These are notes about specific cells in the table, often in form of footnotes with lowercase letters as markers. Each footnote is in the same paragraph. The APA Manual recommends making the the footnote marker in superscript. The space that separates the marker from the the explanatory text should also be in superscript, hence the non-breaking space ^\\u00A0^ at the beginning of th footnote text.\nProbability Note. This note explains the meanings of probability markers (e.g., * p &lt; .05; ** p &lt; .001; *** p &lt; .001)."
  },
  {
    "objectID": "posts/apatables/apa701.html#adding-table-notes-with-flextable",
    "href": "posts/apatables/apa701.html#adding-table-notes-with-flextable",
    "title": "Recreating APA Manual Table 7.1 in R with apa7",
    "section": "Adding Table Notes with flextable",
    "text": "Adding Table Notes with flextable\nAdding footnotes with flextable functions might look like a lot of code in Table 4, but it is mostly straightforward.\nThe general note can be created with the footer_lines function. The note itself can be written inside flextable::as_paragraph or ftExtra::as_paragraph_md. The ftExtra::as_paragraph_md function can make moderately complex formatting a little easier because it uses markdown syntax. If precise fine-tuning is needed, the flextable package has many formatting functions that can be used inside flextable::as_paragraph.\nGetting everything to display in perfect APA style is not always possible with the footnote function. The footnotes are supposed to go in a single paragraph that is separate from the general note. We can make footnotes appear in their own paragraph by setting the inline argument in footnote to TRUE, but this makes the general note also in the same paragraph.\nI did not use the footnote function to create the probability note because I did not want the asterisk to be in superscript. Instead the asterisk is placed in the table with the append_chunk function, and the probability note is added separately with the footer_lines function.\n\n```{r}\n#| label: tbl-71b\n#| tbl-cap: Numbers of Children With and Without Proof of Parental Citizenship\n\nd_citizenship |&gt;\n  apa_flextable(row_title_column = Wave, \n                row_title_prefix = \"Wave\", \n                row_title_align = \"center\",\n                line_spacing = 1.5) |&gt;\n  surround(i = ~ Grade == \"Total\",\n                      border.top = list(color = \"gray\", \n                                        width = 1, \n                                        style = \"solid\")) |&gt; \n  add_footer_lines(\n  as_paragraph_md(\n    \"*Note*. This table demonstrates the elements of a\n    prototypical table. A *general note* to a table appears\n    first and contains information needed to understand the\n    table, including definitions of abbreviations (see Sections\n    7.14–7.15) and the copyright attribution for a reprinted or\n    adapted table (see Section 7.7).\"\n  )\n) |&gt;\nfootnote(\n  i = ~ Wave == 1 &\n    Grade == 3,\n  j = \"Girls_With\",\n  value = as_paragraph_md(\n    \"^\\u00A0^A *specific note* appears in a separate \n      paragraph below the general note. \"),\n  ref_symbols = \"a\"\n) |&gt;\n  footnote(\n  i = ~ Wave == 1 &\n    Grade == 3,\n  j = \"Girls_Without\",\n  value = as_paragraph_md(\n    \"^\\u00A0^Subsequent specific notes follow in the \n    same paragraph (see Section 7.14).\"),\n  ref_symbols = \"b\", \n  inline = TRUE, \n  sep = \"\"\n) |&gt;\nadd_footer_lines(\n  values = as_paragraph_md(\n    \"\\\\* A *probability note* (for *p*-values) appears as a \n    separate paragraph below any specific notes; subsequent \n    probability notes follow in the same paragraph (see \n    Section 7.14).\"\n    )) |&gt;\n  append_chunks(\n    i = ~ Wave == 2 & Grade == \"Total\",\n    j = \"Boys_With\", \n    as_chunk(\"*\")) |&gt; \n  line_spacing(space = 1.5, part = \"footer\")\n```\n\n\n\n\nTable 4\n\n\nNumbers of Children With and Without Proof of Parental Citizenship\n\n\n\n\nGradeGirlsBoysWithWithoutWithWithoutWave 13111a240b28123242972512902645301260306221Total709751877717Wave 2320118921019942141942362105221216239213Total636599685*622Note. This table demonstrates the elements of a prototypical table. A general note to a table appears first and contains information needed to understand the table, including definitions of abbreviations (see Sections 7.14–7.15) and the copyright attribution for a reprinted or adapted table (see Section 7.7).a A specific note appears in a separate paragraph below the general note.b Subsequent specific notes follow in the same paragraph (see Section 7.14).* A probability note (for p-values) appears as a separate paragraph below any specific notes; subsequent probability notes follow in the same paragraph (see Section 7.14)."
  },
  {
    "objectID": "posts/apatables/apa701.html#adding-table-notes-with-apaquartos-apa-note-chunk-option",
    "href": "posts/apatables/apa701.html#adding-table-notes-with-apaquartos-apa-note-chunk-option",
    "title": "Recreating APA Manual Table 7.1 in R with apa7",
    "section": "Adding Table Notes with apaquarto’s apa-note Chunk Option",
    "text": "Adding Table Notes with apaquarto’s apa-note Chunk Option\nIn some formats (e.g., .pdf), long table notes can mess with the column widths and make the table unreadable.\nIf you are rendering with the apaquarto extension, you can append table notes with the apa-note chunk option. Recent versions of apaquarto can separate apa-note text into multiple paragraphs as shown in the chunk below.\nAlso, it bothers me that the footnote symbols mess up the number alignment. I decided to add them in myself. In the same columns that have footnotes, I padded the numbers that do not have footnotes with the numbsp; character (Figure Space) to maintain the alignment.\nAs a final polishing move, I rejiggered the column widths to make everything center correctly. Table 5 is as good as I can make a table in APA Style.\n\n```{r}\n#| label: tbl-71c\n#| image-preview: true\n#| tbl-cap: Numbers of Children With and Without Proof of Parental Citizenship\n#| apa-note: \n#|   - This table demonstrates the elements of a prototypical \n#|     table. A *general note* to a table appears first and \n#|     contains information needed to understand the table, \n#|     including definitions of abbreviations (see Sections \n#|     7.14--7.15) and the copyright attribution for a \n#|     reprinted or adapted table (see Section 7.7).\n#|   - ^a^ Subsequent specific notes follow in the same \n#|     paragraph (see Section 7.14). ^b^ Subsequent specific \n#|     notes follow in the same paragraph (see Section 7.14).\n#|   - \\* A *probability note* (for *p* values) appears as a \n#|     separate paragraph below any specific notes; subsequent \n#|     probability notes follow in the same paragraph (see \n#|     Section 7.14).\n\nd_citizenship |&gt;\n  mutate(Girls_With = paste0(\n    \"^&numsp;^\",\n    Girls_With, \n    ifelse(Wave == 1 & Grade == 3, \n           \"^a^\", \n           \"^&numsp;^\")),\n    Girls_Without = paste0(\n      \"^&numsp;^\",\n    Girls_Without, \n    ifelse(Wave == 1 & Grade == 3, \n           \"^b^\", \n           \"^&numsp;^\")),\n    Boys_With = paste0(\n      \"^&numsp;^\",\n    Boys_With, \n    ifelse(Wave == 2 & Grade == \"Total\", \n           \"^\\\\*^\", \n           \"^&numsp;^\"))\n    ) |&gt; \n  apa_flextable(row_title_column = Wave, \n                row_title_prefix = \"Wave\", \n                row_title_align = \"center\",\n                table_width = 1, \n                line_spacing = 1.5, \n                layout = \"fixed\") |&gt; \n    surround(i = ~ Grade == \"Total\",\n                      border.top = list(color = \"gray\", \n                                        width = 1, \n                                        style = \"solid\")) |&gt; \n  width(width = c(1.34, 1.28, 1.28, .05, 1.28, 1.28)) \n```\n\n\n\n\nTable 5\n\n\nNumbers of Children With and Without Proof of Parental Citizenship\n\n\n\n\nGradeGirlsBoysWithWithoutWithWithoutWave 13 111a 240b 281 2324 297  251  290 2645 301  260  306 221Total 709  751  877 717Wave 23 201  189  210 1994 214  194  236 2105 221  216  239 213Total 636  599  685*622\n\n\n\n\n\n\nNote. This table demonstrates the elements of a prototypical table. A general note to a table appears first and contains information needed to understand the table, including definitions of abbreviations (see Sections 7.14–7.15) and the copyright attribution for a reprinted or adapted table (see Section 7.7).\n\n\na Subsequent specific notes follow in the same paragraph (see Section 7.14). b Subsequent specific notes follow in the same paragraph (see Section 7.14).\n\n\n* A probability note (for p values) appears as a separate paragraph below any specific notes; subsequent probability notes follow in the same paragraph (see Section 7.14).\n\n\n\nNote that in Figure 1, the Total row is left-aligned in the Grade column. I did not do this because I think it looks bad, and there was no note saying that I had to do it to remain in strict APA style. If you think you need it, you would add this line to your table pipeline:\n\nalign(i = ~ Grade == \"Total\", j = \"Grade\") \n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast"
  },
  {
    "objectID": "posts/2025-11-11-winners-of-posit-s-2025-table-contest/index.html",
    "href": "posts/2025-11-11-winners-of-posit-s-2025-table-contest/index.html",
    "title": "Winners of Posit’s 2025 Table Contest",
    "section": "",
    "text": "Every year Posit holds a contest for the best statistical table. The 2025 contest results have some fantastic tables covering a diverse range of purposes, methods, and aesthetics. The winner was Martin Stavro’s interactive app that displays characteristics of the United Airlines Fleet.\nEight special prize categories were also announced. It was gratifying that my 24-part series, in which I recreate the tables in the APA Publication Manual, won the the prize for Best Tutorial. I have been informed that a small prize is in the mail!\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel and Joel Schneider, W.},\n  title = {Winners of {Posit’s} 2025 {Table} {Contest}},\n  date = {2025-11-11},\n  url = {https://wjschne.github.io/posts/2025-11-11-winners-of-posit-s-2025-table-contest/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J., & Joel Schneider, W. (2025, November 11). Winners\nof Posit’s 2025 Table Contest. Schneirographs. https://wjschne.github.io/posts/2025-11-11-winners-of-posit-s-2025-table-contest/"
  },
  {
    "objectID": "posts/2024-03-15-lua-filter/mylua.html",
    "href": "posts/2024-03-15-lua-filter/mylua.html",
    "title": "My lua filters",
    "section": "",
    "text": "For a long time I thought that someone would write a complete Quarto extension for APA-style manuscripts. It turns out that someone was me.\nMaking apaquarto was more of a challenge than I imagined going in. At first the extension relied a lot of hacks—things I knew were fragile, but I did not know how to do it better. The better way was to use Lua filters. It took time before I wrapped my head around Pandoc Lua filters, but eventually I was able to make them work for me.\nPandoc converts text into an AST (abstract syntax tree). For example, a short paragraph like “I like Quarto.” would be\nA Lua filter can select and transform specific elements For example, to transform all the Str (string) elements to uppercase:\nStr = function(s)\n  return string.upper(s)\nend\nAfter running this filter, the paragraph would be transformed like so:\nEach filter you run will walk through the AST, making any changes to it. Quarto walks through the AST many times (e.g., normalizing metadata, processing figure references).\nThe documentation for Pandoc Lua filters is comprehensive and has instructive examples. I will be adding more examples to this post as I go."
  },
  {
    "objectID": "posts/2024-03-15-lua-filter/mylua.html#replacing-ampersands-in-in-text-citations.",
    "href": "posts/2024-03-15-lua-filter/mylua.html#replacing-ampersands-in-in-text-citations.",
    "title": "My lua filters",
    "section": "Replacing ampersands in in-text citations.",
    "text": "Replacing ampersands in in-text citations.\nWhen there are multiple authors, APA Style requires an “and” in in-text citations, but apa.csl cannot do that and still get parenthetical citations right. Lua to the rescue!"
  },
  {
    "objectID": "posts/2024-03-15-lua-filter/mylua.html#first-try",
    "href": "posts/2024-03-15-lua-filter/mylua.html#first-try",
    "title": "My lua filters",
    "section": "First Try",
    "text": "First Try\nAdapted from Samuel Dodson:\n\nfunction Cite (ct)\n  if ct.citations[1].mode == \"AuthorInText\" then\n    ct.content = ct.content:walk {\n      Str = function(s)\n        if s.text == \"&\" then\n          s.text = \"and\"\n          return s\n        end\n      end\n    }\n    return ct.content\n  end\nend\n\nThis function walks down a document and runs every time a citation is found.\nThe pandoc.Cite type has several fields in it, including citations and content. The citations field is a table that can contain several citations because parenthetical citations can have more than one citation. In-text citations have only one citation. To find the first citation in the citation field, we use square brackets and an index of 1. Thus, ct.citations[1] retrieves the first citation.\nEach citation is a special element component that has several fields:\n\nid has the citation identifier\nmode tells the citation type (NormalCitation, AuthorInText, or SuppressAuthor)\nprefix and suffix contain text before (e.g., e.g.,) and after the citation (e.g., pp. 33-49)\n\nWe only want to change AuthorInText citations. Thus, if ct.citations[1].mode == \"AuthorInText\" then means to only proceed if the citation is an in-text citation.\nThe content field of a pandoc.Cite contains the text that is going to be displayed from the citation (e.g., “Schneider & McGrew (2018)”). It is not actually a long string, but a sequence of “inline” elements like so:\n[ Str \"Schneider\"\n  , Space\n  , Str \"&\"\n  , Space\n  , Str \"McGrew\"\n  , Space\n  , Str \"(2018)\"]\nWe want to “walk” through this sequence and find the Str elements that are ampersands and replace them.\nSo, the following command says, “Walk through the ct.content field, and find each Str element. If the text of the Str element has an ampersand, replace the ampersand with \"and\". Return the Str element.\n\nct.content = ct.content:walk {\n  Str = function(s)\n    if s.text == \"&\" then\n      s.text = \"and\"\n      return s\n    end\n  end\n  }"
  },
  {
    "objectID": "posts/2024-03-15-lua-filter/mylua.html#adding-support-for-languages-beyond-english",
    "href": "posts/2024-03-15-lua-filter/mylua.html#adding-support-for-languages-beyond-english",
    "title": "My lua filters",
    "section": "Adding support for languages beyond English",
    "text": "Adding support for languages beyond English\nI wanted people to use the filter in any language. In their metadata, they need to tell apaquarto what their word for and is. For example, Spanish speakers can substitute y for and like so:\n\nlang: es\nlanguage:\n   citation-last-author-separator: y\n\nTo make this work, the Lua filter needs to be able to access the citation-last-author-separator field in the metadata.\nWe set up a local variable andreplacement with a default value \"and\".\nTo pass metadata into a function, we need to set up two local functions, one to retrieve the replacement word for and in the metadata (get_and) and one replace ampersands in the citations (replace_and). Then the return function runs the get_and function on the metadata, followed by the replace_and function on each citation:\n\n-- set default value for and\nlocal andreplacement = \"and\"\n\n-- get andreplacement if it is specified in metadata\nlocal function get_and(m)\n  -- check if m.language exists\n  -- check if m.language[\"citation-last-author-separator\"] exists\n  if m.language and m.language[\"citation-last-author-separator\"] then\n     -- create string from metadata\n     andreplacement = pandoc.utils.stringify(m.language[\"citation-last-author-separator\"])\n  end\nend\n\n-- Replace ampersand for in-text citations\nlocal function replace_and (ct)\n    if ct.citations[1].mode == \"AuthorInText\" then\n    ct.content = ct.content:walk {\n      Str = function(s)\n        if s.text == \"&\" then\n          s.text = andreplacement\n          return s\n        end\n      end\n    }\n    return ct.content\n  end\nend\n\n-- run the get_and function first, \n-- then the replace_and function\nreturn {\n    { Meta = get_and },\n    { Cite = replace_and }\n}"
  },
  {
    "objectID": "posts/2023-07-23-latex-equation-in-ggplot2/index.html",
    "href": "posts/2023-07-23-latex-equation-in-ggplot2/index.html",
    "title": "Annotated equations in ggplot2",
    "section": "",
    "text": "Note\n\n\n\nThe xdvir package, which inserts \\LaTeX equations into grid-based plots (including ggplot2) is now on CRAN. I recommend xdvir over the older methods presented in this post. Other than this message, I have left this post “as is.”\n\n\nR has a mathematical annotation system via plotmath, but I like the look of true \\LaTeX equations better.\nGetting \\LaTeX equations into ggplot2 plots has never been easy. The tikzdevice package is great if you are generating a .pdf document. If you are not, then you might want to consider other options.\nThe easiest, hassle-free option that I know of is to create the equation in a \\TeX editor and then import the resulting .pdf using ggimage. If I need the best possible image quality, I convert the .pdf to .svg using dvisvgm (see example below).\nFor annotated equations, I like using the aptly-named annotate-equations \\LaTeX package. It uses tikz to remember where parts of equations are on the page. The \\eqnmark and \\eqnmarkbox functions work like so:\n\\eqnmark[color]{node_name}{latex equation terms}\nThen use the \\annotate function like so:\n\\annotate[color]{above,left}{node_name}{annotation text}\n\nMy Annotated Equation\nI am going to refer to the same file name with various endings (e.g., .tex, .pdf, .svg), so I will define it here.\n\nmyfile &lt;- \"annotatedequationsimple\"\n\nHere is the code for the annotated equation. It is saved in annotatedequationsimple.tex.\n\n\nLaTeX code\n```{tikz zscorecode}\n#| code-fold: show\n#| code-summary: \"LaTeX code\"\n#| eval: false\n\n\\documentclass[border={25pt 50pt -35pt 52pt}]{standalone}\n%\\documentclass{article}\n\\usepackage{annotate-equations}\n\\usepackage{xcolor}\n\\definecolor{myviolet}{HTML}{440154}\n\\definecolor{myblue}{HTML}{3B528B}\n\\definecolor{myindigo}{HTML}{21908C}\n\\definecolor{mygreen}{HTML}{5DC863}\n\\usepackage[sfdefault,condensed]{roboto}\n\\begin{document}\n    \n\\renewcommand{\\eqnhighlightheight}{\\mathstrut}\n    \n\\huge$\n\\eqnmark[myviolet]{z}{z} = \n\\frac{\n    \\eqnmark[myblue]{x}{X}-\n    \\eqnmark[myindigo]{mu}{\\mu}}{\n    \\eqnmark[mygreen]{sigma}{\\sigma}}\n$\n\n\n\\annotate[\n    yshift=1em, \n    myviolet,\n    align=right]\n    {above, left}\n    {z}\n    {$z$-score}\n    \n\\annotate[\n    yshift=1em, \n    myblue, \n    align=right]\n    {above,left}\n    {x}\n    {Observed\\\\ Score}\n    \n\n\\annotate[\n    yshift=1em, \n    myindigo]\n    {above,right}\n    {mu}\n    {Population\\\\ Mean\\\\ $\\mu = 100$}\n    \n\n    \n\\annotate[\n    yshift=-.4em, \n    mygreen, \n    align=right]\n    {below,left}\n    {sigma}\n    {Population\\\\ Standard\\\\ Deviation\\\\ $\\sigma = 15$}\n    \n\\end{document}\n\n```\n\n\nNow convert the .tex file to .pdf:\n\npaste0('pdflatex -interaction=nonstopmode ', myfile,'.tex') |&gt; \n  shell()\n\nNow we can convert the .pdf to .svg:\n\npaste0(\"dvisvgm --pdf --output=\", myfile,\".svg \", myfile,\".pdf\") |&gt; \n  shell()\n\nFor best image quality, import the .svg file with svgparser. The read_svg function will create a grid grob that can plotted directly using ggplot2::annotation_custom. If you want fine control of the imported grob, you can plot it using the gggrid package. See here for an example.\nIn the simplest case, we can do this:\n\nlibrary(svgparser)\nmy_svg &lt;- svgparser::read_svg(paste0(myfile, \".svg\"))\nggplot() +\n  theme_void() +\n  annotation_custom(my_svg, xmin = 0, xmax = 1, ymin = 0, ymax = 1) \n\n\n\n\n\n\n\nFigure 1: A simple plot with an annotated equation.\n\n\n\n\n\nHowever, this is no better than just displaying the .svg directly. You probably want to embed the equation in a plot. For example:\n\n\nCode\nmu &lt;- 100\nsigma &lt;- 15\nplot_height &lt;- dnorm(mu, mu, sigma)\nlb &lt;- -4 * sigma + mu\nub &lt;- 4 * sigma + mu\n\nggplot() +\n  annotation_custom(my_svg,\n                    xmin = 112,\n                    xmax = 164,\n                    ymin = .33 * plot_height) +\n  stat_function(\n    fun = \\(x) dnorm(x, mean = mu, sd = sigma),\n    geom = \"area\",\n    n = 1000,\n    fill = \"dodgerblue\",\n    alpha = .5\n  ) +\n  theme_classic(base_family = \"Roboto Condensed\",\n                base_size = 18) +\n  theme(\n    axis.text.x = element_markdown(),\n    axis.title.x = element_markdown(),\n    axis.line = element_blank()\n  )  +\n  scale_x_continuous(\n    \"Observed Score *X*&lt;br&gt;*z*\",\n    breaks = seq(lb, ub, sigma),\n    limits = c(lb, ub),\n    labels = \\(x) paste0(\n      signs::signs(x),\n      \"&lt;br&gt;\",\n      ifelse(\n        x == mu,\n        \"&lt;em&gt;&mu;&lt;/em&gt;\",\n        paste0(\n          signs::signs((x - mu) / sigma,\n                       add_plusses = T,\n                       label_at_zero = \"none\"\n          ),\n          \"&lt;em&gt;&sigma;&lt;/em&gt;\"\n        )\n      )\n    )\n  ) +\n  scale_y_continuous(\n    NULL,\n    limits = c(0, plot_height),\n    expand = expansion(),\n    breaks = NULL\n  ) \n\n\n\n\n\n\n\n\nFigure 2: Importing an .svg file via svgparser and annotation_custom.\n\n\n\n\n\nIf you can live with just a little pixelation, the ggimage package can import a .pdf directly with good results and less hassle, provided you render the plot with the ragg package.\n\n\nCode\nggplot() +\n  geom_image(\n    data = tibble(\n      x = 140,\n      y = .65 * plot_height,\n      image = \"annotatedequationsimple.pdf\"\n    ),\n    aes(x, y, image = image),\n    size = .70\n  ) +\n  stat_function(\n    fun = \\(x) dnorm(x, mean = mu, sd = sigma),\n    geom = \"area\",\n    n = 1000,\n    fill = \"dodgerblue\",\n    alpha = .5\n  ) +\n  theme_classic(base_family = \"Roboto Condensed\", \n                base_size = 18) +\n  theme(\n    axis.text.x = element_markdown(),\n    axis.title.x = element_markdown(),\n    axis.line = element_blank()\n  )  +\n  scale_x_continuous(\n    \"Observed Score *X*&lt;br&gt;*z*\",\n    breaks = seq(lb, ub, sigma),\n    limits = c(lb, ub),\n    labels = \\(x) paste0(\n      signs::signs(x),\n      \"&lt;br&gt;\",\n      ifelse(\n        x == mu,\n        \"&lt;em&gt;&mu;&lt;/em&gt;\",\n        paste0(\n          signs::signs((x - mu) / sigma,\n                       add_plusses = T,\n                       label_at_zero = \"none\"\n          ),\n          \"&lt;em&gt;&sigma;&lt;/em&gt;\"\n        )\n      )\n    )\n  ) +\n  scale_y_continuous(\n    NULL,\n    limits = c(0, plot_height),\n    expand = expansion(),\n    breaks = NULL\n  )\n\n\n\n\n\n\n\n\nFigure 3: Importing a .pdf file via ggimage::geom_image.\n\n\n\n\n\n\n\nA more complex example\nIn this example, I used the \\eqnmarkbox function for greater clarity. The .tex file is saved in a file called annotatedequation.tex.\n\n\nLaTeX code\n\\documentclass[border={10pt 48pt -45pt 62pt}]{standalone}\n%\\documentclass{article}\n\\usepackage{annotate-equations}\n\\usepackage{xcolor}\n\\definecolor{myviolet}{HTML}{414487}\n\\definecolor{myblue}{HTML}{2F6C8E}\n\\definecolor{myblue2}{HTML}{21908C}\n\\definecolor{mygreen}{HTML}{2FB47C}\n\\definecolor{mygreen2}{HTML}{7AD151}\n\\usepackage[sfdefault,condensed]{roboto}\n\\begin{document}\n    \n\\renewcommand{\\eqnhighlightheight}{\\mathstrut}\n    \n$\\LARGE\n\\eqnmarkbox[myviolet]{nodeP}{P\\left(T \\le \\tau \\right)} = \n\\eqnmarkbox[myblue]{phi}{\\Phi}\n\\left(\\frac{\n    \\eqnmarkbox[myblue2]{tau}{\\tau}-\n    \\eqnmarkbox[mygreen]{esttrue}{\\hat{T}}}{\n    \\eqnmarkbox[mygreen2]{sigma}{\\sigma_{T - \\hat{T}}}}\n\\right)$\n\n\n\\annotate[\n    yshift=1em, \n    xshift=11mm, \n    myviolet]\n    {above, left}\n    {nodeP}\n    {Probability true score $T$\\\\ is less than threshold $\\tau$ }\n    \n\\annotate[\n    yshift=-.6em,\n    myblue]\n    {below,left}\n    {phi}\n    {Standard Normal Cumulative\\\\ Distribution Function $\\Phi()$}\n    \n\\annotate[\n    yshift=1.4em, \n    xshift=4mm, \n    myblue2]\n    {above,left}\n    {tau}\n    {Threshold $\\tau=70$}\n    \n\\annotate[\n    yshift=3em, \n    xshift=7mm, \n    mygreen]\n    {above,left}\n    {esttrue}\n    {Estimated True Score $\\hat{T}=r_{XX}(X-\\mu)+\\mu$\\\\ \n    Observed Score $X\\sim \\mathcal{N}\\left(\\mu = 100, \\sigma=15\\right)$\\\\ \n    Reliability Coefficient $r_{XX}=\\{.80,.85,.90,.95,.98\\}$}\n    \n\\annotate[\n    yshift=-2em, \n    mygreen2]\n    {below,left}\n    {sigma}\n    {Standard Error of the Estimate\\\\ \n    $\\sigma_{T-\\hat{T}}=\\sigma\\sqrt{r_{XX}-r_{XX}^2}$}\n    \n\\end{document}\n\n\nNow convert .tex to .pdf:\n\n\nCode\nmyfile &lt;- \"annotatedequation\"\n\npaste0('pdflatex -interaction=nonstopmode ', myfile,'.tex') |&gt; \n  shell()\n\n\nAnd we are ready to plot. This plot shows the probability that an observed score will have a true score less than a specific threshold, given a reliability coefficient.\n\n\nCode\nviridis_start &lt;- .2\nviridis_end &lt;- .8\nthreshold &lt;- 70\n\n# Find where a line intersects with the normal cdf\nfind_x &lt;- function(rxx = .8,\n                   slope = 0.0048,\n                   intercept = .66,\n                   mu = 100,\n                   sigma = 15,\n                   start_x = 60,\n                   threshold = 70) {\n  x &lt;- start_x\n  precision &lt;- .00001\n  diff_y &lt;- precision * 10\n  reps &lt;- 0\n  while (abs(diff_y) &gt; precision) {\n    x &lt;- x - diff_y \n    line_y &lt;- x * slope + intercept\n    curve_y &lt;- pnorm(threshold,\n                     mean = rxx * (x - mu) + mu,\n                     sd = sigma * sqrt(rxx - rxx ^ 2))\n    \n    line_y &lt;- x * slope + intercept\n    curve_y &lt;- pnorm(threshold,\n                     mean = rxx * (x - mu) + mu,\n                     sd = sigma * sqrt(rxx - rxx ^ 2))\n    diff_y &lt;- line_y - curve_y\n    reps &lt;- reps + 1\n  }\n  tibble(x = x, p = line_y, mu = mu, sigma = sigma, threshold = threshold, reps = reps)\n}\n\n\n\n\ndimage = data.frame(x = 115, \n                    y = .62, \n                    image = paste0(myfile, \".pdf\"))\nv_rxx &lt;- round(c(seq(0.80, 0.95, 0.05), 0.98), 2)\n\nd_threshold &lt;-\n  crossing(x = round(seq(40, 160, 0.1), 1),\n           rxx = v_rxx,\n           threshold = 70) %&gt;%\n  mutate(\n    see = 15 * sqrt(rxx - rxx ^ 2),\n    mu = (x - 100) * rxx + 100,\n    p = pnorm(threshold, mu, see)\n  ) %&gt;%\n  group_by(rxx) %&gt;%\n  mutate(acceleration = p - lag(p)) %&gt;%\n  ungroup\n\n\n\nd_labels &lt;- tibble(rxx = v_rxx) %&gt;%\n  mutate(x = map_df(rxx, find_x)) |&gt; \n  unnest(x)\n\nd_threshold %&gt;%\n  ggplot(aes(x, p)) +\n  geom_line(aes(color = factor(rxx)), lwd = 1) +\n  geom_vline(\n    aes(xintercept = threshold),\n    lty = 2,\n    lwd = 1,\n    color = \"gray30\"\n  ) +\n  geom_image(data = dimage,\n             aes(x = x,\n                 y = y,\n                 image = image),\n             size = .87) +\n  geom_richtext(\n    aes(label = rxx_label,\n        color = factor(rxx)),\n    data = d_labels %&gt;%\n      mutate(rxx_label = prob_label(rxx)),\n    angle = -67,\n    size = WJSmisc::ggtext_size(13),\n    label.colour = NA,\n    fill = \"#FFFFFF\",\n    family = \"Roboto Condensed\",\n    label.margin = unit(0, \"mm\"),\n    label.r = unit(2, \"mm\"),\n    label.padding = unit(c(0, 0.75, 0, .5), \"mm\")\n  ) +\n  scale_x_continuous(\n    \"Observed Score\",\n    breaks = seq(40, 160, 15),\n    minor_breaks = seq(40, 160, 5),\n    expand = expansion()\n  ) +\n  scale_y_continuous(\n    paste0(\"Probability True Score &lt; \", threshold),\n    expand = expansion(),\n    breaks = seq(0, 1, 0.1),\n    labels = prob_label,\n    limits = c(0, 1)\n  ) +\n  scale_color_viridis_d(begin = viridis_start,\n                        end = viridis_end) +\n  theme_minimal(base_family = \"Roboto Condensed\", \n                base_size = 16) +\n  theme(legend.position = \"none\", \n        plot.margin = unit(c(3, 5, 2, 2), \"mm\")) +\n  coord_fixed(ratio = 100,\n              clip = \"off\",\n              xlim = c(40, 160)) \n\n\n\n\n\n\n\n\nFigure 4: The probability that a true score is less than 70 depends on the observed score’s position and reliability coefficient.\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2023,\n  author = {Schneider, W. Joel},\n  title = {Annotated Equations in Ggplot2},\n  date = {2023-07-24},\n  url = {https://wjschne.github.io/posts/2023-07-23-latex-equation-in-ggplot2/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2023, July 24). Annotated equations in ggplot2.\nSchneirographs. https://wjschne.github.io/posts/2023-07-23-latex-equation-in-ggplot2/"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Schneirographs",
    "section": "",
    "text": "Winners of Posit’s 2025 Table Contest\n\n\n\nR\n\napa7\n\nAPA Style\n\n\n\nPosit’s 2025 Table Contest. My 24-part APA table series named Best Tutorial.\n\n\n\n\n\nNov 11, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.24 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nOct 4, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.23 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nOct 3, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.22 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nOct 2, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.21 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nOct 1, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.20 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 30, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.19 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 29, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.18 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 28, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.17 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 27, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.16 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 26, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.15 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 25, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.14 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 24, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.13 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 23, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.12 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 22, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.11 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 21, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.10 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 20, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.9 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 19, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.8 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 18, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.7 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 17, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.6 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 16, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.5 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 15, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.4 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 14, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.3 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 13, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.2 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 12, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating APA Manual Table 7.1 in R with apa7\n\n\n\nR\n\nAPA Style\n\n\n\nDemonstration of the apa7 package, a flextable extension package\n\n\n\n\n\nSep 10, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nAPA style with Quarto: A shiny web app for creating apaquarto files\n\n\n\nQuarto\n\nAPA Style\n\n\n\nAnnouncing a web app for creating apaquarto documents.\n\n\n\n\n\nSep 5, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncing ggdiagram, a ggplot2 extension for making diagrams programmatically\n\n\n\nggplot2\n\nggdiagram\n\n\n\nMaking diagrams in R with an objected-oriented approach inspired by Tikz\n\n\n\n\n\nAug 19, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nMy lua filters\n\n\n\nlua\n\npandoc\n\n\n\nA collection of lua filters\n\n\n\n\n\nMar 15, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a custom arrowhead for ggplot2 using ggarrow and arrowheadr\n\n\n\nggplot2\n\n\n\nThe ggarrow package allows any polygon to be an arrowhead.\n\n\n\n\n\nAug 26, 2023\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nAnnotated equations in ggplot2\n\n\n\nggplot2\n\n\n\nImporting latex into ggplot2\n\n\n\n\n\nJul 24, 2023\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nCreating collapsible output with knitr chunk hooks\n\n\n\nknitr\n\nmarkdown\n\n\n\nHow to make the output of a knitr code chunk collapsible using knitr hooks.\n\n\n\n\n\nJun 30, 2023\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nSplitting sentence texts into roughly equal segments\n\n\n\nggplot2\n\n\n\nFor long text labels on the y-axis, splitting them into roughly equal segments can look better.\n\n\n\n\n\nMay 31, 2023\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nInsert Text from One Markdown Document into Another\n\n\n\nmarkdown\n\n\n\nA function for retrieving a div or span from another markdown document\n\n\n\n\n\nMar 30, 2023\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nMaking text labels the same size as axis labels in ggplot2\n\n\n\nggplot2\n\n\n\nThe geom_text and geom_label functions do not specifiy text size the same way as the rest of ggplot2 elements do. For consistent text sizes, we can apply a simple conversion.\n\n\n\n\n\nAug 10, 2021\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nBar chart labels on smooth paths in ggplot2\n\n\n\nggplot2\n\n\n\nPlacing stacked bar chart labels on a smooth path makes them easier to compare.\n\n\n\n\n\nJul 31, 2021\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nPoint labels perpendicular to a curve in ggplot2\n\n\n\nggplot2\n\n\n\nHow to place legible labels for points on a curve in ggplot2\n\n\n\n\n\nJul 27, 2021\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nCaption on same line as axis title in ggplot2\n\n\n\nggplot2\n\n\n\nYou can put the caption on the line as the x-axis title\n\n\n\n\n\nJul 21, 2021\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nConcatenating vectors unless a vector is empty\n\n\n\nR\n\n\n\nThe paste and paste0 functions have a recycle0 argument that returns an empty vector when any part of the string is empty.\n\n\n\n\n\nMay 13, 2021\n\n\nW. Joel Schneider\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "AssessingPsyche/2025-05-18-horn-cattell-1966/horncattell1966.html",
    "href": "AssessingPsyche/2025-05-18-horn-cattell-1966/horncattell1966.html",
    "title": "Horn & Cattell (1966)",
    "section": "",
    "text": "Horn & Cattell (1966) was a landmark study in the advancement of intelligence theory. It prompted Cattell to refine his original gf-gc theory, bringing it closer to Thurstone’s (1938) Primary Mental Abilities. The paper’s EFA methods are a little out of date, so I wanted to see what would happen if modern EFA methods were applied. I also removed the personality variables from the analysis, as they seemed to clutter the results.\nA heatmap of the correlation matrix from their Table 3 gives a nice preview of the factor analytic results:\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the superheat package.\n  Please report the issue to the authors.\n\n\n\n\n\n\n\n\n\nI see 3 big clusters, 2 of which split into 2 smaller clusters.\n\nParallel analysis\nA parallel analysis with n = 297 participants suggests 4 factors, perhaps 5:\n\n\n\n\n\n\n\n\n\nLet’s extract up to 5 factors, starting with just a single factor.\n\n\n1 Factor\n\n\n\n\n\n\n\n\n\nNothing to see here except the familiar finding that the reasoning tests have high g loadings and the speeded tests have lower g loadings.\n\n\n2 Factors\nDoes this look like gf-gc theory 1.0? It does to me!\n\n\n\n\n\n\n\n\n\n\n\n3 Factors\nThis looks like the higher-order gf-gc-gs factors from Schneider & McGrew (2018).\n\n\n\n\n\n\n\n\n\n\n\n4 Factors\nHere we see that gf splits into Gf and Gv:\n\n\n\n\n\n\n\n\n\n\n\n5 factors\ngs (General Speediness) splits into Gs and Gr\n\n\n\n\n\n\n\n\n\nThese results are in line with CHC Theory (McGrew, 2005, 2009; Schneider & McGrew, 2012; Schneider & McGrew, 2018), as they should be. Not much has changed! This is a good thing. Consistent results engender trust.\n\n\n\n\n\nReferences\n\nHorn, J. L., & Cattell, R. B. (1966). Refinement and test of the theory of fluid and crystallized general intelligences. Journal of Educational Psychology, 57(5), 253–270. https://doi.org/10.1037/h0023816\n\n\nMcGrew, K. S. (2005). The cattell-horn-carroll theory of cognitive abilities: Past, present, and future. In D. P. Flanagan & P. L. Harrison (Eds.), Contemporary intellectual assessment. Theories, tests, and issues (2nd ed., pp. 136–181). Guilford Press.\n\n\nMcGrew, K. S. (2009). CHC theory and the human cognitive abilities project: Standing on the shoulders of the giants of psychometric intelligence research. Intelligence, 37(1), 1–10. https://doi.org/10.1016/j.intell.2008.08.004\n\n\nSchneider, W. J., & McGrew, K. S. (2012). The cattell-horn-carroll model of intelligence. In D. P. Flanagan & P. L. Harrison (Eds.), Contemporary intellectual assessment: Theories, tests, and issues (3rd ed., pp. 99–144). Guilford Press. https://psycnet.apa.org/record/2012-09043-004\n\n\nSchneider, W. J., & McGrew, K. S. (2018). The Cattell-Horn-Carroll theory of cognitive abilities. In D. P. Flanagan & E. M. McDonough (Eds.), Contemporary intellectual assessment: Theories, tests, and issues (4th ed., pp. 73–130). Guilford Press. https://www.guilford.com/books/Contemporary-Intellectual-Assessment/Flanagan-McDonough/9781462552030\n\n\nThurstone, L. L. (1938). Primary mental abilities. University of Chicago Press.\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel and Joel Schneider, W.},\n  title = {Horn \\& {Cattell} (1966)},\n  date = {2025-05-18},\n  url = {https://wjschne.github.io/AssessingPsyche/2025-05-18-horn-cattell-1966/horncattell1966.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J., & Joel Schneider, W. (2025, May 18). Horn &\nCattell (1966). AssessingPsyche. https://wjschne.github.io/AssessingPsyche/2025-05-18-horn-cattell-1966/horncattell1966.html"
  },
  {
    "objectID": "AssessingPsyche/2014-01-16-reliability-is-for-squares/index.html",
    "href": "AssessingPsyche/2014-01-16-reliability-is-for-squares/index.html",
    "title": "Reliability coefficients are for squares",
    "section": "",
    "text": "The more reliable a score is, the more certain we can be about what it means (provided its validity is close to its reliability). Certain rules-of-thumb about score reliability are sometimes proposed:\n\nBase high-stakes decisions only on scores with reliability coefficients of 0.98 or better.\nBase substantive interpretations on scores with reliability coefficients of 0.90 or better.\nBase decisions to give more tests or not on scores with reliability coefficients of 0.80 or more.\n\nSuch guidelines seem reasonable to me, but I do not find reliability coefficients to be intuitively easy to understand. How much uncertainty is associated with a reliability coefficient of .80? The value of the coefficient (.80) is not directly informative about individual scores. Instead, it refers to the correlation the scores have with a repeated measurement.\nAnother way to think about the reliability coefficient is that it is a ratio of true score variance to observed score variance. In classical test theory, an observed score (X) is influenced by a reliable component, the true score (T), and also by measurement error (e). That is\nX=T+e\nThe true score is static for each person, and the error fluctuates randomly.\nBecause the true score and error are uncorrelated, the variance of X is the sum of the variance of T and the variance fo e:\n\\sigma^2_X=\\sigma^2_T+\\sigma^2_e\nTherefore, the reliability coefficient of an observed score is the ratio of the true score variance over the total variance:\nr_{XX}=\\frac{\\sigma^2_T}{\\sigma^2_X}\nIn other words, what proportion of the observed score’s variability is consistent?\nOkay, so what is variance? Variance is the average squared deviation from the mean. Squared quantities are not easy to think about for most of us. For this reason, I prefer to convert reliability coefficients into confidence interval widths. Confidence interval widths and reliability coefficients have a non-linear relationship:\n\\text{CI Width}=2z\\sigma_{x}\\sqrt{r_{xx}-r^2_{xx}}\nWhere:\nz is the z-score associated with the level of confidence you want (e.g., 1.96 for a 95% confidence interval)\n\\sigma_{x} is the standard deviation of X\nr_{xx} is the classical test theory reliability coefficient for X\nFor index scores (μ = 100, σ = 15), a reliability coefficient of .80 is associated with a 95% confidence interval that is 24 points wide. That to me is much more informative than knowing that 80% of the variance is reliable.\nCalculating a lower and upper bounds of a confidence interval for a score looks complex with all the symbols and subscripts, but after doing it a few times, it is not so bad. Basically, compute the estimated true score (\\hat{T}) and then add (or subtract) the margin of error.\n\\hat{T}=\\mu_x+r_{xx}\\left(X-\\mu_x\\right)\n\\text{CI} = \\hat{T} \\pm z\\sigma_x\\sqrt{r_{xx}-r^2_{xx}}\nThe interactive app in Figure 1 graph below shows the non-linear relationship between reliability and 95% confidence interval widths for different observed index scores. The confidence interval width is widest when the reliability coefficient is .5 and tapers to 0 when the reliability coefficient is 0 or 1.\nThe idea that the confidence interval width is zero when reliability is perfect makes sense. However, it might be counterintuitive that the confidence interval width is also zero when the reliability coefficient is zero.\nHow is this possible? To make sense of this, we have to remember what the true score is. It is the long term average score after repeated measurements (assuming no carryover effects). If a score has no reliable component, it is pure error. When r_{XX}=0, the score is X=T+e, where \\sigma_T^2=0 and \\sigma_e^2=\\sigma_X^2. So the true score has no variance, meaning that its mean is exactly the mean of X for everyone.\nThe reason for this is that when reliability is zero, the true score is a constant.\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(shiny)\nlibrary(munsell)\nlibrary(ggnormalviolin)\nlibrary(ggtext)\nlibrary(scales)\nif (FALSE) {\n  library(ggdiagram)\n}\nui &lt;- fluidPage(\n  sidebarLayout(\n      mainPanel(\n      plotOutput(outputId = \"distPlot\",width = \"400px\", height = \"400px\")\n    ),\n    \n    sidebarPanel(\n      div(style=\"display: inline-block; width: 200px;\",\n      sliderInput(\n        inputId = \"x\",\n        label = HTML(\"Score (&lt;i&gt;X&lt;/i&gt;)\"),\n        min = 40,\n        max = 160,\n        value = 120,\n        step = 1\n      )),\n      div(style=\"display: inline-block; width: 200px;\",\n          sliderInput(\n        inputId = \"my_rxx\",\n        label = HTML(\"Reliability (&lt;i&gt;r&lt;sub&gt;XX&lt;/sub&gt;&lt;/i&gt;)\"),\n        min = 0,\n        max = 1,\n        value = .9,\n        step = .01\n      )),\n      tags$br(),\n      div(style=\"display: inline-block; width: 75px;\",\n      numericInput(\n        inputId = \"mu\",\n        label = \"Mean\",\n        min = 0,\n        max = 100,\n        value = 100,\n        step = 1\n      )),\n      div(style=\"display: inline-block; width: 75px;\",\n      numericInput(\n        inputId = \"sigma\",\n        label = \"SD\",\n        min = 1,\n        max = 15,\n        value = 15, \n        step = 1\n      ))\n    )\n    \n  )\n)\nserver &lt;- function(input, output, session) {\n  output$distPlot &lt;- renderPlot({\n    mu &lt;- input$mu\n    sigma &lt;- input$sigma\n    my_rxx &lt;- input$my_rxx\n    x &lt;- input$x\n\n\np &lt;- .95\nz &lt;- (x - mu)  / sigma\nz_ci &lt;- qnorm(1 - (1 - p) / 2)\nrxx = c(seq(0,.019,.001),seq(.02,.98,.01), seq(.981,1,.001))\nrxx_rev &lt;- rev(rxx)\nlb &lt;- sigma * (z * rxx - z_ci * sqrt(rxx - rxx ^ 2)) + mu\nub &lt;- sigma * (z * rxx_rev + z_ci * sqrt(rxx_rev - rxx_rev ^ 2)) + mu\n\n\nmy_see &lt;- sigma * sqrt(my_rxx - my_rxx ^ 2)\nmy_moe &lt;- z_ci * my_see\nmy_tau &lt;- sigma * z * my_rxx  + mu\nmy_lb &lt;- sigma * z * my_rxx - my_moe + mu\nmy_ub &lt;- sigma * z * my_rxx + my_moe + mu\n\nmy_xhat &lt;- (x - mu) * sqrt(my_rxx) + mu\nmy_see2 &lt;- sqrt(1 - my_rxx) \nmy_moe2 &lt;- z_ci * my_see2\nmy_lb2 &lt;- my_xhat - my_moe2\nmy_ub2 &lt;- my_xhat + my_moe2\n\nlb2 &lt;- sigma * (z * sqrt(rxx) - z_ci * sqrt(1 - rxx )) + mu\nub2 &lt;- sigma * (z * sqrt(rxx_rev) + z_ci * sqrt(1 - rxx_rev)) + mu\n\n\nd_arrow &lt;- tibble(Reliability = my_rxx, \n                  ci = c(my_lb, my_ub))\n\n\n\ntibble(Reliability = c(rxx, rxx_rev),\n       ci = c(lb, ub),\n       ci2 = c(lb2, ub2)) %&gt;% \n  ggplot(aes(Reliability, ci)) +\n  geom_polygon(fill = \"dodgerblue4\", alpha = .2, aes(y = ci2)) +\n  geom_polygon(fill = \"dodgerblue3\", alpha = .5) +\n  geom_normalviolin(data = tibble(mu = my_tau, x = my_rxx, sigma = my_see), fill = \"black\", p_tail = .05, aes(x = x, mu = mu, sigma = sigma, width = .15, face_left = F), inherit.aes = F, color = NA, alpha = .2) +\n  scale_x_continuous(\"Reliability Coefficient\", breaks = seq(0,1,.2), labels = c(\"0\", \".20\", \".40\", \".60\", \".80\", \"1\"), expand = expansion(add = .09)) +\n  scale_y_continuous(\"Score\", breaks = -4:4 * sigma + mu,\n  minor_breaks = seq(-4 * sigma + mu,\n  4 * sigma + mu,\n  ifelse((sigma %% 3) == 0,\n  sigma / 3,\n  sigma / 2)) ) +\n  coord_fixed(ratio = 1 / (sigma * 8),\n              ylim = c(-4 * sigma + mu, 4 * sigma + mu),\n              clip = \"off\") +\n  theme_minimal(16, \"sans\") +\n  theme(panel.spacing.x = unit(5, \"mm\")) +\n  geom_line(data = d_arrow) +\n  geom_text(data = d_arrow,\n            aes(label = scales::number(ci, .1), x = Reliability - .02),\n            hjust = 1,\n            family = \"sans\") +\n  annotate(\n    \"richtext\",\n    x = my_rxx + .02,\n    y = x,\n    hjust = 0,\n    label = paste0(\"***X*** = **\", x, \"**\"),\n    color = \"firebrick\",\n    family = \"sans\",\n    fill = NA,\n    label.colour = NA\n  ) +\n  annotate(\n    \"text\",\n    x = my_rxx - .02,\n    y = my_tau,\n    hjust = 1,\n    label = number(my_tau, .1),\n    family = \"sans\"\n  ) +\n  annotate(\"point\", x = my_rxx, y = my_tau) +\n  annotate(\"point\", x = my_rxx, y = x, size = 3, color = \"firebrick\") +\n  ggtitle(paste0(\"Confidence Interval Width = \", number(my_ub - my_lb, .1)))\n  \n    \n  })\n}\nshinyApp(ui = ui, server = server)\n\n\n\nFigure 1: The relationship of reliability coefficients and confidence interval widths.\n\n\n\nPaying close attention to confidence intervals allows you to do away with rough rules-of-thumb about reliability and make more direct and accurate interpretations about individual scores.\n\n\n\nCitationBibTeX citation:@misc{schneider2014,\n  author = {Schneider, W. Joel},\n  title = {Reliability Coefficients Are for Squares},\n  date = {2014-01-16},\n  url = {https://wjschne.github.io/AssessingPsyche/2014-01-16-reliability-is-for-squares/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2014, January 16). Reliability coefficients are for\nsquares. AssessingPsyche. https://wjschne.github.io/AssessingPsyche/2014-01-16-reliability-is-for-squares/"
  },
  {
    "objectID": "AssessingPsyche/2012-09-13-visualizing-covariance/index.html",
    "href": "AssessingPsyche/2012-09-13-visualizing-covariance/index.html",
    "title": "Visualizing Covariance",
    "section": "",
    "text": "Correlation? I get it. I have a gut-level sense of what it is. Covariance? Somehow it just eludes me. I mean, I know the formulas, and I can give you a conceptual definition of it—but its meaning never really sunk in.\nOne thing about covariance that always seemed counterintuitive to me is that covariance between two variables of unequal variance can sometimes be larger than the variance of the variable with less variance. For example, if X has a variance of 9, Y has a variance of 64 and the correlation between X and Y is 0.5, the covariance between X and Y is 12. How can X have a larger covariance with Y than its own variance (i.e., its covariance with itself)? Never made sense to me.\nTo figure it out, I created a little web app that allows the user to change the standard deviations of variables x and y, as well as the correlation between the two variables.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 650\nlibrary(shiny)\nlibrary(glue)\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ggtext)\nlibrary(munsell)\n\nif (FALSE) {\n  library(ggdiagram)\n}\n\nprob_label &lt;- function(\n  p,\n  accuracy = 0.01,\n  digits = NULL,\n  max_digits = NULL,\n  remove_leading_zero = TRUE,\n  round_zero_one = TRUE\n) {\n  if (is.null(digits)) {\n    l &lt;- scales::number(p, accuracy = accuracy)\n  } else {\n    sig_digits &lt;- abs(ceiling(log10(p + p / 1e+09)) - digits)\n    pgt99 &lt;- p &gt; 0.99\n    sig_digits[pgt99] &lt;- abs(\n      ceiling(log10(1 - p[pgt99])) -\n        digits +\n        1\n    )\n    sig_digits[\n      ceiling(log10(p)) == log10(p) & (-log10(p) &gt;= digits)\n    ] &lt;- sig_digits[\n      ceiling(log10(p)) == log10(p) &\n        (-log10(p) &gt;= digits)\n    ] -\n      1\n    sig_digits[is.infinite(sig_digits)] &lt;- 0\n    l &lt;- purrr::map2_chr(p, sig_digits, formatC, format = \"f\", flag = \"#\")\n  }\n  if (remove_leading_zero) {\n    l &lt;- sub(\"^-0\", \"-\", sub(\"^0\", \"\", l))\n  }\n  if (round_zero_one) {\n    l[p == 0] &lt;- \"0\"\n    l[p == 1] &lt;- \"1\"\n    l[p == -1] &lt;- \"-1\"\n  }\n  if (!is.null(max_digits)) {\n    if (round_zero_one) {\n      l[round(p, digits = max_digits) == 0] &lt;- \"0\"\n      l[round(p, digits = max_digits) == 1] &lt;- \"1\"\n      l[round(p, digits = max_digits) == -1] &lt;- \"-1\"\n    } else {\n      l[round(p, digits = max_digits) == 0] &lt;- paste0(\n        \".\",\n        paste0(rep(\"0\", max_digits), collapse = \"\")\n      )\n      l[round(p, digits = max_digits) == 1] &lt;- paste0(\n        \"1.\",\n        paste0(rep(\"0\", max_digits), collapse = \"\")\n      )\n      l[round(p, digits = max_digits) == -1] &lt;- paste0(\n        \"-1.\",\n        paste0(rep(\"0\", max_digits), collapse = \"\")\n      )\n    }\n  }\n  l &lt;- sub(pattern = \"-\", replacement = \"−\", x = l)\n  Encoding(l) &lt;- \"UTF-8\"\n  dim(l) &lt;- dim(p)\n  l\n}\nui &lt;- fluidPage(\n  sidebarLayout(\n    mainPanel(\n      plotOutput(outputId = \"distPlot\", width = \"400px\", height = \"400px\")\n    ),\n    sidebarPanel(\n      div(\n        style = \"display: inline-block; width: 200px;\",\n        sliderInput(\n          inputId = \"sd_blue\",\n          label = \"SD for x:\",\n          min = 1,\n          max = 10,\n          value = 10\n        )\n      ),\n      div(\n        style = \"display: inline-block; width: 200px;\",\n        sliderInput(\n          inputId = \"sd_red\",\n          label = \"SD for y:\",\n          min = 1,\n          max = 10,\n          value = 10\n        )\n      ),\n      div(\n        style = \"display: inline-block; width: 200px;\",\n        sliderInput(\n          inputId = \"r\",\n          label = \"Correlation (r)\",\n          min = 0,\n          max = 1,\n          value = .5,\n          step = .01\n        )\n      )\n    )\n  )\n)\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    sd_blue &lt;- input$sd_blue\n    sd_red &lt;- input$sd_red\n    r &lt;- input$r\n    x_red &lt;- sd_red / 2\n    y_red &lt;- x_red + sd_blue\n    x_blue &lt;- sd_red + sd_blue / 2\n    y_blue &lt;- sd_blue / 2\n    x_pink &lt;- x_blue\n    y_pink &lt;- y_red\n    x_purple &lt;- x_blue\n    y_purple &lt;- sd_blue + r * sd_red / 2\n    tibble(\n      x = c(x_red, x_blue, x_pink, x_purple),\n      y = c(y_red, y_blue, y_pink, y_purple),\n      width = c(sd_red, sd_blue, sd_blue, sd_blue),\n      height = c(sd_red, sd_blue, sd_red, sd_red * r),\n      color = c(\"firebrick4\", \"dodgerblue4\", \"orchid\", \"orchid4\"),\n      label = c(\n        glue(\"Var(*y*) = {sd_red ^ 2}\"),\n        glue(\"Var(*x*) = {sd_blue ^ 2}\"),\n        \"\",\n        glue(\"Cov(*xy*) = {round(sd_blue * sd_red * r, 2)}\")\n      )\n    ) %&gt;%\n      ggplot(aes(x, y)) +\n      geom_tile(\n        (aes(\n          width = width,\n          height = height,\n          fill = I(color)\n        ))\n      ) +\n      geom_richtext(\n        aes(label = label),\n        color = \"white\",\n        size = 4,\n        fill = NA,\n        label.color = NA\n      ) +\n      geom_segment(\n        data = tibble(\n          x = c(sd_red, sd_red, sd_red + sd_blue),\n          y = c(sd_blue, sd_blue, sd_blue),\n          xend = c(sd_red + sd_blue, sd_red, sd_red + sd_blue),\n          yend = c(sd_blue, sd_red + sd_blue, r * sd_red + sd_blue)\n        ),\n        aes(xend = xend, yend = yend),\n        color = \"white\",\n        arrow = arrow(\n          15,\n          length = unit(10, \"pt\"),\n          ends = \"both\",\n          type = \"closed\"\n        )\n      ) +\n      geom_richtext(\n        data = tibble(\n          x = c(x_blue, sd_red, sd_red + sd_blue),\n          y = c(sd_blue, x_pink, y_purple),\n          label = c(\n            glue(\"SD~*x*~ = {sd_blue}\"),\n            glue(\"SD~*y*~ = {sd_red}\"),\n            glue(\"*r*~*xy*~ = {prob_label(r)}\")\n          ),\n          angle = c(0, 90, 90),\n          vjust = c(.5, .5, 1.1)\n        ),\n        aes(\n          label = label,\n          angle = angle,\n          vjust = vjust\n        )\n      ) +\n      scale_x_continuous(NULL, expand = expansion(add = 1), limits = c(0, 20)) +\n      scale_y_continuous(NULL, expand = expansion(add = 1), limits = c(0, 20)) +\n      coord_equal(clip = \"off\") +\n      theme_void() +\n      theme(panel.background = element_rect(\"black\"))\n  })\n}\nshinyApp(ui = ui, server = server)\nThe area of the blue square is equal to the variance of X. The area of the red square is equal to the variance of Y. The pink rectangle (which is partially occluded by the purple rectangle) is how large covariance could be if X and Y were perfectly correlated. The area of the purple square is equal to the covariance between X and Y. The ratio of the area of the purple rectangle to the area of the pink rectangle is equal to the correlation between X and Y.\nI’m not sure why but this visualization has made me feel better about covariance. It’s like were friends now. 😉\n\n\n\n\n\n\nNoteNote\n\n\n\nOriginally posted 2012-09-13 on AssessingPsyche. The original article had a web app created with Mathematica’s computatable document format. Because link to the app is now dead, I recreated the app in Shinylive.\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2012,\n  author = {Schneider, W. Joel},\n  title = {Visualizing {Covariance}},\n  date = {2012-09-13},\n  url = {https://wjschne.github.io/AssessingPsyche/2012-09-13-visualizing-covariance/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2012, September 13). Visualizing Covariance.\nAssessingPsyche. https://wjschne.github.io/AssessingPsyche/2012-09-13-visualizing-covariance/"
  },
  {
    "objectID": "AssessingPsyche/2010-10-07-overture/index.html",
    "href": "AssessingPsyche/2010-10-07-overture/index.html",
    "title": "Overture Redux",
    "section": "",
    "text": "The original title of this blog (Assessing Psyche, Engaging Gauss, Seeking Sophia) was more of an aspiration than a description. To keep things simple, going forward the blog’s title will be just AssessingPsyche.\nThe purpose of this blog remains the same from the original post in 2010:\n\nWhen we are at our best, clinicians skilled in psychological assessment see beyond the obvious and communicate something useful and true to a person who needs our help. Uncovering something true often requires a bit of science, sometimes a little math, and always a lot of empathy. Communicating what is true so that it is useful requires still more empathy and a bit of art. We facilitate in our clients a deeper understanding of what is happening to them and present to them a different vision of what they can be.\nIn this blog, I hope to communicate something useful and true to other professionals about psychological assessment. I hope that in communicating what I know, I will come to know more than I do now.\n\n\n\n\nCitationBibTeX citation:@misc{schneider2023,\n  author = {Schneider, W. Joel},\n  title = {Overture {Redux}},\n  date = {2023-09-12},\n  url = {https://wjschne.github.io/AssessingPsyche/2010-10-07-overture/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2023, September 12). Overture Redux.\nAssessingPsyche. https://wjschne.github.io/AssessingPsyche/2010-10-07-overture/"
  },
  {
    "objectID": "AssessingPsyche/2014-01-13-gentle-introduction-to-factor-analysis/index.html",
    "href": "AssessingPsyche/2014-01-13-gentle-introduction-to-factor-analysis/index.html",
    "title": "A Gentle, Non-Technical Introduction to Factor Analysis",
    "section": "",
    "text": "When measuring characteristics of physical objects, there may be some disagreement about the best methods to use but there is little disagreement about which dimensions are being measured. We know that we are measuring length when we use a ruler and we know that we are measuring temperature when we use a thermometer. It is true that heating some materials makes them expand but we are virtually never confused about whether heat and length represent distinct dimensions that are independent of each other. That is, they are independent of each other in the sense that things can be cold and long, cold and short, hot and long, or hot and short.\nUnfortunately, we are not nearly as clear about what we are measuring when we attempt to measure psychological dimensions such as personality traits, motivations, beliefs, attitudes, and cognitive abilities. Psychologists often disagree not only about what to name these dimensions but also about how many dimensions there are to measure. For example, you might think that there exists a personality trait called niceness. Another person might disagree with you, arguing that niceness is a vague term that lumps together 2 related but distinguishable traits called friendliness and kindness. Another person could claim that kindness is too general and that we must separate kindness with friends from kindness with strangers.\nAs you might imagine, these kinds of arguments can quickly lead to hypothesizing the existence of as many different traits as our imaginations can generate. The result would be a hopeless confusion among psychological researchers because they would have no way to agree on what to measure so that they can build upon one another’s findings. Fortunately, there are ways to put some limits on the number of psychological dimensions and come to some degree of consensus about what should be measured. One of the most commonly used of such methods is called factor analysis.\nAlthough the mathematics of factor analysis is complicated, the logic behind it is not difficult to understand. The assumption behind factor analysis is that things that co-occur tend to have a common cause. For example, fevers, sore throats, stuffy noses, coughs, and sneezes tend to occur at roughly the same time in the same person. Often, they are caused by the same thing, namely, the virus that causes the common cold. Note that although the virus is one thing, its manifestations are quite diverse. In psychological assessment research, we measure a diverse set of abilities, behaviors and symptoms and attempt to deduce which underlying dimensions cause or account for the variations in behavior and symptoms we observe in large groups of people. We measure the relations between various behaviors, symptoms, and test scores with correlation coefficients and use factor analysis to discover patterns of correlation coefficients that suggest the existence of underlying psychological dimensions.\nAll else being equal, a simple theory is better than a complicated theory. Therefore, factor analysis helps us discover the smallest number of psychological dimensions (i.e., factors) that can account for the correlation patterns in the various behaviors, symptoms, and test scores we observe. For example, imagine that we create 4 different tests that would measure people’s knowledge of vocabulary, grammar, arithmetic, and geometry. If the correlations between all of these tests were 0 (i.e., high scorers on one test are no more likely to score high on the other tests than low scorers), then the factor analysis would suggest to us that we have measured 4 distinct abilities and no simplification of the data is possible. The correlations between all the tests are displayed in Table 1.\nTable 1: Correlation Matrix of Academic Tests\nIn Figure 1, the theoretical model that would be implied is that there are 4 abilities (shown as circles) that influence performance on 4 tests (shown as squares). The numbers beside the arrows imply that the abilities and the tests have high but imperfect correlations of 0.9.\nFigure 1: Four Independent Tests\nOf course, you probably recognize that it is very unlikely that the correlations between these tests would be 0. Therefore, imagine that the correlation between the vocabulary and grammar tests is quite high: .81. This means that high scorers on vocabulary are likely to also score high on grammar and low scorers on vocabulary are likely to score low on grammar. The correlation between arithmetic and geometry is .81 also. Furthermore, the correlations between the language tests and the mathematics tests is 0. The new correlation matrix is in Table 2.\nTable 2: Correlation Matrix of Academic Tests\nFactor analysis would suggest that we have measured not 4 distinct abilities but rather 2 abilities. Researchers interpreting the results of the factor analysis would have to use their best judgment to decide what to call these 2 abilities. In this case, it would seem reasonable to call them language ability and mathematical ability. These 2 abilities (shown below as circles in Figure 2) influence performance on 4 tests (shown as squares).\nFigure 2: Two Independent Academic Abilities\nNow imagine that the correlations between all 4 tests is equally high, as shown in Table 3. That is, for example, vocabulary is just as strongly correlated with geometry as it is with grammar.\nTable 3: Correlation Matrix of Academic Tests\nIn this case, factor analysis would suggest that the simplest explanation for this pattern of correlations is that there is just 1 factor that causes all of these tests to be equally correlated. We might call this factor general academic ability, as shown in Figure 3.\nFigure 3: General Academic Ability\nIn reality, if you were to actually measure these 4 abilities, the results would not be so clear. It is likely that all of the correlations would be positive and substantially above 0. It is also likely that the language subtests would correlate more strongly with each other than with the mathematical subtests. The new correlation matrix is in Table 4.\nTable 4: Correlation Matrix of Academic Tests\nGiven the correlations in Table 4, factor analysis would suggest that language and mathematical abilities are distinct but not entirely independent from each other. That is, language abilities and mathematics abilities are substantially correlated with each other. Factors can be correlated for a variety of reasons, but one possibility is that there is a general academic (or intellectual) ability that influences performance in all academic areas. Figure 4, abilities are arranged in hierarchies with general abilities influencing narrow abilities.\nFigure 4: A Hierarchical Model of Academic Abilities"
  },
  {
    "objectID": "AssessingPsyche/2014-01-13-gentle-introduction-to-factor-analysis/index.html#all-of-the-tests-measure-the-same-ability",
    "href": "AssessingPsyche/2014-01-13-gentle-introduction-to-factor-analysis/index.html#all-of-the-tests-measure-the-same-ability",
    "title": "A Gentle, Non-Technical Introduction to Factor Analysis",
    "section": "All of the tests measure the same ability",
    "text": "All of the tests measure the same ability\nA graphical representation of a hypothesis in confirmatory factor analysis is called a path diagram. Tests are drawn with rectangles and hypothetical factors are drawn with ovals. The correlations between tests and factors are drawn with arrows. The path diagram for this hypothesis would look like Figure 5.\n\n\n\n\n\n\n\n\nFigure 5: General Working Memory"
  },
  {
    "objectID": "AssessingPsyche/2014-01-13-gentle-introduction-to-factor-analysis/index.html#both-digits-forward-and-digits-backward-measure-short-term-memory-storage-capacity-and-are-distinct-from-executive-control.",
    "href": "AssessingPsyche/2014-01-13-gentle-introduction-to-factor-analysis/index.html#both-digits-forward-and-digits-backward-measure-short-term-memory-storage-capacity-and-are-distinct-from-executive-control.",
    "title": "A Gentle, Non-Technical Introduction to Factor Analysis",
    "section": "Both Digits Forward and Digits Backward measure short-term memory storage capacity and are distinct from executive control.",
    "text": "Both Digits Forward and Digits Backward measure short-term memory storage capacity and are distinct from executive control.\nThe path diagram would look like this (the curved arrow allows for the possibility that the 2 factors might be correlated)\n\n\n\n\n\n\n\n\nFigure 6: Two-Factors of Working Memory, Model 1"
  },
  {
    "objectID": "AssessingPsyche/2014-01-13-gentle-introduction-to-factor-analysis/index.html#digits-forward-and-digits-backward-measure-different-abilities.",
    "href": "AssessingPsyche/2014-01-13-gentle-introduction-to-factor-analysis/index.html#digits-forward-and-digits-backward-measure-different-abilities.",
    "title": "A Gentle, Non-Technical Introduction to Factor Analysis",
    "section": "Digits Forward and Digits Backward measure different abilities.",
    "text": "Digits Forward and Digits Backward measure different abilities.\nIf Digits Forward is primarily a measure of Short-Term Storage, and Digits Backward is primary a measure of Executive control, the path diagram would look like Figure 7\n\n\n\n\n\n\n\n\nFigure 7: Two-Factors of Working Memory, Model 2\n\n\n\n\n\nConfirmatory factor analysis produces a number of statistics, called fit statistics that tell us which of the models or hypotheses we tested are most in agreement with the data. Studying the results, we can select the best model or perhaps generate a new model if none of them provide a good “fit” with the data. With structural equation modeling, a procedure that is very similar to confirmatory factor analysis, we can test extremely complex hypotheses about the structure of psychological variables.\n\n\n\n\n\n\nNoteNote\n\n\n\nOriginally posted 2014-01-13 on AssessingPsyche. The article has been revised several times since it was written originally for Cohen & Swerdlik’s Psychological Testing and Assessment: An Introduction To Tests and Measurement (Ninth Edition)"
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "",
    "text": "Conflict of Interest Statement: ATP Assessments, the publisher of the RESCA-E, commissioned me to write a descriptive account of the likely relations among the RESCA-E subtests and CHC Theory constructs. I billed them for the hours that I spent researching this topic and writing my thoughts on the matter. However, I was so impressed the the RESCA-E that I wanted to write a short review of it, and I also conducted additional analyses about its structure. Because ATP Assessments did not ask for my opinion about the quality of the RESCA-E nor for the statistical analyses I conducted, I did not bill for the many additional hours I spent on these activities. If I were not impressed with the RESCA-E, this document would have been much shorter.”)\nThe Receptive, Expressive & Social Communication Assessment–Elementary (RESCA-E) is a new measure of language abilities for children in the elementary school years. The purpose of this review is to evaluate the RESCA-E in terms of the Cattell-Horn-Carroll Theory of Cognitive Abilities [CHC theory; McGrew (2005); Schneider & McGrew (2012)]. However, some preliminary remarks about the test’s design are in order.\n\n\nModest elegance, by its nature, attracts little praise. I will do my part here to rectify this injustice. The RESCA-E test materials, stimuli, and protocols are designed for practical efficiency but sacrifice nothing in aesthetic appeal. This might not seem to matter, but it does. Spending time with ugly, frustrating test materials makes one yearn for early retirement.\nThe application of sound typographical principles has enhanced the readability and ease of use of the protocol; the whole document is thoughtfully coded by font, color, and shading. The protocol does not feel cramped; it has generous space for notes, yet no space is wasted. Sure, the designers could have shortened the protocol by making everything smaller and more compact, but that would have been penny wise, pound foolish. This same care and consistency was extended to everything in the test kit.\n\n\n\nI do not know the test’s authors, Patricia Hamaguchi and Deborah Ross-Swain, and I have had no contact with them. Yet, I can tell something about their work process and their scholarly values. To someone who has never tried to design an ability test, it may not be obvious that the RESCA-E subtest items were labored over for untold hours until they were just right. In most test batteries I find several items (or whole subtests) that seem a bit off, like bum notes in a singer’s solo. I found none here. The items are so smoothly written that they draw no attention to themselves—no small feat.\nEven more importantly, the item content reflects a deep understanding on the part of the authors of what matters in the evaluation of children. No item is merely easy or merely difficult, chosen to meet some psychometric need. No, each item is intended to measure something substantial and relevant to everyday functioning. A rare patience was required to keep working with each item until it was easy to understand, quick to administer, and simple to score, all the while remaining clinically relevant, yet psychometrically sound. For this accomplishment, Patricia Hamaguchi, Deborah Ross-Swain, and their associates at ATP Assessments deserve a tip of the hat and hearty congratulations."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#first-impressions",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#first-impressions",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "",
    "text": "Modest elegance, by its nature, attracts little praise. I will do my part here to rectify this injustice. The RESCA-E test materials, stimuli, and protocols are designed for practical efficiency but sacrifice nothing in aesthetic appeal. This might not seem to matter, but it does. Spending time with ugly, frustrating test materials makes one yearn for early retirement.\nThe application of sound typographical principles has enhanced the readability and ease of use of the protocol; the whole document is thoughtfully coded by font, color, and shading. The protocol does not feel cramped; it has generous space for notes, yet no space is wasted. Sure, the designers could have shortened the protocol by making everything smaller and more compact, but that would have been penny wise, pound foolish. This same care and consistency was extended to everything in the test kit."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#looking-deeper",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#looking-deeper",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "",
    "text": "I do not know the test’s authors, Patricia Hamaguchi and Deborah Ross-Swain, and I have had no contact with them. Yet, I can tell something about their work process and their scholarly values. To someone who has never tried to design an ability test, it may not be obvious that the RESCA-E subtest items were labored over for untold hours until they were just right. In most test batteries I find several items (or whole subtests) that seem a bit off, like bum notes in a singer’s solo. I found none here. The items are so smoothly written that they draw no attention to themselves—no small feat.\nEven more importantly, the item content reflects a deep understanding on the part of the authors of what matters in the evaluation of children. No item is merely easy or merely difficult, chosen to meet some psychometric need. No, each item is intended to measure something substantial and relevant to everyday functioning. A rare patience was required to keep working with each item until it was easy to understand, quick to administer, and simple to score, all the while remaining clinically relevant, yet psychometrically sound. For this accomplishment, Patricia Hamaguchi, Deborah Ross-Swain, and their associates at ATP Assessments deserve a tip of the hat and hearty congratulations."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#comprehension-of-vocabulary",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#comprehension-of-vocabulary",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Comprehension of Vocabulary",
    "text": "Comprehension of Vocabulary\nThe examiner shows four drawings to the examinee and says a word. The examinee points to the one best depicts the word.2 The advantage of this format is that it cleanly measures the CHC narrow ability Lexical Knowledge, a facet of Comprehension/Knowledge (Gc). The “disadvantage” of the multiple choice format is that its loading on general intelligence is likely to be smaller than with tests with more open-ended formats (e.g., WISC-V Vocabulary). Why? No judgment is required to decide which aspects of a definition to emphasize. Wechsler’s (1958) goal was never to measure things like knowledge per se, but to measure intelligence in all its integrative, glorious complexity.3 However, when your goal is to measure word knowledge, not judgment, this item format is exactly what you want.\n2 Similar to the Peabody Picture Vocabulary Test, Fourth Edition and the Receptive One-Word Picture Vocabulary Test-43 Wechsler (1958, p. 15) wrote, “Then, when an examiner employs an arithmetic or a vocabulary test as part of an intelligence scale, the object of the examiner is not to discover the subject’s aptitude for arithmetic or extent of his word knowledge, although these are inevitably involved, but his capacity to function in overall areas which are assumed to require intelligence.”All picture vocabulary tests start with simple objects one encounters frequently (e.g., spoon, ball, dog). Where the test designers go from there matters quite a bit. There are two ways to make a picture vocabulary test more difficult:\n\nShow pictures of increasingly unusual objects (e.g., fob, lappets, ait, manometer).\nShow pictures illustrating increasingly complex concepts (e.g., relieved, hesitant, shrewd, intimacy) or increasingly subtle distinctions between related words (e.g., ask vs. beg, sad vs. sobbing, consider vs. ponder).\n\nWhich approach do you think is more applicable to everyday life? Me, too. Fortunately, this is the approach that the RESCA-E takes, with more difficult items focusing mostly on emotions, interpersonal relations, measurements, and abstractions. None of the words are particularly unusual, technical, or esoteric."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#comprehension-of-oral-directions",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#comprehension-of-oral-directions",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Comprehension of Oral Directions",
    "text": "Comprehension of Oral Directions\nThis measure is similar to the Comprehension of Vocabulary subtest in its format. The examiner shows four drawings to the examinee and says a sentence that contains an instruction. The examinee chooses the picture that is consistent with the instruction. For example, the sentence might be, “You may not ride bicycles in the park.” The four pictures might be various configurations of a child, a bicycle, and a park. The correct answer might be a picture of child in the park with a bicycle chained outside its gates.\nThe items of this test are ingeniously designed to avoid problems in similar tests of oral direction comprehension. Some tests give increasingly long directions that tax working memory instead of comprehension per se.4 This subtest minimizes working memory load by presenting directions that rarely have more than three parts. Items become increasingly difficult mostly because of the complexity of the command rather than its length.5\n4 Which is fine, if we wish to measure the degree to which working memory deficits interfere with comprehension.5 For example, “Write a letter that has only curved lines or only straight lines, but not both kinds of lines. Which letter could be written? [shows letters] B, D, O, or P”6 You might ask, “If CHC theory has holes in it, why not just fill them?” Good question. Theory building, especially the assembly of a comprehensive taxonomy, must be a slow, deliberate, systematic process. Adding new features that are not well validated into the taxonomy will undermine trust in its utility.In terms of CHC theory, it appears that this subtest measures Listening Ability, a facet of Gc. However, it seems likely that Carroll’s Listening Ability factor comprises multiple subfactors, and that this subtest measures a narrow subset of them (e.g., understanding of English syntax). Thus, the model of language that underlies RESCA-E is more elaborated than the model of language ability in CHC theory. Ideally, research using instruments like the RESCA-E will refine and extend CHC theory’s treatment of language abilities.6"
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#comprehension-of-stories-and-questions",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#comprehension-of-stories-and-questions",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Comprehension of Stories and Questions",
    "text": "Comprehension of Stories and Questions\nIn this subtest, the examiner reads a short story to the examinee and asks questions about the examinee’s understanding of it. For each question, there are four possible answers that the examinee can point to (either words that the examiner reads aloud or pictures that are shown). The passages are carefully crafted to be stories rather than barely concealed lists of random details and events. To answer the questions correctly, the examinee must understand the gist of the story rather than recall highly specific details. Thus, it is a true comprehension test rather than a memory test.\nComprehension of Stories and Questions is, like Comprehension of Oral Directions, a measure of Listening Ability, but less about syntax and more about semantics. To some degree, it is also a measure of Meaningful Memory (Gl)."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#comprehsion-of-basic-morphology-and-syntax",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#comprehsion-of-basic-morphology-and-syntax",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Comprehsion of Basic Morphology and Syntax",
    "text": "Comprehsion of Basic Morphology and Syntax\nLike the other receptive language subtests, this supplementary subtest is a multiple-choice test. It is cleverly designed to measure a child’s understanding of syntax and morphology. For example, to measure one’s understanding of plurality, the prompt could be “The birds are swimming.” The child has to choose among pictures of birds swimming, but only one picture has more than one bird. In similar fashion, understanding of a variety of other features of English are tested: past vs. present vs. future tense, negation (e.g., none, not, never), prepositions (e.g., on, in, above, between), gender (he vs. she, him vs. her), singular vs. plural, self vs. other (me vs. her), before vs. after, active vs. passive voice (e.g., the boy touched the dog, the boy was touched by the dog).\nThis subtest is another measure of Listening Ability but with a focus on syntax. From the name of Carroll’s Grammatical Sensitivity factor, it might seem that this is what Comprehension of Basic Morphology and Syntax measures. However, the Grammatical Sensitivity ability factor was a measure of formal knowledge of grammar. Obviously, knowing formal grammatical rules will help on this test but that is not what is being measured here directly."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#executing-oral-directions",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#executing-oral-directions",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Executing Oral Directions",
    "text": "Executing Oral Directions\nThis supplementary test is of obvious importance. It can resolve questions such as, “Does the child understand simple commands?” The subtest is similar to and is most correlated with Comprehension of Oral Directions (r = .52). It differs from that test in that it is not a multiple-choice test. Instead, it requires the examinee to follow simple commands at first (e.g., “Touch your knee. Go.”) and increasingly complex sentences at the end of the test (e.g., “Stand up and walk to the door. When you get there, knock on it three times or open it, but not both. Go.”). Near the end of the test, the commands are long and thus the working memory demands are high."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#expressive-labeling-of-vocabulary",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#expressive-labeling-of-vocabulary",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Expressive Labeling of Vocabulary",
    "text": "Expressive Labeling of Vocabulary\nLike the Comprehension of Vocabulary subtest, this test measures Lexical Knowledge. On some items examinee is asked what object is in a picture. In many confrontational naming tests, the objects in the pictures become increasingly unusual. Not so, here. As with the Comprehension of Vocabulary subtest, the pictures measure knowledge of words related to abstract concepts, emotions, and interpersonal relations.\nAlthough Comprehension of Vocabulary (CV) and Expressive Labeling of Vocabulary (ELV) have the highest correlation of all the RESCA-E subtests (r = .6), the correlation is not so high that they are redundant. In my opinion, the RESCA-E should have a Vocabulary composite score consisting of these tests. Using formulas explained in Schneider (2013), you can compute a custom vocabulary composite score like so:\nVocabulary Composite = 2.795(CV + ELV) − 44.1"
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#expressive-skills-for-describing-and-explaining",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#expressive-skills-for-describing-and-explaining",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Expressive Skills for Describing and Explaining",
    "text": "Expressive Skills for Describing and Explaining\nGuilford (1967) distinguished between tests requiring convergent production (i.e., a single answer is correct) and tests requiring divergent production (e.g., the examinee gives as many correct answers, such as naming as many ice cream flavors as possible within the time allotted). Cattell originally grouped divergent production tests with crystallized intelligence. The reason for this is that his tests had generous time limits so that the tests were measures of how much information was in the person’s knowledge banks instead how fast it could be pulled out of memory. The results will be quite different if one is given 1 minute to name as many words as possible ending in -tion compared to the same task but with a 5-minute time limit. With generous time limits, the test becomes more like a breadth-of-vocabulary test rather than a memory retrieval speed test. Almost all commercially available tests of divergent production have short time limits and thus function as memory retrieval speed tests.\nIn this subtest, the examinee is shown a picture (e.g., a family preparing a meal) or given a scenario (e.g., a child getting ready for bed) and the examinee is prompted to tell the examiner everything he or she knows about this situation. Unlike many divergent processing tests (e.g., COWAT), the child does not get 1 point for every answer. Instead there is a checklist of criteria for scoring points. In general, the examinee is awarded points for mentioning aspects of the picture or scenario that are most salient or of central importance.\nThis is a test paradigm I have never seen before. If it is indeed novel, it is potentially a major advance. It is rare in life to have to name as many exemplars of a category as possible (e.g., sports, furniture, animals, words that begin with H). In contrast, spontaneously describing the most salient aspects of a situation is a hallmark of intelligence. I look forward to seeing validation efforts to evaluate this paradigm’s utility.\nThis subtest has relatively small correlations with the other tests (r in the .2–.3 range), suggesting that it is not merely a Gc test, though it is undoubtedly influenced by Gc. In CHC theory, there is a little-understood narrow ability called Associational Fluency in the Gr (Memory Retrieval Fluency) broad ability cluster. It is distinguished from the better-known Ideational Fluency factor in that the quality of the responses matters more than the number. Given how points are awarded for mentioning important aspects of a picture or scenario, it seems likely that this is what is being measured. It also seems like that having general knowledge, an intermediate factor within Gc, would help a person perform well on this test."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#narrative-skills",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#narrative-skills",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Narrative Skills",
    "text": "Narrative Skills\nIn this subtest, the examinee is prompted to tell the gist of a narrative that the examiner read. Another type of item involves telling the examiner about an experience (e.g., taking a long trip in a car or bus). As with the Expressive Skills for Describing and Explaining subtest, the examiner scores the response for the quality of the answers, not for how much is said. It may sound from this description that this subtest is a bear to score, but it is quite straightforward.\nThe Narrative Skills subtest is most highly correlated with Expressive Skills for Describing and Explaining. It seems likely that it too is a measure of Associational Fluency. It does not seem to draw on background knowledge to the same degree and instead requires Meaningful Memory. Supporting this interpretation is the fact that its second-highest correlation is with Comprehension of Stories and Questions, which is also hypothesized to be influenced by Meaningful Memory."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#expressive-use-of-basic-morphology-and-syntax",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#expressive-use-of-basic-morphology-and-syntax",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Expressive Use of Basic Morphology and Syntax",
    "text": "Expressive Use of Basic Morphology and Syntax\nIn this subtest, the examiner prompts the examinee to answer questions about pictures. The questions are cleverly worded so that the examinee’s answers are expected to conform to certain syntactical rules (e.g., “This girl here is running. What is this other girl doing? Answer: Walking”).\nGiven the similarity in names, it seems reasonable to suppose that this subtest would be strongly correlated with Comprehension of Basic Morphology and Syntax. Its correlation is only r = .45, a value that is similar to the correlations it has with many other subtests. Factor analyses of the RESCA-E reveal no evidence that there is a special relationship between these two measures of morphology and syntax understanding. Examining the pattern of critical difference scores in the Technical Manual, it appears that the correlation between these two tests decreases with age. Why this happens deserves scrutiny. One likely explanation is that both tests have very low ceilings for older children, which means that the scores are imprecise for children with average or better understanding of morphology and syntax.7\n7 A test with a low ceiling does not have enough difficult items to distinguish reliably among high-ability examinees. Low ceilings are a major problem for intelligence tests, but are not so problematic for tests like the RESCA-E, which are designed to identify children with language deficits rather than designed to identify the superstars of syntactic sophistication.In terms of CHC theory, it appears that this subtest measures aspects of Communication Ability, a little-researched factor. As with Comprehension of Basic Morphology & Syntax, it seems likely that Grammatical Sensitivity also influences performance on the test."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#comprehension-of-body-language-and-vocal-emotion",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#comprehension-of-body-language-and-vocal-emotion",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Comprehension of Body Language and Vocal Emotion",
    "text": "Comprehension of Body Language and Vocal Emotion\nIn this subtest, the examiner shows the examinee four pictures of people making different gestures or with different facial expressions. The examiner plays an audio CD that asks a question like, “Which person is thinking, ‘I am confused.’” and the examinee picks the picture of the person who looks confused. The items are well designed and toward the end of the subtest assess fairly subtle social signals. This subtest would fit it with emotional intelligence tests like the MSCEIT’s (Mayer et al., 2002) measures of emotion perception.\nBecause of Guildford’s work in social intelligence, Carroll’s model has a factor called Knowledge of Behavioral Content, an aspect of achievement. However, there is already strong evidence that social and emotional reasoning have several narrow ability factors associated with them (Mayer et al., 2008). It is not clear where these factors belong in CHC theory, but the evidence keeps pouring in that these factors matter. Sooner or later, CHC theory is going to have to provide a more nuanced account of aspects of social and emotional intelligence (MacCann et al., 2014; Schneider et al., 2016)."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#social-and-language-inference",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#social-and-language-inference",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Social and Language Inference",
    "text": "Social and Language Inference\nIn this test, the examiner presents a scenario in which someone uses indirect or idiomatic language. In many items, the examinee selects the correct answer from four answer choices. For example, two children are talking in class and the teacher says, “Hey, knock it off, you two!” The correct inference will be that the teacher wants the children to stop talking.\nIn terms of CHC theory, this seems to be a general Language Development measure with emphasis on Knowledge of Behavioral Content as well."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#situational-language-use",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#situational-language-use",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Situational Language Use",
    "text": "Situational Language Use\nIn this subtest, the examiner presents the examinee with a scenario and prompts the examinee to say how he or she would respond in that situation. For example, “Your mother introduces you to a woman you do not know. The woman smiles and extends her hand, saying ‘Pleased to meet you.’ How would you respond in a friendly and polite manner?” The examinee’s response is scored according to straightforward criteria.\nIn terms of CHC theory, this subtest measures Communication Ability as well as Knowledge of Behavioral Content."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#elicited-body-language",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#elicited-body-language",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Elicited Body Language",
    "text": "Elicited Body Language\nIn this subtest, the examiner asks the examinee to act out various common situations (e.g., Pretend you just took a bite of your favorite food. Pretend you accidentally hurt your finger. Pretend you are listening to someone who whispers a surprising secret.). This test may supplant the SB5 Verbal Absurdities as the most delightful test to administer.\nOnce again, CHC theory’s taxonomy is too sparse in this domain to explain what is going on in this subtest. Nevertheless, the narrow ability that is being measured is mostly likely Knowledge of Behavioral Content."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#exploratory-factor-analysis",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#exploratory-factor-analysis",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Exploratory Factor Analysis",
    "text": "Exploratory Factor Analysis\nI used the RESCA-E correlation matrix for the entire standardization sample (N = 825) to conduct all analyses. I removed the Social Communication Inventory scale from consideration because it is not an ability test. It has low correlations with the other tests and thus would produce an uninformative singleton factor.\nTo see how many factors to extract, I conducted a parallel analysis using the psych package (Revelle, 2016) in R. I used the type of parallel analysis based on principal factors rather than the more commonly-used principal components method because it is more accurate when there is a large general factor (Crawford et al., 2010). As seen in Figure 2, the results suggest that extracting five principal factors is a reasonable choice.\n\n\n\n\n\n\n\n\nFigure 2: Principal Factor Analysis–Based Parallel Analysis of the RESCA-E Subtests\n\n\n\n\n\nI extracted 1, 2, 3, 4, and then 5 principal factors with an oblimin rotation (See Tables 1–5). In no solution did the three language domains—expressive, receptive, and social communication—hang together. Does this result mean that the structure of RESCA-E is not valid? No, but it suggests that the three ability domains are not the same kind of constructs that factor-analysts are used to. The three language domains are not cohesive clusters of relatively unitary abilities, and the subtests in each of the three composite scores will often not hang together as closely as factor-based composites would.\n\n\n\nTable 2. Factor loadings of RESCA-E subtests in a principal factor analysis with oblimin rotation (1-factor solution)\n\n\n\nDomain\nPA1\n\n\n\n\nComprehension of Stories and Questions\nReceptive\n.75\n\n\nExpressive Labeling of Vocabulary\nExpressive\n.75\n\n\nComprehension of Oral Directions\nReceptive\n.73\n\n\nComprehension of Vocabulary\nReceptive\n.72\n\n\nSocial and Language Inference\nSocial\n.69\n\n\nSituational Language Use\nSocial\n.68\n\n\nExpressive Use of Basic Morphology and Syntax\nExpressive\n.66\n\n\nComprehension of Basic Morphology and Syntax\nReceptive\n.65\n\n\nNarrative Skills\nExpressive\n.63\n\n\nExecuting Oral Directions\nReceptive\n.61\n\n\nComprehension of Body Language and Vocal Emotion\nSocial\n.59\n\n\nElicited Body Language\nSocial\n.54\n\n\nExpressive Skills for Describing and Explaining\nExpressive\n.54\n\n\n\n\n\n\nTable 3. Factor loadings of RESCA-E subtests in a principal factor analysis with oblimin rotation (2-factor solution)\n\n\n\n\n\n\n\n\n\nDomain\nPA1\nPA2\n\n\n\n\nSocial and Language Inference\nSocial\n.80\n\n\n\nComprehension of Basic Morphology and Syntax\nReceptive\n.77\n\n\n\nComprehension of Vocabulary\nReceptive\n.73\n\n\n\nComprehension of Oral Directions\nReceptive\n.72\n\n\n\nComprehension of Stories and Questions\nReceptive\n.67\n\n\n\nExpressive Labeling of Vocabulary\nExpressive\n.63\n\n\n\nExecuting Oral Directions\nReceptive\n.59\n\n\n\nExpressive Use of Basic Morphology and Syntax\nExpressive\n.57\n\n\n\nComprehension of Body Language and Vocal Emotion\nSocial\n.55\n\n\n\nSituational Language Use\nSocial\n.47\n.28\n\n\nElicited Body Language\nSocial\n.33\n.28\n\n\nNarrative Skills\nExpressive\n\n.75\n\n\nExpressive Skills for Describing and Explaining\nExpressive\n\n.70\n\n\n\n\n\n\nTable 4. Factor loadings of RESCA-E subtests in a principal factor analysis with oblimin rotation (3-factor solution)\n\n\n\n\n\n\n\n\n\n\nDomain\nPA1\nPA2\nPA3\n\n\n\n\nComprehension of Vocabulary\nReceptive\n.76\n\n\n\n\nExpressive Labeling of Vocabulary\nExpressive\n.74\n\n\n\n\nSocial and Language Inference\nSocial\n.74\n\n\n\n\nComprehension of Stories and Questions\nReceptive\n.69\n\n\n\n\nComprehension of Basic Morphology and Syntax\nReceptive\n.68\n\n\n\n\nComprehension of Oral Directions\nReceptive\n.65\n\n\n\n\nComprehension of Body Language and Vocal Emotion\nSocial\n.53\n\n\n\n\nExpressive Skills for Describing and Explaining\nExpressive\n\n.75\n\n\n\nNarrative Skills\nExpressive\n\n.61\n.23\n\n\nSituational Language Use\nSocial\n\n.20\n.55\n\n\nElicited Body Language\nSocial\n\n.20\n.44\n\n\nExpressive Use of Basic Morphology and Syntax\nExpressive\n.32\n\n.39\n\n\nExecuting Oral Directions\nReceptive\n.36\n\n.37\n\n\n\n\n\n\nTable 5. Factor loadings of RESCA-E subtests in a principal factor analysis with oblimin rotation (4-factor solution)\n\n\n\n\n\n\n\n\n\n\n\nDomain\nPA1\nPA3\nPA2\nPA4\n\n\n\n\nComprehension of Vocabulary\nReceptive\n.78\n\n\n\n\n\nExpressive Labeling of Vocabulary\nExpressive\n.75\n\n\n\n\n\nSocial and Language Inference\nSocial\n.69\n\n\n\n\n\nComprehension of Stories and Questions\nReceptive\n.65\n\n\n\n\n\nComprehension of Basic Morphology and Syntax\nReceptive\n.62\n\n\n\n\n\nComprehension of Oral Directions\nReceptive\n.56\n\n\n.24\n\n\nComprehension of Body Language and Vocal Emotion\nSocial\n.54\n\n\n\n\n\nExpressive Use of Basic Morphology and Syntax\nExpressive\n.32\n.25\n\n\n\n\nSituational Language Use\nSocial\n\n.93\n\n\n\n\nElicited Body Language\nSocial\n\n.34\n\n\n\n\nExpressive Skills for Describing and Explaining\nExpressive\n\n\n.71\n\n\n\nNarrative Skills\nExpressive\n\n\n.68\n\n\n\nExecuting Oral Directions\nReceptive\n\n\n\n.79\n\n\n\n\n\n\nTable 6. Factor loadings of RESCA-E subtests in a principal factor analysis with oblimin rotation (5-factor solution)\n\n\n\n\n\n\n\n\n\n\n\n\nDomain\nPA1\nPA2\nPA5\nPA3\nPA4\n\n\n\n\nComprehension of Vocabulary\nReceptive\n.73\n\n\n\n\n\n\nExpressive Labeling of Vocabulary\nExpressive\n.65\n\n\n\n\n\n\nComprehension of Stories and Questions\nReceptive\n.52\n\n\n\n\n\n\nSocial and Language Inference\nSocial\n.51\n\n.24\n\n\n\n\nComprehension of Body Language and Vocal Emotion\nSocial\n.49\n\n\n\n.23\n\n\nComprehension of Basic Morphology and Syntax\nReceptive\n.39\n\n.31\n\n\n\n\nExpressive Skills for Describing and Explaining\nExpressive\n\n.79\n\n\n\n\n\nNarrative Skills\nExpressive\n\n.65\n\n\n\n\n\nComprehension of Oral Directions\nReceptive\n\n\n.68\n\n\n\n\nExecuting Oral Directions\nReceptive\n\n\n.51\n\n.29\n\n\nElicited Body Language\nSocial\n\n\n\n.83\n\n\n\nSituational Language Use\nSocial\n\n\n\n.33\n.28\n\n\nExpressive Use of Basic Morphology and Syntax\nExpressive\n\n\n\n\n.52"
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#might-mulitdimensional-scaling-detect-a-faceted-relationship-among-the-three-language-domains",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#might-mulitdimensional-scaling-detect-a-faceted-relationship-among-the-three-language-domains",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Might Mulitdimensional Scaling Detect a Faceted Relationship Among the Three Language Domains?",
    "text": "Might Mulitdimensional Scaling Detect a Faceted Relationship Among the Three Language Domains?\nIs there some sort of faceted relationship that the three language domains might have that factor analysis cannot detect (e.g., process × domain)? If so, it should show up in a multidimensional scaling (MDS). Using the correlations subtracted from 1 as distances, a two-dimensional MDS suggests no obvious cohesion of the three domains (See Figure 3).\n\n\n\n\n\n\n\n\nFigure 3: Two-dimensional Multidimensional Scaling of the RESCA-E Subtests. Blue = Receptive, Red = Expressive, Green = Social Communication\n\n\n\n\n\nPerhaps there is order that can be seen in three dimensions that cannot be detected in two. You can grab Figure 4 with your mouse and rotate it. I am unable to detect any patterns that show the three domains to be elegantly cohesive.\n\n\n\n\n\n\n\n\nFigure 4: Three-dimensional Multidimensional Scaling of the RESCA-E Subtests. Blue = Receptive, Red = Expressive, Green = Social Communication"
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#confirmatory-factor-analysis-to-the-rescue",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#confirmatory-factor-analysis-to-the-rescue",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Confirmatory Factor Analysis to the rescue?",
    "text": "Confirmatory Factor Analysis to the rescue?\nThese previous analyses were exploratory analyses. Would confirmatory factor analysis (CFA) have supported the hypothesis that the three domains are cohesive? No, but it might have given the illusion of such a finding.\nIf we conduct a one-factor CFA (Figure 5) and compare it to a three-factor CFA (Figure 6), the model fit improves significantly (p &lt; .001). The loadings all look healthy in Figure 6. What is the problem? The problem is that the factors are almost perfectly correlated, meaning that if the latent variables could be measured without error, there would be little point in distinguishing among them. Had Figure 5.1 of the RESCA-E Technical Manual shown the correlations among the latent factors, this would have been plainly visible (Compare with Figure 7). In contrast, the correlations among the three-factor EFA latent variables are between .62, and .84, which means that they do not yield redundant information.\n\n\n\n\n\n\n\n\nFigure 5: General-factor CFA of RESCA-E Subtests\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Three-factor CFA of RESCA-E Subtests\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Three-factor CFA of RESCA-E Core Subtests"
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#conclusion-the-three-resca-e-language-domains-are-descriptive-categories-not-distinct-abilities",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#conclusion-the-three-resca-e-language-domains-are-descriptive-categories-not-distinct-abilities",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Conclusion: The Three RESCA-E Language Domains Are Descriptive Categories, Not Distinct Abilities",
    "text": "Conclusion: The Three RESCA-E Language Domains Are Descriptive Categories, Not Distinct Abilities\nThe available evidence suggests that the distinction between receptive, expressive, and social communication abilities is mostly descriptive. There are language abilities that are legitimately classified as receptive or expressive, or social, but no cohesive ability constructs we could call receptive language ability, expressive language ability, or social communication ability. If Carroll could not find them with 400+ data sets, we should not be surprised to not find them here in this one. The three RESCA-E composites should be interpreted in this light."
  },
  {
    "objectID": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#alternative-structure",
    "href": "AssessingPsyche/2016-10-26-rescae/RESCAE.html#alternative-structure",
    "title": "The RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs",
    "section": "Alternative Structure",
    "text": "Alternative Structure\nA cluster analysis can sometimes detect patterns that exploratory factor analyses miss. First, we see the correlation plot (Figure 8), with rectangles around 4 clusters (Ward’s method applied to the correlations subtracted from 1). Again, the pattern is not consistent with the 3 groupings.\n\n\n\n\n\n\n\n\nFigure 8: Correlation plot of the RESCA-E Subtests. Blue = Receptive, Red = Expressive, Green = Social Communication\n\n\n\n\n\nInstead, in the dendrogram in Figure 9, four clusters are highlighted plus a very narrow vocabulary cluster is singled out.\n\n\n\n\n\n\n\n\nFigure 9: Hierarchical Cluster Analysis of the RESCA-E Subtest Correlation Matrix (Ward’s Method). Suggested intepretations of clusters in blue.\n\n\n\n\n\nInstead of a three-part division between receptive, expressive, and social communication, this analysis suggests a 2 × 2 matrix (See Figure 10). There are two expressive clusters, one of which emphasizes social communication whereas the other emphasizes narrative communication (not that narrative is non-social). There are two “receptive” clusters, one of which is more social in that it is about understanding others’ intentions, sometimes via understanding English syntax. The other comprehension cluster consists of tests measuring semantic knowledge (e.g., vocabulary) and narratives.\n\n\n\n\n\n\n\n\nFigure 10: Matrix of Concpetual Groupings\n\n\n\n\n\nFigure 11 shows that the 2 × 2 structure suggested by hierarchical cluster analysis is also present in the the 3D MDS model. Grab the picture with your mouse and move it around to see the structure better.\n\n\n\n\n\n\n\n\nFigure 11: A suggested regrouping of the RESCA-E Subtests.\n\n\n\n\nWhy might there be a division between narrative/semantic vs. social/syntactic abilities? I can only speculate. The clumping of narrative and semantic abilities evokes the classic division of declarative memory into episodic memory and semantic memory. Why would social and syntactic subtests clump together? This is deeply speculative on my part, but there is a developing body of evidence that Broca’s Area not only is vital for the comprehension of syntax, but also the intentions of others (Gentilucci et al., 2006; Hamzei et al., 2003). To oversimplify, perhaps the narrative/semantic vs. social/syntactic abilities distinction reflects to some degree the relative health and functioning of Wernicke’s and Broca’s area, respectively."
  },
  {
    "objectID": "assessingpsyche.html",
    "href": "assessingpsyche.html",
    "title": "AssessingPsyche",
    "section": "",
    "text": "Horn & Cattell (1966)\n\n\nA Re-Analysis\n\n\n\n\n\n\n\n\nMay 18, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nOverture Redux\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nThe RESCA-E Subtests Are Thoughtfully Designed and Highly Refined Measures of CHC Constructs\n\n\nA Review of the Receptive, Expressive & Social Communication Assessment–Elementary\n\n\n\n\n\n\n\n\nOct 26, 2016\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nReliability coefficients are for squares\n\n\nConfidence interval widths tell it to you straight.\n\n\n\npsychometrics\n\nreliability\n\nconfidence intervals\n\n\n\nVisualizing the relationship between reliability and confidence interval widths\n\n\n\n\n\nJan 16, 2014\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nA Gentle, Non-Technical Introduction to Factor Analysis\n\n\n\n\n\n\nstatistics\n\n\n\nDiscovering Underlying Dimensions in Data\n\n\n\n\n\nJan 13, 2014\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Covariance\n\n\n\n\n\n\nstatistics\n\n\n\nA visualization of covariance\n\n\n\n\n\nSep 13, 2012\n\n\nW. Joel Schneider\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "W. Joel Schneider, Ph.D.\nProfessor\nTemple University\nPsychological Studies in Education\n\n\n\nCounseling Psychology\n\n\nSchool Psychology\n\n\n\n\n\n Contact Me\n Curriculum Vitae\n ORCID\n Google Scholar\n Open Science Framework\n ResearchGate\n Mastodon\n Zotero\n Stackoverflow\n YouTube\n GitHub\n\n\n\n\n\n\n\nI am a professor at Temple University in the College of Education and Human Development in the Psychological Studies in Education Department. I belong to both the School Psychology and Counseling Psychology programs.\nAlthough I have diverse interests, my primary focus is trying to understand and improve the validity of psychological assessment practices. I teach courses in assessment, counseling, statistics, and research methods.\nI grew up in Southern California (1970–1990) and lived in Córdoba, Argentina (1990–1992). I completed my undergraduate degree in Psychology at the University of California at Berkeley in 1995 and my doctoral studies in clinical psychology at Texas A&M University in 2003. My internship year (2001–2002) was spent in the Dutchess County Department of Behavioral & Community Health in Poughkeepsie, NY. Since then I have been a faculty member at Illinois State University (2002–2017) and Temple University (2017–Present)."
  },
  {
    "objectID": "posts/2023-08-26-making-a-custom-arrowhead-for-ggplot2-using-ggarrow-and-arrowheadr/index.html",
    "href": "posts/2023-08-26-making-a-custom-arrowhead-for-ggplot2-using-ggarrow-and-arrowheadr/index.html",
    "title": "Making a custom arrowhead for ggplot2 using ggarrow and arrowheadr",
    "section": "",
    "text": "Base plot for demonstrations\nlibrary(tidyverse)\n# install.packages(\"remotes\")\n# remotes::install_github(\"teunbrand/ggarrow\")\nlibrary(ggarrow)\n\nnode_radius &lt;- .2\nresection_length &lt;- .03\n\nd_node &lt;- tibble(node = c(\"A\", \"B\", \"C\"),\n       x = c(0, .5, 1),\n       y = c(0, sqrt(3) / 2, 0))\n\nd_edge &lt;- d_node |&gt; \n  mutate(theta_next = atan((y - lag(y)) / (x - lag(x))),\n         x_from = lag(x) + cos(theta_next) * (node_radius + resection_length),\n         y_from = lag(y) + sin(theta_next) * (node_radius + resection_length),\n         x_to = x + cos(theta_next + pi) * (node_radius + resection_length),\n         y_to = y + sin(theta_next + pi) * (node_radius + resection_length)) |&gt; \n  filter(!is.na(x_from))\n\np &lt;- d_edge |&gt;\n  ggplot(aes(x = x_from, y = y_from, xend = x_to, yend = y_to)) +\n  ggforce::geom_circle(aes(\n    x0 = x,\n    y0 = y,\n    r = .2,\n    fill = node\n  ),\n  data = d_node,\n  color = NA, inherit.aes = F) +\n  geom_text(aes(label = node, x = x, y = y), \n            data = d_node, \n            size = 20,\n            family = \"Roboto Condensed\",\n            inherit.aes = F) +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.position = \"none\") +\n     scale_fill_viridis_d(\n       option = \"D\",\n       begin = .2,\n       end = .8,\n       alpha = .5\n     )\n\np\n\n\n\n\n\n\n\n\n\nThe default arrows in ggplot2 are perfectly serviceable. There is the open variety:\n\n\nOpen arrow code\np + geom_segment(arrow = arrow())\n\n\n\n\n\n\n\n\nFigure 1: The ggplot2 open arrow\n\n\n\n\n\nYou can specify which ends of the segment have arrowheads, whether the arrows are open or closed, the length of the arrow, and how sharp the arrow’s point is with the angle argument.\nI happen to prefer a longer, sharper, closed arrow:\n\n\nClosed arrow code\np + geom_segment(\n  arrow = arrow(\n    angle = 15,\n    length = unit(8, \"mm\"),\n    type = \"closed\"\n))\n\n\n\n\n\n\n\n\nFigure 2: The ggplot2 closed arrow\n\n\n\n\n\nNevertheless, I miss the variety of arrows in available in TikZ. I particularly like the latex' arrow:\n\n\nTikZ code with latex’ arrow\n\\usetikzlibrary{arrows}\n\\begin{tikzpicture}\n\\node[fill=violet!50!white, circle] (A) at (0,0) {A};\n\\node[fill=cyan!80!black, circle] (B) at (1,1.732) {B};\n\\node[fill=green!40!black!40, circle] (C) at (2,0) {C};\n\\path[-&gt;,\n    draw,\n    shorten &gt;=2pt,\n    shorten &lt;=2pt,\n    &gt;=latex',\n    thick,\n    color = black!40,\n    text = black] (A) -&gt; (B);\n    \\path[-&gt;,\n    draw,\n    shorten &gt;=2pt,\n    shorten &lt;=2pt,\n    &gt;=latex',\n    thick,\n    color = black!40,\n    text = black] (B) -&gt; (C);\n\\end{tikzpicture}\n\n\n\n\n\n\n\n\nFigure 3: The TikZ latex’ arrow\n\n\n\n\n\n\nThe ggarrow package\nIf you want more variety in drawing arrows in ggplot2, Teun van den Brand’s ggarrow package expands your limits to whatever your imagination can provide.\nThe default arrowhead is already a nice improvement:\n\n\nCode for wings arrow (i.e, stealth arrow in TikZ)\np + geom_arrow_segment(length_head = 10)\n\n\n\n\n\n\n\n\nFigure 4: The “wings” arrow\n\n\n\n\n\nYou can play around with the sharpness of the point (offset) and the sharpness of the barb (inset). You can also add feathers.\n\n\nCode for sharp barbs and feathers\np +\n  geom_arrow_segment(length_head = 4,\n                     arrow_head = arrow_head_wings(offset = 20, \n                                                   inset = 10), \n                     arrow_fins = arrow_fins_feather(), \n                     length_fins = 11)\n\n\n\n\n\n\n\n\nFigure 5: Sharp barbs and feathers\n\n\n\n\n\n\n\nCode for kite arrowhead\np +\n  geom_arrow_segment(length_head = 15,\n                     arrow_head = arrow_head_wings(offset = 22.5,\n                                                   inset = 115),\n                     arrow_fins = arrow_head_wings(offset = 22.5,\n                                                   inset = 115),\n                     length_fins = 15)\n\n\n\n\n\n\n\n\nFigure 6: Double-headed arrow with kite arrowhead\n\n\n\n\n\n\n\nCode for reverse kite arrowhead\np +\n  geom_arrow_segment(length_head = 20,\n                     arrow_head = arrow_head_wings(offset = 45,\n                                                   inset = 120))\n\n\n\n\n\n\n\n\nFigure 7: Reverse kite arrowhead\n\n\n\n\n\nIf the arrow goes too far, you can pull it back with the resect arguments.\n\n\nCode for resecting an arrowhead\np +\n  geom_arrow_segment(length_head = 6,\n                     arrow_head = arrow_head_wings(offset = 120,\n                                                   inset = 35),\n                     resect_head = 2)\n\n\n\n\n\n\n\n\nFigure 8: Demonstration of resecting arrowheads\n\n\n\n\n\nThere is much, much more that can be done. See ggarrow’s arrow ornament vignette for more options.\n\n\nCustom Arrowheads\nNot only does ggarrow offer great arrow geoms with excellent features like resection, one can create any custom arrowhead or feather that can be made with a single polygon.\nThe polygon generally falls between -1 and 1 on x and y, though you can plot outside those limits. In most cases, the point is at (0,1), and the line ends at (0,0):\n\n\nCode\npar(pty = \"s\")\nx &lt;- c(0,1)\ny &lt;- c(0,0)\nplot(x , y, xlim = c(-1, 1), ylim = c(-1, 1))\nrect(-1,-.1,0,.1)\ntext(-.5,0, labels = \"Line\")\ntext(1,0, labels = \"Arrow Point\", adj = 1.2)\npolygon(ggarrow::arrow_head_wings() |&gt; `colnames&lt;-`(c(\"a\", \"b\")))\n\n\n\n\n\n\n\n\nFigure 9: Grid for custom arrowheads\n\n\n\n\n\nThe polygon you create should be be a 2-column matrix with named columns (e.g., x and y). Here I make a elliptical arrowhead.\n\n\nCode\nmake_ellipse &lt;- function(a = 1, b = .5){\n  t &lt;- seq(0,2*pi, length.out = 361)\n  cbind(x = a * cos(t), y = b * sin(t)) \n}\n\np +\n  geom_arrow_segment(length_head = 5,\n                     arrow_head = make_ellipse())\n\n\n\n\n\n\n\n\nFigure 10: Make ellipse\n\n\n\n\n\n\n\nThe arrowheadr package\nI made the arrowheadr package to make custom arrowheads quickly. Some of them are admittedly silly. However, it is now easy to make my favorite kind of arrow in ggplot2:\n\n\nCode\nlibrary(arrowheadr)\n\np + \n  geom_arrow_segment(length_head = 5,\n                     arrow_head = arrow_head_latex())\n\n\n\n\n\nMimicking the latex prime arrowhead\n\n\n\n\nMore examples:\nA catenary:\n\n\nCode\nstlouis &lt;- arrow_head_catenary(base_width = .25, thickness = .15)\n\np + \n  geom_arrow_segment(length_head = 5,\n                     arrow_head = stlouis)\n\n\n\n\n\n\n\n\nFigure 11: The Gateway Arch in St. Louis is shaped like a catenary, not a parabola.\n\n\n\n\n\nThe Cauchy function:\n\n\nCode\np + \n  geom_arrow_segment(length_head = 5,\n                     arrow_head = arrow_head_function(dt, df = 1))\n\n\n\n\n\n\n\n\nFigure 12: Any function can make an arrowhead.\n\n\n\n\n\nRazors:\n\n\nCode\nrazors &lt;- c(1,0,\n  0,.5,\n  -.35,.25,\n  -.35, .21,\n  0,.35,\n  .90,0\n  ) |&gt; \n  v2matrix() |&gt; \n  reflecter() \n\np + \n  geom_arrow_segment(length_head = 10,\n                     arrow_head = razors)\n\n\n\n\n\n\n\n\nFigure 13: The reflecter function takes a set of points and makes them symmetrical.\n\n\n\n\n\nA candle flame:\n\n\nCode\ncandleflame &lt;- arrow_head_wittgenstein_rod(\n  fixed_point = c(-2.75, 0),\n  rod_length = 3.75,\n  nudge = c(1, 0),\n  rescale = .95\n)\n\np + \n  geom_arrow_segment(length_head = 12,\n                     arrow_head = candleflame)\n\n\n\n\n\n\n\n\nFigure 14: Wittgenstein’s rod makes some nice shapes\n\n\n\n\n\nUsing bezier curve control points:\n\n\nCode\ncurved_arrowhead &lt;- arrow_head_bezier(list(\n  c(1,  0,\n    .5, .5,\n    .2, .5),\n  c(.2, .5,\n    .2, .1,\n    -.1, .25,\n    -.3, .25),\n  c(-.3, .25,\n    0, 0,\n    -.3, -.25),\n  c(-.3, -.25,\n    -.1, -.25,\n    .2,  -.1,\n    .2, -.5),\n  c(.2, -.5,\n    .5, -.5,\n    1,  0)\n))\n\np + \n  geom_arrow_segment(length_head = 8,\n                     arrow_head = curved_arrowhead)\n\n\n\n\n\n\n\n\nFigure 15: Bezier curves can make almost anything.\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2023,\n  author = {Schneider, W. Joel},\n  title = {Making a Custom Arrowhead for Ggplot2 Using Ggarrow and\n    Arrowheadr},\n  date = {2023-08-26},\n  url = {https://wjschne.github.io/posts/2023-08-26-making-a-custom-arrowhead-for-ggplot2-using-ggarrow-and-arrowheadr/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2023, August 26). Making a custom arrowhead for\nggplot2 using ggarrow and arrowheadr. Schneirographs. https://wjschne.github.io/posts/2023-08-26-making-a-custom-arrowhead-for-ggplot2-using-ggarrow-and-arrowheadr/"
  },
  {
    "objectID": "posts/2025-09-05-apaquartoshiny/index.html",
    "href": "posts/2025-09-05-apaquartoshiny/index.html",
    "title": "APA style with Quarto: A shiny web app for creating apaquarto files",
    "section": "",
    "text": "Quarto is great for writing scientific documents with reproducible analyses. The aqaquarto extension for Quarto will create documents in APA style, rendering to the document format of your choice: .docx, .html, or .pdf.\nAlthough I have worked hard to document all of apaquarto’s options and features, even I was starting to lose track of them all. To make the initial setup of an apaquarto document a little easier, try the apaquarto web app.\nBecause the app is made with shinylive, your data stays entirely private because it runs locally on your machine. Be patient, it takes a moment or two to load.\nUsing the app is straightforward: Fill in the text boxes, select the check boxes, etc. The table for completing the authors and affiliations is a little finicky, so be sure to finish editing by pressing the Enter/Return key in each cell.\nWhen you are ready, select the Make Document page on the side menu. Click Update, copy the resulting text, and paste it into your Quarto document.\nLet me know your ideas for improving the app and/or apaquarto. schneider@temple.edu\n\n\n\n\n\n\nFigure 1: Screenshot of apaquarto web app\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {APA Style with {Quarto:} {A} Shiny Web App for Creating\n    Apaquarto Files},\n  date = {2025-09-05},\n  url = {https://wjschne.github.io/posts/2025-09-05-apaquartoshiny/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 5). APA style with Quarto: A shiny web\napp for creating apaquarto files. Schneirographs. https://wjschne.github.io/posts/2025-09-05-apaquartoshiny/"
  },
  {
    "objectID": "posts/announcing-ggdiagram/index.html",
    "href": "posts/announcing-ggdiagram/index.html",
    "title": "Announcing ggdiagram, a ggplot2 extension for making diagrams programmatically",
    "section": "",
    "text": "I am pleased to announce that ggdiagram has been published on CRAN. The package site has many vignettes, but I will give a brief introduction here as well.\n\nlibrary(ggdiagram)\nlibrary(ggplot2)\n\nThe ggdiagram package has functions to create a variety of graphical objects:\n\npoints\n\nob_point\nob_polar\n\nlines and arrows\n\nob_line\nob_segment\nob_path\nob_bezier\nob_arc\nob_variance\nob_covariance\nconnect\n\nshapes\n\nob_circle\nob_ellipse\nob_rectangle\nob_ngon\nob_wedge\nob_reuleaux\nob_circular_segment\nob_intercept\n\ntext\n\nob_label\nob_latex\n\n\nObjects created in ggdiagram can be inserted into a standard ggplot2 pipeline.\n\nggplot() +\n  coord_equal() +\n  ob_circle(x = -2, y = -1) +\n  ob_rectangle(x = 1, y = 2, width = 2)\n\n\n\n\n\n\n\nFigure 1: A circle and a rectangle in a standard ggplot2 pipeline\n\n\n\n\n\nThere is nothing wrong with the code in Figure 1, but it could have easily been written with standard ggplot2 code. That is, the ggplot2 ecosystem already has the ability to display points, lines, shapes, and text. Why do we need a new package? The grammar of graphics is optimally designed to display data. After the graphical objects are made, it is not easy to use features of those objects to make new graphical objects. For example,\n\nWhere is the midpoint of a line segment?\nWhere do a circle and a rectangle intersect?\nWhere is the leftmost point of a group of rotated ellipses?\n\nThe ggdiagram package allows users to identify features from objects to create new objects.\nIn Figure 2, the ggdiagram function sets the ggplot2 theme (theme_void by default), as well as the font size and font family of the labels. Circle B is placed at a 20-degree angle from circle A, with 2 units of separation between them. A connecting arrow is placed between the two circles, with 2 points of resection. A label (.56) is placed at the midpoint of the arrow segment.\nNote that in Figure 2, variables A and B are defined in the middle of the pipeline. In ggplot2 code, any variable assignments for variables that are to be used later in the pipeline can be enclosed in curly braces {}.\n\nggdiagram(font_size = 20) +\n  {\n    A &lt;- ob_circle(label = \"A\")\n  } +\n  {\n    B &lt;- ob_circle(label = \"B\") |&gt;\n      place(from = A,\n            where = degree(20),\n            sep = 2)\n  } +\n  connect(\n    from = A,\n    to = B,\n    label = ob_label(\".56\"),\n    resect = 2\n  )\n\n\n\n\n\n\n\nFigure 2: An arrow between two circles\n\n\n\n\n\n\nAutomation\nPlacing objects one at a time can become tiresome if the there are repetitive elements. The ggdiagram package has a number of methods to automate code. In Figure 3, we create a path diagram with three sets of objects:\n\nA circle of radius 1.5, with the label “A”\nA horizontal array of 4 indicator variables (as superellipses or squircles) 4 units below circle A, labeled A1 through A4, with a separation of .4 units between the variables.\nArrows from circle A to the four indicators with coefficients aligned horizontally.\n\n\nggdiagram(font_family = \"Roboto Condensed\", font_size = 32) +\n  {\n    A &lt;- ob_circle(\n      radius = 1.5,\n      label = ob_label(\"*A*\", size = 64, nudge_y = -.1)\n    )\n  } +\n  {\n    a4 &lt;- ob_ellipse(m1 = 10) %&gt;%\n      place(A, where = \"below\", sep = 4) %&gt;%\n      ob_array(k = 4, label = paste0(\"*A*~\", 1:4, \"~\"), sep = .4)\n  } +\n  connect(\n    from = A,\n    to = a4,\n    resect = 2,\n    label = ob_label(\n      round_probability(c(.75, .87, .92, .67)),\n      label.padding = margin(t = 3),\n      angle = 0,\n      size = 20\n    )\n  )@set_label_y()\n\n\n\n\n\n\n\nFigure 3: A latent variable with four indicators\n\n\n\n\n\nCode can be shortened considerably with for loops, lapply, or map functions from the purrr package. The map_ob function is a specialized version of purrr::map that unbinds the elements of an object, applies a function to each one, and binds the results into a single object. If multiple objects of different types are returned in a list, as they are here, they are consolidated into a single ob_shape_list.\nIn Figure 4, six latent variables are arranged in a circle. Using map-ob, the indicators for each latent variable are created, along with their connecting arrows.\nIf this seems like a lot of code, imagine what it would take to create 30 objects, 30 labels, and 39 arrows line by line!\n\n# Latent variables\nk &lt;- 6\n\n# labels/id\nid &lt;- LETTERS[1:k]\n\n# Degree positions\ntheta &lt;- seq(0, 360, length.out = k + 1)[1:k]\n\n# Fills\nmy_colors &lt;- class_color(hue = theta + 90, saturation = .25, brightness = .55)\n\n# Indicators per variable\nj &lt;- 4\n\n# All unique pairs of latent variables\npath_connections &lt;- combn(id, 2)\n\nggdiagram(font_family = \"Roboto Condensed\", font_size = 20) +\n  {\n    # latent variables\n    l &lt;- ob_circle(\n      ob_polar(degree(theta + 90), r = 3.37),\n      label = ob_label(\n        id,\n        size = 40,\n        nudge_y = -.1,\n        fill = NA,\n        color = \"white\"\n      ),\n      radius = 1,\n      id = id,\n      fill = my_colors,\n      color = NA\n    )\n  } +\n  # Connect each unique pair of latent variables\n  connect(\n    l[path_connections[1, ]],\n    l[path_connections[2, ]],\n    resect = 2,\n    color = l[path_connections[1, ]]@fill\n  ) +\n  {\n    # Make indictors for each latent variable in l\n    map_ob(l, \\(ll) {\n      # Get the angle\n      th &lt;- ll@center@theta\n\n      # Get the color for latent variable\n      ll_fill &lt;- class_color(ll@fill)\n\n      # Make color gradient for observed indicators\n      cc &lt;- seq(-180 / k, 180 / k, length.out = j) * .6\n      o_fill &lt;- class_color(\n        hue = ll_fill@hue + cc,\n        saturation = ll_fill@saturation * .9,\n        brightness = ll_fill@brightness * 1.1\n      )\n\n      # Subscripts\n      i &lt;- seq(j)\n      if (th@positive &lt; degree(180)) {\n        i &lt;- rev(i)\n      }\n\n      # Make observed indicator variables\n      o &lt;- ob_ellipse(m1 = 15, a = .75, angle = th) |&gt;\n        place(from = ll, where = ll@center@theta, sep = 1.45) |&gt;\n        ob_array(\n          k = j,\n          where = th + degree(90),\n          sep = .2,\n          fill = o_fill,\n          label = ob_label(\n            paste0(ll@label@label, \"~\", i, \"~\"),\n            nudge_y = -.05,\n            fill = NA,\n            color = \"white\"\n          )\n        )\n\n      # Connect latent variable to observed indicators\n      a &lt;- connect(ll, o, resect = 2, color = o@fill)\n      list(o, a)\n    })\n  }\n\n\n\n\n\n\n\nFigure 4: Latent variables arranged in a hexagon\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Announcing Ggdiagram, a Ggplot2 Extension for Making Diagrams\n    Programmatically},\n  date = {2025-08-19},\n  url = {https://wjschne.github.io/posts/announcing-ggdiagram/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, August 19). Announcing ggdiagram, a ggplot2\nextension for making diagrams programmatically. Schneirographs.\nhttps://wjschne.github.io/posts/announcing-ggdiagram/"
  },
  {
    "objectID": "posts/apatables/apa702.html",
    "href": "posts/apatables/apa702.html",
    "title": "Recreating APA Manual Table 7.2 in R with apa7",
    "section": "",
    "text": "In this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nHanging indents with the hanging_indent\nConditional indentation with padding\nDecimal alignment with align_chr\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\nFigure 1 Shows some demographic summary data. Getting the data into the proper shape took a fair amount of code, but such data wrangling is not the point of this tutorial. The part that was tricky was getting the text wrapping and hanging indentation right.\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.1\n\n\n\n\n\n\n\n\n\nIndentation is usually easy with flextable’s padding function. Here, some columns needed indentation, some hanging indentation, and some needed both. The best solution I found was to seize control of the formatting using stringr::str_wrap to find where to break the lines and then markdown (\\\\ to make new lines, and \\u00A0, the figure space, to indent). The hanging_indent function implements these steps automatically.\n\n\n\nTo my eye, numeric text often looks better when decimal aligned. The align_chr function allows text to be centered within a column and yet still aligned on the decimal (or any other character). By default, it pads the numeric vector with the figure space (&numsp; or \\u2007) character, which is as wide as an integer.\nDoes flextable already have decimal alignment? Yes! However, its method requires the user to supply the position of a decimal tab within the column, which may require a fair bit of trial and error. Of course, if you need decimal alignment somewhere other than in the center of a column, the flextable method is the best option.\n\n\n\nBy default, apa_flextable will automatically format columns with specific names (or endings). The Variable column is one of those, and by default, it will transform the text to title case. In this instance, we want sentence casing, so we list Variable in no_format_columns.\nThe remaining columns that normally get automatic formatting (i.e, columns ending with “_n”) will still get it. Setting auto_format_columns = FALSE, will turn off all automatic formatting.\n\n```{r}\n#| label: tbl-72\n#| tbl-cap: Sociodemographic Characteristics of Participants at Baseline\n#| apa-note: \n#|   - \"*N*&nbsp;=&nbsp;150 (*n*&nbsp;=&nbsp;50 for each condition). Participants \n#|      were on average 39.5 years old (SD&nbsp;=&nbsp;10.1), and \n#|      participant age did not differ by condition.\"\n#|   - \"^a\\u00A0^Reflects the number and percentage of\n#|      participants answering “yes” to this question.\"\n\n# Make data\nd &lt;- tibble(\n  Variable = c(\n    rep(\"Gender\", 2),\n    rep(\"Marital status\",4), \n    rep(\"Children\",2),\n    rep(\"Cohabiting\", 2),\n    rep(\"Highest educational level\", 3),\n    rep(\"Employment\", 5),\n    rep(\"Previous\\u00A0psychological treatment\",2),\n    rep(\"Previous\\u00A0psychotropic medication\",2)\n    ),\n  Group = c(\n    \"Female\",\"Male\",\"Single\",\"Married/partnered\",\n    \"Divorced/widowed\",\"Other\", \n    \"Yes\",\"No\",\n    \"Yes\",\"No\",\n    \"Middle school\", \"High school/some college\",\n    \"University or post-graduate degree\",\n    \"Unemployed\",\"Student\",\"Employed\",\n    \"Self-employed\",\"Retired\",\"Yes\",\"No\",\"Yes\",\"No\"),\n    `Guided self-help` = c(\n      25L, 25L, 13L, 35L, 1L, 1L, 26L, 24L, 37L, 13L,\n      0L, 22L, 27L, 3L, 8L, 30L, 9L, 0L, 17L, 33L, 6L, 44L),\n  `Unguided self-help` = c(\n    20L, 30L, 11L, 38L, 1L, 0L, 26L, 24L, 36L, 14L, 1L,\n    17L, 30L, 5L, 7L, 29L, 7L, 2L, 18L, 32L, 13L, 37L),\n   `Wait-list control` = c(\n     23L, 27L, 17L, 28L, 4L, 1L, 22L, 28L, 26L, 24L, \n     1L, 13L, 32L, 2L, 3L, 40L, 5L, 0L, 24L, 26L, 11L, 39L)\n) \n\n# format data\nd_formatted &lt;- d |&gt; \n  pivot_longer(-c(Variable, Group), \n               values_to = \"n\") |&gt;\n  mutate(`%` = round(100  * n / sum(n)), \n         .by = c(Variable, name)) |&gt;\n  mutate(`Full sample_n` = sum(n),\n         .by = c(Variable, Group)) |&gt;\n  pivot_wider_name_first(values_from = c(n , `%`)) |&gt; \n  relocate(`Full sample_n`, .after = last_col()) |&gt;\n  mutate(\n    `Full sample_%` = 100 * `Full sample_n` /\n      sum(`Full sample_n`),\n    .by = c(Variable)\n  ) |&gt;\n  filter(Group !=  \"No\") |&gt;\n  mutate(Group = ifelse(\n    Group == \"Yes\", \n    paste0(Variable, \"^a^\"), Group) |&gt;\n      hanging_indent(width = 22)) |&gt;\n  rename(`Baseline\\ncharacteristic` = Group) |&gt; \n  mutate(across(c(ends_with(\"_\\\\*n\\\\*\"), \n                  ends_with(\"_%\")), \n                .fns = \\(x) align_chr(x, accuracy = .1)))\n\n# Make table\nd_formatted |&gt; \n  apa_flextable(\n    row_title_column = Variable,\n    row_title_border = NA,\n    line_spacing = 1.5, \n    horizontal_padding = 0,\n    font_size = 10.5,\n    left_column_padding = 10, \n    no_format_columns = Variable) |&gt;\n  align(j = 1, align = \"left\") |&gt;\n  width(width = c(1.55, rep(c(.6, .6, .05), 4)[-12]))\n```\n\n\n\n\nTable 1\n\n\nSociodemographic Characteristics of Participants at Baseline\n\n\n\n\nBaseline characteristicGuided self-helpUnguided self-helpWait-list controlFull samplen%n%n%n%GenderFemale255020402346 6845.3Male255030602754 8254.7Marital statusSingle132611221734 4127.3Married/partnered35703876285610167.3Divorced/widowed 1 2 1 2 4 8  6 4.0Other 1 2 0 0 1 2  2 1.3Childrena265226522244 7449.3Cohabitinga377436722652 9966.0Highest educational levelMiddle school 0 0 1 2 1 2  2 1.4High school/some    college224517351328 5236.4University or post-    graduate degree275530623270 8962.2EmploymentUnemployed 3 6 510 2 4 10 6.7Student 816 714 3 6 1812.0Employed306029584080 9966.0Self-employed 918 714 510 2114.0Retired 0 0 2 4 0 0  2 1.3Previous psychological    treatmenta173418362448 5939.3Previous psychotropic    medicationa 61213261122 3020.0\n\n\n\n\n\n\nNote. N = 150 (n = 50 for each condition). Participants were on average 39.5 years old (SD = 10.1), and participant age did not differ by condition.\n\n\na Reflects the number and percentage of participants answering “yes” to this question.\n\n\n\nBecause I used apa-note instead of flextable functions, the notes would only appear if using the apaquarto extension.\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast"
  },
  {
    "objectID": "posts/apatables/apa702.html#hanging-indentation",
    "href": "posts/apatables/apa702.html#hanging-indentation",
    "title": "Recreating APA Manual Table 7.2 in R with apa7",
    "section": "",
    "text": "Indentation is usually easy with flextable’s padding function. Here, some columns needed indentation, some hanging indentation, and some needed both. The best solution I found was to seize control of the formatting using stringr::str_wrap to find where to break the lines and then markdown (\\\\ to make new lines, and \\u00A0, the figure space, to indent). The hanging_indent function implements these steps automatically."
  },
  {
    "objectID": "posts/apatables/apa702.html#decimal-alignment",
    "href": "posts/apatables/apa702.html#decimal-alignment",
    "title": "Recreating APA Manual Table 7.2 in R with apa7",
    "section": "",
    "text": "To my eye, numeric text often looks better when decimal aligned. The align_chr function allows text to be centered within a column and yet still aligned on the decimal (or any other character). By default, it pads the numeric vector with the figure space (&numsp; or \\u2007) character, which is as wide as an integer.\nDoes flextable already have decimal alignment? Yes! However, its method requires the user to supply the position of a decimal tab within the column, which may require a fair bit of trial and error. Of course, if you need decimal alignment somewhere other than in the center of a column, the flextable method is the best option."
  },
  {
    "objectID": "posts/apatables/apa702.html#turn-off-automatic-formatting",
    "href": "posts/apatables/apa702.html#turn-off-automatic-formatting",
    "title": "Recreating APA Manual Table 7.2 in R with apa7",
    "section": "",
    "text": "By default, apa_flextable will automatically format columns with specific names (or endings). The Variable column is one of those, and by default, it will transform the text to title case. In this instance, we want sentence casing, so we list Variable in no_format_columns.\nThe remaining columns that normally get automatic formatting (i.e, columns ending with “_n”) will still get it. Setting auto_format_columns = FALSE, will turn off all automatic formatting.\n\n```{r}\n#| label: tbl-72\n#| tbl-cap: Sociodemographic Characteristics of Participants at Baseline\n#| apa-note: \n#|   - \"*N*&nbsp;=&nbsp;150 (*n*&nbsp;=&nbsp;50 for each condition). Participants \n#|      were on average 39.5 years old (SD&nbsp;=&nbsp;10.1), and \n#|      participant age did not differ by condition.\"\n#|   - \"^a\\u00A0^Reflects the number and percentage of\n#|      participants answering “yes” to this question.\"\n\n# Make data\nd &lt;- tibble(\n  Variable = c(\n    rep(\"Gender\", 2),\n    rep(\"Marital status\",4), \n    rep(\"Children\",2),\n    rep(\"Cohabiting\", 2),\n    rep(\"Highest educational level\", 3),\n    rep(\"Employment\", 5),\n    rep(\"Previous\\u00A0psychological treatment\",2),\n    rep(\"Previous\\u00A0psychotropic medication\",2)\n    ),\n  Group = c(\n    \"Female\",\"Male\",\"Single\",\"Married/partnered\",\n    \"Divorced/widowed\",\"Other\", \n    \"Yes\",\"No\",\n    \"Yes\",\"No\",\n    \"Middle school\", \"High school/some college\",\n    \"University or post-graduate degree\",\n    \"Unemployed\",\"Student\",\"Employed\",\n    \"Self-employed\",\"Retired\",\"Yes\",\"No\",\"Yes\",\"No\"),\n    `Guided self-help` = c(\n      25L, 25L, 13L, 35L, 1L, 1L, 26L, 24L, 37L, 13L,\n      0L, 22L, 27L, 3L, 8L, 30L, 9L, 0L, 17L, 33L, 6L, 44L),\n  `Unguided self-help` = c(\n    20L, 30L, 11L, 38L, 1L, 0L, 26L, 24L, 36L, 14L, 1L,\n    17L, 30L, 5L, 7L, 29L, 7L, 2L, 18L, 32L, 13L, 37L),\n   `Wait-list control` = c(\n     23L, 27L, 17L, 28L, 4L, 1L, 22L, 28L, 26L, 24L, \n     1L, 13L, 32L, 2L, 3L, 40L, 5L, 0L, 24L, 26L, 11L, 39L)\n) \n\n# format data\nd_formatted &lt;- d |&gt; \n  pivot_longer(-c(Variable, Group), \n               values_to = \"n\") |&gt;\n  mutate(`%` = round(100  * n / sum(n)), \n         .by = c(Variable, name)) |&gt;\n  mutate(`Full sample_n` = sum(n),\n         .by = c(Variable, Group)) |&gt;\n  pivot_wider_name_first(values_from = c(n , `%`)) |&gt; \n  relocate(`Full sample_n`, .after = last_col()) |&gt;\n  mutate(\n    `Full sample_%` = 100 * `Full sample_n` /\n      sum(`Full sample_n`),\n    .by = c(Variable)\n  ) |&gt;\n  filter(Group !=  \"No\") |&gt;\n  mutate(Group = ifelse(\n    Group == \"Yes\", \n    paste0(Variable, \"^a^\"), Group) |&gt;\n      hanging_indent(width = 22)) |&gt;\n  rename(`Baseline\\ncharacteristic` = Group) |&gt; \n  mutate(across(c(ends_with(\"_\\\\*n\\\\*\"), \n                  ends_with(\"_%\")), \n                .fns = \\(x) align_chr(x, accuracy = .1)))\n\n# Make table\nd_formatted |&gt; \n  apa_flextable(\n    row_title_column = Variable,\n    row_title_border = NA,\n    line_spacing = 1.5, \n    horizontal_padding = 0,\n    font_size = 10.5,\n    left_column_padding = 10, \n    no_format_columns = Variable) |&gt;\n  align(j = 1, align = \"left\") |&gt;\n  width(width = c(1.55, rep(c(.6, .6, .05), 4)[-12]))\n```\n\n\n\n\nTable 1\n\n\nSociodemographic Characteristics of Participants at Baseline\n\n\n\n\nBaseline characteristicGuided self-helpUnguided self-helpWait-list controlFull samplen%n%n%n%GenderFemale255020402346 6845.3Male255030602754 8254.7Marital statusSingle132611221734 4127.3Married/partnered35703876285610167.3Divorced/widowed 1 2 1 2 4 8  6 4.0Other 1 2 0 0 1 2  2 1.3Childrena265226522244 7449.3Cohabitinga377436722652 9966.0Highest educational levelMiddle school 0 0 1 2 1 2  2 1.4High school/some    college224517351328 5236.4University or post-    graduate degree275530623270 8962.2EmploymentUnemployed 3 6 510 2 4 10 6.7Student 816 714 3 6 1812.0Employed306029584080 9966.0Self-employed 918 714 510 2114.0Retired 0 0 2 4 0 0  2 1.3Previous psychological    treatmenta173418362448 5939.3Previous psychotropic    medicationa 61213261122 3020.0\n\n\n\n\n\n\nNote. N = 150 (n = 50 for each condition). Participants were on average 39.5 years old (SD = 10.1), and participant age did not differ by condition.\n\n\na Reflects the number and percentage of participants answering “yes” to this question.\n\n\n\nBecause I used apa-note instead of flextable functions, the notes would only appear if using the apaquarto extension.\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast"
  },
  {
    "objectID": "posts/apatables/apa704.html",
    "href": "posts/apatables/apa704.html",
    "title": "Recreating APA Manual Table 7.4 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 4 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nflextable::merge_v to collapse text acros rows.\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.4\n\n\n\n\n\n\n\nAPA Manual Table 7.4 in Figure 1 is not entirely visible, so I have reproduced the visible rows only. This table is straightforward to make. I used flextable::merge_v to remove redundant text in the Study column.\nThis table is intended to illustrate repeated headers, which is not relevant in the .html display here. Fortunately, when rendered to .docx, flextable repeats headers by default.\n\n```{r}\n#| label: tbl-74\n#| tbl-cap: Sample and Task Information for Studies Included in the Meta-Analysis.\n#| apa-note: \"AX-CPT = AX–continuous performance task; DPX = dot–pattern expectancy task.\"\n\n# Make data and restructure data\nd &lt;- tibble(\n  Study = c(\n    \"Barch et al. (2001)\",\n    \"Barch et al. (2008)\",\n    \"Becker (2012)\",\n    \"Braver et al. (1999)\",\n    \"Chung et al. (2011)\",\n    \"MacDonald & Carter (2003)\",\n    \"Poppe et al. (2016)\",\n    \"Reilly et al. (2017)\",\n    \"Sheffield et al. (2014)\",\n    \"Todd et al. (2014)\",\n    \"Zhang et al. (2015)\"\n  ),\n  `with first-episode schizophrenia` = c(\n    14L, NA, NA, 16L, NA, NA, NA, NA, NA, NA, NA),\n  `with chronic episode schizophrenia` = c(\n    NA, 57L, 49L, NA, 41L, 17L, 47L, 402L, 104L, 33L, 339L),\n  `healthy control participants` = c(\n    12L, 37L, 28L, 16L, 27L, 17L, 56L, 304L, 132L, 58L, 665L),\n  Task = c(\n    \"AX-CPT\",\n    \"AX-CPT\",\n    \"AX-CPT\",\n    \"AX-CPT\",\n    \"AX-CPT\",\n    \"AX-CPT\",\n    \"DPX\",\n    \"DPX\",\n    \"AX-CPT, DPX\",\n    \"AX-CPT\",\n    \"DPX\"\n  )\n) |&gt; \n  pivot_longer(where(is.integer)) |&gt; \n  filter(!is.na(value)) |&gt; \n  unite(Sample, c(value, name), sep = \" \") |&gt; \n  select(Study, Sample, Task)\n\n\n# Make table\nd |&gt; \n  apa_flextable() |&gt; \n  merge_v(j = \"Study\") |&gt; \n  align(j = \"Sample\") \n```\n\n\n\n\nTable 1\n\n\nSample and Task Information for Studies Included in the Meta-Analysis.\n\n\n\n\nStudySampleTaskBarch et al. (2001)14 with first-episode schizophreniaAX-CPT12 healthy control participantsAX-CPTBarch et al. (2008)57 with chronic episode schizophreniaAX-CPT37 healthy control participantsAX-CPTBecker (2012)49 with chronic episode schizophreniaAX-CPT28 healthy control participantsAX-CPTBraver et al. (1999)16 with first-episode schizophreniaAX-CPT16 healthy control participantsAX-CPTChung et al. (2011)41 with chronic episode schizophreniaAX-CPT27 healthy control participantsAX-CPTMacDonald & Carter (2003)17 with chronic episode schizophreniaAX-CPT17 healthy control participantsAX-CPTPoppe et al. (2016)47 with chronic episode schizophreniaDPX56 healthy control participantsDPXReilly et al. (2017)402 with chronic episode schizophreniaDPX304 healthy control participantsDPXSheffield et al. (2014)104 with chronic episode schizophreniaAX-CPT, DPX132 healthy control participantsAX-CPT, DPXTodd et al. (2014)33 with chronic episode schizophreniaAX-CPT58 healthy control participantsAX-CPTZhang et al. (2015)339 with chronic episode schizophreniaDPX665 healthy control participantsDPX\n\n\n\n\n\n\nNote. AX-CPT = AX–continuous performance task; DPX = dot–pattern expectancy task.\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.4 in {R} with Apa7},\n  date = {2025-09-14},\n  url = {https://wjschne.github.io/posts/apatables/apa704.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 14). Recreating APA Manual Table 7.4\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa704.html"
  },
  {
    "objectID": "posts/apatables/apa706.html",
    "href": "posts/apatables/apa706.html",
    "title": "Recreating APA Manual Table 7.6 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 6 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nCentering text on a space rather than a decimal with align_chr\nAutomatic column formatting with apa-format_columns\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.6\n\n\n\n\n\n\n\nTo simply enter the data from this table would be a straightforward process. To make things interesting, I am going simulate data from the group means and standard deviations, run a series of t-tests, and display the data. This makes the process much more like what would happen in a real write-up. The means, standard deviations, and p-values come from the simulated data, not from Figure 1.\nNormally non-significant p-values would be rounded to 2 digits, but they were rounded to 3 digits via apa_format_columns, which also italicized the p column. The apa_format_columns is invoked by apa_flextable, but sometimes we want to take care of some custom formatting ahead of that step.\nThe two middle columns are aligned on the space between the group means and standard deviations, via align_chr(x, center = \" \").\n\n```{r}\n#| label: tbl-76\n#| tbl-cap: Means and Standard Deviations of Scores on Baseline Measures\n#| apa-note: Standard deviations are presented in parentheses. \n#|           BAS&nbsp;=&nbsp;Behavioral Activation System; \n#|           BAS-T&nbsp;=&nbsp;Behavioral Activation System–Total \n#|           scores from the Behavioral Inhibition System/Behavioral \n#|           Activation System Scales; SR&nbsp;=&nbsp;Sensitivity \n#|           to Reward scores from the Sensitivity \n#|           to Punishment and Sensitivity to Reward Questionnaire; \n#|           BDI&nbsp;=&nbsp;Beck Depression Inventory scores; \n#|           ASRM&nbsp;=&nbsp;Altman Self-Rating Mania Scale scores; \n#|           M-SRM&nbsp;=&nbsp;Modified Social Rhythm Metric \n#|           Regularity scores.\nset.seed(12)\n\n# make data\nd &lt;- tibble::tribble(\n   ~Scale,     ~Group,    ~M,  ~SD,\n  \"BAS-T\",     \"High\", 46.17, 2.87,\n     \"SR\",     \"High\", 17.94, 1.88,\n    \"BDI\",     \"High\",  7.11,  6.5,\n   \"ASRM\",     \"High\",  6.46, 4.01,\n  \"M-SRM\",     \"High\", 11.05, 3.36,\n  \"BAS-T\", \"Moderate\", 37.99, 1.32,\n     \"SR\", \"Moderate\", 11.52, 1.84,\n    \"BDI\", \"Moderate\",  6.18, 6.09,\n   \"ASRM\", \"Moderate\",  5.63, 3.69,\n  \"M-SRM\", \"Moderate\", 11.76, 2.75\n  ) |&gt; \n  mutate(x = map2(M, SD, \\(m, s) rnorm(75, mean = m, sd = s))) |&gt;\n  unnest(x) |&gt;\n  nest(data = -Scale)\n\n# Analyze data\nd_analysis &lt;- d |&gt; \n  mutate(\n    descriptives = map(\n      data,\n      \\(d) select(d, Group, x) |&gt;\n        summarise(m = mean(x) , s = sd(x), .by = Group) |&gt;\n        mutate(value = paste0(\n          scales::number(m, accuracy = .01),\n          \" (\",\n          scales::number(s, accuracy = .01),\n          \")\"\n        )) |&gt;\n        select(-c(m, s)) |&gt;\n        mutate(Group = paste0(Group, \" BAS group\")) |&gt; \n        pivot_wider(names_from = Group)\n    ),\n    p = map_dbl(data, \\(d) {\n      t.test(x ~ Group, data = d)$p.value \n    }),\n  ) |&gt;\n  select(-data) |&gt;\n  unnest(descriptives)\n\n# Format data\nd_formatted &lt;- d_analysis |&gt; \n  mutate(across(-c(Scale), \\(x) align_chr(x, center = \" \"))) |&gt; \n  apa_format_columns(accuracy = .001)\n\n# Make table\nd_formatted |&gt; \n  apa_flextable(table_width = 1, \n                layout = \"fixed\", \n                line_spacing = 1.5) |&gt;\n  width(width = rep(6.5 / 4, 4))\n```\n\n\n\n\nTable 1\n\n\nMeans and Standard Deviations of Scores on Baseline Measures\n\n\n\n\nScaleHigh BAS groupModerate BAS grouppBAS-T46.14 (2.60)37.95 (1.22)&lt;.001SR17.93 (1.80)11.24 (1.82)&lt;.001BDI 6.89 (6.48) 5.67 (5.46) .210ASRM 6.38 (3.96) 5.30 (3.63) .080M-SRM10.92 (3.10)11.70 (2.76) .100\n\n\n\n\n\n\nNote. Standard deviations are presented in parentheses. BAS = Behavioral Activation System; BAS-T = Behavioral Activation System–Total scores from the Behavioral Inhibition System/Behavioral Activation System Scales; SR = Sensitivity to Reward scores from the Sensitivity to Punishment and Sensitivity to Reward Questionnaire; BDI = Beck Depression Inventory scores; ASRM = Altman Self-Rating Mania Scale scores; M-SRM = Modified Social Rhythm Metric Regularity scores.\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.6 in {R} with Apa7},\n  date = {2025-09-16},\n  url = {https://wjschne.github.io/posts/apatables/apa706.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 16). Recreating APA Manual Table 7.6\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa706.html"
  },
  {
    "objectID": "posts/apatables/apa708.html",
    "href": "posts/apatables/apa708.html",
    "title": "Recreating APA Manual Table 7.8 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 8 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nPre-formatting columns and turning off auto-formatting\nIntentional use of inconsistent rounding within columns\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nlibrary(signs)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.8\n\n\n\n\n\n\n\nIf it were simply a matter of copying the text into the table, this would be fairly simple. However, in a real analysis, the text does not come pre-formatted. Thus, I simulated the data first, and then proceeded as if I had to conduct the t-tests and report the results from scratch.\nFor this one, the trick is to get the rounding right for the means and standard deviations. Each row is on a different scale, and needs different rounding rules. The easiest way I could do this was to first apply regular rounding with round and then round to a set number of significant digits with formatC.\nNormally, I would respect the direction of the effect, but Figure 1 makes t and Cohen’s d positive for all three rows despite the change in direction for the middle row.\n\n```{r}\n#| label: tbl-78\n#| tbl-cap: Results of Curve-Fitting Analysis Examining the \n#|          Time Course of Fixations to the Target\n#| apa-note: \"For each participant, the logistic function was \n#|           fit to target fixations separately. The maximum \n#|           asymptote is the asymptotic degree of looking at \n#|           the end of the time course of fixations. The \n#|           crossover point is the point in time the function \n#|           crosses the midway point between peak and baseline. \n#|           The slope represents the rate of change in the \n#|           function measured at the crossover. Mean parameter \n#|           values for each of the analyses are shown for the \n#|           9-year-olds (*n*&nbsp;=&nbsp;24) and 16-year-olds (*n*&nbsp;=&nbsp;18), \n#|           as well as the results of t tests (assuming unequal \n#|           variance) comparing the parameter estimates between \n#|           the two ages.\"\nset.seed(10)\n\n# Make data\nd &lt;- tibble::tribble(\n                      ~`Logistic Parameter`, ~Age,    ~M,    ~SD,\n          \"Maximum asymptote, proportion\",   9L, 0.843,  0.135,\n                       \"Crossover, in ms\",   9L,   759,     87,\n  \"Slope, as change in proportion per ms\",   9L, 0.001,  .0002,\n          \"Maximum asymptote, proportion\",  16L, 0.877,  0.082,\n                       \"Crossover, in ms\",  16L,   694,     42,\n  \"Slope, as change in proportion per ms\",  16L, 0.002,  .0002\n  ) |&gt; \n  mutate(n = ifelse(Age == 9L, 24, 18)) |&gt; \n  mutate(x = pmap(list(M, SD, n), \n                  \\(M, SD, n) rnorm(n, M, SD))) |&gt; \n  select(data = -c(M,SD, n)) |&gt; \n  unnest(x) |&gt; \n  nest(data = -`Logistic Parameter`) \n\n# Analyze data\nd_analysis &lt;- d |&gt; \n  mutate(t_fit = map(data, \\(d) t.test(\n    x ~ Age, data = d)),\n         cohen = map(data, \\(d) effectsize::cohens_d(\n           x ~ Age, \n           data = d))) |&gt; \n  unnest(cohen)\n\n# Format data\nd_formatted &lt;- d_analysis |&gt; \n  select(-starts_with(\"CI\")) |&gt; \n  mutate(`t(40)` = map_dbl(t_fit, \"statistic\") |&gt; \n           abs() |&gt; \n           align_chr(accuracy = .001), \n         p = map_dbl(t_fit, \"p.value\"),\n         .before = Cohens_d) |&gt; \n  select(-t_fit) |&gt; \n  mutate(data = pmap(\n    list(d = data, \n         round_digits = c(3, 0, 4),\n         sig_digits = c(3, 3, 1)), \n    \\(d, round_digits, sig_digits) {\n      d |&gt; \n        summarise(M = mean(x), \n                  SD = sd(x),\n                  .by = Age) |&gt; \n        mutate(across(c(M, SD), \\(x) {\n          round(x, round_digits) |&gt; \n            formatC(digits = sig_digits) |&gt; \n            trimws() |&gt; \n            str_remove(\"^0\")\n          }))\n  })) |&gt; \n  unnest(data) |&gt; \n  mutate(Age = paste0(Age, \"-year-olds\"), \n         Cohens_d = signs(abs(Cohens_d), \n                                 accuracy = .001)) |&gt; \n  pivot_wider_name_first(\n    values_from = c(M, SD), \n    names_from = Age) |&gt; \n  select(`Logistic Parameter`, \n         contains(\"olds\"), \n         everything()) |&gt; \n  mutate(`Logistic Parameter` = hanging_indent(`Logistic Parameter`)) \n\n\n# Make table\nd_formatted |&gt; \n  apa_flextable(\n    no_format_columns = contains(\"olds\"),\n    column_formats = column_formats(accuracy = .001))\n```\n\n\n\n\nTable 1\n\n\nResults of Curve-Fitting Analysis Examining the Time Course of Fixations to the Target\n\n\n\n\nLogistic Parameter9-year-olds16-year-oldst(40)pCohen’s dMSDMSDMaximum asymptote, proportion.805.127.882.086 2.325 .0250.687Crossover, in ms7227069139 1.802 .0800.521Slope, as change in proportion    per ms.001.0002.002.000218.671&lt;.0015.748\n\n\n\n\n\n\nNote. For each participant, the logistic function was fit to target fixations separately. The maximum asymptote is the asymptotic degree of looking at the end of the time course of fixations. The crossover point is the point in time the function crosses the midway point between peak and baseline. The slope represents the rate of change in the function measured at the crossover. Mean parameter values for each of the analyses are shown for the 9-year-olds (n = 24) and 16-year-olds (n = 18), as well as the results of t tests (assuming unequal variance) comparing the parameter estimates between the two ages.\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.8 in {R} with Apa7},\n  date = {2025-09-18},\n  url = {https://wjschne.github.io/posts/apatables/apa708.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 18). Recreating APA Manual Table 7.8\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa708.html"
  },
  {
    "objectID": "posts/apatables/apa710.html",
    "href": "posts/apatables/apa710.html",
    "title": "Recreating APA Manual Table 7.10 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 10 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nCorrelation matrices and descriptive statistics with apa_cor\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.10\n\n\n\n\n\n\n\nThe apa_cor function does a lot under the hood. By default, it gives the mean and standard deviation of each variable. By supplying a list of summary functions summary_functions = c(\"n\", \"M\", \"SD\"), we can get the n for each variable, too.\n\nset.seed(1234)\n\n# Descriptives\nd &lt;- tibble::tribble(\n                     ~Variable,    ~n,    ~M,  ~SD,\n    \"Internal--external status\", 3697L,  0.43, 0.49,\n     \"Manager job performance\", 2134L,  3.14, 0.62,\n             \"Starting salary\", 3697L,  1.01, 0.27,\n        \"Subsequent promotion\", 3697L,  0.33, 0.47,\n         \"Organization tenure\", 3697L,  6.45, 6.62,\n    \"Unit service performance\", 3505L,    85, 6.98,\n  \"Unit financial performance\",  694L, 42.61, 5.86\n  )\n\n# Correlations\nR &lt;- \"\nr       1       2       3       4       5       6       7   \n1   0.00    0.00    0.00    0.00    0.00    0.00    0.00\n2   -0.08   0.00    0.00    0.00    0.00    0.00    0.00\n3   0.45    -0.01   0.00    0.00    0.00    0.00    0.00\n4   0.08    -0.07   0.04    0.00    0.00    0.00    0.00\n5   -0.29   0.09    0.01    0.09    0.00    0.00    0.00\n6   0.25    -0.39   0.24    0.08    0.01    0.00    0.00\n7   0.00    0.03    0.12    -0.07   -0.02   0.16    0.00\" |&gt; \n    readr::read_tsv() |&gt; \n  suppressMessages() |&gt; \n    tibble::column_to_rownames(\"r\") |&gt; \n    as.matrix() |&gt; \n  `dimnames&lt;-`(list(d$Variable, d$Variable)) \n\n# Make R symmetric and fill diagonal with 1s\nR &lt;- (R + t(R)) |&gt; `diag&lt;-`(1)\n\n# Make data\nd_simulated &lt;- mvtnorm::rmvnorm(\n  n = max(d$n), \n  mean = deframe(d |&gt; select(Variable, M)),\n  sigma = diag(d$SD) %*% R %*% diag(d$SD)) |&gt; \n  as_tibble() |&gt; \n  map2_df(d$n, \\(x, n) {\n    k &lt;- length(x)\n    x[sample(1:k, size = k - n)] &lt;- NA\n    x\n  }) |&gt; \n  rename_with(hanging_indent, width = 15, indent = 0)\n\n\n# Make table\nd_simulated |&gt; \n  apa_cor(\n    summary_functions = c(\"n\", \"M\", \"SD\"), \n    significance_note = FALSE, \n    p_value = c(.05, .01)) |&gt;   \n  footnote(\n    i = 1, \n    j = \"Variable\", \n    value = as_paragraph(\n      \"0 = internal hires and 1 = external hires.\"), \n    ref_symbols = \"a \",\n    inline = TRUE, \n    sep = \" \") |&gt; \n  footnote(\n    i = 3, \n    j = \"Variable\", \n    value = as_paragraph(\n      paste(\"A linear transformation was performed on\",\n      \"the starting salary values to maintain pay practice\",\n      \"confidentiality. The standard deviation (0.27) can\",\n      \"be interpreted as 27% of the average starting salary\",\n      \"for all managers. Thus, ±1 SD includes a range of\",\n      \"starting salaries from 73% (i.e., 1.00 – 0.27) to\",\n      \"127% (i.e., 1.00 + 0.27) of the average starting\",\n      \"salaries for all managers.\")), \n    ref_symbols = \"b \", \n    inline = TRUE, \n    sep = \" \") |&gt; \n  footnote(\n    i = c(6,7), \n    j = \"Variable\", \n    value = as_paragraph(\n      \"Values reflect the average across 3 years of data.\"), \n    ref_symbols = \"c \", \n    inline = TRUE, \n    sep = \" \") |&gt; \n  add_footer_lines(\n    as_paragraph_md(\n      \"^\\\\*&numsp;^*p* &lt; .05. ^\\\\*\\\\*&numsp;^*p* &lt; .01.\"))\n\n\n\n\nTable 1\n\n\nDescriptive Statistics and Correlations for Study Variables\n\n\n\n\nVariablenMSD12345671. Internal–external statusa 3,697 0.430.48—2. Manager jobperformance2,134 3.120.61−.08**—3. Starting salaryb 3,697 1.010.27.47**−.02—4. Subsequentpromotion3,697 0.330.47.08**−.07**.05**—5. Organizationtenure3,697 6.516.65−.27**.08**.03*.12**—6. Unit serviceperformancec 3,50584.926.92.25**−.35**.25**.08**.02—7. Unit financialperformancec   69442.295.84−.02.07.05−.09*−.01.11**—a 0 = internal hires and 1 = external hires. b A linear transformation was performed on the starting salary values to maintain pay practice confidentiality. The standard deviation (0.27) can be interpreted as 27% of the average starting salary for all managers. Thus, ±1 SD includes a range of starting salaries from 73% (i.e., 1.00 – 0.27) to 127% (i.e., 1.00 + 0.27) of the average starting salaries for all managers. c Values reflect the average across 3 years of data. * p &lt; .05. ** p &lt; .01.\n\n\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.10 in {R} with Apa7},\n  date = {2025-09-20},\n  url = {https://wjschne.github.io/posts/apatables/apa710.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 20). Recreating APA Manual Table 7.10\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa710.html"
  },
  {
    "objectID": "posts/apatables/apa712.html",
    "href": "posts/apatables/apa712.html",
    "title": "Recreating APA Manual Table 7.12 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 12 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nSelective indentation with flextable::padding\nAligned significance stars with add_star_column\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.12\n\n\n\n\n\n\n\nTo make the process more realistic, I simulated data using the means and standard deviations from Figure 1, calculated descriptive statistics, conducted one-way ANOVAs, and extracted significance tests and effect sizes.\nNote that the F column’s stars were added using the add_star_column function, which looks for a column called p, and converts p-values to stars.\n\nset.seed(123)\n\n# Make data\nd_descriptives &lt;- \"\nMeasure                 Urban_M Urban_SD        Rural_M     Rural_SD\nSelf-esteem             2.91        0.49        3.35        0.35\nSocial support          4.22        1.50        5.56        1.20\nThreat                  2.78        0.87        1.99        0.88\nChallenge               2.48        0.88        2.83        1.20\nSelf-efficacy           2.65        0.79        3.53        0.92\" |&gt;\n  readr::read_tsv() |&gt;\n  suppressMessages() |&gt; \n  pivot_longer(-Measure) |&gt; \n  separate(name, c(\"type\", \"name\")) |&gt; \n  pivot_wider() |&gt; \n  mutate(data = map2(M, SD, \\(m,sd) rnorm(148, m, sd))) |&gt; \n  select(Measure, type, data) |&gt; \n  unnest(data) |&gt; \n  rename(y = data) |&gt; \n  nest(.by = Measure)\n\n# Analysis\nd_analysis &lt;- d_descriptives |&gt; \n  mutate(\n    msd = map(data, \\(d) {\n      d |&gt; \n        summarise(M = mean(y),\n                  SD = sd(y), \n                  .by = type) \n    }),\n    fit = map(data, \\(d) aov(y ~ type, data = d)),\n    mp = map(fit, \\(fit) {\n      parameters::model_parameters(fit) |&gt;\n        as_tibble() |&gt;\n        slice(1)\n      }\n      ),\n    `F` = map_dbl(mp, \"F\"),\n    p = map_dbl(mp, \"p\"),\n    eta2 = map_dbl(fit, \\(fit) {\n      eta2 &lt;- suppressMessages(\n        effectsize::effectsize(fit, type = \"eta2\" ))\n      \n      as_tibble(eta2)$Eta2\n        \n        }))\n\n# Get residual degrees of freedom\ndf_residual &lt;- d_analysis$fit[[1]]$df.residual\n\n# Format data\nd_formatted &lt;- d_analysis |&gt; \n  unnest(msd) |&gt; \n  pivot_wider_name_first(names_from = type, \n                         values_from = c(M, SD)) |&gt; \n  select(Measure, contains(\"_\"), `F`, p, eta2)  |&gt; \n  mutate(`F` = align_chr(`F`)) |&gt; \n  add_star_column(`F`, merge = TRUE) |&gt; \n  select(-p) |&gt; \n  rename_with(\\(x) ifelse(x == \"F\", \n                          paste0(\"*F*(1,\", df_residual, \")\"), \n                          x)) |&gt; \n  dplyr::add_row(Measure = \"Cognitive appraisals\", .after = 2) \n\n# Make table\nd_formatted |&gt; \n  apa_flextable() |&gt; \n  padding(i = 4:6, j = 1, padding.left = 20) |&gt; \n  width(width = c(1.65, .8, .8, .05, .8, .8, .9, .7))\n\n\n\n\nTable 1\n\n\nMeans, Standard Deviations, and One-Way Analyses of Variance in Psychological and Social Resources and Cognitive Appraisals\n\n\n\n\nMeasureUrbanRuralF(1,294)η2MSDMSDSelf-esteem2.900.463.380.33   109.38***.27Social support4.201.555.611.13    79.88***.21Cognitive appraisalsThreat2.710.882.090.95    34.57***.11Challenge2.510.932.821.13   6.61*.02Self-efficacy2.660.793.500.89    72.78***.20\n\n\n\n\n\n\n* p &lt; .05; ** p &lt; .01 *** p &lt; .001\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.12 in {R} with Apa7},\n  date = {2025-09-22},\n  url = {https://wjschne.github.io/posts/apatables/apa712.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 22). Recreating APA Manual Table 7.12\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa712.html"
  },
  {
    "objectID": "posts/apatables/apa714.html",
    "href": "posts/apatables/apa714.html",
    "title": "Recreating APA Manual Table 7.14 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 14 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nAligning to only the left side of a decimal using the side parameter of the align_chr function\nBold markdown with bold_md\nUse of non-breaking space (&nbsp;) to prevent markdown from treating numbered text as ordered lists\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.14\n\n\n\n\n\n\n\nThe PCAT items have row titles, have hanging indents, and are decimal aligned. This kind of formatting is not too hard in MS Word, but requires some pre-processing for flextable. We can use align_chr to align by the first decimal. However, we need to prevent additional spaces to be added to the right side of the first decimals. Doing so will prevent strange formatting side effects after applying hanging_indent. So we specify side = left in align_chr to add additional figures spaces on the left side only.\nThe colformat_md function automatically formats text to markdown. Unfortunately, the PCAT items look like ordered lists to the markdown parser. To prevent this conversion, we insert a non-breaking space (&nbsp;) after the first decimal.\nThe factor loadings need to be bolded selectively (if &gt; .3). The flextable::bold function is great for selective bolding. However, the loadings are not really numbers by the time they get to flextable (i.e., they have text minus signs, figure spaces, and leading zeroes removed), which makes conditional processing tricky. It was easier to preprocess the loadings, selectively surrounding them with markdown bold tags with bold_md.\n\n# Make data\nd &lt;- tibble(\n       Factor = c(\"Factor 1: Tenderness--Positive\",\n                  \"Factor 1: Tenderness--Positive\",\"Factor 1: Tenderness--Positive\",\n                  \"Factor 1: Tenderness--Positive\",\n                  \"Factor 1: Tenderness--Positive\",\"Factor 2: Liking\",\"Factor 2: Liking\",\n                  \"Factor 2: Liking\",\"Factor 2: Liking\",\"Factor 2: Liking\",\n                  \"Factor 3: Protection\",\"Factor 3: Protection\",\"Factor 3: Protection\",\n                  \"Factor 3: Protection\",\"Factor 3: Protection\"),\n    `PCAT Item` = c(\"20. You make a baby lagh over and over again by making silly faces.\",\n                  \"22. A child blows you kisses to say goodbye.\",\n                  \"16. A newborn baby curls its hand around your finger.\",\n                  \"19. You watch as a toddler takes their first step and tumboles gently back down.\",\n                  \"25. You see a father tossing his giggling baby up into the air as a game.\",\n                  \"5. I think that kids are annoying. (R)\",\n                  \"8. I can't stand how children whine all the time. (R)\",\n                  \"2. When I hear a child crying, my first thought is \\\"shut up!\\\" (R)\",\n                  \"11. I don't like to be around babies. (R)\",\n                  \"14. If I could, I would hire a nanny to take care of my children. (R)\",\n                  \"7. I would hurt anyone who was a threat to a child.\",\n                  \"12. I would show no mercy to someone who was a danger to a child.\",\n                  \"15. I would use any means necessary to protect a child, even if I had to hurt others.\",\n                  \"4. I would feel compelled to punish anyone who tried to hurt a child.\",\n                  \"9. I would sooner go to bed hungry than let a child go without food.\"),\n    `Factor loading_1` = c(\n      0.86, 0.85, 0.84, 0.77, 0.7, -0.01, -0.12,\n      0.04, 0.11, 0.08, -0.13, 0.0, 0.06, 0.07, 0.46),\n    `Factor loading_2` = c(\n      0.04, -0.02, -0.06, 0.05, 0.1, 0.95, 0.83,\n      0.72, 0.7, 0.58, -0.02, -0.05, 0.08, 0.03, -0.03),\n    `Factor loading_3` = c(\n      0.01, -0.01, 0, -0.07, -0.03, 0.06, -0.03,\n      0.01, -0.01, -0.02, 0.95, 0.74, 0.72, 0.68, 0.36)\n) \n\n# Format data\nd_formatted &lt;- d |&gt; \n  mutate(`PCAT Item` = align_chr(`PCAT Item`, side = \"left\") |&gt; \n           hanging_indent(width = 75, indent = 6) |&gt; \n           str_replace(\"\\\\. \", \".&nbsp;\"))  |&gt;  \n    mutate(\n    across(\n      starts_with(\"Factor loading\"), \n      \\(x) {\n        y &lt;- align_chr(x, trim_leading_zeros = TRUE)\n        ifelse(abs(x) &gt; .3, bold_md(y), y)\n        }\n      )\n    )\n\n# Make table\nd_formatted |&gt; \n  apa_flextable(row_title_column = Factor, \n                line_spacing = 1.5) |&gt; \n  align(j = 1, align = \"left\") \n\n\n\n\nTable 1\n\n\nResults From a Factor Analysis of the Parental Care and Tenderness (PCAT) Questionnaire\n\n\n\n\nPCAT ItemFactor loading123Factor 1: Tenderness–Positive20. You make a baby lagh over and over again by making silly faces. .86 .04 .0122. A child blows you kisses to say goodbye. .85−.02−.0116. A newborn baby curls its hand around your finger. .84−.06 .0019. You watch as a toddler takes their first step and tumboles gently back      down. .77 .05−.0725. You see a father tossing his giggling baby up into the air as a game. .70 .10−.03Factor 2: Liking 5. I think that kids are annoying. (R)−.01 .95 .06 8. I can’t stand how children whine all the time. (R)−.12 .83−.03 2. When I hear a child crying, my first thought is “shut up!” (R) .04 .72 .0111. I don’t like to be around babies. (R) .11 .70−.0114. If I could, I would hire a nanny to take care of my children. (R) .08 .58−.02Factor 3: Protection 7. I would hurt anyone who was a threat to a child.−.13−.02 .9512. I would show no mercy to someone who was a danger to a child. .00−.05 .7415. I would use any means necessary to protect a child, even if I had to      hurt others. .06 .08 .72 4. I would feel compelled to punish anyone who tried to hurt a child. .07 .03 .68 9. I would sooner go to bed hungry than let a child go without food. .46−.03 .36\n\n\n\n\n\n\nNote. N = 307. The extraction method was principal axis factoring with an oblique (promax with Kaiser Normalization) rotation. Factor loadings above .30 are in bold. Reverse-scored items are denoted with (R). Adapted from “Individual Differences in Activation of the Parental Care Motivational System: Assessment, Prediction, and Implications,” by E. E. Buckels, A. T. Beall, M. K. Hofer, E. Y. Lin, Z. Zhou, and M. Schaller, 2015, Journal of Personality and Social Psychology, 108(3), p. 501 (https://doi.org/10.1037/pspp0000023). Copyright 2015 by the American Psychological Association.\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.14 in {R} with Apa7},\n  date = {2025-09-24},\n  url = {https://wjschne.github.io/posts/apatables/apa714.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 24). Recreating APA Manual Table 7.14\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa714.html"
  },
  {
    "objectID": "posts/apatables/apa716.html",
    "href": "posts/apatables/apa716.html",
    "title": "Recreating APA Manual Table 7.16 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 16 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nTwo levels of grouping with flextable::as_grouped_data\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.16\n\n\n\n\n\n\n\nHere I simulate and analyze path models of the actor-partner interdependence model. Getting the row titles to be centered under the t column took some trial-and-error.\n\n```{r}\n#| label: tbl-716\n#| tbl-cap: \"Regressions of Associations Between Marital \n#|           Satisfaction and Average Levels of Marital Behavior\"\n#| apa-note: \"CI&nbsp;=&nbsp;confidence interval; H → H&nbsp;=&nbsp;husband-as-actor \n#|           effect on the husband’s own marital satisfaction; \n#|           W → W&nbsp;=&nbsp;wife-as-actor effect on the wife's own marital \n#|           satisfaction; W → H&nbsp;=&nbsp;wife-as-partner effect on the \n#|           husband’s satisfaction; H → W&nbsp;=&nbsp;husband-as-partner \n#|           effect on the wife's satisfaction.\"\nset.seed(123)\n# Make Coefficients\nd_coefficients &lt;- tibble::tribble(\n        ~Dependent,    ~Group, ~Predictor,     ~b,\n  \"Angry Behavior\",   \"Actor\",       \"H_H\",  -98.9,\n  \"Angry Behavior\",   \"Actor\",       \"W_W\", -87.11,\n  \"Angry Behavior\", \"Partner\",       \"W_H\", -76.18,\n  \"Angry Behavior\", \"Partner\",       \"H_W\",  -91.8,\n       \"Disregard\",   \"Actor\",       \"H_H\", -38.62,\n       \"Disregard\",   \"Actor\",       \"W_W\", -47.54,\n       \"Disregard\", \"Partner\",       \"W_H\", -82.81,\n       \"Disregard\", \"Partner\",       \"H_W\", -79.36,\n      \"Distancing\",   \"Actor\",       \"H_H\", -47.42,\n      \"Distancing\",   \"Actor\",       \"W_W\",   3.04,\n      \"Distancing\", \"Partner\",       \"W_H\",  -0.05,\n      \"Distancing\", \"Partner\",       \"H_W\",  -53.5\n  ) \n\n# Make Model\nmodel &lt;- d_coefficients |&gt; \n  mutate(b = b / 100) |&gt;\n  unite(Predictor, c(Group, Predictor)) |&gt; \n  mutate(across(c(Dependent, Predictor), snakecase::to_snake_case)) |&gt; \n  unite(b, c(b, Predictor), sep = \" * \") |&gt; \n  summarise(b = paste(b, collapse = \" + \"), .by = Dependent) |&gt; \n  unite(Model, c(Dependent, b), sep = \" ~ \") |&gt; \n  pull(Model) \n\n# Make data\nd_simulated &lt;- lavaan::simulateData(model, sample.nobs = 50) |&gt; \n  as_tibble() |&gt; \n  mutate(id = row_number(), .before = 0) |&gt; \n  pivot_longer(angry_behavior:distancing, \n               values_to = \"y\", names_to = \"Dependent\") |&gt; \n  mutate(y = y  * 100) |&gt; \n  nest(data = -Dependent)\n\n\n# Analyze data\nd_analysis &lt;- d_simulated |&gt; \n  mutate(\n    fit = map(data, \\(d) {\n      lm(y ~ ., data = d |&gt; select(-id))\n      }),\n    tbl = map(fit, parameters::parameters))\n  \n# Format data\nd_formatted &lt;- d_analysis |&gt; \n  select(Dependent, tbl) |&gt; \n  unnest(tbl) |&gt; \n  apa_format_columns() |&gt; \n  filter(Variable != \"Constant\") |&gt; \n  separate(Variable, c(\"Type\", \"i\", \"d\")) |&gt; \n  mutate(across(c(i,d), toupper)) |&gt; \n  unite(Variable, c(i, d), sep = c(\" \\u2192 \")) |&gt; \n  relocate(`95% CI`, .after = last_col()) |&gt; \n  as_grouped_data(groups = c(\"Dependent\", \"Type\")) |&gt; \n  mutate(across(c(Type, Variable), \\(x) tidyr::replace_na(x, \"\"))) |&gt; \n  mutate(Variable = paste0(Type, Variable, recycle0 = TRUE),\n         Dependent = snakecase::to_sentence_case(Dependent)) |&gt; \n  fill(Dependent) |&gt; \n  filter(Variable != \"\") |&gt; \n  select(-Type)\n\n# Make Table\nd_formatted |&gt; \n  apa_flextable(row_title_column = Dependent, \n                row_title_align = \"center\",\n                line_spacing = 1.5, \n                no_format_columns = Variable) |&gt; \n  padding(j = 1, \n          i = ~`*B*` != \"\", \n          padding.left = 20) |&gt; \n  width(width = c(1.1, rep(.85, 4), 2)) \n```\n\n\n\n\nTable 1\n\n\nRegressions of Associations Between Marital Satisfaction and Average Levels of Marital Behavior\n\n\n\n\nVariableBSEtp95% CIAngry behaviorActorH → H−88.7714.91−5.96&lt;.001[−118.79, −58.75]W → W−83.7213.84−6.05&lt;.001[−111.60, −55.84]PartnerW → H−70.9217.41−4.07&lt;.001[−105.98, −35.86]H → W−74.3816.11−4.62&lt;.001[−106.83, −41.93]DisregardActorH → H−33.5114.08−2.38 .02 [ −61.87,  −5.15]W → W−65.6713.07−5.02&lt;.001[ −92.00, −39.34]PartnerW → H−78.8916.44−4.80&lt;.001[−112.01, −45.77]H → W−83.3015.22−5.47&lt;.001[−113.95, −52.65]DistancingActorH → H−31.4714.10−2.23 .03 [ −59.86,  −3.08]W → W  9.4713.09 0.72 .47 [ −16.90,  35.83]PartnerW → H−14.7916.47−0.90 .37 [ −47.95,  18.37]H → W−73.3415.24−4.81&lt;.001[−104.03, −42.65]\n\n\n\n\n\n\nNote. CI = confidence interval; H → H = husband-as-actor effect on the husband’s own marital satisfaction; W → W = wife-as-actor effect on the wife’s own marital satisfaction; W → H = wife-as-partner effect on the husband’s satisfaction; H → W = husband-as-partner effect on the wife’s satisfaction.\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.16 in {R} with Apa7},\n  date = {2025-09-26},\n  url = {https://wjschne.github.io/posts/apatables/apa716.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 26). Recreating APA Manual Table 7.16\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa716.html"
  },
  {
    "objectID": "posts/apatables/apa718.html",
    "href": "posts/apatables/apa718.html",
    "title": "Recreating APA Manual Table 7.18 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 18 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nPresenting a sequence of regression models\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(lavaan)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.18\n\n\n\n\n\n\n\nTo make the conversion of analysis to table more realistic, I simulated data to resemble the Step 3 model.\n\n```{r}\n#| label: tbl-718\n#| tbl-cap: \"Hierarchical Regression: Results for Well-Being\"\n#| apa-note: \n#|   - \"CI = confidence interval; *LL*&nbsp;=&nbsp;lower limit; \n#|   *UL*&nbsp;=&nbsp;upper limit; familismo = the collective \n#|   importance of family unity that emphasizes interdependence\n#|   and solidarity; Mex Am margin&nbsp;=&nbsp;Mexican \n#|   American marginalization.\"\n#|   - \"^\\\\*^*p* &lt; .05. ^\\\\*\\\\*^*p* &lt; .01. ^\\\\*\\\\*\\\\*^*p* &lt; .001.\"\nset.seed(123)\n# Make coefficients\nd_coefficients &lt;- tibble::tribble(\n  ~Step,                ~Variable, ~beta,    ~b,\n     1L, \"Perceived social class\",  0.31,  0.45,\n     1L,       \"Generation level\", -0.01, -0.01,\n     2L,              \"Familismo\",  0.23,  0.37,\n     2L,          \"Acculturation\",  0.05,  0.11,\n     2L,          \"Enculturation\",  0.24,  0.35,\n     2L,        \"Mex {Am} margin\", -0.17, -0.23,\n     3L,   \"Masculinity ideology\", -0.18, -0.05\n  ) |&gt; \n  mutate(vname = snakecase::to_any_case(\n    Variable, \n    case = \"parsed\") |&gt; fct_inorder(),\n    Variable = fct_inorder(Variable))\n\n# Make data\nd &lt;- d_coefficients |&gt; \n  unite(vname, c(beta, vname), \n        sep = \" * \") |&gt; \n  pull(vname) |&gt; \n  paste(collapse = \" + \") |&gt; \n  {\n    \\(x) paste0(\"y ~ \", x)\n    }() |&gt; \n  simulateData(standardized = TRUE, sample.nobs = 125) |&gt; \n  mutate(y = y + 4) |&gt; \n  as_tibble()\n\n# Rescale data\nfor (i in seq_len(nrow(d_coefficients))) {\n  d[,d_coefficients[i,]$vname] &lt;- d[,d_coefficients[i,]$vname] * \n    d_coefficients[i,]$beta / \n    d_coefficients[i,]$beta\n}\n\n# Analyze data\nd_analysis &lt;- d_coefficients |&gt;\n  select(Step, vname) |&gt;\n  crossing(Model = 1:3) |&gt;\n  arrange(Model, Step) |&gt;\n  filter(Step &lt;= Model) |&gt;\n  select(-Step) |&gt;\n  summarise(f = paste0(vname, collapse = \" + \"), \n            .by = Model) |&gt;\n  mutate(\n    eq = paste0(\"y ~ \", f) |&gt; map(formula),\n    fit = map(eq, lm, data = d),\n    Model = paste(\"Model\", Model)\n  )\n\n# Format data\nd_formatted &lt;- d_analysis |&gt;\n  mutate(\n    parameters = map(fit, model_parameters),\n    std_parameters = map(fit, \n                         model_parameters, \n                         standardize  = \"basic\") |&gt; \n      map(\\(d) select(d, Std_Coefficient))\n  ) |&gt;\n  select(Model, parameters, std_parameters) |&gt;\n  unnest(c(parameters, std_parameters)) |&gt;\n  select(-CI, -t, -df_error) |&gt;\n  rename(pp = p) |&gt;\n  relocate(SE, .before = Std_Coefficient) |&gt;\n  apa_format_columns() |&gt;\n  rename(`95% CI for *B*_*LL*` = LL,\n         `95% CI for *B*_*UL*` = UL) |&gt;\n  as_grouped_data(\"Model\") |&gt;\n  left_join(\n    d_analysis |&gt;\n      pull(fit) |&gt;\n      apa_performance_comparison(\n        starred = \"deltaR2\", \n        metrics = c(\"R2\", \"deltaR2\")),\n    by = join_by(Model)\n  ) |&gt;\n  replace_na(list(Model = \"\", Variable = \"\")) |&gt;\n  unite(Variable, c(Variable, Model), sep = \"\") |&gt;\n  mutate(`&beta;` = ifelse(Variable == \"Constant\", NA, `&beta;`)) |&gt; \n  add_star_column(c(`*B*`, `&beta;`), p = pp) |&gt; \n  select(-pp) |&gt;\n  mutate(Variable = str_replace(Variable, \"^Model\", \"Step\") |&gt; \n           snakecase::to_sentence_case() |&gt; \n           str_replace(\" am \", \" Am \"))\n\n# Make table\nd_formatted |&gt; \n  apa_flextable(line_spacing = 1.5, \n                layout = \"fixed\") |&gt;\n  padding(i = ~!is.na(`*B*`), \n          j = 1, \n          padding.left = 15) |&gt;\n  surround(i = ~is.na(`*B*`), \n           border.top = fp_border_default(color = \"gray30\")) |&gt; \n  width(width = 6.5 * c(3, .6, .4, 1, 1, 1, \n                           .6, .4, 1, 1) / 10)\n```\n\n\n\n\nTable 1\n\n\nHierarchical Regression: Results for Well-Being\n\n\n\n\nVariableB95% CI for BSEβR2ΔR2LLULStep 1.03Constant 4.00*** 3.85 4.160.08Perceived Social Class 0.15−0.01 0.300.08 .16Generation Level−0.04−0.19 0.110.08−.04Step 2.16.13***Constant 4.03*** 3.89 4.180.07Perceived Social Class 0.19* 0.04 0.340.08 .21*Generation Level−0.04−0.18 0.100.07−.05Familismo 0.18* 0.03 0.320.07 .21*Acculturation 0.06−0.09 0.220.08 .07Enculturation 0.09−0.06 0.240.08 .10Mex Am Margin−0.23**−0.38−0.080.08−.26**Step 3.23.06** Constant 4.03*** 3.89 4.170.07Perceived Social Class 0.18* 0.03 0.320.07 .20*Generation Level−0.08−0.22 0.060.07−.09Familismo 0.18* 0.04 0.320.07 .21*Acculturation 0.06−0.09 0.210.07 .07Enculturation 0.09−0.06 0.240.07 .10Mex Am Margin−0.22**−0.37−0.080.07−.26**Masculinity Ideology−0.22**−0.37−0.080.07−.26**\n\n\n\n\n\n\nNote. CI = confidence interval; LL = lower limit; UL = upper limit; familismo = the collective importance of family unity that emphasizes interdependence and solidarity; Mex Am margin = Mexican American marginalization.\n\n\n*p &lt; .05. **p &lt; .01. ***p &lt; .001.\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.18 in {R} with Apa7},\n  date = {2025-09-28},\n  url = {https://wjschne.github.io/posts/apatables/apa718.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 28). Recreating APA Manual Table 7.18\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa718.html"
  },
  {
    "objectID": "posts/apatables/apa720.html",
    "href": "posts/apatables/apa720.html",
    "title": "Recreating APA Manual Table 7.20 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 20 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nDisplay of multilevel models\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(lme4)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.20\n\n\n\n\n\n\n\nI wanted the experience of making the table close to what would happen in a real analysis. Instead of copying table’s text exactly, I simulated multilevel data based on the results in Figure 1. Although there are functions that can simulate multilevel data, In my experience, I can more easily get exactly what I want if I write out the equations and them simulate the data myself. The only information I lacked were the sample sizes at levels 1 and 2 and the number of times these behaviors were measured in the observational study. I supplied some numbers that made the models run without error.\n\nLevel 1\n\n\\begin{aligned}\nY_{ij} &= b_{0j} + b_{1j}Time_{ij} + b_{2j}Time_{ij}^2 + e_{ij}\\\\\ne_{ij}&\\sim\\mathcal{N}\\left(0,\\tau_1\\right)\n\\end{aligned}\n\n\n\nLevel 2\n\n\\begin{aligned}\nb_{0j} &= b_{00}+b_{01}PBS_j+b_{02}SFB_j+e_{0j}\\\\\nb_{1j} &= b_{10}+b_{11}PBS_j+b_{12}SFB_j+e_{1j}\\\\\nb_{2j} &= b_{20}+b_{21}PBS_j+b_{22}SFB_j+e_{2j}\\\\\ne_2&=\\begin{bmatrix}\ne_{0j}\\\\\ne_{1j}\\\\\ne_{2j}\n\\end{bmatrix}\\\\\n\\tau_2&=\\begin{bmatrix}\n\\tau_{00}&0&0\\\\\n0&\\tau_{11}&0\\\\\n0&0&\\tau_{22}\\\\\n\\end{bmatrix}\\\\\ne_2&\\sim\\mathcal{N}\\left(0,\\tau_2\\right)\n\\end{aligned}\n\nBecause of the stars after the coefficients, I was not able to find a good option for aligning both the coefficients and standard errors in the “Model” columns, so I aligned the coefficients only.\nGetting coefficients, variance components, and fit statistics in the same table required a lot of custom formatting. Automating some of this would be a good idea.\n\nset.seed(123)\n# Number of people\nn_2 &lt;- 400\n# Number of time points\nn_1 &lt;- 15\n\nd_parameter &lt;- tibble::tibble(\n  symbol = c(\n    \"b_00\", \"b_01\", \"b_02\",\n    \"b_10\", \"b_11\", \"b_12\",\n    \"b_20\", \"b_21\", \"b_22\",\n    \"s_e\", \"s_0\", \"s_1\", \"s_2\"),\n  Positive = c(\n    3.34, 0.39, 0.26, -0.002, -0.06, 0.001,\n    -0.001, -0.03, -0.01, 0.52, 1.31, 0.04, 0.03),\n  Negative = c(\n    1.82, -0.36, -0.16, 0.01, 0.02, -0.04, 0.02, \n    0.01, -0.01, 0.51, 1, 0.02, 0.02),\n  Parameter = c(\n     \"(Intercept)\",\n     \"PBS\",\n     \"SFB\",\n     \"Time\",\n     \"PBS:Time\",\n     \"Time:SFB\",\n     \"I(Time^2)\",\n     \"PBS:I(Time^2)\",\n     \"I(Time^2):SFB\",\n     \"SD (Observations)\",\n     \"SD (Intercept)\",\n     \"SD (Time)\", \n     \"SD (I(Time^2))\"),\n  pname = c(\n    \"Intercept\",\n    \"Prosocial behavior\",\n    \"Self-focused behavior\",\n    \"Time\",\n    \"Prosocial behavior\", \n    \"Self-focused behavior\",\n    \"Time^2\",\n    \"Prosocial behavior\", \n    \"Self-focused behavior\",\n    \"Level 1\",\n    \"Level 2\",\n    \"Level 2\",\n    \"Level 2\"),\n  l2 = c(\n    rep(\"Status at postest, &pi;~0j~\", 3),\n    rep(\"Linear rate of change, &pi;~1j~\", 3),\n    rep(\"Quadratic rate of change, &pi;~2j~\", 3),\n    rep(\"Variance components\", 4)),\n  effect_type = c(\n    rep(\"Fixed\", 9),\n    rep(\"Random\", 4))\n) |&gt; \n  mutate(symbol = fct_inorder(symbol),\n         pname = fct_inorder(pname))\n\n# Level 1 variances\ntau_1 &lt;- d_parameter |&gt; \n  filter(symbol == \"s_e\") |&gt; \n  select(symbol, Positive, Negative) |&gt;\n  pivot_longer(-symbol) |&gt; \n  select(name, value) |&gt; \n  deframe()\n\n# Level 2 variances\ntau_2 &lt;- d_parameter |&gt; \n  filter(str_starts(symbol, \"s_\")) |&gt; \n  filter(symbol != \"s_e\") |&gt; \n  select(symbol, Positive, Negative) |&gt;\n  pivot_longer(-symbol) |&gt;\n  select(-symbol) |&gt; \n  nest(value = -name) |&gt; \n  mutate(value = map(\n    value, \n    \\(x) diag(unlist(x)))) |&gt;\n  deframe()\n\n# Fixed effects\nd_parameter_fixed &lt;- d_parameter |&gt; \n  filter(effect_type == \"Fixed\") |&gt; \n  select(symbol, Positive, Negative) |&gt; \n  pivot_longer(-symbol, \n               names_to = \"dv\", \n               values_to = \"b\") |&gt;\n  pivot_wider(names_from = symbol, \n              values_from = b)\n\n# Level 2 simulated data\nd_2 &lt;- tibble(\n  id = seq_len(n_2),\n  Intercept = 1,\n  PBS = rnorm(n_2),\n  SFB = rnorm(n_2)\n) |&gt; \n  crossing(\n    dv = fct_inorder(c(\"Positive\", \n                       \"Negative\"))) |&gt; \n  nest(data = -dv) |&gt; \n    mutate(\n      tau_2 = tau_2, \n      e_2 = map(\n        tau_2, \n        \\(x) {\n          mvtnorm::rmvnorm(\n            n = n_2, \n            mean = c(\n              e_0j = 0, \n              e_1j = 0, \n              e_2j = 0), \n            sigma = x) |&gt; \n            as_tibble()\n        })) |&gt; \n    select(-tau_2) |&gt; \n    unnest(c(data, e_2)) |&gt; \n  left_join(d_parameter_fixed, by = join_by(dv)) |&gt; \n  mutate(b_0j = b_00 + b_01 * PBS + b_02 * SFB + e_0j,\n         b_1j = b_10 + b_11 * PBS + b_12 * SFB + e_1j,\n         b_2j = b_20 + b_21 * PBS + b_22 * SFB + e_2j)\n\n# Level 1 simulated data\nd_1 &lt;- d_2 |&gt; \n  crossing(\n    Time = seq(0, n_1 - 1)\n  ) |&gt; \n  nest(data = -dv) |&gt; \n  mutate(tau_1 = tau_1,\n         e_ij = map2(tau_1, data, \\(tau, d) {\n           rnorm(nrow(d), sd = sqrt(tau))\n           })) |&gt; \n  unnest(c(data, e_ij)) |&gt;\n  mutate(\n    y = b_0j + b_1j * Time + b_2j * Time^2 + e_ij) |&gt; \n  select(dv, id, PBS, SFB, Time, y) \n\n# Analyze data\nfit &lt;- d_1 |&gt; \n  nest(data = -dv) |&gt; \n  mutate(\n    `Model 1` = map(\n      data, \n      \\(d) {\n        lmer(y ~ 1 + Time + I(Time^2) + \n               (1 + Time + I(Time^2) || id), \n             data = d)\n      }),\n    `Model 2` = map(\n      data, \n      \\(d) {\n        lmer(y ~ 1 + PBS * Time + PBS * I(Time^2) + \n               SFB * Time + SFB * I(Time^2) + \n               (1 + Time + I(Time^2) || id), \n             data = d)\n        })\n      ) |&gt; \n  select(-data) |&gt; \n  pivot_longer(-c(dv), \n               names_to = \"Model\", \n               values_to = \"fit\") |&gt; \n  mutate(\n    parameters = map(\n      fit, parameters::parameters, \n      include_sigma = TRUE))\n\n# Peformance statistics\nd_performance &lt;- fit |&gt; \n  select(dv, fit) |&gt; \n  summarise(fit = list(fit), .by = dv) |&gt; \n  mutate(p = map(fit, \\(f1) {\n    anova(f1[[1]], f1[[2]]) |&gt; \n      as_tibble() |&gt; \n      rename(Deviance = `-2*log(L)`, \n             deltachi2 = Chisq,\n             df_diff = Df,\n             p = `Pr(&gt;Chisq)`) |&gt; \n      mutate(Model = paste0(\"Model \", 1:2),\n             chistar = p2stars(p),\n             Deviance = align_chr(Deviance)) |&gt; \n      select(\n        Model, \n        Deviance, \n        deltachi2, \n        df_diff, \n        chistar) |&gt; \n      apa_format_columns() |&gt; \n      unite(`&Delta;&chi;^2^`, \n            `&Delta;&chi;^2^`, \n            chistar, \n            sep = \"\")\n    })) |&gt; \n  select(-fit) |&gt; \n  arrange(desc(dv)) |&gt; \n  unnest(p) |&gt; \n  mutate(dv = paste0(dv, \" emotions\")) |&gt;\n  unite(dv, c(dv, Model)) |&gt; \n  mutate(dv = fct_inorder(dv)) |&gt; \n  pivot_longer(-dv, names_to = \"Effect\") |&gt; \n  pivot_wider(names_from = dv) |&gt; \n  mutate(effect_type = \"Goodness of fit\") |&gt; \n  suppressMessages()\n  \n# Format data\nd &lt;- fit |&gt; \n  unnest(parameters) |&gt; \n  select(dv, Model, Parameter, Coefficient, SE, p) |&gt; \n  mutate(Coefficient = align_chr(Coefficient)) |&gt; \n  add_star_column(Coefficient, merge = TRUE) |&gt; \n  mutate(SE = align_chr(SE, \n                                    accuracy = .01) |&gt; \n           tagger(\" (\", \")\"), \n         b = paste0(Coefficient, SE),\n         dv = fct_inorder(paste(dv, \"emotions\"))) |&gt; \n  select(dv, Model, Parameter, b) |&gt; \n  unite(dv, c(dv, Model)) |&gt; \n  pivot_wider(names_from = dv, values_from = b) |&gt; \n  left_join(\n    d_parameter |&gt; \n      select(Parameter, \n             symbol, \n             pname, \n             l2, \n             effect_type), \n    by = join_by(Parameter)) |&gt; \n  arrange(symbol) |&gt; \n  rename(Effect = pname) |&gt; \n  select(-Parameter) |&gt; \n  relocate(Effect, symbol, .before = 0) |&gt; \n  mutate(symbol = as.character(symbol) |&gt; \n           str_replace_all(\n             c(`s_` = \"&sigma;~\",\n               `b_` = \"&gamma;~\")) |&gt; \n           paste0(\"~\", ifelse(\n             str_detect(symbol, \"^s\"), \n             \"^2^\", \n             \"\"))) |&gt; \n  mutate(Effect = as.character(Effect) |&gt; str_replace(\"\\\\^2\", \"^2^\")) |&gt; \n  as_grouped_data(c(\"effect_type\", \"l2\"))  |&gt; \n  mutate(Effect = ifelse(is.na(l2), Effect, l2)) |&gt; \n  select(-l2) |&gt; \n  tidyr::fill(effect_type) |&gt; \n  filter(!is.na(Effect))  |&gt; \n  bind_rows(d_performance) |&gt; \n  mutate(across(\n    everything(), \n    \\(x) replace_na(x, replace = \"\"))) |&gt; \n  rename(`Parameter ` = symbol) \n\n# Make table\nd |&gt; \n  apa_flextable(\n    row_title_column = effect_type, \n    row_title_align = \"center\",\n    font_size = 10, \n    line_spacing = 1.5) |&gt; \n  align(j = 1, i = ~is.na(row_title)) |&gt; \n  padding(\n    j = 1, \n    i = ~(is.na(row_title) & \n            `Parameter ` != \"\"), \n    padding.left = 15) |&gt; \n  merge_v(j = \"Effect\") |&gt; \n  width(width = c(1.70, .55, \n                  1.05, 1.05, .05, \n                  1.05, 1.05))\n\n\n\n\nTable 1\n\n\nModel Parameters and Goodness of Fit for Linear and Quadratic Changes in Emotions by Behavior Type\n\n\n\n\nEffectParameterNegative emotionsPositive emotionsModel 1Model 2Model 1Model 2FixedStatus at postest, π0jInterceptγ00    1.79*** (0.06)    1.80*** (0.06)    3.40*** (0.06)    3.39*** (0.06)Prosocial behaviorγ01   −0.34*** (0.06)    0.25*** (0.06)Self-focused behaviorγ02  −0.17** (0.06)    0.22*** (0.06)Linear rate of change, π1jTimeγ10 0.00 (0.01) 0.00 (0.01) 0.00 (0.01) 0.00 (0.01)Prosocial behaviorγ11 0.02 (0.01)   −0.08*** (0.01)Self-focused behaviorγ12   −0.04*** (0.01) 0.02 (0.01)Quadratic rate of change, π2jTime2γ20   0.02** (0.01)   0.02** (0.01) 0.02 (0.01) 0.02 (0.01)Prosocial behaviorγ21 0.00 (0.01)   −0.03*** (0.01)Self-focused behaviorγ22 0.00 (0.01) −0.02* (0.01)RandomVariance componentsLevel 1σe2 0.73 0.73 0.71 0.71Level 2σ02 1.08 1.01 1.15 1.11σ12 0.13 0.13 0.22 0.20σ22 0.13 0.13 0.18 0.18Goodness of fitDeviance17,862.0717,800.9518,214.8118,133.36Δχ261.12***81.45***Δdf66\n\n\n\n\n\n\nNote. Standard errors are in parentheses. All p values in this table are two tailed. In Model 1 (unconditional quadratic growth), the intercept parameter estimate (γ00) represents the average positive or negative emotions score at posttest across the sample. In Model 2 (prosocial and self-focused behavior vs. control), the intercept parameter estimate (γ00) represents the average positive or negative emotions score in the control condition at posttest, γ02 represents the difference at posttest between the prosocial behavior conditions and the control condition, and γ03 represents the difference at posttest between the self-focused behavior condition and the control condition. γ10 represents the average linear rate of change in the control condition, γ11 represents additional effects of prosocial behavior on linear rate of change, and γ12 represents additional effects of self-focused behavior on linear rate of change. Finally, γ20 represents the average quadratic rate of change in the control condition, γ21 represents additional effects of prosocial behavior on quadratic rate of change, and γ22 represents additional effects of self-focused behavior on quadratic rate of change. In all models, the intercept, linear slope (time), and quadratic slope (time2) were free to vary.\n\n\n* p &lt; .05. ** p &lt; .01. *** p &lt; .001.\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.20 in {R} with Apa7},\n  date = {2025-09-30},\n  url = {https://wjschne.github.io/posts/apatables/apa720.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, September 30). Recreating APA Manual Table 7.20\nin R with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa720.html"
  },
  {
    "objectID": "posts/apatables/apa722.html",
    "href": "posts/apatables/apa722.html",
    "title": "Recreating APA Manual Table 7.22 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 22 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nUse of hanging_indent\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(lme4)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.22\n\n\n\n\n\n\n\nMaking this table was relatively straightforward, except that I needed to play with the column widths and hanging indents until they fit in a table that is 6.5 inches wide.\n\n```{r}\n#| label: tbl-722\n#| tbl-cap: \"Master Narrative Voices: Struggle and\n#|           Success and Emancipation\"\ntibble(\n  theme = c(\n    \"Struggle and success\",\n    \"Struggle and success\",\n    \"Emancipation\",\n    \"Emancipation\"),\n  `Discourse and dimension` = c(\n    paste(\"Self actualization as a member of a larger\",\n          \"gay community is the end goal of healthy sexual\",\n          \"identity development or \\\"coming out\\\"\"),\n    paste(\"Maintaining healthy sexual identity entails\",\n          \"vigilance against internalization of\",\n          \"societal discrimination\"),\n    paste(\"Open exploration of an individually fluid\",\n          \"sexual self is the goal of healthy sexual\",\n          \"identity development\"),\n    paste(\"Questioning discrete, monolithic categories of\",\n          \"sexual identity\")),\n  `Example quote` = c(\n    paste(\n      \"\\\"My path of gayness...going from denial to saying,\",\n      \"'well, this is it,' and then the process of coming\",\n      \"out, and the process of just sort of looking around\",\n      \"and seeing, well where do I stand in the world? And\",\n      \"sort of having, uh, political feelings.\\\"\",\n      \"(Carl, age 50)\"),\n    paste(\"\\\"When I'm, like, thinking of criticisms of\",\n      \"more mainstream gay culture, I try to...make sure\",\n      \"it's coming from an appropriate place and not, like,\",\n      \"a place of self loathing.\\\" (Patrick,age 20)\"),\n    paste(\"\\\"[For heterosexuals] the man penetrates the\",\n      \"woman, whereas with gay people, I feel like there\",\n      \"is this potential for really playing around with\",\n      \"that model a lot, you know, and just experimenting\",\n      \"and exploring.\\\" (Orion, age 31)\"),\n    paste(\"\\\"LGBTQI, you know, and added on so many letters.\",\n      \"It does start to raise the question about what the\",\n      \"terms mean and whether...any term can adequately be\",\n      \"descriptive.\\\" (Bill, age 50 )\"))\n) |&gt; \n  mutate(\n    `Discourse and dimension` = hanging_indent(\n      `Discourse and dimension`,\n      width = 29, indent = 4),\n        `Example quote` = hanging_indent(\n      `Example quote`,\n      width = 55, indent = 4)\n    ) |&gt; \n  apa_flextable(\n    row_title_column = theme, \n    line_spacing = 1.5) |&gt; \n  align() |&gt; \n  width(width = c(2.5, 4)) \n```\n\n\n\n\nTable 1\n\n\nMaster Narrative Voices: Struggle and Success and Emancipation\n\n\n\n\nDiscourse and dimensionExample quoteStruggle and successSelf actualization as a    member of a larger gay    community is the end goal    of healthy sexual identity    development or “coming out”“My path of gayness…going from denial to saying,    ‘well, this is it,’ and then the process of coming    out, and the process of just sort of looking around and    seeing, well where do I stand in the world? And sort of    having, uh, political feelings.” (Carl, age 50)Maintaining healthy sexual    identity entails vigilance    against internalization of    societal discrimination“When I’m, like, thinking of criticisms of more    mainstream gay culture, I try to…make sure it’s    coming from an appropriate place and not, like, a place    of self loathing.” (Patrick,age 20)EmancipationOpen exploration of an    individually fluid sexual    self is the goal of healthy    sexual identity development“[For heterosexuals] the man penetrates the woman,    whereas with gay people, I feel like there is this    potential for really playing around with that model a    lot, you know, and just experimenting and exploring.”    (Orion, age 31)Questioning discrete,    monolithic categories of    sexual identity“LGBTQI, you know, and added on so many letters.    It does start to raise the question about what the    terms mean and whether…any term can adequately be    descriptive.” (Bill, age 50 )\n\n\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.22 in {R} with Apa7},\n  date = {2025-10-02},\n  url = {https://wjschne.github.io/posts/apatables/apa722.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, October 2). Recreating APA Manual Table 7.22 in\nR with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa722.html"
  },
  {
    "objectID": "posts/apatables/apa724.html",
    "href": "posts/apatables/apa724.html",
    "title": "Recreating APA Manual Table 7.24 in R with apa7",
    "section": "",
    "text": "Making tables in APA style (Part 24 of 24)\nIn this 24-part series, each of the tables in Chapter 7 of the Publication Manual of the American Psychological Association (7th Edition) is recreated with apa7, flextable, easystats, and tidyverse functions.\n\n\n\n\n\n\nNoteHighlights\n\n\n\n\nUse of hanging_indent\n\n\n\n\nlibrary(apa7)\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(lme4)\nset_flextable_defaults(theme_fun = theme_apa, \n                       font.family = \"Times New Roman\")\n\n\n\n\n\nFigure 1\n\n\nScreenshot of the APA Manual’s Table 7.24\n\n\n\n\n\n\n\nMaking this table was relatively straightforward, except that I needed to play with the column widths and hanging indents until they fit in a table that is 6.5 inches wide.\n\n```{r}\n#| label: tbl-724\n#| tbl-cap: \"Integrated Results Matrix for the Effect of\n#|           Topic Familiarity on Reliance on Author Expertise\"\n#| apa-note: \"We integrated quantitative data (whether\n#|            students selected a card about nuclear power\n#|            or about climate change) and qualitative data\n#|            (interviews with students) to provide a more\n#|            comprehensive description of students’ card\n#|            selections between the two topics.\"\n\ntibble(\n  `Quantitative result` = c(\n    paste(\"When the topic was more (climate change) and\", \n          \"cards were more relevant, participants placed\", \n          \"less value on author expertise.\"),\n    paste(\"When the topic is less familiar (nuclear power)\", \n          \"and cards were more relevant, participants\", \n          \"placed more value on author expertise.\")),\n   `Qualitative result` = c(\n     paste(\"When an assertion was considered to be more\", \n          \"familiar and to be general knowledge,\", \n          \"participants perceived less need to rely on\", \n          \"author expertise.\"),\n     paste(\"When an assertion was considered to be less\", \n          \"familiar and not general knowledge,\", \n          \"participants perceived more need to rely on\", \n          \"offer as brief expertise author expertise.\")),\n  `Example quote` = c(\n    paste(\"Participant 144: \\\"I feel that I know more\", \n          \"about climate, and there are several things\", \n          \"on climate cards that are obvious, and that\", \n          \"if I sort of know it already, then the source\", \n          \"is not so critical... whereas with nuclear\", \n          \"energy. I don't know so much, so then I may\", \n          \"be more interested in who says what.\\\"\"),\n    paste(\"Participant 3: \\\"[Nuclear power], which I know\", \n          \"much, much less about, I would back up my\", \n          \"arguments more with what I trust from the\", \n          \"professors.\\\"\"))\n) |&gt; \n  mutate(\n    `Quantitative result` = hanging_indent(\n      `Quantitative result`, width = 25),\n    `Qualitative result` = hanging_indent(\n      `Qualitative result`, width = 25),\n    `Example quote` = hanging_indent(\n      `Example quote`, width = 32)) |&gt;\n  apa_flextable(line_spacing = 1.5, font_size = 12) |&gt;\n  align() |&gt;\n  width(width = c(2,2,2.5))\n```\n\n\n\n\nTable 1\n\n\nIntegrated Results Matrix for the Effect of Topic Familiarity on Reliance on Author Expertise\n\n\n\n\nQuantitative resultQualitative resultExample quoteWhen the topic was more    (climate change) and    cards were more relevant,    participants placed    less value on author    expertise.When an assertion    was considered to be    more familiar and to    be general knowledge,    participants perceived    less need to rely on    author expertise.Participant 144: “I feel that    I know more about climate, and    there are several things on    climate cards that are obvious,    and that if I sort of know it    already, then the source is    not so critical… whereas with    nuclear energy. I don’t know    so much, so then I may be more    interested in who says what.”When the topic is less    familiar (nuclear power)    and cards were more    relevant, participants    placed more value on    author expertise.When an assertion was    considered to be less    familiar and not general    knowledge, participants    perceived more need    to rely on offer as    brief expertise author    expertise.Participant 3: “[Nuclear power],    which I know much, much less    about, I would back up my    arguments more with what I trust    from the professors.”\n\n\n\n\n\n\nNote. We integrated quantitative data (whether students selected a card about nuclear power or about climate change) and qualitative data (interviews with students) to provide a more comprehensive description of students’ card selections between the two topics.\n\n\n\n\n\n\nFirst\n\n\nPrevious\n\n\nNext\n\n\nLast\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2025,\n  author = {Schneider, W. Joel},\n  title = {Recreating {APA} {Manual} {Table} 7.24 in {R} with Apa7},\n  date = {2025-10-04},\n  url = {https://wjschne.github.io/posts/apatables/apa724.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2025, October 4). Recreating APA Manual Table 7.24 in\nR with apa7. Schneirographs. https://wjschne.github.io/posts/apatables/apa724.html"
  },
  {
    "objectID": "posts/caption-on-same-line-as-axis-title-in-ggplot2/index.html",
    "href": "posts/caption-on-same-line-as-axis-title-in-ggplot2/index.html",
    "title": "Caption on same line as axis title in ggplot2",
    "section": "",
    "text": "Sometimes I want to put a plot caption in the lower right corner of the plot.\n\nlibrary(tidyverse)\np &lt;- ggplot() +\n  stat_function(xlim = c(-4,4), fun = dnorm, n = 801) + \n  labs(x = \"z-scores\", caption = \"Note: Mean = 0, SD = 1\")\n\np\n\n\n\n\n\n\n\n\nHowever, I want the caption to be a little higher, on the same line as the x-axis title. To do so, set a a negative top margin:\n\np + theme(plot.caption = element_text(margin = margin(t = -10, unit = \"pt\")))\n\n\n\n\n\n\n\n\nHere is a more finished example.\n\nlibrary(ggnormalviolin)\nlibrary(ggtext)\nmy_font_size = 16\nmy_font &lt;- \"Roboto Condensed\"\nupdate_geom_defaults(geom = \"text\",new = list(family = my_font) )\nupdate_geom_defaults(geom = \"label\",new = list(family = my_font) )\nupdate_geom_defaults(\"richtext\", list(family = my_font))\ntheme_set(theme_minimal(base_size = my_font_size, base_family = my_font))\n\nd_rect &lt;- tibble(SS = 100, \n                 width = c(20, 40, 60, 80, 122), \n                 fill = paste0(\"gray\", c(95, 90, 80, 70, 65) - 25)) %&gt;% \n  arrange(-width)\n\ntibble(\n  Scale = c(\n    \"Fluid Reasoning\",\n    \"Verbal Comprehension\",\n    \"Visual-Spatial Processing\",\n    \"Working Memory\",\n    \"Processing Speed\",\n    \"Population\"),\n  y = c(5:1 - 0.5, 0),\n  SS = c(115, 111, 109, 86, 79, 100),\n  rxx = c(.93, .92, .92, .92, .88, 0),\n  width = c(rep(1.4, 5), 10.4),\n  alpha = c(rep(1, 5), .3)\n) %&gt;% \n  mutate(true_hat = rxx * (SS - 100) + 100, \n         see = ifelse(rxx == 0, 15, 15 * sqrt(rxx - rxx ^ 2)),\n         Scale = fct_inorder(Scale) %&gt;% fct_rev()) %&gt;% \n  ggplot(aes(y, SS)) + \n  geom_tile(data = d_rect, aes(width = 5.8, x = 2.9, \n                               fill = fill, \n                               height = width, \n                               y = SS)) + \n  geom_normalviolin(aes(mu = true_hat, \n                                        sigma = see, \n                                        width = width,\n                                        alpha = alpha), \n                                    face_left = F, \n                    fill = \"white\") + \n  geom_richtext(aes(label = ifelse(\n    Scale == \"Population\", \n    \"Population Mean\", \n    paste0(\"&lt;span style='font-size:8.5pt;color:white'&gt;(\", \n           round(100 * pnorm(SS, 100, 15),0),\n           \") &lt;/span&gt;\",\n           SS, \n           \"&lt;span style='font-size:9pt;color:#666666'&gt; (\", \n           round(100 * pnorm(SS, 100, 15),0),\n           \")&lt;/span&gt;\"))), \n    vjust = -0.2, \n    lineheight = .8, \n    fill = NA, \n    color = \"gray20\",\n    label.color = NA,\n    label.padding = unit(0,\"mm\")) +\n  geom_text(aes(y = true_hat, \n                label = ifelse(Scale == \"Population\", \n                               \"\", \n                               as.character(Scale))), \n            vjust = 1.5, \n            color = \"gray15\",\n            lineheight = .8) +\n  geom_linerange(aes(ymin = true_hat - 1.96 * see,\n                      ymax = true_hat + 1.96 * see), \n                 linewidth = .5) +\n  geom_pointrange(aes(ymin = true_hat - see,\n                      ymax = true_hat + see), \n                  size = 1.2, \n                  fatten = 1.5) +\n  geom_text(aes(x = x, y = y, label = label), \n            data = tibble(\n              y = c(49.5, 65, 75, 85, 100, 115, 125, 135, 150.5), \n              x = 5.5, \n              label = c(\"Extremely\\nLow Range\", \n                                  \"Very\\nLow\", \n                                  \"Low\\nRange\", \n                                  \"Low\\nAverage\", \n                                  \"Average\\nRange\", \n                                  \"High\\nAverage\", \n                                  \"High\\nRange\", \n                                  \"Very\\nHigh\", \n                                  \"Extremely\\nHigh Range\")), \n            color = \"white\", lineheight = .8, size = 4.25) +\n  scale_y_continuous(\n    \"Standard Scores &lt;span style='font-size:11.7pt;color:#656565'&gt;&lt;br&gt;\n    (and Percentile Ranks)&lt;/span&gt;\", \n    breaks = seq(40, 160, 10), \n    limits = c(37, 163),\n    labels = \\(x) paste0(x,\n                         \"&lt;br&gt;&lt;span style='font-size:10pt;color:#656565'&gt;(\", \n                         pnorm(x,100, 15) %&gt;% \n                           WJSmisc::proportion2percentile(digits = 2) %&gt;%\n                           str_trim(), \n                         \")&lt;/span&gt;\"),\n    expand = expansion()) +\n  scale_x_continuous(NULL, expand = expansion(), breaks = NULL) +\n  scale_alpha_identity() +\n  scale_fill_identity() +\n  coord_flip(clip = \"off\") + \n  theme(axis.title.x = element_markdown(hjust = 0, \n                                        margin = margin(t = 1.25, \n                                                        l = 2,\n                                                        unit = \"mm\")),\n        axis.text.x = element_markdown(colour = \"gray20\"),\n        plot.caption = element_markdown(hjust = 0, \n                                        size = 10, \n                                        margin = margin(t = -10.25, \n                                                        l = 100, \n                                                        unit = \"mm\"), \n                                        color = \"gray40\"),\n        plot.title = element_text(hjust = .025)) +\n  labs(\n    title = \"Display of Cognitive Test Scores\",\n    caption = \"**Notes:** The white normal curves represent the expected \n       true&lt;br&gt;score distributions for each observed score. The black lines&lt;br&gt;\n       underneath span the 68% and 95% confidence intervals.\")\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@misc{schneider2021,\n  author = {Schneider, W. Joel},\n  title = {Caption on Same Line as Axis Title in Ggplot2},\n  date = {2021-07-21},\n  url = {https://wjschne.github.io/posts/caption-on-same-line-as-axis-title-in-ggplot2/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2021, July 21). Caption on same line as axis title in\nggplot2. Schneirographs. https://wjschne.github.io/posts/caption-on-same-line-as-axis-title-in-ggplot2/"
  },
  {
    "objectID": "posts/creating-collapsible-output-with-knitr-chunk-hooks/index.html",
    "href": "posts/creating-collapsible-output-with-knitr-chunk-hooks/index.html",
    "title": "Creating collapsible output with knitr chunk hooks",
    "section": "",
    "text": "I am writing a (still very much unfinished) psychometrics book with suggested exercises using R. I wanted to provide solutions to the questions, but I wanted to hide the answers until the reader clicks them.\nAn easy way to create collapsible content is with html’s &lt;details&gt; tag. For example:\nCreate a variable `x`, and assign it a value of 5.\n\n&lt;details&gt;\n  &lt;summary&gt;Suggested Solution&lt;/summary&gt;\n\n```{r}\nx &lt;- 5\n```\n&lt;/details&gt;\nCreate a variable x, and assign it a value of 5.\n\n\nSuggested Solution\n\n\nx &lt;- 5\n\n\n\nHowever, I did not want to worry about all the &lt;details&gt; every time I wrote a question. To automate this process, I used two knitr hooks: a chunk hook and an option hook.\nChunk hooks are for customizing the output of a chunk.\nHere I create a new chunk hook that is run before and after the chunk. When it is run before the chunk, it adds the &lt;details&gt; and &lt;summary&gt; tags. After the chunk, it closes the &lt;details&gt; tag.\n\n# Add the details tag before running the chunk \n# and close the tag after running the chunk.\nknitr::knit_hooks$set(\n  solutionsetter = function(before,options) {\n  \n  if (before) {\n    \n    \"\\n\\n&lt;details&gt;&lt;summary&gt;Suggested Solution&lt;/summary&gt;\\n\\n\"\n    \n  } else {\n    \n    \"\\n\\n&lt;/details&gt;\\n\\n\"\n    \n  }\n})\n\nIf all we needed to do was to enclose the output into a &lt;details&gt; tag, then just the solutionsetter hook would be needed. However, I also wanted to set echo=TRUE so that the output would be visible even if the default for echo was FALSE.\nOption hooks are great for when you have a kind of chunk you will use often, and it requires setting many chunk options at once (e.g., a plot variant with different dimensions than your default plot has).\nHere I create a new option hook called solution. It sets the current chunk’s echo option to TRUE and also triggers the new solutionsetter code chunk below. The updated options list needs to be returned explicitly, or the hook will not work.\n\nknitr::opts_hooks$set(solution = function(options) {\n  options$echo &lt;- TRUE\n  options$solutionsetter &lt;- TRUE\n  return(options)\n})\n\nNow all need to do is to set my chunk option to solution = TRUE and the output will be collapsible:\n\nCreate a variable `x`, and assign it a value of 5.\n\n```{r, solution = TRUE}\nx &lt;- 5\n```\nCreate a variable x, and assign it a value of 5.\n\n\n\nSuggested Solution\n\nx &lt;- 5\n\n\n\n\nThe details package\nIf you want a function to enclose the output of a chunk or inline code in &lt;details&gt; tags, use the details package by Jonathan Sidi. For example, an inline chunk like this:\n`r details::details(\"Answer\", summary = \"Question\")`\nWill output like so:\n\n\n Question \n\n\nAnswer\n\n\nThe package has many other uses, which are explained in the package’s vignettes.\n\n\n\n\nCitationBibTeX citation:@misc{schneider2023,\n  author = {Schneider, W. Joel},\n  title = {Creating Collapsible Output with Knitr Chunk Hooks},\n  date = {2023-06-30},\n  url = {https://wjschne.github.io/posts/creating-collapsible-output-with-knitr-chunk-hooks/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2023, June 30). Creating collapsible output with knitr\nchunk hooks. Schneirographs. https://wjschne.github.io/posts/creating-collapsible-output-with-knitr-chunk-hooks/"
  },
  {
    "objectID": "posts/making-text-labels-the-same-size-as-axis-labels-in-ggplot2/index.html",
    "href": "posts/making-text-labels-the-same-size-as-axis-labels-in-ggplot2/index.html",
    "title": "Making text labels the same size as axis labels in ggplot2",
    "section": "",
    "text": "As explained in this ggplot2 vignette, the size parameter in geom_text and geom_label is in millimeters, and the size parameter in all other text elements in ggplot2 is in points.\nIf I specify the base_size of the plot and the size of a label to 16, you can see that the text label is much bigger than 16.\n\nlibrary(tidyverse)\ntextsize &lt;- 16\nggplot() +\n  stat_function(\n    fun = dnorm,\n    xlim = c(-4, 4),\n    geom = \"area\",\n    alpha = .3\n  ) +\n  theme_minimal(base_size = textsize) +\n  annotate(\n    geom = \"text\",\n    x = 0,\n    y = 0,\n    label = \"Mean = 0\",\n    size = textsize,\n    vjust = -.1\n  )\n\n\n\n\n\n\n\n\nIf you do not mind a little trial and error, you can fiddle with the size parameter until you find a value that looks good to you. However, what if we want the axis text labels and the output of geom_text to be exactly the same size?\nWe are in luck because ggplot2 has a .pt constant that will help us convert point sizes to millimeters.\n\nggplot2::.pt\n\n[1] 2.845276\n\n\nWe know that, by default, axis text is .8 times as large as the base_size of the theme.\nLet’s make a function to automate the conversion:\n\nggtext_size &lt;- function(base_size, ratio = 0.8) {\n  ratio * base_size / ggplot2::.pt\n}\n\nNow we can make the label and axis text exactly the same size:\n\nggplot() + \n  stat_function(fun = dnorm, xlim = c(-4,4), geom = \"area\", alpha = .3) +\n  theme_minimal(base_size = textsize) + \n  annotate(\n    geom = \"text\",\n    x = 0,\n    y = 0,\n    label = \"Mean = 0\",\n    size = ggtext_size(textsize),\n    vjust = -.3\n  )\n\n\n\n\n\n\n\n\nFor my own convenience (and possibly yours), I put the ggtext_size function in the WJSmisc package.\nFor more on ggplot font size matters, I found this post by Christophe Nicault to be informative.\n\n\n\nCitationBibTeX citation:@misc{schneider2021,\n  author = {Schneider, W. Joel},\n  title = {Making Text Labels the Same Size as Axis Labels in Ggplot2},\n  date = {2021-08-10},\n  url = {https://wjschne.github.io/posts/making-text-labels-the-same-size-as-axis-labels-in-ggplot2/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSchneider, W. J. (2021, August 10). Making text labels the same size as\naxis labels in ggplot2. Schneirographs. https://wjschne.github.io/posts/making-text-labels-the-same-size-as-axis-labels-in-ggplot2/"
  },
  {
    "objectID": "posts/splitting-sentence-texts-into-roughly-equal-segments/index.html",
    "href": "posts/splitting-sentence-texts-into-roughly-equal-segments/index.html",
    "title": "Splitting sentence texts into roughly equal segments",
    "section": "",
    "text": "When analyzing questionnaire data, I sometimes put the text of several questions on the y-axis of a plot. Longer text strings will usually make the plot too narrow.\nSuppose we give an admittedly silly questionnaire about preference for darker food. We want to display the percentages of people answering “Yes” to each question.\nHere are the questions and the (completely made-up) percentages:\n# To install the WJSmisc package:\n# remotes::install_github(\"wjschne/WJSmisc\")\nlibrary(WJSmisc)\nlibrary(tidyverse)\nd &lt;- tibble(Item = c(\"When you drink coffee, do you prefer darker roasts?\",\n                     \"Is pumpernickel the best bread?\",\n                     \"Do you like dark chocolate more than you like milk chocolate?\",\n                     \"Are black olives better than green olives?\",\n                     \"Do you enjoy the taste of prunes more than can be prudently admitted aloud?\",\n                     \"Is black licorice the secret to a happly life?\"),\n            Percentage = c(45,4,37, 84,2, 3)) |&gt; \n  mutate(Item = fct_reorder(Item, Percentage))\nA preliminary plot might look like this:\nmyfont &lt;- \"Roboto Condensed\"\nmycolors &lt;- viridis::viridis(n = nrow(d), begin = .2, end = .8) |&gt; tinter::lighten(.5)\nmytextsize &lt;- 17\nmytext &lt;- \"gray60\"\np &lt;- ggplot(d, aes(Percentage, Item)) + \n  geom_col(aes(fill = factor(Percentage))) + \n  geom_richlabel(aes(label = paste0(Percentage, \"%\"), \n                     color = factor(Percentage)), \n                 hjust = 0, fill = \"black\",\n                 text_size = mytextsize,\n                 family = myfont) +\n  labs(y = NULL,\n       caption = \"Note: These numbers were made up for illustrative purposes.\") +\n  theme_minimal(base_size = mytextsize, base_family = myfont) +\n  scale_x_continuous(\"Percent Yes\", \n                     limits = c(0,100), \n                     breaks = NULL,\n                     expand = expansion(mult = c(0,.2))) +\n  scale_fill_manual(values = mycolors) +\n  scale_color_manual(values = mycolors) +\n  theme(plot.title.position = \"plot\", \n        plot.background = element_rect(\"black\"),\n        plot.title = element_text(colour = mytext),\n        plot.caption = element_text(color = \"gray40\"),\n        legend.position = \"none\",\n        axis.title = element_text(color = mytext),\n        panel.grid.major.y = element_blank(),\n        axis.text = element_text(color = mycolors))\np + ggtitle(\"Y-axis labels with no text wrapping\")\nThe questions are taking up a lot of space. We can fix this by using stringr::str_wrap to make the text lines no longer than a specified width:\np + \n  scale_y_discrete(labels = \\(x) str_wrap(x, width = 30)) + \n  ggtitle(\"Splitting Y-axis labels with str_wrap\")\nThis is a definite improvement, but I wish the lines of text were wrapped more evenly. For example, I do not like the fact that “chocolate” and “bread” are by themselves on their own lines. I have spent a fair amount of time breaking lines up by hand and hoped to create a function to automate the process. To achieve a more balanced style of text splitting, I made the str_wrap_equal function.\np + scale_y_discrete(labels = \\(x) str_wrap_equal(x, max_width = 30)) + \n  ggtitle(\"Splitting Y-axis labels with str_wrap_equal\")\nTo each their own, but this style of text wrapping feels better to me here. It is the sort of thing no one but the analyst will notice if it is right but will feel a bit off if not.\nFor your convenience and mine, I put the str_wrap_equal function in the WJSmisc package."
  },
  {
    "objectID": "posts/splitting-sentence-texts-into-roughly-equal-segments/index.html#technical-details",
    "href": "posts/splitting-sentence-texts-into-roughly-equal-segments/index.html#technical-details",
    "title": "Splitting sentence texts into roughly equal segments",
    "section": "Technical Details",
    "text": "Technical Details\nI hesitate to post a new function because the odds are quite high that someone has already done what I did. However, my initial search did not locate a function that does exactly what I needed. If you know of a function have I duplicated unnecessarily, let me know.\nIn deciding whether to break the line, the function does the following:\n\nCalculate the overall length of the text (i.e., number of characters).\nFind k (the number of lines likely needed) by dividing the overall text length by max_width and round up to the nearest integers.\nCalculate the preferred width of the line by dividing the overall text length by the number of lines likely needed.\nAdd to the line one word at a time as long as doing so makes the current line width closer to the preferred width. If not, a new line is started.\nThe function will not make a line longer than what is specified by max_width unless it has to place a single, lonely word that is longer than max_width.\n\nHere is the code for the function. Any suggestions to improve it are welcome.\n\nstr_wrap_equal &lt;- function(x, max_width = 30L, sep = \"\\n\") {\n  purrr::map_chr(x, \\(xi) {\n    # Find overall text length\n    xlen &lt;- stringr::str_length(xi)\n    # Remove any line breaks and allow lines to break at forward slashes\n    xi &lt;- stringr::str_replace(xi, \"\\n\", \" \") |&gt; \n      stringr::str_replace(\"/\", \"/ \")\n    # Number of lines likely needed\n    k &lt;- ceiling(xlen / max_width)\n    # Optimal line length\n    preferred_width &lt;- xlen / k\n    # Split text into words\n    words &lt;- stringr::str_split(xi, pattern = \" \", simplify = F)[[1]]\n    # Number of words in text\n    k_words &lt;- length(words)\n    # Length of each word in text\n    word_len &lt;- stringr::str_length(words)\n    # Create empty text lines with a few extra, if needed\n    textlines &lt;- rep(\"\", k + 10)\n    # Current text line\n    i &lt;- 1\n    # Decide whether to add a word to the current line or to start a new line\n    for (w in seq(k_words)) {\n      # Width of current line before adding a new word\n      current_width &lt;- stringr::str_length(textlines[i])\n      # Width of current line if a new word is added\n      proposed_width &lt;- current_width + word_len[w] + 1\n      # Difference between current width and preferred width\n      current_difference &lt;- abs(current_width - preferred_width)\n      # Difference between proposed width and preferred width\n      proposed_difference &lt;- abs(proposed_width - preferred_width)\n      # Should we start a new line?\n      if (current_difference &lt; proposed_difference | proposed_width &gt; max_width) {\n        i &lt;- i + 1\n      }\n      # Add word to current line, remove spaces, and rejoin words divided by forward slashes\n      textlines[i] &lt;- stringr::str_trim(paste(textlines[i], words[w])) |&gt;\n        stringr::str_replace(\"/ \", \"/\")\n    }\n    # Collapse non-empty lines by separation character\n    paste0(textlines[stringr::str_length(textlines) &gt; 0], collapse = sep)\n  })\n\n}"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "A free online book about psychometric tools for psychological assessment\n\nI have been working on a book project off and on since 2012. I have ambitious goals for this book, but it is not nearly complete. The preliminary, unfinished version is here.\n\n\n\n\n\nAn introduction to tests and measurements\nRenée Tobin and I helped update Ronald Cohen’s Psychological Testing and Assessment: An Introduction to Tests and Measurements. It is a comprehensive textbook suitable for introductory psychological assessment courses.\n\n\n\n\n\n\n\n\n\nA book about making reader-friendly assessment reports that instill hope and promote change.\nMy co-authors and I worked hard to fill the second edition of Essentials of Assessment Report Writing with non-obvious guidance about how to write reports so that the meet the needs of the reports’ readers."
  },
  {
    "objectID": "projects.html#individual-psychometrics-an-assessment-toolkit-with-applications-in-r",
    "href": "projects.html#individual-psychometrics-an-assessment-toolkit-with-applications-in-r",
    "title": "Projects",
    "section": "",
    "text": "A free online book about psychometric tools for psychological assessment\n\nI have been working on a book project off and on since 2012. I have ambitious goals for this book, but it is not nearly complete. The preliminary, unfinished version is here."
  },
  {
    "objectID": "projects.html#psychological-testing-and-assessment",
    "href": "projects.html#psychological-testing-and-assessment",
    "title": "Projects",
    "section": "",
    "text": "An introduction to tests and measurements\nRenée Tobin and I helped update Ronald Cohen’s Psychological Testing and Assessment: An Introduction to Tests and Measurements. It is a comprehensive textbook suitable for introductory psychological assessment courses."
  },
  {
    "objectID": "projects.html#essentials-of-assessment-report-writing",
    "href": "projects.html#essentials-of-assessment-report-writing",
    "title": "Projects",
    "section": "",
    "text": "A book about making reader-friendly assessment reports that instill hope and promote change.\nMy co-authors and I worked hard to fill the second edition of Essentials of Assessment Report Writing with non-obvious guidance about how to write reports so that the meet the needs of the reports’ readers."
  },
  {
    "objectID": "tutorials/ANOVA/ANOVA.html",
    "href": "tutorials/ANOVA/ANOVA.html",
    "title": "ANOVA in R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(easystats)"
  },
  {
    "objectID": "tutorials/ANOVA/ANOVA.html#basic-descriptives",
    "href": "tutorials/ANOVA/ANOVA.html#basic-descriptives",
    "title": "ANOVA in R",
    "section": "Basic descriptives",
    "text": "Basic descriptives\nNumeric variables can be described in terms of means, standard deviations (SD), interquartile ranges (IQR), skewness, and kurtosis:\n\ndescribe_distribution(d)\n\nVariable              |  Mean |    SD |   IQR |          Range | Skewness\n-------------------------------------------------------------------------\nAnxiety_Physiological | 60.49 | 12.79 | 16.43 | [22.78, 93.88] |    -0.29\nAnxiety_Questionnaire | 55.11 | 13.30 | 18.24 | [21.79, 92.38] |     0.05\n\nVariable              | Kurtosis |   n | n_Missing\n--------------------------------------------------\nAnxiety_Physiological |     0.24 | 300 |         0\nAnxiety_Questionnaire |    -0.16 | 300 |         0\n\n\nCategorical variables are often described in terms of frequencies tables.\n\nd %&gt;% \n  select(Attachment) %&gt;% \n  data_tabulate()\n\nAttachment (Attachment) &lt;categorical&gt;\n# total N=300 valid N=300\n\nValue      |   N | Raw % | Valid % | Cumulative %\n-----------+-----+-------+---------+-------------\nSecure     | 110 | 36.67 |   36.67 |        36.67\nAvoidant   |  85 | 28.33 |   28.33 |        65.00\nAmbivalent | 105 | 35.00 |   35.00 |       100.00\n&lt;NA&gt;       |   0 |  0.00 |    &lt;NA&gt; |         &lt;NA&gt;\n\n\nWe can get descriptives statistics of our numeric variables separated by group. There are many ways to do this, but the simplest is the describeBy function from the psych package\n\npsych::describeBy(d, group = \"Attachment\")\n\n\n Descriptive statistics by group \nAttachment: 1\n                      vars   n mean sd median trimmed mad min max range  skew\nAttachment               1 110    1  0      1       1   0   1   1     0   NaN\nAnxiety_Physiological    2 110   50 11     51      51  12  23  70    48 -0.44\nAnxiety_Questionnaire    3 110   50 11     51      51  12  23  70    46 -0.36\n                      kurtosis se\nAttachment                 NaN  0\nAnxiety_Physiological    -0.41  1\nAnxiety_Questionnaire    -0.61  1\n------------------------------------------------------------ \nAttachment: 2\n                      vars  n mean   sd median trimmed mad min max range skew\nAttachment               1 85    2  0.0      2       2 0.0   2   2     0  NaN\nAnxiety_Physiological    2 85   67  9.8     67      67 9.5  46  94    48 0.27\nAnxiety_Questionnaire    3 85   48 10.6     48      48 9.9  22  74    52 0.11\n                      kurtosis  se\nAttachment                 NaN 0.0\nAnxiety_Physiological    -0.13 1.1\nAnxiety_Questionnaire    -0.10 1.1\n------------------------------------------------------------ \nAttachment: 3\n                      vars   n mean  sd median trimmed mad min max range skew\nAttachment               1 105    3 0.0      3       3 0.0   3   3     0  NaN\nAnxiety_Physiological    2 105   66 9.3     65      66 7.8  45  94    48 0.19\nAnxiety_Questionnaire    3 105   66 9.9     66      66 9.3  44  92    48 0.31\n                      kurtosis   se\nAttachment                 NaN 0.00\nAnxiety_Physiological     0.08 0.91\nAnxiety_Questionnaire    -0.10 0.97\n\n\nThe datawizard package has some really nice functions for descriptives:\n\nd %&gt;% \n  datawizard::means_by_group(\n    select = contains(\"Anxiety\") , \n    by = \"Attachment\")\n\n# Mean of Anxiety_Physiological by Attachment\n\nCategory   |  Mean |   N |    SD |         95% CI |      p\n----------------------------------------------------------\nSecure     | 49.95 | 110 | 10.67 | [48.08, 51.83] | &lt; .001\nAvoidant   | 67.20 |  85 |  9.84 | [65.07, 69.33] | &lt; .001\nAmbivalent | 66.09 | 105 |  9.33 | [64.17, 68.00] | &lt; .001\nTotal      | 60.49 | 300 | 12.79 |                |       \n\nAnova: R2=0.395; adj.R2=0.391; F=96.988; p&lt;.001\n\n# Mean of Anxiety_Questionnaire by Attachment\n\nCategory   |  Mean |   N |    SD |         95% CI |      p\n----------------------------------------------------------\nSecure     | 50.09 | 110 | 10.90 | [48.13, 52.05] | &lt; .001\nAvoidant   | 47.82 |  85 | 10.57 | [45.59, 50.06] | &lt; .001\nAmbivalent | 66.27 | 105 |  9.90 | [64.26, 68.28] | &lt; .001\nTotal      | 55.11 | 300 | 13.30 |                |       \n\nAnova: R2=0.385; adj.R2=0.381; F=92.975; p&lt;.001\n\n\nThe skimr package is also quite nice:\n\nlibrary(skimr)\n# Overall summary\nd %&gt;% \n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n300\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nAttachment\n0\n1\nFALSE\n3\nSec: 110, Amb: 105, Avo: 85\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAnxiety_Physiological\n0\n1\n60\n13\n23\n53\n61\n69\n94\n▁▃▇▅▁\n\n\nAnxiety_Questionnaire\n0\n1\n55\n13\n22\n46\n55\n64\n92\n▂▆▇▅▁\n\n\n\n\n# Grouped summaries\nd %&gt;% \n  group_by(Attachment) %&gt;% \n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n300\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nAttachment\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nAttachment\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAnxiety_Physiological\nSecure\n0\n1\n50\n10.7\n23\n43\n51\n58\n70\n▂▅▆▇▃\n\n\nAnxiety_Physiological\nAvoidant\n0\n1\n67\n9.8\n46\n61\n67\n73\n94\n▂▇▇▅▁\n\n\nAnxiety_Physiological\nAmbivalent\n0\n1\n66\n9.3\n45\n61\n65\n72\n94\n▃▆▇▃▁\n\n\nAnxiety_Questionnaire\nSecure\n0\n1\n50\n10.9\n23\n42\n51\n59\n70\n▂▅▇▇▆\n\n\nAnxiety_Questionnaire\nAvoidant\n0\n1\n48\n10.6\n22\n41\n48\n54\n74\n▁▅▇▃▂\n\n\nAnxiety_Questionnaire\nAmbivalent\n0\n1\n66\n9.9\n44\n60\n66\n72\n92\n▂▆▇▃▂\n\n\n\n\n\nThese are convenience functions. If they do exactly what you want, great! However, I often need statistics in a data frame in a format optimal for further processing (e.g., for publication-worthy tables or plots).\n\nd_descriptives &lt;- d %&gt;% \n  pivot_longer(contains(\"Anxiety\"), \n               names_to = \"Measure\") %&gt;% \n  summarise(Mean = mean(value),\n            SD = sd(value),\n            n = n(),\n            .by = c(Attachment, Measure)) %&gt;% \n  mutate(Measure = snakecase::to_title_case(Measure))\n\nd_descriptives\n\n# A tibble: 6 × 5\n  Attachment Measure                Mean    SD     n\n  &lt;fct&gt;      &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 Secure     Anxiety Physiological  50.0 10.7    110\n2 Secure     Anxiety Questionnaire  50.1 10.9    110\n3 Ambivalent Anxiety Physiological  66.1  9.33   105\n4 Ambivalent Anxiety Questionnaire  66.3  9.90   105\n5 Avoidant   Anxiety Physiological  67.2  9.84    85\n6 Avoidant   Anxiety Questionnaire  47.8 10.6     85\n\n\nThis output is not pretty, but it was not intended to be. It is in a format that is easy to adapt for other things. For example, I can use the gt (“Great Tables”) package to get exactly what I want to display:\n\n\nCode\nlibrary(gt)\nd_descriptives %&gt;% \n  gt(groupname_col = \"Measure\", \n     rowname_col = \"Attachment\") %&gt;%\n  tab_stub_indent(everything(), indent = 5)\n\n\n\n\n\n\n\n\n\nMean\nSD\nn\n\n\n\n\nAnxiety Physiological\n\n\nSecure\n50\n10.7\n110\n\n\nAmbivalent\n66\n9.3\n105\n\n\nAvoidant\n67\n9.8\n85\n\n\nAnxiety Questionnaire\n\n\nSecure\n50\n10.9\n110\n\n\nAmbivalent\n66\n9.9\n105\n\n\nAvoidant\n48\n10.6\n85\n\n\n\n\n\n\n\nOr I can use ggplot2 to display means and standard deviations graphically:\n\n\nCode\nd_descriptives %&gt;%\n  mutate(mean_label = scales::number(Mean, .1)) %&gt;% \n  ggplot(aes(Attachment, Mean)) +\n  facet_grid(cols = vars(Measure)) +\n  geom_pointrange(aes(ymin = Mean - SD,\n                      ymax = Mean + SD)) +\n  geom_label(\n    aes(label = mean_label), \n    hjust = -.3)"
  },
  {
    "objectID": "tutorials/ANOVA/ANOVA.html#diagnostic-plots",
    "href": "tutorials/ANOVA/ANOVA.html#diagnostic-plots",
    "title": "ANOVA in R",
    "section": "Diagnostic Plots",
    "text": "Diagnostic Plots\n\ncheck_model(fit)\n\n\n\n\n\n\n\n\nNone of the diagnostic checks raise any alarms. Specifically, the residuals are reasonably normal and have consistent variance acrross groups (i.e., the homogeneeity of variance assumption is reasonable)."
  },
  {
    "objectID": "tutorials/ANOVA/ANOVA.html#summary",
    "href": "tutorials/ANOVA/ANOVA.html#summary",
    "title": "ANOVA in R",
    "section": "Summary",
    "text": "Summary\nThe base R function summary gives us most of what we might want to know.\n\nsummary(fit)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nAttachment    2  20359   10179      93 &lt;2e-16 ***\nResiduals   297  32517     109                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can see that overall model is significant, meaning that at least two groups have reliably different means. At this point we do not know which means differ, but from the plot we have a good guess that the Ambivalent group scores higher on the Anxiety Questionnaire than the other two groups.\nA similar, but tidier display can be found with the parameters function:\n\nparameters(fit)\n\nParameter  | Sum_Squares |  df | Mean_Square |     F |      p\n-------------------------------------------------------------\nAttachment |    20358.60 |   2 |    10179.30 | 92.98 | &lt; .001\nResiduals  |    32516.83 | 297 |      109.48 |       |       \n\nAnova Table (Type 1 tests)"
  },
  {
    "objectID": "tutorials/ANOVA/ANOVA.html#other-summary-functions",
    "href": "tutorials/ANOVA/ANOVA.html#other-summary-functions",
    "title": "ANOVA in R",
    "section": "Other Summary Functions",
    "text": "Other Summary Functions\nThe overall “fit” or “performance” of the model:\n\nperformance(fit)\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |   RMSE |  Sigma\n--------------------------------------------------------------------\n2265.082 | 2265.218 | 2279.897 | 0.385 |     0.381 | 10.411 | 10.463\n\n\n\nAIC: Akaike’s Information Criterion\nAICc: AIC with a correction for small sample sizes\nBIC: Bayesian Information Criterion\nR2: R-squared: Coefficient of Determination\nR2_adj: Adjusted r-squared\nRMSE: Root mean squared error =\\sqrt{\\frac{\\sum_{i = 1}^{n}{e_i}}{n}}\nSIGMA: Residual standard deviation (The SD of the residuals =\\sqrt{\\frac{\\sum_{i = 1}^{n}{e_i}}{n-k-1}})"
  },
  {
    "objectID": "tutorials/ANOVA/ANOVA.html#automated-interpretation",
    "href": "tutorials/ANOVA/ANOVA.html#automated-interpretation",
    "title": "ANOVA in R",
    "section": "Automated Interpretation",
    "text": "Automated Interpretation\n\nreport(fit)\n\nThe ANOVA (formula: Anxiety_Questionnaire ~ Attachment) suggests that:\n\n  - The main effect of Attachment is statistically significant and large (F(2,\n297) = 92.98, p &lt; .001; Eta2 = 0.39, 95% CI [0.32, 1.00])\n\nEffect sizes were labelled following Field's (2013) recommendations."
  },
  {
    "objectID": "tutorials/ANOVA/ANOVA.html#post-hoc-tests",
    "href": "tutorials/ANOVA/ANOVA.html#post-hoc-tests",
    "title": "ANOVA in R",
    "section": "Post-hoc Tests",
    "text": "Post-hoc Tests\nWe would like to compare all means but control for family-wise error.\n\nfit_contrast &lt;- estimate_contrasts(fit, \n                   contrast = \"Attachment\", \n                   backend = \"emmeans\")\nfit_contrast \n\nMarginal Contrasts Analysis\n\nLevel1   | Level2     | Difference |           95% CI |   SE | t(297) |      p\n------------------------------------------------------------------------------\nAvoidant | Ambivalent |     -18.44 | [-21.45, -15.44] | 1.53 | -12.08 | &lt; .001\nSecure   | Ambivalent |     -16.18 | [-18.99, -13.37] | 1.43 | -11.33 | &lt; .001\nSecure   | Avoidant   |       2.27 | [ -0.71,   5.24] | 1.51 |   1.50 |  0.135\n\nVariable predicted: Anxiety_Questionnaire\nPredictors contrasted: Attachment\np-values are uncorrected.\n\nreport(fit_contrast)\n\nThe marginal contrasts analysis suggests the following. The difference between\nAvoidant and Ambivalent is negative and statistically significant (difference =\n-18.44, 95% CI [-21.45, -15.44], t(297) = -12.08, p &lt; .001). The difference\nbetween Secure and Ambivalent is negative and statistically significant\n(difference = -16.18, 95% CI [-18.99, -13.37], t(297) = -11.33, p &lt; .001). The\ndifference between Secure and Avoidant is positive and statistically\nnon-significant (difference = 2.27, 95% CI [ -0.71, 5.24], t(297) = 1.50, p =\n0.135)"
  },
  {
    "objectID": "tutorials/DataSimulation/data_simulation.html",
    "href": "tutorials/DataSimulation/data_simulation.html",
    "title": "Data Simulation for Multilevel Models",
    "section": "",
    "text": "Once you learn how to simulate data, you will possess a data-science superpower. Generating simulated data allows you to run complicated “What-If” thought experiments, which can help identify implications of models, methods, and procedures that are otherwise difficult to discern. For example, you can ask, if my hypothesized model is correct, what sample size will I need to detect my hypothesized results? Of course, this is what power analysis is for. However, there are many models for which no analytic solution exists, and only simulated data can generate approximate answers to your questions.\nSimulated data can help you understand implicit implications of statistical procedures. For example, you might have heard that ANOVA is robust to mild to moderate violations of the normality assumption. How do we know this? Scholars simulated data with normal and non-normal residuals, and found that ANOVA generally came to the correct decision except when the normality assumption was violated to extreme degrees.\nSimulated data can help you understand non-obvious implications of complex decisions. For example, Schneider & Roman (2018) used simulated data to show that some well-intentioned diagnostic procedures cause diagnosis of learning disabilities to be less accurate, not more.\nSimulated data have a primary limitation: they can only tell you about truths implicit in your model, method, or procedure. To observe genuinely new truths, you must observe genuinely new data."
  },
  {
    "objectID": "tutorials/DataSimulation/data_simulation.html#specify-means",
    "href": "tutorials/DataSimulation/data_simulation.html#specify-means",
    "title": "Data Simulation for Multilevel Models",
    "section": "Specify Means",
    "text": "Specify Means\nYou already know how to set up a vector with the c function. We are going to create a named vector of means. That is, we are going to associate the mean values with their respective variable names. The reason we are doing this is that when we set the mean vector with names, the data generating function will automatically name the columns.\nThere are, of course, several ways to give vector elements names. Pick one that you like.\n\n# Supply names directly. Does the job with the least typing.\nm &lt;- c(x = 50, y = 75, z = 80)\n\n# Using the names function.\nm &lt;- c(50, 75, 80)\nnames(m) &lt;- c(\"x\", \"y\", \"z\")\n\n# Using the names function after a pipe:\nm &lt;- c(50, 75, 80) %&gt;% \n  `names&lt;-`(c(\"x\", \"y\", \"z\"))\n\nm\n\n x  y  z \n50 75 80"
  },
  {
    "objectID": "tutorials/DataSimulation/data_simulation.html#specify-standard-deviations",
    "href": "tutorials/DataSimulation/data_simulation.html#specify-standard-deviations",
    "title": "Data Simulation for Multilevel Models",
    "section": "Specify Standard Deviations",
    "text": "Specify Standard Deviations\nWe set up the vector of standard deviations in the standard way, though you can apply names to it if you like.\n\ns &lt;- c(5, 8, 9)"
  },
  {
    "objectID": "tutorials/DataSimulation/data_simulation.html#specify-the-correlation-matrix",
    "href": "tutorials/DataSimulation/data_simulation.html#specify-the-correlation-matrix",
    "title": "Data Simulation for Multilevel Models",
    "section": "Specify the Correlation Matrix",
    "text": "Specify the Correlation Matrix\nThe matrix function takes a vector, and you tell it how many rows and columns the matrix has. If you omit the number of columns, it will figure out how many columns are needed from the number of elements in your vector. In this case, there are 3 elements and 3 rows, so it divides 9 elements by 3 rows and assumes there are 3 columns.\n\\[\\boldsymbol{R}=\\left[\\begin{array}{r} 1 & .6 & .7 \\\\ .6 & 1 & .8 \\\\ .7 & .8 & 1\\end{array}\\right]\\]\n\n# Correlation matrix\nR &lt;- matrix(c(1, .6, .7,\n             .6,  1, .8,\n             .7, .8,  1), \n            nrow = 3)\nR\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.6  0.7\n[2,]  0.6  1.0  0.8\n[3,]  0.7  0.8  1.0\n\n\n\n\nIn a k × k symmetric matrix, the number of redundant elements is\n\\[\\frac{k(k-1)}{2}\\]\nWith a 3 × 3 matrix, it is easy to specify a symmetric matrix because there are only 3 redundant entries. However, imagine how tedious specifying a 20 × 20 matrix with 190 redundant entries would be. With larger matrices, consider specifying just the lower triangle of the matrix, and converting it to a full symmetric matrix using lavaan’s lower2full function. I believe this approach is not only less tedious but also less error-prone.\n\\[\\boldsymbol{R}_{\\text{Lower}}=\\left[\\begin{array}{r} 1 & \\\\ .6 & 1 \\\\ .7 & .8 & 1\\end{array}\\right]\\]\nThe input for lavaan’s lav_matrix_lower2full function is just a vector, but I put line breaks in it to mimic the configuration of the lower triangle of the matrix.\n\nR &lt;- lavaan::lav_matrix_lower2full(c(1,\n                                    .6,  1,\n                                    .7, .8, 1))"
  },
  {
    "objectID": "tutorials/DataSimulation/data_simulation.html#convert-correlation-matrix-to-covariance-matrix",
    "href": "tutorials/DataSimulation/data_simulation.html#convert-correlation-matrix-to-covariance-matrix",
    "title": "Data Simulation for Multilevel Models",
    "section": "Convert Correlation Matrix to Covariance Matrix",
    "text": "Convert Correlation Matrix to Covariance Matrix\nThe covariance of x and y (σxy) is the product of the correlation of x and y (rxy) and the standard deviations of x and y.\n\\[\\sigma_{xy}=r_{xy}\\sigma_x\\sigma_y\\]\nTo convert a correlation matrix R into a covariance matrix Σ using just matrix algebra requires converting the standard deviations into a diagonal matrix D (i.e., standard deviations on the diagonal and zeroes everywhere else).\n\\[\\boldsymbol{D}=\\left[\\begin{array}{l} \\sigma_x&0\\\\0&\\sigma_y \\end{array}\\right]\\]\n\\[\\boldsymbol{\\Sigma = DRD}\\]\n\n# Using matrix operations:\n# The diag function converts a vector to a diagonal matrix\nD &lt;- diag(s)\n\n# Convert the correlation matrix to covariances\n# The %*% function is the matrix multiplication operator\nSigma = D %*% R %*% D\n\nIf this conversion process seems like a lot of work, lavaan’s cor2cov function is a quicker alternative:\n\nSigma &lt;- lavaan::cor2cov(R = R, sds = s)"
  },
  {
    "objectID": "tutorials/DataSimulation/data_simulation.html#generate-multivariate-data",
    "href": "tutorials/DataSimulation/data_simulation.html#generate-multivariate-data",
    "title": "Data Simulation for Multilevel Models",
    "section": "Generate Multivariate Data",
    "text": "Generate Multivariate Data\nNow we are ready to generate our multivariate normal data. Let’s generate 1000 cases. The rmvnorm function outputs a matrix, so we usually need to convert it to a tibble.\n\nd &lt;- mvtnorm::rmvnorm(n = 1000, mean = m, sigma = Sigma) %&gt;% \n  as_tibble()\n\nd\n\n# A tibble: 1,000 × 3\n       x     y     z\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  44.5  68.0  69.7\n 2  44.8  67.6  62.2\n 3  51.7  85.7  88.4\n 4  54.7  76.3  89.3\n 5  43.8  61.8  71.0\n 6  43.8  75.2  73.1\n 7  56.8  79.9  89.8\n 8  45.1  54.8  53.4\n 9  49.7  76.1  80.6\n10  55.1  97.0 104. \n# ℹ 990 more rows\n\n\nLet’s plot the simulated data\n\nggplot(d, aes(x, y)) + \n  geom_point()\n\n\n\n\n\n\n\n\nIf you want to add marginal density plots, ggExtra’s ggMarginal function looks nice to me.\n\n# Here we save a plot as a variable p\np &lt;- ggplot(d, aes(x, y)) + \n  geom_point()\n\n# Marginal histograms will be added to our saved plot\nggExtra::ggMarginal(p, data = d, type = \"histogram\")"
  },
  {
    "objectID": "tutorials/DataSimulation/data_simulation.html#step-1-write-out-your-equations",
    "href": "tutorials/DataSimulation/data_simulation.html#step-1-write-out-your-equations",
    "title": "Data Simulation for Multilevel Models",
    "section": "Step 1: Write Out Your Equations",
    "text": "Step 1: Write Out Your Equations\nI cannot stress this point strongly enough. Taking the time to write your equations makes subsequent steps so much easier.\nSuppose we want to simulate data in which first-graders’ reading ability is predicted by their vocabulary measured at the beginning of the year and by their first grade teacher’s conscientiousness. Suppose that teacher conscientiousness is expected to help all students learn to read, but it is expected that teacher conscientiousness will be particularly important for students with low vocabulary. That is, the effect of teacher conscientiousness is expected to be stronger for students with low vocabulary compared to students with strong vocabulary. Thus, teacher conscientiousness will have an effect on vocabulary’s random slope.\nSo, we know that vocabulary is a level-1 variable, and teacher conscientiousness is a level-2 variable. Teacher conscientiousness is expected to have an effect on both the random intercept and vocabulary’s random slope.\nLet’s write out the entire equations:\n\\[\n\\begin{align*}\n\\textbf{Level 1:}\\\\[1.5ex]\nReading_{ij}&=b_{0j} + b_{1j}V_{ij}+e_{ij}\\\\\ne&\\sim\\mathcal{N}(0,\\tau_1)\\\\[1.5ex]\n\\text{Where}\\\\\nReading_{ij}&=\\text{Reading score for student } i \\text{ in class }  j\\\\\nb_{0j} &= \\text{Random intercept for class } j\\\\\nb_{1j} &= \\text{Random slope for Vocabulary for class } j\\\\\nV_{ij}&=\\text{Vocabulary score for student } i \\text{ in class }  j\\\\\ne_{ij}&=\\text{Level-1 error for student } i \\text{ in class }  j\\\\\n\\tau_1 &= \\text{Level-1 Error Variance}\\\\[2ex]\n\\textbf{Level 2:}\\\\[1.5ex]\nb_{0j}&=b_{00} + b_{01}C_{j}+e_{0j}\\\\\nb_{1j}&=b_{10} + b_{11}C_{j}+e_{1j}\\\\\n\\begin{bmatrix}e_{0j}\\\\e_{1j}\\end{bmatrix}&\\sim\\mathcal{N}\\left(\\begin{bmatrix}0\\\\ 0 \\end{bmatrix}\\begin{matrix} \\\\ ,\\end{matrix}\\begin{bmatrix}\\tau_{00}&\\\\ \\tau_{10}&\\tau_{11}\\end{bmatrix}\\right)\\\\[1.5ex]\n\\text{Where}\\\\\nb_{0j} &= \\text{Random intercept for class } j\\\\\nb_{00} &= \\text{The expected intercept when } C_j \\text{ is 0}\\\\\nb_{01} &= \\text{The effect of } C_j \\text{ on the random intercept}\\\\\nC_j &= \\text{Teacher conscientiousness in class } j\\\\\ne_{0j} &= \\text{The random intercept's error for class } j \\\\\nb_{1j} &= \\text{Random slope for Vocabulary for class } j\\\\\nb_{10} &= \\text{The expected Vocabulary slope when } C_j \\text{ is 0}\\\\\nb_{11} &= \\text{The effect of } C_j \\text{ on the Vocabulary slope}\\\\\ne_{1j} &= \\text{The Vocabulary slope's error for class } j \\\\\n\\tau_{00} &= \\text{The variance of } e_{0j}\\\\\n\\tau_{11} &= \\text{The variance of } e_{1j}\\\\\n\\tau_{10} &= \\text{The covariance of } e_{0j} \\text{ and } e_{1j}\n\\end{align*}\n\\]\nCombining the level-1 and level-2 formulas:\n\\[Reading_{ij}=\\underbrace{b_{00} + b_{01}C_{j}+e_{0j}}_{b_{0j}} + (\\underbrace{b_{10} + b_{11}C_{j}+e_{1j}}_{b_{1j}})V_{ij}+e_{ij}\\]\nBy multiplying everything out, we can see exactly how everything needs to be calculated.\n\\[Reading_{ij}=b_{00} + b_{01}C_{j}+e_{0j} + b_{10}V_{ij} + b_{11}C_{j}V_{ij}+e_{1j}V_{ij}+e_{ij}\\]\nNow we are ready!"
  },
  {
    "objectID": "tutorials/DataSimulation/data_simulation.html#step-2-specify-all-model-parameters",
    "href": "tutorials/DataSimulation/data_simulation.html#step-2-specify-all-model-parameters",
    "title": "Data Simulation for Multilevel Models",
    "section": "Step 2: Specify all model parameters",
    "text": "Step 2: Specify all model parameters\n\nSpecifying the fixed coefficients\nIn this case, we have four coefficients, and it is straightforward to specify them all.\nWhich values should you choose? That is determined by theory. With complex models like this one, sometimes there is a bit of trial-and-error to get parameter values that produce data that are consistent with the theory you have in mind.\n\n# Fixed coefficients\n# Fixed intercept\nb_00 &lt;- 500\n# Teacher conscientiousness effect on intercept\nb_01 &lt;- 20\n# Fixed slope for Vocabulary\nb_10 &lt;- 30\n# Teacher conscientiousness effect on Vocabulary's slope\nb_11 &lt;- -4\n\n\n\nSpecify the level-1 error standard deviation (or variance)\nYou need to remember whether you set this number up as a variance or as a standard deviation. I am setting it up as a variance.\n\n# Level-1 error variance\ntau_1 &lt;- 5 ^ 2\n\n\n\nSpecify tau_2, the level-2 error covariance matrix\n\\[\\boldsymbol{\\tau}_2=\\begin{bmatrix}\\tau_{00}&\\\\ \\tau_{10}&\\tau_{11}\\end{bmatrix}\\]\nI have written out the \\(\\boldsymbol{\\tau}_2\\) matrix showing only the lower triangle of the matrix because the upper triangle is redundant. I think it is safer to write the lower triangle rather than specifying the full matrix with its duplicate covariancs.\n\n\nSpecifying the full matrix would look like this: ::: {.cell}\nmatrix(\n  c(tau_00, tau_10,\n    tau_10, tau_11), \n  nrow = 2)\n:::\nWe can create the \\(\\boldsymbol{\\tau}_2\\) matrix like so:\n\n# Variance of intercepts\ntau_00 &lt;- 25 ^ 2\n# Variance of Vocabulary slope\ntau_11 &lt;- 0.4 ^ 2\n# Correlation of intercepts and Vocabulary slopes\nr_10 &lt;- -.4\n# Covariance of intercepts and slopes\ntau_10 &lt;- sqrt(tau_00 * tau_11) * r_10\n\n# tau lower triangle in vector form\ntau_lower_triangle &lt;- c(tau_00,\n                        tau_10, tau_11)\n\n# Convert lower triangle to a full symmetric matrix\ntau_2 &lt;- lavaan::lav_matrix_lower2full(tau_lower_triangle)"
  },
  {
    "objectID": "tutorials/DataSimulation/data_simulation.html#step-2-simulate-the-level-2-variables.",
    "href": "tutorials/DataSimulation/data_simulation.html#step-2-simulate-the-level-2-variables.",
    "title": "Data Simulation for Multilevel Models",
    "section": "Step 2: Simulate the level-2 variables.",
    "text": "Step 2: Simulate the level-2 variables.\n\nHow many level-2 clusters will there be?\nIt is generally best to start with a small number so that you simulations will run quickly. You can increase the number of clusters later. Let’s start with 100 groups. We will use the variable k to refer to the number of clusters.\n\n# Number of clusters\nk &lt;- 100\n\n# Cluster ID variable\n# Makes a sequence from 1 to k\ncluster_id &lt;- seq(k)\n\n\n\nHow many people in each cluster?\nOption 1 Every cluster has the same size.\n\n# Every class has 28 students\ncluster_size = 28\n\nOption 2 Cluster size is determined by a random variable.\nWhich random variable? It is up to you. Some common choices include the normal distribution, Poisson distribution, or the uniform distribution.\n\nUse the normal distribution, and round to the nearest integer. With small means and/or large standard deviations, there is a chance that you will have cluster sizes less than one, so you will want to plan for that. To make sure we never have a cluster size less than 1, we will use the pmax function. It replaces any vector element less than the lower limit with the lower limit. For example:\n\n\npmax(c(2,0,-1, 4), 1)\n\n[1] 2 1 1 4\n\n\nHere we generate a normal variate with a mean of 28 and a standard deviation of 2.\n\ncluster_size &lt;- rnorm(k, mean = 28, sd = 2) %&gt;% \n  round(digits = 0) %&gt;% \n  pmax(1)\n \n\nd2 &lt;- tibble(cluster_id, cluster_size)\n\nggplot(d2, aes(cluster_size)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nUse the Poisson distribution to create integers with a mean (and variance) of λ. With a small λ, it is possible to have cluster sizes of zero.\n\n\ncluster_size = rpois(k, lambda = 28) %&gt;% \n  pmax(1)\n\nd2 &lt;- tibble(cluster_id, cluster_size)\n\nggplot(d2, aes(cluster_size)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nThe binomial distribution is also a good choice for generating non-negative numbers.\n\nUse the sample function to have cluster sizes uniformly distributed between 2 values.\n\n\ncluster_size = sample(x = 25:35, size = k, replace = TRUE) \n\nd2 &lt;- tibble(cluster_id, cluster_size)\n\nggplot(d2, aes(cluster_size)) +\n  geom_bar() \n\n\n\n\n\n\n\n\nOption 3 Make your own distribution function.\n\nIf you know the probability of each member of your sample space, you can specify each probability in the prob argument in the sample function.\n\n\n# A tribble is a \"transposed tibble\" that is easy to view.\n# I pasted it directly from Excel using the datapasta RStudio addin\nd_probs &lt;- tibble::tribble(\n  ~sample_space, ~probability,\n            25L,         0.01,\n            26L,         0.02,\n            27L,         0.04,\n            28L,         0.10,\n            29L,         0.20,\n            30L,         0.30,\n            31L,         0.20,\n            32L,         0.08,\n            33L,         0.03,\n            34L,         0.01,\n            35L,         0.01)\n\n\ncluster_size = sample(\n  x = d_probs$sample_space, \n  prob = d_probs$probability,\n  size = k, \n  replace = TRUE) \n\nd2 &lt;- tibble(cluster_id, cluster_size)\n\nggplot(d2, aes(cluster_size)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nIf you have real data that you want to mimic, you can use the sample function to generate new data with the same proportions as the real data.\n\n\n# Pretend that x is a vector with real data on class sizes. \n# Also pretend that we do not know how it was made. \n# All we know is the data in x.\n\nd_x &lt;- tibble(x = round((rpois(10000, 28) - 28) * 0.3, 0) + 28) %&gt;% \n  group_by(x) %&gt;% \n  summarise(probability = n() / nrow(.))\n\n# Generate new data based on same proportions\ncluster_size &lt;- sample(x = d_x$x,\n       size = k,\n       replace = TRUE,\n       prob = d_x$probability) %&gt;% \n  as.integer()\n\nd2 &lt;- tibble(cluster_id, cluster_size)\n\nggplot(d2, aes(cluster_size)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nSimulate level-2 error terms.\nWe will use the τ matrix to simulate u0j and u1j.\nRecall that\n\\[\\begin{bmatrix}e_{0j}\\\\e_{1j}\\end{bmatrix}\\sim\\mathcal{N}\\left(\\begin{bmatrix}0\\\\ 0 \\end{bmatrix}\\begin{matrix} \\\\ ,\\end{matrix}\\begin{bmatrix}\\tau_{00}&\\\\ \\tau_{10}&\\tau_{11}\\end{bmatrix}\\right)\\]\nCreate level-2 error terms.\n\ne2 &lt;- rmvnorm(k, \n             mean = c(e_0j = 0, e_1j = 0), \n             sigma = tau_2) %&gt;% \n  as_tibble()\n\ne2\n\n# A tibble: 100 × 2\n      e_0j    e_1j\n     &lt;dbl&gt;   &lt;dbl&gt;\n 1   1.67   0.0213\n 2  32.7   -0.212 \n 3 -25.7   -0.311 \n 4  -9.27  -0.0941\n 5  28.2   -0.859 \n 6 -48.6   -0.213 \n 7 -10.0    0.0453\n 8  -0.120  0.230 \n 9  22.9    0.0493\n10 -10.3    0.0828\n# ℹ 90 more rows\n\n\n\n\nSimulate level-2 predictors\nHow you simulate the level-2 predictors depends on theory. Personality variables like conscientiousness usually have a normal distribution. I am going to make things simple by creating a standard normal variate (mean = 0, sd = 1).\n\nC_j = rnorm(k)\n\n\n\nCompute all level-2 data in one sequence of steps.\nThe bind_cols function binds the columns of two data.frames (or tibbles).\n\nd2 &lt;- tibble(cluster_id = 1:k,\n       cluster_size = rpois(k, 28) %&gt;% pmax(1),\n       C_j = rnorm(k)) %&gt;% \n  bind_cols(e2) %&gt;% \n  mutate(b_0j = b_00 + b_01 * C_j + e_0j,\n         b_1j = b_10 + b_11 * C_j + e_1j)\nd2\n\n# A tibble: 100 × 7\n   cluster_id cluster_size    C_j    e_0j    e_1j  b_0j  b_1j\n        &lt;int&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1          1           33 -0.614   1.67   0.0213  489.  32.5\n 2          2           28  0.405  32.7   -0.212   541.  28.2\n 3          3           19  0.944 -25.7   -0.311   493.  25.9\n 4          4           24  0.418  -9.27  -0.0941  499.  28.2\n 5          5           30 -1.20   28.2   -0.859   504.  33.9\n 6          6           36 -0.423 -48.6   -0.213   443.  31.5\n 7          7           29  1.10  -10.0    0.0453  512.  25.7\n 8          8           41  0.989  -0.120  0.230   520.  26.3\n 9          9           37 -0.618  22.9    0.0493  511.  32.5\n10         10           29  0.586 -10.3    0.0828  501.  27.7\n# ℹ 90 more rows"
  },
  {
    "objectID": "tutorials/DataSimulation/data_simulation.html#step-3-simulate-level-1-data",
    "href": "tutorials/DataSimulation/data_simulation.html#step-3-simulate-level-1-data",
    "title": "Data Simulation for Multilevel Models",
    "section": "Step 3: Simulate Level-1 Data",
    "text": "Step 3: Simulate Level-1 Data\n\nConvert Level-2 Data to Level-1 Data\nThe uncount function makes this step easy. It duplicates a row the number of times specified in the weights variable—the cluster size in this case.\n\nd1 &lt;- d2 %&gt;% \n  uncount(cluster_size)\n\nd1\n\n# A tibble: 2,787 × 6\n   cluster_id    C_j  e_0j   e_1j  b_0j  b_1j\n        &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1          1 -0.614  1.67 0.0213  489.  32.5\n 2          1 -0.614  1.67 0.0213  489.  32.5\n 3          1 -0.614  1.67 0.0213  489.  32.5\n 4          1 -0.614  1.67 0.0213  489.  32.5\n 5          1 -0.614  1.67 0.0213  489.  32.5\n 6          1 -0.614  1.67 0.0213  489.  32.5\n 7          1 -0.614  1.67 0.0213  489.  32.5\n 8          1 -0.614  1.67 0.0213  489.  32.5\n 9          1 -0.614  1.67 0.0213  489.  32.5\n10          1 -0.614  1.67 0.0213  489.  32.5\n# ℹ 2,777 more rows\n\n\n\n\nCompute the level-1 error term and the level-1 covariates.\nThe rnorm function needs to know how many rows there are. Because the number of rows was generated randomly, we do not usually know how many rows there will be. Fortunately, we can use the n function, which tells us how many rows are in a data frame.\n\nd1 &lt;- d1 %&gt;% \n  mutate(V_ij = rnorm(n()),\n         e = rnorm(n(), sd = sqrt(tau_1)))\nd1\n\n# A tibble: 2,787 × 8\n   cluster_id    C_j  e_0j   e_1j  b_0j  b_1j    V_ij      e\n        &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1          1 -0.614  1.67 0.0213  489.  32.5 -1.65    1.34 \n 2          1 -0.614  1.67 0.0213  489.  32.5 -0.0635 -5.36 \n 3          1 -0.614  1.67 0.0213  489.  32.5 -1.79   -0.924\n 4          1 -0.614  1.67 0.0213  489.  32.5 -0.326   0.730\n 5          1 -0.614  1.67 0.0213  489.  32.5  0.180   1.77 \n 6          1 -0.614  1.67 0.0213  489.  32.5  1.40   -2.98 \n 7          1 -0.614  1.67 0.0213  489.  32.5  0.546  -6.83 \n 8          1 -0.614  1.67 0.0213  489.  32.5 -0.0393  1.59 \n 9          1 -0.614  1.67 0.0213  489.  32.5 -0.0401 -0.279\n10          1 -0.614  1.67 0.0213  489.  32.5  1.57   -7.86 \n# ℹ 2,777 more rows"
  },
  {
    "objectID": "tutorials/DataSimulation/data_simulation.html#simulate-the-outcome-variable",
    "href": "tutorials/DataSimulation/data_simulation.html#simulate-the-outcome-variable",
    "title": "Data Simulation for Multilevel Models",
    "section": "Simulate the outcome variable",
    "text": "Simulate the outcome variable\nIf we have written out the level-1 equation, this step is easy.\n\\[Reading_{ij}=b_{0j} + b_{1j}V_{ij}+e_{ij}\\]\n\nd1 &lt;- d1 %&gt;% \n  mutate(Reading = b_0j + b_1j * V_ij + e)\nd1\n\n# A tibble: 2,787 × 9\n   cluster_id    C_j  e_0j   e_1j  b_0j  b_1j    V_ij      e Reading\n        &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1          1 -0.614  1.67 0.0213  489.  32.5 -1.65    1.34     437.\n 2          1 -0.614  1.67 0.0213  489.  32.5 -0.0635 -5.36     482.\n 3          1 -0.614  1.67 0.0213  489.  32.5 -1.79   -0.924    430.\n 4          1 -0.614  1.67 0.0213  489.  32.5 -0.326   0.730    480.\n 5          1 -0.614  1.67 0.0213  489.  32.5  0.180   1.77     497.\n 6          1 -0.614  1.67 0.0213  489.  32.5  1.40   -2.98     532.\n 7          1 -0.614  1.67 0.0213  489.  32.5  0.546  -6.83     500.\n 8          1 -0.614  1.67 0.0213  489.  32.5 -0.0393  1.59     490.\n 9          1 -0.614  1.67 0.0213  489.  32.5 -0.0401 -0.279    488.\n10          1 -0.614  1.67 0.0213  489.  32.5  1.57   -7.86     532.\n# ℹ 2,777 more rows"
  },
  {
    "objectID": "tutorials/EFA/index.html",
    "href": "tutorials/EFA/index.html",
    "title": "Exploratory Factor Analysis with R",
    "section": "",
    "text": "The following code will install the tidyverse, GPArotation, and psych packages if they are not installed already.\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"GPArotation\")) install.packages(\"GPArotation\")\nif (!require(\"psych\")) install.packages(\"psych\")\nif (!require(\"psych\")) install.packages(\"easystats\")\n\n\nlibrary(tidyverse)\nlibrary(GPArotation)\nlibrary(psych)\nlibrary(easystats)"
  },
  {
    "objectID": "tutorials/EFA/index.html#junk-factors",
    "href": "tutorials/EFA/index.html#junk-factors",
    "title": "Exploratory Factor Analysis with R",
    "section": "Junk Factors",
    "text": "Junk Factors\nWe want to make sure that each factor makes sense theoretically and that there are no “junk” factors. Junk factors group unrelated items that have no underlying theoretical construct that unites them. EFA is “exploratory” and sometimes it groups items together because of chance fluctuations in the correlations. Junk factors typically have low loadings on all items (&lt; 0.4 or so).\nIf you find a junk factor, you can leave it in and not interpret it or you can extract 1 factor fewer.\nIn this case, all the factors are interpretable:\n\nPA1 = Extraversion\nPA2 = Neuroticism\nPA3 = Conscientiousness\nPA4 = Openness\nPA5 = Agreeableness"
  },
  {
    "objectID": "tutorials/EFA/index.html#singleton-factors",
    "href": "tutorials/EFA/index.html#singleton-factors",
    "title": "Exploratory Factor Analysis with R",
    "section": "Singleton Factors",
    "text": "Singleton Factors\nWe also worry about “singleton” factors. These have a high loading (&gt;0.5 or so) on a single item and all other loadings are small (&lt;0.3 or so). EFA is for finding factors that explain variability across items. Singleton factors do not do this. When your solution has a singleton factor, it is generally best to extract 1 factor fewer. However, doing so does not always get rid of the singleton factor. It might collapse two legitimate factors into one factor. In this case, you probably want to return to the previous solution with the singleton (or exclude the problematic item from the analysis)."
  },
  {
    "objectID": "tutorials/EFA/index.html#factor-correlations",
    "href": "tutorials/EFA/index.html#factor-correlations",
    "title": "Exploratory Factor Analysis with R",
    "section": "Factor correlations",
    "text": "Factor correlations\nThe correlation matrrix of the latent factors in a an EFA is sometimes referred to as the Φ (Phi) matrix. To extract the factor correlations from the fa function’s output, we can do like so:\n\nfa(d_bfi, nfactors = 5, fm = \"pa\")$Phi\n\n         PA2     PA1     PA3     PA5      PA4\nPA2  1.00000  0.2282 -0.1897 -0.0733  0.03735\nPA1  0.22822  1.0000 -0.2204 -0.3185 -0.18808\nPA3 -0.18967 -0.2204  1.0000  0.2160  0.18971\nPA5 -0.07330 -0.3185  0.2160  1.0000  0.26729\nPA4  0.03735 -0.1881  0.1897  0.2673  1.00000\n\n\nPlotted, the Φ matrix look like this:\n\n\nCode\nfactor_labels &lt;- c(\"Extraversion\", \"Neuroticism\", \"Conscientiousness\", \"Openness\", \"Agreeableness\")\n\nfa(d_bfi, nfactors = 5, fm = \"pa\")$Phi %&gt;% \n  corrr::as_cordf(diagonal = 1) %&gt;% \n  corrr::stretch() %&gt;% \n  mutate(x = factor(x, labels = factor_labels),\n         y = factor(y, labels = factor_labels)) %&gt;% \n  ggplot(aes(x,y)) + \n  geom_tile(aes(fill = r)) + \n  geom_text(aes(label = WJSmisc::prob_label(r)), family  = \"Roboto Condensed\", hjust = 1, nudge_x = .1, size.unit = \"pt\", size = 12) + \n  scale_fill_gradient2(limits = c(-1,1), labels = \\(x) WJSmisc::prob_label(x, .1), breaks = seq(-1,1,.1)) + \n  theme_minimal(base_family = \"Roboto Condensed\", base_size = 12) + \n  theme(legend.key.height = unit(1, \n                                 units = \"null\"), \n        legend.text = element_text(hjust = 1)) +\n  scale_x_discrete(NULL, expand = expansion()) + \n  scale_y_discrete(NULL, expand = expansion()) +\n  coord_equal()\n\n\n\n\n\n\n\n\n\nThe correlations among these factors are generally low (&lt; 0.3 or so). Thus, the principal components method of parallel analysis is likely more accurate than the factor analysis version of parallel analysis. Thus, we should probably stick with 5 factors.\nHowever, we should probably extract 6 factors and see what we get. After all, this is exploratory work.\n\nfa(d_bfi, nfactors = 6, fm = \"pa\") %&gt;% \n  fa.sort() %&gt;% \n  print(cut = 0.2)\n\nFactor Analysis using method =  pa\nCall: fa(r = d_bfi, nfactors = 6, fm = \"pa\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                                            PA2   PA1   PA3   PA5   PA4   PA6\nGet angry easily.                          0.84                              \nGet irritated easily.                      0.83                              \nHave frequent mood swings.                 0.67                              \nPanic easily.                              0.45  0.24                        \nOften feel blue.                           0.43  0.42                        \nFind it difficult to approach others.            0.69                        \nDon't talk a lot.                                0.59                        \nMake friends easily.                            -0.52        0.21        0.30\nTake charge.                                    -0.40  0.27        0.25      \nContinue until everything is perfect.                  0.67                  \nDo things in a half-way manner.                       -0.65              0.28\nDo things according to a plan.                         0.55                  \nWaste my time.                                        -0.55                  \nAm exacting in my work.                                0.54                  \nInquire about others' well-being.                            0.68            \nKnow how to comfort others.                                  0.62            \nAm indifferent to the feelings of others.                   -0.56        0.29\nMake people feel at ease.                                    0.45        0.23\nLove children.                                               0.40            \nCarry the conversation to a higher level.                          0.65      \nAm full of ideas.                                                  0.57      \nWill not probe deeply into a subject.                             -0.45  0.38\nKnow how to captivate people.                   -0.33              0.39  0.22\nSpend time reflecting on things.                 0.35              0.39      \n                                            h2   u2 com\nGet angry easily.                         0.68 0.32 1.0\nGet irritated easily.                     0.66 0.34 1.0\nHave frequent mood swings.                0.55 0.45 1.2\nPanic easily.                             0.34 0.66 2.3\nOften feel blue.                          0.49 0.51 2.4\nFind it difficult to approach others.     0.56 0.44 1.1\nDon't talk a lot.                         0.39 0.61 1.3\nMake friends easily.                      0.55 0.45 2.0\nTake charge.                              0.40 0.60 2.9\nContinue until everything is perfect.     0.49 0.51 1.3\nDo things in a half-way manner.           0.55 0.45 1.5\nDo things according to a plan.            0.31 0.69 1.1\nWaste my time.                            0.43 0.57 1.5\nAm exacting in my work.                   0.34 0.66 1.3\nInquire about others' well-being.         0.50 0.50 1.1\nKnow how to comfort others.               0.51 0.49 1.2\nAm indifferent to the feelings of others. 0.34 0.66 1.7\nMake people feel at ease.                 0.48 0.52 2.3\nLove children.                            0.28 0.72 2.2\nCarry the conversation to a higher level. 0.47 0.53 1.0\nAm full of ideas.                         0.35 0.65 1.1\nWill not probe deeply into a subject.     0.34 0.66 2.0\nKnow how to captivate people.             0.48 0.52 2.9\nSpend time reflecting on things.          0.27 0.73 2.4\n\n                       PA2  PA1  PA3  PA5  PA4  PA6\nSS loadings           2.46 2.14 2.04 1.88 1.57 0.69\nProportion Var        0.10 0.09 0.08 0.08 0.07 0.03\nCumulative Var        0.10 0.19 0.28 0.35 0.42 0.45\nProportion Explained  0.23 0.20 0.19 0.17 0.15 0.06\nCumulative Proportion 0.23 0.43 0.62 0.79 0.94 1.00\n\n With factor correlations of \n      PA2   PA1   PA3   PA5   PA4   PA6\nPA2  1.00  0.25 -0.19 -0.11  0.03  0.12\nPA1  0.25  1.00 -0.21 -0.29 -0.20 -0.10\nPA3 -0.19 -0.21  1.00  0.19  0.19  0.01\nPA5 -0.11 -0.29  0.19  1.00  0.26  0.15\nPA4  0.03 -0.20  0.19  0.26  1.00  0.08\nPA6  0.12 -0.10  0.01  0.15  0.08  1.00\n\nMean item complexity =  1.7\nTest of the hypothesis that 6 factors are sufficient.\n\ndf null model =  276  with the objective function =  6.99 with Chi Square =  19512\ndf of  the model are 147  and the objective function was  0.34 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic n.obs is  2760 with the empirical chi square  579.3  with prob &lt;  1.2e-52 \nThe total n.obs was  2800  with Likelihood Chi Square =  960.3  with prob &lt;  1.7e-119 \n\nTucker Lewis Index of factoring reliability =  0.921\nRMSEA index =  0.044  and the 90 % confidence intervals are  0.042 0.047\nBIC =  -206.5\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   PA2  PA1  PA3  PA5  PA4  PA6\nCorrelation of (regression) scores with factors   0.93 0.89 0.88 0.87 0.85 0.74\nMultiple R square of scores with factors          0.86 0.79 0.78 0.76 0.72 0.55\nMinimum correlation of possible factor scores     0.72 0.58 0.56 0.53 0.44 0.10\n\n\nThe output does not all fit so the loadings for PA6 are below the loadings of the other factors.\nNotice that all the PA6 loadings are small. Also notice that they do not really make sense to group together. For example, what construct causes people to be “indifferent to the feelings of others” and at the same time “Make people feel at ease”?\nTwo hypotheses:\n\nRead in the right manner, all of the items are consistent with a person who uses charm and guile to manipulate others.\nFour of the six items have words or phrases that require a higher reading level (indifferent, probe, captivate, half-way manner). Perhaps the factor emerged because of differences in literacy.\n\nBoth these hypotheses might be true. They are not mutually exclusive.\nHowever, given that PA6 has no strong loadings, it is a weak factor and possibly a junk factor. Furthermore, our parallel analyses suggested we retain 5 factors, not 6."
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html",
    "title": "Getting Ready to Use R and RStudio",
    "section": "",
    "text": "“Blooming, buzzing confusion?” I see what you did there. —WJ\n\nFor R to pay off, you have to buy in. Learning challenging material does not have to be “fun” at every moment to be worthwhile. The beginning data analyst, assailed by statistical concepts, coding conventions, and baffling error messages, feels it all as one great blooming, buzzing confusion.\nThe challenge of learning R is indeed going to be challenging at times, but just there will also be many little a-ha moments, several larger check-out-what-I-can-do celebrations, and a few peak experiences of eudaemonic reverie."
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#r-is-a-programming-language-built-for-statistics.",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#r-is-a-programming-language-built-for-statistics.",
    "title": "Getting Ready to Use R and RStudio",
    "section": "R is a programming language built for statistics.",
    "text": "R is a programming language built for statistics.\n\n\n\n\nJohn Chambers\n\n\n\n\nRobert Gentleman\n\n\n\n\nRoss Ihaka\n\n\n\nIn 1976, John Chambers and colleagues at Bell Labs developed S, a programming language specifically designed to facilitate statistical analyses and data visualization.\nIn 1995, Robert Gentleman and Ross Ihaka released an open-source variant of S and named it “R” after their shared first initial.\nMost S code looks identical to R code, but R has a number of subtle enhancements such as better memory management.\nMore about the origins of R"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#r-is-a-computer-program-that-runs-code-in-the-programming-language-r.",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#r-is-a-computer-program-that-runs-code-in-the-programming-language-r.",
    "title": "Getting Ready to Use R and RStudio",
    "section": "R is a computer program that runs code in the programming language R.",
    "text": "R is a computer program that runs code in the programming language R.\nThe primary way that the R language is used is to perform analyses in the R software environment for statistical computing and graphics. This program is free and open source, meaning that not only can anyone download it for free, but its source code can be reused by anyone for any purpose."
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#r-is-a-data-science-ecosystem-in-which-the-r-community-flourishes",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#r-is-a-data-science-ecosystem-in-which-the-r-community-flourishes",
    "title": "Getting Ready to Use R and RStudio",
    "section": "R is a data science ecosystem in which the R community flourishes",
    "text": "R is a data science ecosystem in which the R community flourishes\nStrictly speaking, R is just one program. However, people use R in startlingly creative ways, often in combination with other programs and in coordination with other people. I like to think of R as the entire ecosystem of software, services, standards, and cultural practices shared by the entire community of R users.\n\n\n\nArtwork by [@allison_horst]\n\n\nThe technical merits of the R language and software do matter, but the success of R has more to do with the inclusive, welcoming, diverse, and expanding culture of the R community. When an individual R user has a great idea, the R community has as integrated set of technical standards, web services, and cultural practices such that innovations spread quickly to everyone in the community—often with multiple tutorials aimed at users of all levels of expertise. With the support of the R community, ordinary people can leverage R to accomplish far more than anyone would have thought possible not so very long ago.\n\n\n\nArtwork by [@allison_horst]"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#windows",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#windows",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Windows",
    "text": "Windows\nClick here and download the latest version of R for Windows. Open the file and follow installation instructions.\n\n\n\nScreenshot of Windows Installation Link"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#mac",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#mac",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Mac",
    "text": "Mac\nClick here and download the latest version of R for Mac. Double-click the file and follow installation instructions.\nThe instructions can be overwhelming to newcomers so I have included a picture with the download link highlighted:\n\n\n\nScreenshot of Mac Installation Link"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#linux",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#linux",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Linux",
    "text": "Linux\nInstall R on Ubuntu with this Bash script:\n\nsudo apt-get install r-base \n\nIf you are a Linux user, you probably know what this means. If not, you probably do not. More detailed instructions here."
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#rstudio-desktop",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#rstudio-desktop",
    "title": "Getting Ready to Use R and RStudio",
    "section": "RStudio Desktop",
    "text": "RStudio Desktop\nRStudio is free. Please do not pay for anything. The paid versions of RStudio have nothing you will need for this course. They are not better than the free version. They simply have features tailored to businesses and developers.\nInstall the free version of RStudio Desktop here.\nThe version recommended for your operation system (I’m using Windows) will appear here:\n\n\n\nScreenshot of RStudio Installation Link"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#rstudio-cloud",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#rstudio-cloud",
    "title": "Getting Ready to Use R and RStudio",
    "section": "RStudio Cloud",
    "text": "RStudio Cloud\nOne alternative to installing R and RStudio on your machine is to use RStudio in a web browser in RStudio Cloud. After signing up for a free account, you can use RStudio online. As the internet speeds up, I imagine that this option will become increasingly attractive."
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#the-console",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#the-console",
    "title": "Getting Ready to Use R and RStudio",
    "section": "The Console",
    "text": "The Console\nThe console (lower left) is where you submit quick temporary calculations and run code you have no intention of saving. Hit Enter to submit code to R. R is interactive in the sense that it will display the result of the code in the console where you typed."
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#scripts",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#scripts",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Scripts",
    "text": "Scripts\nThe script pane (upper left) is for writing code you want to preserve in a script file (.R) or RMarkdown file (.Rmd). More on RMarkdown later. Save your script files frequently.\nYou can submit code to the console by hitting Ctrl+Enter (or ⌘+Enter on Macs). You can also submit code by hitting the Run button. If you have code selected, only the selected part will be submitted. If you have no code selected, the current line (wherever the cursor is) will be submitted."
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#environment-variables",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#environment-variables",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Environment Variables",
    "text": "Environment Variables\nIn the Environment tab in the upper right pane, you can see which variables have been created and a preview of what they contain."
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#viewer",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#viewer",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Viewer",
    "text": "Viewer\nThe lower right pane has several tabs:\n\nFiles: Interact with project files\nPlots: Preview plots\nPackages: Install and update packages\nHelp: Search for help\nViewer: View documents created by RStudio"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#give-your-files-long-descriptive-names.",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#give-your-files-long-descriptive-names.",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Give your files long, descriptive names.",
    "text": "Give your files long, descriptive names.\nMake it clear to future-you exactly what is in the file.\n\nBad: data.xlsx\nBetter: dissertation_data.csv\nEven Better: student_questionnaire_time_1.csv"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#separate-words-with-underscores-not-spaces.",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#separate-words-with-underscores-not-spaces.",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Separate words with underscores, not spaces.",
    "text": "Separate words with underscores, not spaces.\nReplace spaces with underscores (_). Spaces often work fine, but sometimes they do not, which can result in hours and hours of debugging. Play it safe and don’t use spaces in file names.\nstudent scores.xlsx → student_scores.xlsx"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#no-special-characters-in-file-names",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#no-special-characters-in-file-names",
    "title": "Getting Ready to Use R and RStudio",
    "section": "No special characters in file names:",
    "text": "No special characters in file names:\nAvoid including in file names characters that have special meanings in many programming languages such as *@^$! and many others. Otherwise unexpected results can make your life complicated.\nparent@emotion*survey.csv → parent_emotion_survey.csv"
  },
  {
    "objectID": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#separate-dates-with-hyphens-using-iso-format",
    "href": "tutorials/GettingReady/getting_ready_to_use_RStudio.html#separate-dates-with-hyphens-using-iso-format",
    "title": "Getting Ready to Use R and RStudio",
    "section": "Separate dates with hyphens using ISO format",
    "text": "Separate dates with hyphens using ISO format\n\n\n\n\nfrom XKCD\n\n\n\nISO format for dates is YYYY-MM-DD, meaning that a four-digit year comes first, followed by a two-digit month, followed by a two-digit day. This format makes sorting order much easier than the formats used in the U.S.\nHere is a data file name that begins with a date, followed by the school district from from the data were collected:\n2020-01-12_district_A.csv\nOne of the benefits of using dates in the ISO format is that they sort chronologically. Which file names would you prefer to deal with?\n\n\n\n\n\nSorted Traditional Dates\nSorted ISO Dates\n\n\n\n\nDecember 04, 2010_District_P.csv\n2007-07-19_District_G.csv\n\n\nFebruary 21, 2009_District_Q.csv\n2008-01-27_District_H.csv\n\n\nFebruary 27, 2013_District_Q.csv\n2009-02-21_District_Q.csv\n\n\nJanuary 27, 2008_District_H.csv\n2009-09-08_District_G.csv\n\n\nJuly 19, 2007_District_G.csv\n2010-07-27_District_Z.csv\n\n\nJuly 27, 2010_District_Z.csv\n2010-11-17_District_X.csv\n\n\nNovember 17, 2010_District_X.csv\n2010-12-04_District_P.csv\n\n\nSeptember 08, 2009_District_G.csv\n2013-02-27_District_Q.csv"
  },
  {
    "objectID": "tutorials/MultiLevel3/multilevel3.html",
    "href": "tutorials/MultiLevel3/multilevel3.html",
    "title": "Multilevel Models with Three Levels",
    "section": "",
    "text": "# Load packages, installing them if needed\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(broom.mixed)\nlibrary(easystats)\nlibrary(sjPlot)\nlibrary(gt)"
  },
  {
    "objectID": "tutorials/MultiLevel3/multilevel3.html#level-1-equation",
    "href": "tutorials/MultiLevel3/multilevel3.html#level-1-equation",
    "title": "Multilevel Models with Three Levels",
    "section": "Level 1 Equation",
    "text": "Level 1 Equation\n\n\n\n\n\n\nNoteNotation Change\n\n\n\nIn this tutorial, I am going to use \\(b\\) for both fixed and random coefficients instead of the tradition of using \\(\\gamma\\) for fixed coefficients and \\(\\beta\\) for random coefficients. Any coefficient that has numbers only in its subscript is fixed (e.g., \\(b_{010}\\)). Any coefficient that has one or more letters in its subscript is random (e.g., \\(b_{0jk}\\)).\nIn addition, I am going to further simplify the notation for residuals and variances. All residuals will be \\(e\\) and all variances will be \\(\\tau\\). Their levels can be inferred from their subscripts. For example, \\(\\tau_1\\) is the variance of the level 1 residual.\n\n\nIf you can ignore the subscripts, this is a simple model in which student engagement predicts reading ability:\n\\[\n\\begin{aligned}\nReading_{ijk}&=b_{0jk}+b_{1jk}S_{ijk}+e_{ijk}\\\\\ne_{ijk}&\\sim\\mathcal{N}(0,\\tau_1)\n\\end{aligned}\n\\]\n\n\n\n\n\n  \n    \n      Type\n      Symbol\n      Interpretation\n    \n  \n  \n    Variables\n\\(Reading_{ijk}\\)\nThe reading comprehension score for student \\(i\\) in classroom \\(j\\) in school \\(k\\).\n    \\(S_{ijk}\\)\nStudent Engagement (L1 predictor). The number of hours per week spent reading for student \\(i\\) in classroom \\(j\\) in school \\(k\\).\n    Random Coefficients\n\\(b_{0jk}\\)\nL2 random intercept. In this model, \\(b_{0jk}\\) refers to the predicted reading score for a student in classroom \\(j\\) in school \\(k\\) who spent 0 hours per week reading.\n    \\(b_{1jk}\\)\nL2 random slope of L1 predictor. In this model, \\(b_{1jk}\\) refers to the predicted effect of student engagement for a student in classroom \\(j\\) in school \\(k\\).\n    Residuals\n\\(e_{ijk}\\)\nL1 residual. In this model, \\(e_{ijk}\\) refers to how much the reading comprehension score for student \\(i\\) in classroom \\(j\\) in school \\(k\\) differs from expectations.\n    Residual Variance\n\\(\\tau_1\\)\nL1 residual variance. The variance of \\(e_{ijk}\\)"
  },
  {
    "objectID": "tutorials/MultiLevel3/multilevel3.html#level-2-equations",
    "href": "tutorials/MultiLevel3/multilevel3.html#level-2-equations",
    "title": "Multilevel Models with Three Levels",
    "section": "Level 2 Equations",
    "text": "Level 2 Equations\nThe L2 predictor, teacher engagement \\(T_{jk}\\), can have an effect on the L2 random intercept \\(b_{0jk}\\) and on the L2 random slope \\(b_{1jk}\\):\n\\[\n\\begin{aligned}\nb_{0jk}&=b_{00k} + b_{01k}T_{jk}+e_{0jk}\\\\\nb_{1jk}&=b_{10k} + b_{11k}T_{jk}+e_{1jk}\\\\\n\\boldsymbol{e_2}&=\\begin{bmatrix}e_{0jk}\\\\ e_{1jk}\\end{bmatrix}\\\\\n\\boldsymbol{\\tau_2}&=\\begin{bmatrix}\\tau_{2.00} & \\\\ \\tau_{2.10} & \\tau_{2.11}\\end{bmatrix}\\\\\n\\boldsymbol{e_2}&\\sim \\mathcal{N}\\left(\\boldsymbol{0},\\boldsymbol{\\tau_2}\\right)\n\\end{aligned}\n\\]\n\n\n\n\n\n  \n    \n      Type\n      Symbol\n      Interpretation\n    \n  \n  \n    Variables\n\\(T_{jk}\\)\nTeacher engagement (L2 predictor). Number of hours teacher \\(j\\) in school \\(k\\) spend on intervention-related activities.\n    Random Coefficients\n\\(b_{0jk}\\)\nL2 random intercept. In this model, \\(b_{0jk}\\) refers to the predicted reading score for a student in classroom \\(j\\) in school \\(k\\) who spent 0 hours per week reading.\n    \\(b_{1jk}\\)\nL2 random slope for L1 predictor. In this model, \\(b_{1jk}\\) refers to the predicted effect of student engagement for a student in classroom \\(j\\) in school \\(k\\).\n    \\(b_{00k}\\)\nL3 random intercept. In this model, \\(b_{00k}\\) refers to the predicted reading score for classroom \\(j\\) in school \\(k\\) whose teacher spent 0 hours per month on intervention-related activitoes.\n    \\(b_{01k}\\)\nL3 random conditional slope for L2 predictor. In this model, \\(b_{01k}\\) refers to the effect of teacher engagement in school \\(k\\) when the student engagement is 0.\n    \\(b_{10k}\\)\nL3 random conditional slope for L1 predictor. In this model, \\(b_{10k}\\) refers to the effect of student engagement in school \\(k\\) when the teacher spends 0 hours per month on intervention-related activitoes.\n    \\(b_{11k}\\)\n*L3 random interaction effect for L1 and L2 predictors *. In this model, \\(b_{11k}\\) refers to the interaction of student and teacher engagement in school \\(k\\).\n    Residuals\n\\(\\boldsymbol{e_2}\\)\nThe vector of L2 residuals, \\(\\{e_{0jk},e_{1jk} \\}\\)\n    \\(e_{0jk}\\)\nResidual for L2 random intercept. In this model, \\(e_{0jk}\\) refers to how much the reading comprehension score for classroom \\(j\\) in school \\(k\\) differs from expectations when the teacher spent 0 hours per month on intervention-related activities.\n    \\(e_{1jk}\\)\nResidual for L2 random slope of L1 predictor. In this model, \\(e_{1jk}\\) allows the effect of student engagement to vary from class to class after accounting for teacher engagement.\n    Residual Variance\n\\(\\boldsymbol{\\tau_2}\\)\n2 × 2 covariance matrix for L2 residuals\n    \\(\\tau_{2.00}\\)\nVariance of the L2 random intercept residual, \\(e_{0jk}\\)\n    \\(\\tau_{2.11}\\)\nVariance of the L2 slope residual, \\(e_{1jk}\\)\n    \\(\\tau_{2.10}\\)\nCovariance of the L2 intercept and slope residuals, \\(e_{0jk}\\) and \\(e_{1jk}\\)"
  },
  {
    "objectID": "tutorials/MultiLevel3/multilevel3.html#level-3-equations",
    "href": "tutorials/MultiLevel3/multilevel3.html#level-3-equations",
    "title": "Multilevel Models with Three Levels",
    "section": "Level 3 Equations",
    "text": "Level 3 Equations\nThe L3 predictor, principal engagement \\(P_{jk}\\), can have have an effect on all four L3 random coefficients:\n\\[\n\\begin{aligned}\nb_{00k}&=b_{000} + b_{001}P_{k}+e_{00k}\\\\\nb_{01k}&=b_{010} + b_{011}P_{k}+e_{01k}\\\\\nb_{10k}&=b_{100} + b_{101}P_{k}+e_{10k}\\\\\nb_{11k}&=b_{110} + b_{111}P_{k}+e_{11k}\\\\\n\\boldsymbol{e_3}&=\\begin{bmatrix}e_{00k}\\\\ e_{01k}\\\\ e_{10k}\\\\ e_{11k}\\end{bmatrix}\\\\\n\\boldsymbol{\\tau_3}&=\\begin{bmatrix}\n\\tau_{3.00} & \\\\\n\\tau_{3.10} & \\tau_{3.11}\\\\\n\\tau_{3.20} & \\tau_{3.21} &  \\tau_{3.22}\\\\\n\\tau_{3.30} & \\tau_{3.31} &  \\tau_{3.32} &  \\tau_{3.33}\n\\end{bmatrix}\\\\\n\\boldsymbol{e_3}&\\sim \\mathcal{N}\\left(\\boldsymbol{0},\\boldsymbol{\\tau_3}\\right)\n\\end{aligned}\n\\]\n\n\n\n\n\n  \n    \n      Type\n      Symbol\n      Interpretation\n    \n  \n  \n    Variables\n\\(P_{k}\\)\nPrincipal engagement (L3 predictor). Number of hours the principal in school \\(k\\) spend on intervention-related activities.\n    Random Coefficients\n\\(b_{00k}\\)\nL3 random intercept. In this model, \\(b_{00k}\\) refers to the predicted reading score for classroom \\(j\\) in school \\(k\\) whose teacher spent 0 hours per month on intervention-related activitoes.\n    \\(b_{01k}\\)\nL3 random conditional slope of L2 predictor. In this model, \\(b_{01k}\\) refers to the effect of teacher engagement in school \\(k\\) when the student engagement is 0.\n    \\(b_{10k}\\)\nL3 random conditional slope of L1 predictor. In this model, \\(b_{10k}\\) refers to the effect of student engagement in school \\(k\\) when the teacher spends 0 hours per month on intervention-related activitoes.\n    \\(b_{11k}\\)\nL3 random interaction effect. In this model, \\(b_{11k}\\) refers to the interaction of student and teacher engagement in school \\(k\\).\n    Fixed Coefficients\n\\(b_{000}\\)\nFixed Intercept. In this model, \\(b_{000}\\) refers to the predicted reading score when student, teacher, and principal engagement is 0.\n    \\(b_{001}\\)\nFixed Conditional Slope of L3 predictor. In this model, \\(b_{001}\\) refers to the effect of principal engagement when student and teacher engagement is 0.\n    \\(b_{010}\\)\nFixed Conditional Slope of L2 predictor. In this model, \\(b_{010}\\) refers to the effect of teacher engagement when student and principal engagement is 0.\n    \\(b_{011}\\)\nFixed L2-L3 Conditional Interaction. In this model, \\(b_{011}\\) refers to the interaction of teacher and principal engagement when student engagement is 0.\n    \\(b_{100}\\)\nFixed Conditional Slope of L1 predictor. In this model, \\(b_{100}\\) refers to the effect of student engagement when teacher and principal engagement is 0.\n    \\(b_{101}\\)\nFixed L1-L3 Conditional Interaction. In this model, \\(b_{101}\\) refers to the interaction of student and principal engagement when teacher engagement is 0.\n    \\(b_{110}\\)\nFixed L1-L2 Conditional Interaction. In this model, \\(b_{110}\\) refers to the interaction of student and principal engagement when teacher engagement is 0.\n    \\(b_{111}\\)\nFixed L1-L2-L3 Interaction. In this model, \\(b_{111}\\) refers to the interaction of student, teacher, and principal engagement.\n    Residuals\n\\(\\boldsymbol{e_3}\\)\nThe vector of L3 residuals, \\(\\{e_{00k},e_{01k},e_{10k},e_{11k} \\}\\)\n    \\(e_{00k}\\)\nResidual for L3 intercept. In this model, \\(e_{00k}\\) refers to how much the reading comprehension score for school \\(k\\) differs from expectations when student, teacher, and principal engagement is 0.\n    \\(e_{01k}\\)\nResidual for random L3 conditional slope of L2 predictor. In this model, \\(e_{01k}\\) refers how much the conditional effect of teacher engagement (when student and principal engagement is 0) differs from expectations in school \\(k\\).\n    \\(e_{10k}\\)\nResidual for random L3 conditional slope of the L1 predictor. In this model, \\(e_{10k}\\) refers how much the conditional effect of student engagement (when teacher and principal engagement is 0) differs from expectations in school \\(k\\).\n    \\(e_{11k}\\)\nResidual for random L3 interaction of L1 and L2 predictors. In this model, \\(e_{11k}\\) refers how much the interaction of student and teacher engagement differs from expectations in school \\(k\\).\n    Residual Variance\n\\(\\boldsymbol{\\tau_3}\\)\n4 × 4 covariance matrix for L3 residuals\n    \\(\\tau_{3.00}\\)\nVariance of \\(e_{00k}\\). After controling for the L3 predictor (principal engagement), how much does the L3 intercept vary?\n    \\(\\tau_{3.11}\\)\nVariance of \\(e_{01k}\\). After controling for the L3 predictor (principal engagement), how much does the L3 slope of the L2 predictor (teacher engagement) vary?\n    \\(\\tau_{3.22}\\)\nVariance of \\(e_{10k}\\). After controling for the L3 predictor (principal engagement), how much does the L3 slope of the L1 predictor (student engagement) vary?\n    \\(\\tau_{3.33}\\)\nVariance of \\(e_{11k}\\). After controling for the L3 predictor (principal engagement), how much does the L3 interaction of the L1–L2 predictors (student and teacher engagement) vary?\n    \\(\\tau_{3.10}\\)\nCovariance of the \\(e_{00k}\\) and \\(e_{01k}\\).\n    \\(\\tau_{3.20}\\)\nCovariance of the \\(e_{00k}\\) and \\(e_{10k}\\)\n    \\(\\tau_{3.30}\\)\nCovariance of the \\(e_{00k}\\) and \\(e_{11k}\\)\n    \\(\\tau_{3.10}\\)\nCovariance of the \\(e_{01k}\\) and \\(e_{10k}\\)\n    \\(\\tau_{3.10}\\)\nCovariance of the \\(e_{01k}\\) and \\(e_{11k}\\)\n    \\(\\tau_{3.10}\\)\nCovariance of the \\(e_{10k}\\) and \\(e_{10k}\\)"
  },
  {
    "objectID": "tutorials/MultiLevel3/multilevel3.html#combined-equation",
    "href": "tutorials/MultiLevel3/multilevel3.html#combined-equation",
    "title": "Multilevel Models with Three Levels",
    "section": "Combined Equation",
    "text": "Combined Equation\n\\[\n\\begin{multline}\nReading_{ijk}=\\underbrace{\\underbrace{b_{000} + b_{001}P_{k}+e_{00k}}_{b_{00k}} + \\underbrace{(b_{010} + b_{011}P_{k}+e_{01k})}_{b_{01k}}T_{jk}+e_{0jk}}_{b_{0jk}}+\\\\\n\\underbrace{(\\underbrace{b_{100}+b_{101}P_k+e_{10k}}_{b_{10k}} + (\\underbrace{b_{110}+b_{111}P_k+e_{11k}}_{b_{11k}})T_{jk}+e_{1jk})}_{b_{1jk}}S_{ijk}+e_{ijk}\\\\\n~\n\\end{multline}\n\\]"
  },
  {
    "objectID": "tutorials/MultiLevel3/multilevel3.html#combined-fixed-vs.-random-effects-equation",
    "href": "tutorials/MultiLevel3/multilevel3.html#combined-fixed-vs.-random-effects-equation",
    "title": "Multilevel Models with Three Levels",
    "section": "Combined Fixed vs. Random Effects Equation",
    "text": "Combined Fixed vs. Random Effects Equation\n\\[\n\\begin{aligned}\nReading_{ijk}&=\n\\underbrace{b_{000}}_{\\text{Fixed Intercept}} +\\\\ &\\underbrace{b_{001}P_{k}+b_{010}T_{jk} + b_{100}S_{ijk}}_{\\text{Fixed Conditional Slopes}} +\\\\\n&\\underbrace{b_{110}S_{ijk}T_{jk}+b_{101}S_{ijk}P_{k}+b_{011}T_{jk}P_{k}}_{\\text{Fixed Conditional 2-Way Interactions}}+\\\\\n&\\underbrace{b_{111}S_{ijk}T_{jk}P_{k}}_{\\text{Fixed L1-L2-L3 Interaction}}+\\\\\n&\\underbrace{e_{00k}+e_{01k}T_{jk}+e_{10k}S_{ijk}+e_{11k}S_{ijk}T_{jk}}_{\\text{L3 Random Effects}}+\\\\\n&\\underbrace{e_{0jk}+e_{1jk}S_{ijk}}_{\\text{L2 Random Effects}}+\\\\\n&\\underbrace{e_{ijk}}_{\\text{L1 Random Effect}}\n\\end{aligned}\n\\]\nThis last equation is equivalent to this model in R:\nReading ~ \n  1 + \n  P + T + S + \n  S:T + T:P + \n  S:T:P +\n  (1 + T + S + S:T | id_school) +\n  (1 + S | id_classrom)\nSome helpful hints about notation:\n\nTerms with letters in the subscripts are random. These include variables (e.g., \\(Reading_{ijk}, S_{ijk}, T_{jk}, P_k\\)), residuals (e.g., \\(e_{ijk}, e_{0jk}, e_{10k}\\)), and random coefficients (e.g., \\(b_{0jk}, b_{10k}\\)).\nRandom coefficients have a mix of numbers and letters in the subscripts (e.g., \\(b_{0jk}, b_{10k}\\)).\nFixed coefficients have numbers only in the subscripts (e.g., \\(b_{000}, b_{110}\\)).\nCoefficients with numbers that are exclusively 0 are intercepts (e.g., \\(b_{000}, b_{0jk}, b_{00k}\\)).\nCoefficients with only 1 non-zero number are slopes (e.g., \\(b_{001}, b_{010}, b_{100}, b_{1jk}, b_{01k}\\)). If interaction effects involving the same variable are present in the model, these slopes are conditional slopes (AKA simple slopes).\nCoefficients with multiple non-zero numbers are interaction effects (e.g., \\(b_{011}, b_{110}, b_{101},b_{111}, b_{11k}\\)).\n\nTo determine the level of the variable:\n\nAll fixed coefficients are specified at the highest level (i.e., 3 in this model)\nAll variables, random coefficients, and error terms belong to the level that corresponds to the number of letters like so:\n\n\\[\n\\text{Level}=1+\\text{Highest Level}-\\text{Number of Letters in Subscript}\n\\]\nThus, in this model, any symbol with \\(ijk\\) in the subscript is a level-1 variable (e.g., \\(Reading_{ijk}, S_{ijk}, e_{ijk}\\)).\nOtherwise, any symbol with \\(jk\\) in the subscript belongs to level 2 . Otherwise, any symbol with \\(k\\) in the subscript belongs to level 3. Fixed coefficients are included in the highest level’s equations (i.e., 3).\nPredictors are paired with coefficients from the next highest level or with fixed coefficients if no higher level exists. Thus,\n\nLevel-1 predictors are paired with level-2 coefficients (e.g., \\(b_{1jk}S_{ijk}\\)).\nLevel-2 predictors are paired with level-3 coefficients (e.g., \\(b_{01k}T_{jk}\\) and \\(b_{11k}T_{jk}\\)).\nLevel-3 predictors (highest level) are paired with fixed coefficients (e.g., \\(b_{001}P_{k}\\), \\(b_{011}P_{k}\\), \\(b_{101}P_{k}\\), and \\(b_{111}P_{k}\\))."
  },
  {
    "objectID": "tutorials/MultiLevel3/multilevel3.html#check-assumptions",
    "href": "tutorials/MultiLevel3/multilevel3.html#check-assumptions",
    "title": "Multilevel Models with Three Levels",
    "section": "Check Assumptions",
    "text": "Check Assumptions\nThe normality assumption holds at all levels. That is, all random variables, at all levels, should be normal.\nFirst, let’s just look at the density plot:\n\ncheck_model(m_null_3)\n\n\n\n\n\n\n\n\nThe level-2 and level-3 intercepts look a little positively skewed. However, this level of non-normality is not usually fatal to the model.\nLets check the assumptions again after we have added our fixed effects to the model."
  },
  {
    "objectID": "tutorials/MultiLevel3/multilevel3.html#level-1-predictors",
    "href": "tutorials/MultiLevel3/multilevel3.html#level-1-predictors",
    "title": "Multilevel Models with Three Levels",
    "section": "Level 1 Predictors",
    "text": "Level 1 Predictors\nLet’s add our L1 predictor first.\n\nm_1_L1fixed &lt;- update(m_null_3, . ~ .  + engagement_student)\n\nFirst, we need a new function that will compute variance reduction.\n\n\n\n\n\n\nWarningWarning!\n\n\n\nVariance reduction does not always behave as it does in ordinary least squares. Think carefully about what is happening. Sometimes the variance reduction will be negative, meaning that the new variances are larger than the old variances. This does not necessarily mean that the new model is worse.\nIn general, do not interpret variance reduction when models have different random slopes structure [@mccoachMultilevelModelSelection2022, 70].\n\n\n\n# Let's make a function that automates the process of computing variance reduction\n# This function will find which random components the 2 models have in common and compares them.\n# Any non-shared variance components will be omitted.\nvariance_reduction &lt;- function(model_new, model_old) {\n  \n    bind_rows(\n      as.data.frame(VarCorr(model_new), order = \"lower.tri\") %&gt;% \n        mutate(model = \"New Model\"),\n      as.data.frame(VarCorr(model_old), order = \"lower.tri\") %&gt;% \n        mutate(model = \"Old Model\")) %&gt;% \n    select(-sdcor) %&gt;% \n    mutate(model = factor(model, levels = c(\"New Model\", \"Old Model\"))) %&gt;% \n    arrange(grp, var1, var2, model) %&gt;% \n    group_by(grp, var1, var2) %&gt;% \n    mutate(n = n()) %&gt;% \n    filter(n &gt; 1) %&gt;% \n    ungroup() %&gt;% \n    select(-n) %&gt;% \n    pivot_wider(names_from = model, values_from = vcov) %&gt;% \n    unite(term, var1, var2, na.rm = T) %&gt;% \n    mutate(`Variance Reduction` = (`Old Model` - `New Model`) / `Old Model`,\n           term = ifelse(grp == \"Residual\", \"Level 1 Variance\", term) %&gt;% \n             str_replace(\"\\\\(Intercept\\\\)\", \"Intercept\")) \n    \n}\n\nWe are going to want a bunch of things every time we get a new model.\n\nAn assumption check plot (optional)\nAn automatic interpretation report\nA summary of the model, preferably in a nice table\nA plot of the fixed effects\nA test of conditional slopes, if the model has interactions.\nA statistical test of difference of the new model with a previous model\nA statistical comparison of performance metrics from the new and previous models\nVariance reduction statistics for the new model compared to a previous model\n\nThe easiest way to get all this with only minimal fussing with code is to specify the old and new models each time like this:\n\nm_old &lt;- m_null_3\nm_new &lt;- m_1_L1fixed\ncheckassumptions = FALSE # Set to TRUE if you want the check assumptions.\n\nThen run the following code:\n\n# Check assumptions if checkassumptions is TRUE\nif (checkassumptions) {\n  performance::check_model(m_new)\n  }\n\n# Tabular display of new model\nsjPlot::tab_model(m_new, show.std = TRUE)\n\n# Automated report of new model\nreport::report(m_new, include_effectsize = T) \n\n\n\n# Plot fixed effects in new model\nif (!is.null(insight::find_predictors(m_new)$conditional)) {\n  sjPlot::plot_model(m_new, \n           type = \"pred\", \n           title = deparse1(as.formula(m_new)), \n           terms = insight::find_predictors(m_new)$conditional\n           )\n}\n\n\n\n# Test simple slopes, if interactions are present\nif (!is.null(insight::find_interactions(m_new))) {\n  reghelper::reghelper(m_new)\n}\n\n# Test comparison\nanova(m_old, m_new) %&gt;% \n  gt::gt(caption = \"Test Model difference\") %&gt;% \n  gt::fmt_number() %&gt;% \n  gt::sub_missing(missing_text = \"\")\n\n# Compare fixed parameters with unstandardized (b) and standardized (beta) coefficients\ntab_model(m_old, m_new, \n          title = \"Model Comparison\", \n          show.std = TRUE, \n          show.ci = FALSE, \n          show.aic = TRUE, \n          string.est = \"b\", \n          string.std = \"&beta;\")\n\n  \n# Variance reduction\nvariance_reduction(m_new, m_old) %&gt;% \n    gt(caption = \"Variance Reduction\") \n\nIf you want an easy way to run a lot of code repeatedly without making a function, you can “re-use” a previous code chunk. This way, if you make changes, you do so in one location only. The chunk with re-usable code was called compare2models. We can run it again by enclosing the code chunk name in brackets like so:\nm_old &lt;- m_null_3\nm_new &lt;- m_1_L1fixed\n&lt;&lt;compare2models&gt;&gt;\nWhen rendered, this chunk will expand to have the compare2models code copied into the chunk like so:\n\nm_old &lt;- m_null_3\nm_new &lt;- m_1_L1fixed\n# Check assumptions if checkassumptions is TRUE\nif (checkassumptions) {\n  performance::check_model(m_new)\n  }\n\n# Tabular display of new model\nsjPlot::tab_model(m_new, show.std = TRUE)\n\n# Automated report of new model\nreport::report(m_new, include_effectsize = T) \n\n\n\n# Plot fixed effects in new model\nif (!is.null(insight::find_predictors(m_new)$conditional)) {\n  sjPlot::plot_model(m_new, \n           type = \"pred\", \n           title = deparse1(as.formula(m_new)), \n           terms = insight::find_predictors(m_new)$conditional\n           )\n}\n\n\n\n# Test simple slopes, if interactions are present\nif (!is.null(insight::find_interactions(m_new))) {\n  reghelper::reghelper(m_new)\n}\n\n# Test comparison\nanova(m_old, m_new) %&gt;% \n  gt::gt(caption = \"Test Model difference\") %&gt;% \n  gt::fmt_number() %&gt;% \n  gt::sub_missing(missing_text = \"\")\n\n# Compare fixed parameters with unstandardized (b) and standardized (beta) coefficients\ntab_model(m_old, m_new, \n          title = \"Model Comparison\", \n          show.std = TRUE, \n          show.ci = FALSE, \n          show.aic = TRUE, \n          string.est = \"b\", \n          string.std = \"&beta;\")\n\n  \n# Variance reduction\nvariance_reduction(m_new, m_old) %&gt;% \n    gt(caption = \"Variance Reduction\") \n\nHowever, this will give you a ton of output at once, which is fine for a rendered document but is cumbersome for interactive sessions. When you are first running the analyses, I recommend just copying the code and running the parts you want to see. Later, you can run the whole compare2models chunk for the rendered document, if desired.\nNow let’s run the code and see what happens:\n\nm_old &lt;- m_null_3\nm_new &lt;- m_1_L1fixed\n# Check assumptions if checkassumptions is TRUE\nif (checkassumptions) {\n  performance::check_model(m_new)\n  }\n\n# Tabular display of new model\nsjPlot::tab_model(m_new, show.std = TRUE)\n\n\n\n \nreading\n\n\nPredictors\nEstimates\nstd. Beta\nCI\nstandardized CI\np\n\n\n(Intercept)\n108.51\n-0.00\n105.97 – 111.04\n-0.11 – 0.11\n&lt;0.001\n\n\nengagement student\n3.90\n0.35\n3.75 – 4.05\n0.33 – 0.36\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n251.47\n\n\n\nτ00 id_classroom\n106.28\n\n\nτ00 id_school\n92.00\n\n\nICC\n0.44\n\n\nN id_classroom\n1191\n\n\nN id_school\n60\n\nObservations\n11914\n\n\nMarginal R2 / Conditional R2\n0.118 / 0.507\n\n\n\n\n# Automated report of new model\nreport::report(m_new, include_effectsize = T) \n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading with engagement_student (formula: reading ~\nengagement_student). The model included id_classroom as random effects\n(formula: list(~1 | id_classroom, ~1 | id_school)). The model's total\nexplanatory power is substantial (conditional R2 = 0.51) and the part related\nto the fixed effects alone (marginal R2) is of 0.12. The model's intercept,\ncorresponding to engagement_student = 0, is at 108.51 (95% CI [105.97, 111.04],\nt(11909) = 83.85, p &lt; .001). Within this model:\n\n  - The effect of engagement student is statistically significant and positive\n(beta = 3.90, 95% CI [3.75, 4.05], t(11909) = 51.27, p &lt; .001; Std. beta =\n0.35, 95% CI [0.33, 0.36])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n# Plot fixed effects in new model\nif (!is.null(insight::find_predictors(m_new)$conditional)) {\n  sjPlot::plot_model(m_new, \n           type = \"pred\", \n           title = deparse1(as.formula(m_new)), \n           terms = insight::find_predictors(m_new)$conditional\n           )\n}\n\n\n\n\n\n\n\n# Test simple slopes, if interactions are present\nif (!is.null(insight::find_interactions(m_new))) {\n  reghelper::reghelper(m_new)\n}\n\n# Test comparison\nanova(m_old, m_new) %&gt;% \n  gt::gt(caption = \"Test Model difference\") %&gt;% \n  gt::fmt_number() %&gt;% \n  gt::sub_missing(missing_text = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnpar\nAIC\nBIC\nlogLik\ndeviance\nChisq\nDf\nPr(&gt;Chisq)\n\n\n\n\n4.00\n104,117.09\n104,146.63\n−52,054.54\n104,109.09\n\n\n\n\n\n\n\n\n5.00\n101,759.69\n101,796.62\n−50,874.85\n101,749.69\n2,359.39\n1.00\n0.00\n\n\n\n\nTest Model difference\n\n# Compare fixed parameters with unstandardized (b) and standardized (beta) coefficients\ntab_model(m_old, m_new, \n          title = \"Model Comparison\", \n          show.std = TRUE, \n          show.ci = FALSE, \n          show.aic = TRUE, \n          string.est = \"b\", \n          string.std = \"&beta;\")\n\n\n\n\n \nreading\nreading\n\n\nPredictors\nb\nβ\np\nb\nβ\np\n\n\n(Intercept)\n116.19\n-0.00\n&lt;0.001\n108.51\n-0.00\n&lt;0.001\n\n\nengagement student\n\n\n\n3.90\n0.35\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n312.04\n251.47\n\n\n\nτ00\n105.17 id_classroom\n106.28 id_classroom\n\n\n\n89.34 id_school\n92.00 id_school\n\n\nICC\n0.38\n0.44\n\n\nN\n1191 id_classroom\n1191 id_classroom\n\n\n\n60 id_school\n60 id_school\n\nObservations\n11914\n11914\n\n\nMarginal R2 / Conditional R2\n0.000 / 0.384\n0.118 / 0.507\n\n\nAIC\n104114.778\n101760.677\n\n\n\nModel Comparison\n# Variance reduction\nvariance_reduction(m_new, m_old) %&gt;% \n    gt(caption = \"Variance Reduction\") \n\n\n\n\n\n\n\n\ngrp\nterm\nNew Model\nOld Model\nVariance Reduction\n\n\n\n\nResidual\nLevel 1 Variance\n251\n312.0\n0.1941\n\n\nid_classroom\nIntercept\n106\n105.2\n-0.0105\n\n\nid_school\nIntercept\n92\n89.3\n-0.0298\n\n\n\n\nVariance Reduction\n\n\n\\[\n\\begin{aligned}\n  \\operatorname{reading}_{i}  &\\sim N \\left(\\alpha_{j[i],k[i]}, \\sigma^2 \\right) \\\\\n    \\alpha_{j}  &\\sim N \\left(\\mu_{\\alpha_{j}}, \\sigma^2_{\\alpha_{j}} \\right)\n    \\text{, for id\\_classroom j = 1,} \\dots \\text{,J} \\\\\n    \\alpha_{k}  &\\sim N \\left(\\mu_{\\alpha_{k}}, \\sigma^2_{\\alpha_{k}} \\right)\n    \\text{, for id\\_school k = 1,} \\dots \\text{,K}\n\\end{aligned}\n\\]\nThere was a lot going on in that output, but it is clear that engagement_student is a significant predictor of reading."
  },
  {
    "objectID": "tutorials/MultiLevel3/multilevel3.html#level-2-predictors",
    "href": "tutorials/MultiLevel3/multilevel3.html#level-2-predictors",
    "title": "Multilevel Models with Three Levels",
    "section": "Level 2 Predictors",
    "text": "Level 2 Predictors\nNow let’s add the L2 predictor, engagement_teacher:\n\nm_2_L2fixed &lt;- update(m_1_L1fixed, . ~ . + engagement_teacher)\nm_old &lt;- m_1_L1fixed\nm_new &lt;- m_2_L2fixed\n# Check assumptions if checkassumptions is TRUE\nif (checkassumptions) {\n  performance::check_model(m_new)\n  }\n\n# Tabular display of new model\nsjPlot::tab_model(m_new, show.std = TRUE)\n\n\n\n \nreading\n\n\nPredictors\nEstimates\nstd. Beta\nCI\nstandardized CI\np\n\n\n(Intercept)\n101.70\n-0.01\n99.10 – 104.31\n-0.12 – 0.11\n&lt;0.001\n\n\nengagement student\n3.91\n0.35\n3.76 – 4.06\n0.33 – 0.36\n&lt;0.001\n\n\nengagement teacher\n3.34\n0.31\n3.08 – 3.60\n0.28 – 0.33\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n251.33\n\n\n\nτ00 id_classroom\n57.74\n\n\nτ00 id_school\n96.06\n\n\nICC\n0.38\n\n\nN id_classroom\n1191\n\n\nN id_school\n60\n\nObservations\n11914\n\n\nMarginal R2 / Conditional R2\n0.209 / 0.509\n\n\n\n\n# Automated report of new model\nreport::report(m_new, include_effectsize = T) \n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading with engagement_student and engagement_teacher (formula:\nreading ~ engagement_student + engagement_teacher). The model included\nid_classroom as random effects (formula: list(~1 | id_classroom, ~1 |\nid_school)). The model's total explanatory power is substantial (conditional R2\n= 0.51) and the part related to the fixed effects alone (marginal R2) is of\n0.21. The model's intercept, corresponding to engagement_student = 0 and\nengagement_teacher = 0, is at 101.70 (95% CI [99.10, 104.31], t(11908) = 76.45,\np &lt; .001). Within this model:\n\n  - The effect of engagement student is statistically significant and positive\n(beta = 3.91, 95% CI [3.76, 4.06], t(11908) = 51.68, p &lt; .001; Std. beta =\n0.35, 95% CI [0.33, 0.36])\n  - The effect of engagement teacher is statistically significant and positive\n(beta = 3.34, 95% CI [3.08, 3.60], t(11908) = 25.51, p &lt; .001; Std. beta =\n0.31, 95% CI [0.28, 0.33])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n# Plot fixed effects in new model\nif (!is.null(insight::find_predictors(m_new)$conditional)) {\n  sjPlot::plot_model(m_new, \n           type = \"pred\", \n           title = deparse1(as.formula(m_new)), \n           terms = insight::find_predictors(m_new)$conditional\n           )\n}\n\n\n\n\n\n\n\n# Test simple slopes, if interactions are present\nif (!is.null(insight::find_interactions(m_new))) {\n  reghelper::reghelper(m_new)\n}\n\n# Test comparison\nanova(m_old, m_new) %&gt;% \n  gt::gt(caption = \"Test Model difference\") %&gt;% \n  gt::fmt_number() %&gt;% \n  gt::sub_missing(missing_text = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnpar\nAIC\nBIC\nlogLik\ndeviance\nChisq\nDf\nPr(&gt;Chisq)\n\n\n\n\n5.00\n101,759.69\n101,796.62\n−50,874.85\n101,749.69\n\n\n\n\n\n\n\n\n6.00\n101,245.12\n101,289.43\n−50,616.56\n101,233.12\n516.57\n1.00\n0.00\n\n\n\n\nTest Model difference\n\n# Compare fixed parameters with unstandardized (b) and standardized (beta) coefficients\ntab_model(m_old, m_new, \n          title = \"Model Comparison\", \n          show.std = TRUE, \n          show.ci = FALSE, \n          show.aic = TRUE, \n          string.est = \"b\", \n          string.std = \"&beta;\")\n\n\n\n\n \nreading\nreading\n\n\nPredictors\nb\nβ\np\nb\nβ\np\n\n\n(Intercept)\n108.51\n-0.00\n&lt;0.001\n101.70\n-0.01\n&lt;0.001\n\n\nengagement student\n3.90\n0.35\n&lt;0.001\n3.91\n0.35\n&lt;0.001\n\n\nengagement teacher\n\n\n\n3.34\n0.31\n&lt;0.001\n\n\nRandom Effects\n\n\n\nσ2\n251.47\n251.33\n\n\n\nτ00\n106.28 id_classroom\n57.74 id_classroom\n\n\n\n92.00 id_school\n96.06 id_school\n\n\nICC\n0.44\n0.38\n\n\nN\n1191 id_classroom\n1191 id_classroom\n\n\n\n60 id_school\n60 id_school\n\nObservations\n11914\n11914\n\n\nMarginal R2 / Conditional R2\n0.118 / 0.507\n0.209 / 0.509\n\n\nAIC\n101760.677\n101248.328\n\n\n\nModel Comparison\n# Variance reduction\nvariance_reduction(m_new, m_old) %&gt;% \n    gt(caption = \"Variance Reduction\") \n\n\n\n\n\n\n\n\ngrp\nterm\nNew Model\nOld Model\nVariance Reduction\n\n\n\n\nResidual\nLevel 1 Variance\n251.3\n251\n0.000525\n\n\nid_classroom\nIntercept\n57.7\n106\n0.456652\n\n\nid_school\nIntercept\n96.1\n92\n-0.044126\n\n\n\n\nVariance Reduction\n\n\nAgain, that is a lot of output to wade through, but it is clear that engagement_teacher is a significant predictor of reading."
  },
  {
    "objectID": "tutorials/MultiLevel3/multilevel3.html#level-3-predictors",
    "href": "tutorials/MultiLevel3/multilevel3.html#level-3-predictors",
    "title": "Multilevel Models with Three Levels",
    "section": "Level 3 Predictors",
    "text": "Level 3 Predictors\nLet’s add the L3 predictor, engagement_principal:\n\nm_3_L3fixed &lt;- update(m_2_L2fixed, . ~ . + engagement_principal)\nm_old &lt;- m_2_L2fixed\nm_new &lt;- m_3_L3fixed\n# Check assumptions if checkassumptions is TRUE\nif (checkassumptions) {\n  performance::check_model(m_new)\n  }\n\n# Tabular display of new model\nsjPlot::tab_model(m_new, show.std = TRUE)\n\n\n\n \nreading\n\n\nPredictors\nEstimates\nstd. Beta\nCI\nstandardized CI\np\n\n\n(Intercept)\n97.99\n-0.01\n94.73 – 101.25\n-0.11 – 0.09\n&lt;0.001\n\n\nengagement student\n3.91\n0.35\n3.76 – 4.06\n0.33 – 0.36\n&lt;0.001\n\n\nengagement teacher\n3.34\n0.31\n3.08 – 3.60\n0.28 – 0.33\n&lt;0.001\n\n\nengagement principal\n1.69\n0.17\n0.70 – 2.69\n0.07 – 0.27\n0.001\n\n\nRandom Effects\n\n\n\nσ2\n251.34\n\n\n\nτ00 id_classroom\n57.75\n\n\nτ00 id_school\n81.23\n\n\nICC\n0.36\n\n\nN id_classroom\n1191\n\n\nN id_school\n60\n\nObservations\n11914\n\n\nMarginal R2 / Conditional R2\n0.237 / 0.508\n\n\n\n\n# Automated report of new model\nreport::report(m_new, include_effectsize = T) \n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading with engagement_student, engagement_teacher and\nengagement_principal (formula: reading ~ engagement_student +\nengagement_teacher + engagement_principal). The model included id_classroom as\nrandom effects (formula: list(~1 | id_classroom, ~1 | id_school)). The model's\ntotal explanatory power is substantial (conditional R2 = 0.51) and the part\nrelated to the fixed effects alone (marginal R2) is of 0.24. The model's\nintercept, corresponding to engagement_student = 0, engagement_teacher = 0 and\nengagement_principal = 0, is at 97.99 (95% CI [94.73, 101.25], t(11907) =\n58.92, p &lt; .001). Within this model:\n\n  - The effect of engagement student is statistically significant and positive\n(beta = 3.91, 95% CI [3.76, 4.06], t(11907) = 51.68, p &lt; .001; Std. beta =\n0.35, 95% CI [0.33, 0.36])\n  - The effect of engagement teacher is statistically significant and positive\n(beta = 3.34, 95% CI [3.08, 3.60], t(11907) = 25.51, p &lt; .001; Std. beta =\n0.31, 95% CI [0.28, 0.33])\n  - The effect of engagement principal is statistically significant and positive\n(beta = 1.69, 95% CI [0.70, 2.69], t(11907) = 3.34, p &lt; .001; Std. beta = 0.17,\n95% CI [0.07, 0.27])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n# Plot fixed effects in new model\nif (!is.null(insight::find_predictors(m_new)$conditional)) {\n  sjPlot::plot_model(m_new, \n           type = \"pred\", \n           title = deparse1(as.formula(m_new)), \n           terms = insight::find_predictors(m_new)$conditional\n           )\n}\n\n\n\n\n\n\n\n# Test simple slopes, if interactions are present\nif (!is.null(insight::find_interactions(m_new))) {\n  reghelper::reghelper(m_new)\n}\n\n# Test comparison\nanova(m_old, m_new) %&gt;% \n  gt::gt(caption = \"Test Model difference\") %&gt;% \n  gt::fmt_number() %&gt;% \n  gt::sub_missing(missing_text = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnpar\nAIC\nBIC\nlogLik\ndeviance\nChisq\nDf\nPr(&gt;Chisq)\n\n\n\n\n6.00\n101,245.12\n101,289.43\n−50,616.56\n101,233.12\n\n\n\n\n\n\n\n\n7.00\n101,236.58\n101,288.28\n−50,611.29\n101,222.58\n10.54\n1.00\n0.00\n\n\n\n\nTest Model difference\n\n# Compare fixed parameters with unstandardized (b) and standardized (beta) coefficients\ntab_model(m_old, m_new, \n          title = \"Model Comparison\", \n          show.std = TRUE, \n          show.ci = FALSE, \n          show.aic = TRUE, \n          string.est = \"b\", \n          string.std = \"&beta;\")\n\n\n\n\n \nreading\nreading\n\n\nPredictors\nb\nβ\np\nb\nβ\np\n\n\n(Intercept)\n101.70\n-0.01\n&lt;0.001\n97.99\n-0.01\n&lt;0.001\n\n\nengagement student\n3.91\n0.35\n&lt;0.001\n3.91\n0.35\n&lt;0.001\n\n\nengagement teacher\n3.34\n0.31\n&lt;0.001\n3.34\n0.31\n&lt;0.001\n\n\nengagement principal\n\n\n\n1.69\n0.17\n0.001\n\n\nRandom Effects\n\n\n\nσ2\n251.33\n251.34\n\n\n\nτ00\n57.74 id_classroom\n57.75 id_classroom\n\n\n\n96.06 id_school\n81.23 id_school\n\n\nICC\n0.38\n0.36\n\n\nN\n1191 id_classroom\n1191 id_classroom\n\n\n\n60 id_school\n60 id_school\n\nObservations\n11914\n11914\n\n\nMarginal R2 / Conditional R2\n0.209 / 0.509\n0.237 / 0.508\n\n\nAIC\n101248.328\n101239.498\n\n\n\nModel Comparison\n# Variance reduction\nvariance_reduction(m_new, m_old) %&gt;% \n    gt(caption = \"Variance Reduction\") \n\n\n\n\n\n\n\n\ngrp\nterm\nNew Model\nOld Model\nVariance Reduction\n\n\n\n\nResidual\nLevel 1 Variance\n251.3\n251.3\n-1.85e-06\n\n\nid_classroom\nIntercept\n57.7\n57.7\n-5.51e-05\n\n\nid_school\nIntercept\n81.2\n96.1\n1.54e-01\n\n\n\n\nVariance Reduction\n\n\nNow we see that all three engagement variables are significant preductors.\nLet’s see how much variance our fixed effects explain compared to our null model:\n\ncompare_performance(m_3_L3fixed, m_null_3)\n\n# Comparison of Model Performance Indices\n\nName        |   Model |   AIC (weights) |  AICc (weights) |   BIC (weights)\n---------------------------------------------------------------------------\nm_3_L3fixed | lmerMod | 1.0e+05 (&gt;.999) | 1.0e+05 (&gt;.999) | 1.0e+05 (&gt;.999)\nm_null_3    | lmerMod | 1.0e+05 (&lt;.001) | 1.0e+05 (&lt;.001) | 1.0e+05 (&lt;.001)\n\nName        | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n---------------------------------------------------------------\nm_3_L3fixed |      0.508 |      0.237 | 0.356 | 15.291 | 15.854\nm_null_3    |      0.384 |      0.000 | 0.384 | 16.974 | 17.665\n\nvariance_reduction(m_3_L3fixed, m_null_3)\n\n# A tibble: 3 × 5\n  grp          term             `New Model` `Old Model` `Variance Reduction`\n  &lt;chr&gt;        &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt;                &lt;dbl&gt;\n1 Residual     Level 1 Variance       251.        312.                0.195 \n2 id_classroom Intercept               57.7       105.                0.451 \n3 id_school    Intercept               81.2        89.3               0.0908\n\n\nNot bad!"
  },
  {
    "objectID": "tutorials/MultiLevel3/multilevel3.html#uncorrelated-slopes-and-intercepts-at-l2",
    "href": "tutorials/MultiLevel3/multilevel3.html#uncorrelated-slopes-and-intercepts-at-l2",
    "title": "Multilevel Models with Three Levels",
    "section": "Uncorrelated slopes and intercepts at L2",
    "text": "Uncorrelated slopes and intercepts at L2\nWe are going to fit two models with random slopes for student engagement. First, we will make the L2 slopes uncorrelated with the L2 intercepts. Then we will allow them to correlate.\nAlthough I normally use the update function to specify new models, my experience is that updating random components gets messy very fast. It seems safer to just specify the models by hand (but carefully!).\n\nm_4_L2_uncorrelated_slopes &lt;- lmer(reading ~ engagement_student  + engagement_teacher + engagement_principal + (1 + engagement_student || id_classroom) + (1 | id_school), data = d)\n\nm_old &lt;- m_3_L3fixed\nm_new &lt;- m_4_L2_uncorrelated_slopes\n# Check assumptions if checkassumptions is TRUE\nif (checkassumptions) {\n  performance::check_model(m_new)\n  }\n\n# Tabular display of new model\nsjPlot::tab_model(m_new, show.std = TRUE)\n\n\n\n \nreading\n\n\nPredictors\nEstimates\nstd. Beta\nCI\nstandardized CI\np\nstd. p\n\n\n(Intercept)\n98.83\n-0.01\n95.60 – 102.06\n-0.11 – 0.10\n&lt;0.001\n0.861\n\n\nengagement student\n3.98\n0.35\n3.77 – 4.20\n0.33 – 0.37\n&lt;0.001\n&lt;0.001\n\n\nengagement teacher\n2.94\n0.31\n2.71 – 3.17\n0.28 – 0.33\n&lt;0.001\n&lt;0.001\n\n\nengagement principal\n1.61\n0.17\n0.63 – 2.60\n0.07 – 0.27\n0.001\n0.001\n\n\nRandom Effects\n\n\n\nσ2\n226.74\n\n\n\nτ00 id_classroom\n33.84\n\n\nτ00 id_school\n81.17\n\n\nτ11 id_classroom.engagement_student\n5.99\n\n\nρ01\n \n\n\nρ01\n \n\n\nICC\n0.34\n\n\nN id_classroom\n1191\n\n\nN id_school\n60\n\nObservations\n11914\n\n\nMarginal R2 / Conditional R2\n0.246 / 0.500\n\n\n\n\n# Automated report of new model\nreport::report(m_new, include_effectsize = T) \n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading with engagement_student, engagement_teacher and\nengagement_principal (formula: reading ~ engagement_student +\nengagement_teacher + engagement_principal). The model included id_classroom as\nrandom effects (formula: list(~1 | id_classroom, ~0 + engagement_student |\nid_classroom, ~1 | id_school)). The model's total explanatory power is\nsubstantial (conditional R2 = 0.50) and the part related to the fixed effects\nalone (marginal R2) is of 0.25. The model's intercept, corresponding to\nengagement_student = 0, engagement_teacher = 0 and engagement_principal = 0, is\nat 98.83 (95% CI [95.60, 102.06], t(11906) = 59.94, p &lt; .001). Within this\nmodel:\n\n  - The effect of engagement student is statistically significant and positive\n(beta = 3.98, 95% CI [3.77, 4.20], t(11906) = 35.88, p &lt; .001; Std. beta =\n0.35, 95% CI [0.33, 0.37])\n  - The effect of engagement teacher is statistically significant and positive\n(beta = 2.94, 95% CI [2.71, 3.17], t(11906) = 24.63, p &lt; .001; Std. beta =\n0.31, 95% CI [0.28, 0.33])\n  - The effect of engagement principal is statistically significant and positive\n(beta = 1.61, 95% CI [0.63, 2.60], t(11906) = 3.20, p = 0.001; Std. beta =\n0.17, 95% CI [0.07, 0.27])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n# Plot fixed effects in new model\nif (!is.null(insight::find_predictors(m_new)$conditional)) {\n  sjPlot::plot_model(m_new, \n           type = \"pred\", \n           title = deparse1(as.formula(m_new)), \n           terms = insight::find_predictors(m_new)$conditional\n           )\n}\n\n\n\n\n\n\n\n# Test simple slopes, if interactions are present\nif (!is.null(insight::find_interactions(m_new))) {\n  reghelper::reghelper(m_new)\n}\n\n# Test comparison\nanova(m_old, m_new) %&gt;% \n  gt::gt(caption = \"Test Model difference\") %&gt;% \n  gt::fmt_number() %&gt;% \n  gt::sub_missing(missing_text = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnpar\nAIC\nBIC\nlogLik\ndeviance\nChisq\nDf\nPr(&gt;Chisq)\n\n\n\n\n7.00\n101,236.58\n101,288.28\n−50,611.29\n101,222.58\n\n\n\n\n\n\n\n\n8.00\n100,619.42\n100,678.50\n−50,301.71\n100,603.42\n619.17\n1.00\n0.00\n\n\n\n\nTest Model difference\n\n# Compare fixed parameters with unstandardized (b) and standardized (beta) coefficients\ntab_model(m_old, m_new, \n          title = \"Model Comparison\", \n          show.std = TRUE, \n          show.ci = FALSE, \n          show.aic = TRUE, \n          string.est = \"b\", \n          string.std = \"&beta;\")\n\n\n\n\n \nreading\nreading\n\n\nPredictors\nb\nβ\np\nb\nβ\np\nstd. p\n\n\n(Intercept)\n97.99\n-0.01\n&lt;0.001\n98.83\n-0.01\n&lt;0.001\n0.861\n\n\nengagement student\n3.91\n0.35\n&lt;0.001\n3.98\n0.35\n&lt;0.001\n&lt;0.001\n\n\nengagement teacher\n3.34\n0.31\n&lt;0.001\n2.94\n0.31\n&lt;0.001\n&lt;0.001\n\n\nengagement principal\n1.69\n0.17\n0.001\n1.61\n0.17\n0.001\n0.001\n\n\nRandom Effects\n\n\n\nσ2\n251.34\n226.74\n\n\n\nτ00\n57.75 id_classroom\n33.84 id_classroom\n\n\n\n81.23 id_school\n81.17 id_school\n\n\nτ11\n \n5.99 id_classroom.engagement_student\n\n\nρ01\n \n \n\n\nρ01\n \n \n\n\nICC\n0.36\n0.34\n\n\nN\n1191 id_classroom\n1191 id_classroom\n\n\n\n60 id_school\n60 id_school\n\nObservations\n11914\n11914\n\n\nMarginal R2 / Conditional R2\n0.237 / 0.508\n0.246 / 0.500\n\n\nAIC\n101239.498\n100621.770\n\n\n\nModel Comparison\n# Variance reduction\nvariance_reduction(m_new, m_old) %&gt;% \n    gt(caption = \"Variance Reduction\") \n\n\n\n\n\n\n\n\ngrp\nterm\nNew Model\nOld Model\nVariance Reduction\n\n\n\n\nResidual\nLevel 1 Variance\n226.7\n251.3\n0.09785\n\n\nid_classroom\nIntercept\n33.8\n57.7\n0.41393\n\n\nid_school\nIntercept\n81.2\n81.2\n0.00069\n\n\n\n\nVariance Reduction\n\n\nInterpretation: We need to include the L2 random slopes for student engagment in our model. Why? The comparison using the anova function was significant. Also, the AIC-wt from the compare_performance function tells us we should prefer the random slopes model over the fixed effects model."
  },
  {
    "objectID": "tutorials/MultiLevel3/multilevel3.html#correlated-slopes-and-intercepts-at-l2",
    "href": "tutorials/MultiLevel3/multilevel3.html#correlated-slopes-and-intercepts-at-l2",
    "title": "Multilevel Models with Three Levels",
    "section": "Correlated slopes and intercepts at L2",
    "text": "Correlated slopes and intercepts at L2\nFor the sake of completeness, we can let the random slopes and intercepts correlate.\n\nm_5_L2_correlated_slopes &lt;- lmer(reading ~ engagement_student  + engagement_teacher + engagement_principal + (1 + engagement_student | id_classroom) + (1 | id_school), data = d)\n\nm_old &lt;- m_4_L2_uncorrelated_slopes\nm_new &lt;- m_5_L2_correlated_slopes\n# Check assumptions if checkassumptions is TRUE\nif (checkassumptions) {\n  performance::check_model(m_new)\n  }\n\n# Tabular display of new model\nsjPlot::tab_model(m_new, show.std = TRUE)\n\n\n\n \nreading\n\n\nPredictors\nEstimates\nstd. Beta\nCI\nstandardized CI\np\nstd. p\n\n\n(Intercept)\n98.84\n-0.01\n95.61 – 102.07\n-0.11 – 0.10\n&lt;0.001\n0.864\n\n\nengagement student\n3.98\n0.35\n3.77 – 4.20\n0.34 – 0.37\n&lt;0.001\n&lt;0.001\n\n\nengagement teacher\n2.93\n0.27\n2.70 – 3.17\n0.25 – 0.29\n&lt;0.001\n&lt;0.001\n\n\nengagement principal\n1.61\n0.16\n0.62 – 2.60\n0.06 – 0.26\n0.001\n0.001\n\n\nRandom Effects\n\n\n\nσ2\n226.82\n\n\n\nτ00 id_classroom\n33.45\n\n\nτ00 id_school\n81.18\n\n\nτ11 id_classroom.engagement_student\n5.94\n\n\nρ01 id_classroom\n0.02\n\n\nICC\n0.42\n\n\nN id_classroom\n1191\n\n\nN id_school\n60\n\nObservations\n11914\n\n\nMarginal R2 / Conditional R2\n0.223 / 0.547\n\n\n\n\n# Automated report of new model\nreport::report(m_new, include_effectsize = T) \n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading with engagement_student, engagement_teacher and\nengagement_principal (formula: reading ~ engagement_student +\nengagement_teacher + engagement_principal). The model included\nengagement_student as random effects (formula: list(~1 + engagement_student |\nid_classroom, ~1 | id_school)). The model's total explanatory power is\nsubstantial (conditional R2 = 0.55) and the part related to the fixed effects\nalone (marginal R2) is of 0.22. The model's intercept, corresponding to\nengagement_student = 0, engagement_teacher = 0 and engagement_principal = 0, is\nat 98.84 (95% CI [95.61, 102.07], t(11905) = 59.95, p &lt; .001). Within this\nmodel:\n\n  - The effect of engagement student is statistically significant and positive\n(beta = 3.98, 95% CI [3.77, 4.20], t(11905) = 35.96, p &lt; .001; Std. beta =\n0.35, 95% CI [0.34, 0.37])\n  - The effect of engagement teacher is statistically significant and positive\n(beta = 2.93, 95% CI [2.70, 3.17], t(11905) = 24.59, p &lt; .001; Std. beta =\n0.27, 95% CI [0.25, 0.29])\n  - The effect of engagement principal is statistically significant and positive\n(beta = 1.61, 95% CI [0.62, 2.60], t(11905) = 3.20, p = 0.001; Std. beta =\n0.16, 95% CI [0.06, 0.26])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n# Plot fixed effects in new model\nif (!is.null(insight::find_predictors(m_new)$conditional)) {\n  sjPlot::plot_model(m_new, \n           type = \"pred\", \n           title = deparse1(as.formula(m_new)), \n           terms = insight::find_predictors(m_new)$conditional\n           )\n}\n\n\n\n\n\n\n\n# Test simple slopes, if interactions are present\nif (!is.null(insight::find_interactions(m_new))) {\n  reghelper::reghelper(m_new)\n}\n\n# Test comparison\nanova(m_old, m_new) %&gt;% \n  gt::gt(caption = \"Test Model difference\") %&gt;% \n  gt::fmt_number() %&gt;% \n  gt::sub_missing(missing_text = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnpar\nAIC\nBIC\nlogLik\ndeviance\nChisq\nDf\nPr(&gt;Chisq)\n\n\n\n\n8.00\n100,619.42\n100,678.50\n−50,301.71\n100,603.42\n\n\n\n\n\n\n\n\n9.00\n100,621.38\n100,687.85\n−50,301.69\n100,603.38\n0.04\n1.00\n0.84\n\n\n\n\nTest Model difference\n\n# Compare fixed parameters with unstandardized (b) and standardized (beta) coefficients\ntab_model(m_old, m_new, \n          title = \"Model Comparison\", \n          show.std = TRUE, \n          show.ci = FALSE, \n          show.aic = TRUE, \n          string.est = \"b\", \n          string.std = \"&beta;\")\n\n\n\n\n \nreading\nreading\n\n\nPredictors\nb\nβ\np\nstd. p\nb\nβ\np\nstd. p\n\n\n(Intercept)\n98.83\n-0.01\n&lt;0.001\n0.861\n98.84\n-0.01\n&lt;0.001\n0.864\n\n\nengagement student\n3.98\n0.35\n&lt;0.001\n&lt;0.001\n3.98\n0.35\n&lt;0.001\n&lt;0.001\n\n\nengagement teacher\n2.94\n0.31\n&lt;0.001\n&lt;0.001\n2.93\n0.27\n&lt;0.001\n&lt;0.001\n\n\nengagement principal\n1.61\n0.17\n0.001\n0.001\n1.61\n0.16\n0.001\n0.001\n\n\nRandom Effects\n\n\n\nσ2\n226.74\n226.82\n\n\n\nτ00\n33.84 id_classroom\n33.45 id_classroom\n\n\n\n81.17 id_school\n81.18 id_school\n\n\nτ11\n5.99 id_classroom.engagement_student\n5.94 id_classroom.engagement_student\n\n\nρ01\n \n0.02 id_classroom\n\n\nICC\n0.34\n0.42\n\n\nN\n1191 id_classroom\n1191 id_classroom\n\n\n\n60 id_school\n60 id_school\n\nObservations\n11914\n11914\n\n\nMarginal R2 / Conditional R2\n0.246 / 0.500\n0.223 / 0.547\n\n\nAIC\n100621.770\n100623.733\n\n\n\nModel Comparison\n# Variance reduction\nvariance_reduction(m_new, m_old) %&gt;% \n    gt(caption = \"Variance Reduction\") \n\n\n\n\n\n\n\n\ngrp\nterm\nNew Model\nOld Model\nVariance Reduction\n\n\n\n\nResidual\nLevel 1 Variance\n226.8\n226.7\n-3.61e-04\n\n\nid_classroom\nIntercept\n33.5\n33.8\n1.16e-02\n\n\nid_school\nIntercept\n81.2\n81.2\n-7.03e-05\n\n\n\n\nVariance Reduction\n\n\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nOf the models we have seen so far, which should we prefer, fixed effects only (m_3_L3fixed), uncorrelated L2 random effects (m_4_L2_uncorrelated_slopes), or correlated L2 random effects (m_5_L2_correlated_slopes)?\n\n\n\n\n\n\n\n\nTipHint (click to see)\n\n\n\n\n\nLook at the output of these functions:\n\nanova(m_3_L3fixed, m_4_L2_uncorrelated_slopes, m_5_L2_correlated_slopes)\ncompare_performance(m_3_L3fixed, m_4_L2_uncorrelated_slopes, m_5_L2_correlated_slopes)"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html",
    "href": "tutorials/RandomIntercepts/random_intercepts.html",
    "title": "Random Intercept Models in R",
    "section": "",
    "text": "In a one-way ANOVA, there is a fixed variable and a random variable. Usually, our focus is on the fixed variable—so much so that we might not realize that the random variable is a variable at all.\nA fixed effect is used when the comparisons between groups of scores are meaningful. There is a small, defined number of groups, and all of them are sampled. A random effect is one where the number of groups is open-ended, and the differences between specific groups are not of theoretical interest.\nFor example, suppose that we compare an outcome variable collected from children who have been given no treatment, a short intervention, and a long-lasting intervention.\n\nOutcome_i=Treatment_i+e_i\n\n\nThe fixed effect is the treatment variable with 3 group means for the control group, the short intervention group, and the long intervention group.\nThe random effect is the difference of each child (e). It is a random effect because there is an open-ended number of children who could have been in our study, and we have no theoretical interest in comparing specific children with other specific children.\n\nThe one-way ANOVA comparing the three groups of children will reveal something about the group mean differences on the outcome variable, but nothing substantive about individual children."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#lme4",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#lme4",
    "title": "Random Intercept Models in R",
    "section": "lme4",
    "text": "lme4\nThe primary package for multilevel modeling we will use in this class is lme4."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#easystats",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#easystats",
    "title": "Random Intercept Models in R",
    "section": "Easystats",
    "text": "Easystats\nThe easystats package, like the tidyverse package, loads a family of packages that can make many aspects of analysis much, much easier than they used to be. You should know that the packages are still very much in development.\n\n parameters gets information about model parameters.\n performance helps you evaluate the performance of your models.\n modelbased computes marginal means and model-based predictions.\n insight extracts fundamental information about your model.\n effectsize computes model effect sizes.\n correlation makes working with correlations easier.\n datawizard makes transforming your data easier.\n see makes publication-ready data visualizations.\n report automates model interpretation.\n\n\n# Install packages, if needed.\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"lme4\")) install.packages(\"lme4\")\nif (!require(\"easystats\")) install.packages(\"easystats\")\nif (!require(\"marginaleffects\")) install.packages(\"marginaleffects\")\n\n\n# Load packages\nlibrary(lme4) # Multilevel modeling\nlibrary(tidyverse) # General data management and plotting\nlibrary(easystats) # General modeling enhancements\nlibrary(marginaleffects) # Predicted values"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#lme4-formula-syntax",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#lme4-formula-syntax",
    "title": "Random Intercept Models in R",
    "section": "lme4 Formula Syntax",
    "text": "lme4 Formula Syntax\nThe lmer function from the lme4 package is our primary tool for estimating multilevel models. The same syntax for OLS regression applies to lmer formulas, with one addition.\n\n\n\nlme4 syntax for a random intercepts model\n\n\n\nThe variable on the left is the outcome variable, reading_comprehension.\nThe ~ means “predicted by.”\nThe first 1 is the overall intercept. In this model it represents the weighted mean of all the classrooms on the outcome variable (reading_comprehension).\nThe parentheses are for specifying a random variable.\nThe 1 inside the parentheses represents the random intercepts for each classroom.\nThe | means “nested in.”"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#model-fit-object",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#model-fit-object",
    "title": "Random Intercept Models in R",
    "section": "Model Fit Object",
    "text": "Model Fit Object\nYou can assign the model’s fit object to a variable. I am calling mine m_0.\n\nm_0 &lt;- lmer(reading_comprehension ~ 1 + (1 | classroom_id), data = d)\n\nThus far nothing appears to have happened. However, the m_0 variable contains all the information we need to evaluate the model."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#checking-model-assumptions",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#checking-model-assumptions",
    "title": "Random Intercept Models in R",
    "section": "Checking Model Assumptions",
    "text": "Checking Model Assumptions\nThe performance package has a nice, overall check:\n\ncheck_model(m_0)\n\n\n\n\n\n\n\n\nEverything looks good. If we want, we can check things one at a time.\n\nNormality Assumptions\nWe want the level 1 and level 2 residuals to be normal. Here we check the level 1 residuals:\n\nnorm_check &lt;- check_normality(m_0)\nnorm_check\n\nOK: residuals appear as normally distributed (p = 0.663).\n\n\nThe output says that the residuals are approximately normal. If the output says that the residuals are not normal, it might not be a major concern. Small departures from normality can trigger a significant test result. In general, we worry about large departures from normality, not statistically significant small ones. They best way to detect large departures from normality is to plot the residuals.\n\nplot(norm_check)\n\n\n\n\n\n\n\nFigure 1: Checking the normality assumption with a Q-Q plot\n\n\n\n\n\nFigure 1 is called a Q-Q plot (Quantile-Quantile plot), which plots the standard normal distribution against the observed residuals. If the residuals are perfectly normal, the dots will be on a diagonal line. If a few points are outside the confidence interval in gray, there probably is nothing to worry about. However, if the points are not even close to the line or the gray confidence region, then maybe you need a generalized linear mixed model (lme4::glmer) instead of the linear mixed model (lme4:lmer). More on that later in the course.\nThe Level 1 residuals look approximately normal in Figure 1.\nWe can also check the normality of the level-2 residuals.\n\nnorm_check_level2 &lt;- check_normality(m_0, effects = \"random\")\nnorm_check_level2\n\nOK: Random effects 'classroom_id: (Intercept)' appear as normally distributed (p = 0.342).\n\n\nFigure 2 suggests that the level-2 random effects are approximately normal. If the error bars did not straddle the regression line, then we would start to worry about the plausibility of the normality assumption at level 2.\n\nplot(norm_check_level2)\n\n[[1]]\n\n\n\n\n\n\n\n\nFigure 2: The Q-Q plot of the level-2 random effects\n\n\n\n\n\nThe performance package has an admittedly experimental function that uses machine learning to predict the distribution of your residuals and response variable (i.e., Y, the variable you are predicting):\n\ncheck_distribution(m_0)\n\n# Distribution of Model Family\n\nPredicted Distribution of Residuals\n\n Distribution Probability\n       cauchy         75%\n       normal         19%\n  exponential          3%\n\nPredicted Distribution of Response\n\n Distribution Probability\n       normal         44%\n       cauchy         41%\n    lognormal          6%\n\n\nFigure 3 plots these estimates along with showing plots of the two residual distributions.\n\nplot(check_distribution(m_0))\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nThis function will become useful when we look at generalized linear mixed models and need to select a distribution family for our residuals.\n\n\nCheck for auto-correlation\nIf your data were gathered independently, the order of the cases should show no discernible trends. If adjacent cases are more similar to each other than to non-adjacent cases, then you have auto-correlation in your data. That might be a sign that the independence assumption has been violated.\n\ncheck_autocorrelation(m_0) \n\nOK: Residuals appear to be independent and not autocorrelated (p = 0.242).\n\n\nFortunately, there is no evidence of autocorrelation here.\n\n\nCheck for non-constant error variance (heteroscedasticity)\n\ncheck_heteroscedasticity(m_0)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p &lt; .001).\n\n\nThere is some of evidence of non-constant error variance. By itself, the warning that heteroscedasticity has been detected is not a big deal. In a large sample, statistically significant results sometimes can be triggered by trivially small departures from homoscedasticity. A plot can help us decide if this violation is worrisome.\n\nplot(check_heteroscedasticity(m_0))\n\n\n\n\n\n\n\nFigure 4: Checking the homoscedasticity assumption\n\n\n\n\n\nFigure 4 shows a mostly flat line. If you see major departures from a flat line, we might need to worry about why the residuals have larger variances for some predicted values than for others. This plot suggests that there is little to worry about.\n\n\nCheck for outliers\nWhen the sample size is small, outliers can distort your findings.\n\ncheck_outliers(m_0)\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.5).\n- For variable: (Whole model)\n\n\n\n# Checking for outliers\nplot(check_outliers(m_0))\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\nThis plot looks squished because none of the points have extreme leverage. Observations with high leverage are not necessarily bad. What we do not want to see is the combination of extreme residuals with extreme leverage.\nAn extreme residual is, by definition, far from the regression line. An observation with extreme leverage, by definition, is an outlier in terms of the predictor variables (i.e., in multiple regression, it is a multivariate outlier in the predictor space).\nIn the plot below, the blue line is the estimated regression line before the red outlier is added. The red line hows how the estimated regression line changes in the presence of the red outlier point.\n{fig-leverageplot}\nOutliers in the predictor space are said to have leverage (the ability to influence coefficients). Notice that the red point barely changes the regression line when the predictor is near the mean of the predictor variable (i.e., when it has little leverage). When the red point is at either extreme of the predictor distribution, it has a lot of leverage to change the regression line. If the red point is near the original regression line, there is no harm done. However, if the red point has the combination of high leverage and a large residual (i.e., it is an outlier in terms of the predictors and also in terms of the residuals), the red regression line can become quite misleading."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#model-summary",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#model-summary",
    "title": "Random Intercept Models in R",
    "section": "Model Summary",
    "text": "Model Summary\nAs with OLS (ordinary least squares) models generated with the lm function, we can see the overall results with the summary function.\n\nsummary(m_0)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_comprehension ~ 1 + (1 | classroom_id)\n   Data: d\n\nREML criterion at convergence: 8078\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.613 -0.676  0.037  0.651  2.971 \n\nRandom effects:\n Groups       Name        Variance Std.Dev.\n classroom_id (Intercept) 304      17.4    \n Residual                 420      20.5    \nNumber of obs: 900, groups:  classroom_id, 30\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)    57.04       3.25    17.5\n\n\nHere we are given the method by which the parameters were estimated: REML (Restricted Maximum Likelihood). We are told the range and interquartile range of the residuals. We are told the variance and SD of the random effects. We are given the estimates, standard errors, and t statistics for the fixed variables, though in this case the only fixed effect is the overall intercept."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#model-statistics",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#model-statistics",
    "title": "Random Intercept Models in R",
    "section": "Model Statistics",
    "text": "Model Statistics\n\nperformance(m_0)\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC | R2 (cond.) | R2 (marg.) |   ICC |   RMSE |  Sigma\n----------------------------------------------------------------------------------\n8084.290 | 8084.317 | 8098.697 |      0.420 |      0.000 | 0.420 | 20.159 | 20.488\n\n\nHere we see the Sigma, the individual-level variance. The root-mean squared error is also a measure of individual-level variance but does not correct for degrees of freedom. The AIC and BIC statistics are useful for comparing models that are non-nested (i.e., models that differ in ways other than merely adding predictors). The R2 (cond.) is the variance explained by all terms, including random effects. The R2 (marg.) is the variance explained by the fixed effect effects. Because there are no fixed effects, this statistic is 0. ICC is the proportion of variance explained by the random effects, which in this case is equal to the R2 (cond.)\nThe report package has an automated interpretation function:\n\nreport(m_0)\n\nWe fitted a constant (intercept-only) linear mixed model (estimated using REML\nand nloptwrap optimizer) to predict reading_comprehension (formula:\nreading_comprehension ~ 1). The model included classroom_id as random effect\n(formula: ~1 | classroom_id). The model's intercept is at 57.04 (95% CI [50.66,\n63.43], t(897) = 17.53, p &lt; .001).\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\nIf you do not want the entire report, it can be broken down into several components:\n\nreport_info(m_0)\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using a Wald t-distribution approximation.\n\nreport_model(m_0)\n\nconstant (intercept-only) linear mixed model (estimated using REML and nloptwrap optimizer) to predict reading_comprehension (formula: reading_comprehension ~ 1). The model included classroom_id as random effect (formula: ~1 | classroom_id)\n\nreport_effectsize(m_0)\n\nEffect sizes were labelled following Cohen's (1988) recommendations. \n\nvery small (Std. beta = 1.27e-15, 95% CI [-0.24, 0.24])\n\nreport_random(m_0)\n\nThe model included classroom_id as random effect (formula: ~1 | classroom_id)\n\nreport_intercept(m_0)\n\nThe model's intercept is at 57.04 (95% CI [50.66, 63.43], t(897) = 17.53, p &lt; .001).\n\nreport_parameters(m_0)\n\n  - The intercept is statistically significant and positive (beta = 57.04, 95% CI [50.66, 63.43], t(897) = 17.53, p &lt; .001; Std. beta = 1.27e-15, 95% CI [-0.24, 0.24])\n\nreport_statistics(m_0)\n\nbeta = 57.04, 95% CI [50.66, 63.43], t(897) = 17.53, p &lt; .001; Std. beta = 1.27e-15, 95% CI [-0.24, 0.24]"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#intraclass-correlation-coefficient-icc",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#intraclass-correlation-coefficient-icc",
    "title": "Random Intercept Models in R",
    "section": "Intraclass Correlation Coefficient (ICC)",
    "text": "Intraclass Correlation Coefficient (ICC)\nIf there is little variability in the level-2 intercepts, then a multilevel model might not be all that important. The intraclass correlation coefficient estimates how much variability in the outcome variable is explained by the level-2 intercepts:\n\\text{ICC}=\\frac{\\tau_{00}}{\\tau_{00}+\\sigma^2} = \\frac{\\text{Level 2 Residual Variance}}{\\text{Level 2 + Level 1 Residual Variance}}\nThe performance package extracts the intraclass correlation coefficient.\n\nicc(m_0)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.420\n  Unadjusted ICC: 0.420\n\n\nThe ICC here means that letting each classroom have its own mean explained 42% of the residual variance. Had the value\nThe regular ICC (“adjusted”) takes into account all random effects. The unadjusted ICC takes both fixed and random effects into account. Because there are no fixed effects in this model, the adjusted and conditional ICC are the same.\n\\text{ICC}_{\\text{Unadjusted}}=\\frac{\\tau_{00}}{\\tau_{00}+\\sigma_e^2+\\sigma_{\\text{Fixed}}^2} = \\frac{\\text{Level 2 Residual Variance}}{\\text{Total Variance}}\n\n\n\n\n\n\n\nICC\nMeaning\n\n\n\n\nAdjusted\nThe proportion of random variance explained by the grouping structure.\n\n\nUnadjusted\nThe proportion of all variance (fixed and random) explained by the grouping structure.\n\n\n\nThe performance package also gives us an alternate set of model statistics along with the ICC."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#variance-explained-r2",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#variance-explained-r2",
    "title": "Random Intercept Models in R",
    "section": "Variance Explained (R2)",
    "text": "Variance Explained (R2)\nThe variance explained by a model is one of the ways we evaluate its explanatory scope.\n\nperformance::r2(m_0)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.420\n     Marginal R2: 0.000\n\n\nThe Conditional R2 is the variance explained by both the fixed and random variables. Note that in this model, because \\sigma_{\\text{fixed}}^2=0, \\text{R}_{\\text{Conditional}}^2=\\text{ICC}.\n\\text{R}_{\\text{Conditional}}^2=\\frac{\\tau_{00}+\\sigma_{\\text{Fixed}}^2}{\\tau_{00}+\\sigma_e^2+\\sigma_{\\text{Fixed}}^2} = \\frac{\\text{Level 2 Residual Variance + Fixed Variance}}{\\text{Total Variance}}\nThe Marginal R2 is the variance explained by just the fixed variables. In this case, there are no fixed variables. Thus, Marginal R2 = 0.\n\\text{R}_{\\text{Marginal}}^2=\\frac{\\sigma_{\\text{Fixed}}^2}{\\tau_{00}+\\sigma_e^2+\\sigma_{\\text{Fixed}}^2} = \\frac{\\text{Fixed Variance}}{\\text{Total Variance}}"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#fixed-and-random-coefficients",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#fixed-and-random-coefficients",
    "title": "Random Intercept Models in R",
    "section": "Fixed and Random Coefficients",
    "text": "Fixed and Random Coefficients\nCoefficient-level statistics can be viewed with the parameters function.\n\nparameters(m_0)\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |         95% CI | t(897) |      p\n-------------------------------------------------------------------\n(Intercept) |       57.04 | 3.25 | [50.66, 63.43] |  17.53 | &lt; .001\n\n# Random Effects\n\nParameter                    | Coefficient\n------------------------------------------\nSD (Intercept: classroom_id) |       17.42\nSD (Residual)                |       20.49\n\n\nThe only fixed coefficient is the overall intercept, which is not usually of theoretical interest.\nIf you only want fixed effect printed:\n\nparameters(m_0, effects = \"fixed\")\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |         95% CI | t(897) |      p\n-------------------------------------------------------------------\n(Intercept) |       57.04 | 3.25 | [50.66, 63.43] |  17.53 | &lt; .001\n\n\nIf you only want random effect printed:\n\nparameters(m_0, effects = \"random\")\n\n# Random Effects\n\nParameter                    | Coefficient | 95% CI\n---------------------------------------------------\nSD (Intercept: classroom_id) |       17.42 |       \nSD (Residual)                |       20.49 |       \n\n\nThe random parameters are standard deviations. Squaring the estimate for sd__(Intercept) will give \\tau_{00}. Squaring the estimate for sd__Observation will give \\sigma^2.\nIf you want estimated parameters for each group:\n\nparameters(m_0, effects = \"random\", group_level = TRUE)\n\n# Random Effects\n\nParameter        | Coefficient |   SE |           95% CI\n--------------------------------------------------------\n(Intercept) [11] |      -28.30 | 3.66 | [-35.46, -21.13]\n(Intercept) [26] |      -25.27 | 3.66 | [-32.44, -18.11]\n(Intercept) [7]  |      -23.33 | 3.66 | [-30.50, -16.17]\n(Intercept) [13] |      -22.16 | 3.66 | [-29.32, -14.99]\n(Intercept) [21] |      -21.82 | 3.66 | [-28.99, -14.65]\n(Intercept) [3]  |      -21.29 | 3.66 | [-28.46, -14.12]\n(Intercept) [15] |      -13.16 | 3.66 | [-20.33,  -5.99]\n(Intercept) [23] |      -11.85 | 3.66 | [-19.02,  -4.69]\n(Intercept) [29] |       -9.81 | 3.66 | [-16.98,  -2.64]\n(Intercept) [30] |       -8.60 | 3.66 | [-15.77,  -1.43]\n(Intercept) [5]  |       -7.86 | 3.66 | [-15.02,  -0.69]\n(Intercept) [9]  |       -7.08 | 3.66 | [-14.24,   0.09]\n(Intercept) [28] |       -6.95 | 3.66 | [-14.12,   0.22]\n(Intercept) [6]  |       -5.40 | 3.66 | [-12.57,   1.77]\n(Intercept) [27] |       -3.05 | 3.66 | [-10.22,   4.12]\n(Intercept) [12] |        1.61 | 3.66 | [ -5.56,   8.78]\n(Intercept) [18] |        1.83 | 3.66 | [ -5.34,   9.00]\n(Intercept) [17] |        4.50 | 3.66 | [ -2.66,  11.67]\n(Intercept) [10] |        7.02 | 3.66 | [ -0.14,  14.19]\n(Intercept) [4]  |        7.55 | 3.66 | [  0.38,  14.72]\n(Intercept) [24] |        8.07 | 3.66 | [  0.91,  15.24]\n(Intercept) [14] |       10.14 | 3.66 | [  2.97,  17.30]\n(Intercept) [25] |       17.51 | 3.66 | [ 10.34,  24.68]\n(Intercept) [8]  |       17.86 | 3.66 | [ 10.69,  25.03]\n(Intercept) [16] |       18.11 | 3.66 | [ 10.94,  25.28]\n(Intercept) [20] |       18.43 | 3.66 | [ 11.26,  25.60]\n(Intercept) [19] |       21.06 | 3.66 | [ 13.89,  28.22]\n(Intercept) [22] |       23.32 | 3.66 | [ 16.15,  30.49]\n(Intercept) [1]  |       27.11 | 3.66 | [ 19.94,  34.28]\n(Intercept) [2]  |       31.81 | 3.66 | [ 24.64,  38.98]\n\n\nWhy would you want the estimated values? Maybe you want to make a plot of the random intercepts like in Figure 6.\n\nparameters(m_0, effects = \"random\", group_level = TRUE) %&gt;% \n  mutate(Level = fct_inorder(Level)) %&gt;%  # Reorder classroom id\n  ggplot(aes(Level, Coefficient)) + \n  geom_pointrange(aes(ymin = CI_low,\n                      ymax = CI_high )) + \n  labs(x = \"Classroom ID\", \n       y = \"Random Intercepts\")\n\n\n\n\n\n\n\nFigure 6: Group coefficients"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#basic-plot",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#basic-plot",
    "title": "Random Intercept Models in R",
    "section": "Basic Plot",
    "text": "Basic Plot\nSuppose we want the predicted value for each classroom. There are a variety of ways to do this. We are going to get case-level predictions from the marginaleffects package’s predictions function. A nice feature of his function is that it will average the predictions over any variable. In this case, the prediction is the same for all 30 groups, so instead of 900 predictions (one for each person), we only need 30 predictions (one for each classroom).\n\nd_predicted &lt;- marginaleffects::predictions(\n  m_0, \n  by = \"classroom_id\") \n\n\n# plot distributions and predicted values\nd %&gt;%\n  ggplot(aes(x = classroom_id,\n             y = reading_comprehension)) +\n  geom_violin(color = NA, fill = \"dodgerblue\", alpha = .25) +\n  geom_point(data = d_predicted,\n             aes(y = estimate)) +\n  geom_hline(aes(yintercept = get_intercept(m_0)))\n\n\n\n\n\n\n\n\nHere a fancier version of the same plot.\n\n# ggtext displays rich text\nlibrary(ggtext)\n\nd %&gt;% \n  ggplot(aes(x = classroom_id,\n             y = reading_comprehension)) + \n  # Within-group variability\n  geom_violin(color = NA, scale = \"width\", fill = \"dodgerblue\", alpha = .25) +\n  # Display raw data\n  ggbeeswarm::geom_quasirandom(size = .5, alpha = 0.5, pch = 16) +\n  # Group means as horizontal segments\n  geom_segment(\n    data = d_predicted,\n    aes(\n      x = as.numeric(classroom_id) - 0.5,\n      xend = as.numeric(classroom_id) + 0.5,\n      y = estimate ,\n      yend = estimate ),\n    linewidth =  .1) +\n  # Group means as text\n  geom_label(aes(\n    y = estimate, \n    label = scales::number(estimate,.1)), \n    data = d_predicted,\n    fill = tinter::lighten('dodgerblue', .25),\n    vjust = -0.3, \n    size = 2.8, \n    color = \"gray30\", \n    label.padding = unit(0,\"mm\"), label.size = 0) +\n  # Display Overall intercept coefficient as text\n  geom_richtext(data = tibble(\n    classroom_id = 1,\n    reading_comprehension = get_intercept(m_0),\n    label =  glue::glue(\"*&gamma;*&lt;sub&gt;00&lt;/sub&gt; = {round(fixef(m_0), 2)}\")\n    ),\n    aes(label = label),\n    label.margin = unit(1,\"mm\"),\n    label.padding = unit(0,\"mm\"),\n    label.color = NA,\n    fill = scales::alpha(\"white\", 0.5), \n    vjust = 0,\n    hjust = 0\n  ) +\n  # Display Overall intercept as horizontal line\n  geom_hline(yintercept = get_intercept(m_0)) +\n  # Axis labels\n  labs(x = \"Classrooms\", y = \"Reading Comprehension\")\n\n\n\n\n\n\n\nFigure 7: Model 0 Group intercepts"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#fit-object",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#fit-object",
    "title": "Random Intercept Models in R",
    "section": "Fit object",
    "text": "Fit object\nRepresenting the fixed intercept, the initial 1 is optional. I included it to be explicit, but most of the time it is omitted.\n\nm_1 &lt;- lmer(reading_comprehension ~ 1 + treatment + (1 | classroom_id), \n              data = d)\n\nSometimes these models can become large and unwieldy, making you vulnerable to coding errors. A safer way to create a nested model is to use the update function that only changes the part you specify. In this syntax, a . means that everything is the same.\n\nm_1 &lt;- update(m_0, . ~ . + treatment)\n\nThe formula above means “Make a new model based on the m_0 model. On the left-hand side, the outcome variable is the same. On the right-hand side, everything is the same except that we want to add the treatment variable as a predictor. Also, use the same data.”\nIt is best practice to use the update function wherever possible, because it reduces errors by making you explicitly state which part of the model will change."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#assumption-checks",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#assumption-checks",
    "title": "Random Intercept Models in R",
    "section": "Assumption Checks",
    "text": "Assumption Checks\n\ncheck_model(m_1)\n\n\n\n\n\n\n\n\nStill no problems."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#summary",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#summary",
    "title": "Random Intercept Models in R",
    "section": "Summary",
    "text": "Summary\n\nsummary(m_1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: reading_comprehension ~ (1 | classroom_id) + treatment\n   Data: d\n\nREML criterion at convergence: 8053.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5797 -0.6787  0.0420  0.6488  3.0029 \n\nRandom effects:\n Groups       Name        Variance Std.Dev.\n classroom_id (Intercept) 149.4    12.22   \n Residual                 419.8    20.49   \nNumber of obs: 900, groups:  classroom_id, 30\n\nFixed effects:\n                   Estimate Std. Error t value\n(Intercept)          45.416      3.196  14.212\ntreatmentTreatment   24.908      4.678   5.325\n\nCorrelation of Fixed Effects:\n            (Intr)\ntrtmntTrtmn -0.683\n\n\nWith a t value of 5.32, the effect of treatment is surely significant.\nNote that lmer displays the treatment variable as treatmentTreatment. This means that it is how much the Treatment group differs from the reference group, which in this case the control group. R is not smart enough to know in advance that the control group is the reference group. Instead, it assumes that the first factor level listed is the reference group. In this case, the control group is listed first. If you want a different reference group, you need to change the order of the factor levels. The base R way is to use the relevel function, but see the forcats package for other useful ways to do this.\nThe report package has an automated interpretation function. Note that unlike the summary function, the report function includes p-values for the fixed effects.\n\n# Overall report\nreport(m_1)\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer)\nto predict reading_comprehension with treatment (formula: reading_comprehension\n~ treatment). The model included classroom_id as random effect (formula: ~1 |\nclassroom_id). The model's total explanatory power is substantial (conditional\nR2 = 0.42) and the part related to the fixed effects alone (marginal R2) is of\n0.21. The model's intercept, corresponding to treatment = Control, is at 45.42\n(95% CI [39.14, 51.69], t(896) = 14.21, p &lt; .001). Within this model:\n\n  - The effect of treatment [Treatment] is statistically significant and positive\n(beta = 24.91, 95% CI [15.73, 34.09], t(896) = 5.32, p &lt; .001; Std. beta =\n0.93, 95% CI [0.59, 1.28])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#plot-model",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#plot-model",
    "title": "Random Intercept Models in R",
    "section": "Plot Model",
    "text": "Plot Model\nFrom Figure 8, it is easy to see that the treatment group has higher reading comprehension, on average, than the control group.\n\nmodelbased::estimate_relation(m_1) %&gt;% \n  plot() + \n  labs(x = NULL, y = \"Reading Comprehension\")\n\n\n\n\n\n\n\nFigure 8: Model-based estimate of the relationship between treatment and reading comprehension"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#fixed-and-random-coefficients-1",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#fixed-and-random-coefficients-1",
    "title": "Random Intercept Models in R",
    "section": "Fixed and Random Coefficients",
    "text": "Fixed and Random Coefficients\n\nparameters(m_1)\n\n# Fixed Effects\n\nParameter             | Coefficient |   SE |         95% CI | t(896) |      p\n-----------------------------------------------------------------------------\n(Intercept)           |       45.42 | 3.20 | [39.14, 51.69] |  14.21 | &lt; .001\ntreatment [Treatment] |       24.91 | 4.68 | [15.73, 34.09] |   5.32 | &lt; .001\n\n# Random Effects\n\nParameter                    | Coefficient\n------------------------------------------\nSD (Intercept: classroom_id) |       12.22\nSD (Residual)                |       20.49\n\n\nHere we see that the treatment variable’s estimate is statistically significant.\nIf we want to see what the estimated means are for the 2 treatment groups:\n\nmodelbased::estimate_means(m_1)\n\nEstimated Marginal Means\n\ntreatment |  Mean |   SE |         95% CI\n-----------------------------------------\nControl   | 45.42 | 3.20 | [38.87, 51.96]\nTreatment | 70.32 | 3.42 | [63.33, 77.32]\n\nMarginal means estimated at treatment"
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#intraclass-correlation-coefficient",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#intraclass-correlation-coefficient",
    "title": "Random Intercept Models in R",
    "section": "Intraclass Correlation Coefficient",
    "text": "Intraclass Correlation Coefficient\n\nperformance::icc(m_1)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.262\n  Unadjusted ICC: 0.206\n\n\nCompare these values to Model 0’s ICC\n\nperformance::icc(m_0)\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.420\n  Unadjusted ICC: 0.420\n\n\nHere we see that the ICC’s are reduced—because treatment explained some of the between-group variability."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#variance-explained-r2-1",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#variance-explained-r2-1",
    "title": "Random Intercept Models in R",
    "section": "Variance Explained (R2)",
    "text": "Variance Explained (R2)\nIn OLS regression, the variance explained is fairly straightforward because there is only one random residual. Its size changes with each new predictor added to the model. With multilevel models, variance explained is more complicated because there are multiple residuals to worry about.\nOur model consists of fixed effects (intercept and predictors) and random effects at level 1 and level 1:\n\nReading_{ij}=\\underbrace{\\gamma_{00}+\\gamma_{01}Treatment_{j}}_{\\text{Fixed}}+\\underbrace{u_{0j}}_{\\text{Random}_2}+\\underbrace{e_{ij}}_{\\text{Random}_1}\n\nWe can calculate the variance of the fixed effects, and the 2 random effects.\n\n\\begin{align}\n\\sigma_{\\text{Fixed}}^2&=\\text{Var}(\\gamma_{00}+\\gamma_{01}Treatment_{j})\\\\\n\\sigma_{1}^2&=\\text{Var}(e_{ij})=\\sigma_e^2\\\\\n\\sigma_{2}^2&=\\text{Var}(u_{0j})=\\tau_{00}\n\\end{align}\n\nThe marginal R^2 is the ratio of the variance of the fixed effects and the total variance (fixed, level 1, and level 2). That is, what percentage of variance is explained by the fixed effects.\n\nR^2_{\\text{Marginal}}=\\frac{\\sigma_{\\text{Fixed}}^2}{\\sigma_{\\text{Fixed}}^2+\\sigma_{2}^2+\\sigma_{1}^2}\n\nThe conditional R^2 compares the fixed and level 2 variances to the total variance (fixed, level 1, and level 2). In this context, what percentage of variance is explained by the fixed effects and the random intercepts.\n\nR^2_{\\text{Conditional}}=\\frac{\\sigma_{\\text{Fixed}}^2+\\sigma_{2}^2}{\\sigma_{\\text{Fixed}}^2+\\sigma_{2}^2+\\sigma_{1}^2}\n\n\nperformance::r2(m_1)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.420\n     Marginal R2: 0.214\n\n\nHere we see that Marginal R2 increased from 0 to 0.21, meaning that a substantial portion of the between-group variability is accounted for by treatment.\nThe Conditional R2 is the same as it was in model 0, because treatment cannot account for additional variability beyond classroom_id (which “accounts” for 100% of the between-group variability)."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#compare-models",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#compare-models",
    "title": "Random Intercept Models in R",
    "section": "Compare Models",
    "text": "Compare Models\nTo compare model coefficients, use the parameters::compare_models function:\n\ncompare_models(m_0, m_1, effects = \"all\")\n\n# Fixed Effects\n\nParameter             |                  m_0 |                  m_1\n-------------------------------------------------------------------\n(Intercept)           | 57.04 (50.66, 63.43) | 45.42 (39.14, 51.69)\ntreatment [Treatment] |                      | 24.91 (15.73, 34.09)\n\n# Random Effects\n\nParameter                    |   m_0 |   m_1\n--------------------------------------------\nSD (Intercept: classroom_id) | 17.42 | 12.22\nSD (Residual)                | 20.49 | 20.49\n\n\nTo test the whether one variable explains more variance than a different nested variable, use the anova function.\n\nanova(m_0, m_1)\n\nData: d\nModels:\nm_0: reading_comprehension ~ 1 + (1 | classroom_id)\nm_1: reading_comprehension ~ (1 | classroom_id) + treatment\n    npar  AIC  BIC logLik deviance Chisq Df Pr(&gt;Chisq)    \nm_0    3 8088 8103  -4041     8082                        \nm_1    4 8069 8089  -4031     8061    21  1    4.6e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value is in Pr(&gt;Chisq) column. It means that m_1’s deviance statistic is significantly smaller than that of m_0.\nThe performance function compare_peformance gives many metrics of change. The Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) can be used to compare non-nested models. The AIC_wt and BIC_wt columns tell you the conditional probabilities in favor of each model.\n\ncompare_performance(m_0, m_1) \n\n# Comparison of Model Performance Indices\n\nName |   Model |  AIC (weights) | AICc (weights) |  BIC (weights) | R2 (cond.)\n------------------------------------------------------------------------------\nm_0  | lmerMod | 8088.5 (&lt;.001) | 8088.5 (&lt;.001) | 8102.9 (&lt;.001) |      0.420\nm_1  | lmerMod | 8069.6 (&gt;.999) | 8069.6 (&gt;.999) | 8088.8 (0.999) |      0.420\n\nName | R2 (marg.) |   ICC |   RMSE |  Sigma\n-------------------------------------------\nm_0  |      0.000 | 0.420 | 20.159 | 20.488\nm_1  |      0.214 | 0.262 | 20.172 | 20.488\n\n\nIn this case, it is clear that model m_1 is preferred compared to m_0.\n\ntest_performance(m_0, m_1) \n\nName |   Model |     BF\n-----------------------\nm_0  | lmerMod |       \nm_1  | lmerMod | &gt; 1000\nModels were detected as nested (in terms of fixed parameters) and are compared in sequential order.\n\n\nWe can also compare the models with a Bayes Factor (BF). If BF &gt; 1, there is more evidence for the second model. If BF &lt; 1, there is greater evidence for the first model.\nHere are some guidelines for interpreting BF Raftery (1995):\n\nbf = 1–3: Weak\nbf = 3–20: Positive\nbf = 20–150: Strong\nbf &gt; 150: Very strong\n\nRaftery, A. E. (1995). Bayesian model selection in social research. Sociological Methodology 25, 111–64.\n\ntest_bf(m_0, m_1) %&gt;% \n  report()\n\nBayes factors were computed using the BIC approximation, by which BF10 =\nexp((BIC0 - BIC1)/2). Compared to the 1 + (1 | classroom_id) model, we found\nextreme evidence (BF = 1.17e+03) in favour of the treatment + (1 |\nclassroom_id) model (the least supported model).\n\n\nThus we see that there is strong evidence that m_1 is a better model than m_0."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#reduction-in-random-variable-variance",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#reduction-in-random-variable-variance",
    "title": "Random Intercept Models in R",
    "section": "Reduction in random variable variance",
    "text": "Reduction in random variable variance\nAlthough the Model 0 \\tau_{00} has the same symbol as the Model 1 \\tau_{00}, they are not the same. I therefore will distinguish the Model 0 and Model1 \\tau_{00} with second-level subscripts: \\tau_{00_0} and \\tau_{00_1}, respectively.\n\\Delta R_2^2(\\text{approx})=\\frac{\\tau_{00_0}-\\tau_{00_1}}{\\tau_{00_0}}\n\ntau_00_0 &lt;- get_variance_random(m_0)\ntau_00_1 &lt;- get_variance_random(m_1)\n\n(tau_00_0 - tau_00_1) / tau_00_0\n\nvar.random \n    0.5077 \n\n100*round((tau_00_0 - tau_00_1) / tau_00_0, 3)\n\nvar.random \n      50.8 \n\n\nThis means that τ00/1 = 149.41 is 50.8% smaller than τ00/0 = 303.51.\nWe can likewise see if there is any reduction in level-1 variance by comparing the residual variance of both models:\n\\Delta R_1^2(\\text{approx})=\\frac{\\sigma_{0}^2-\\sigma_{1}^2}{\\sigma_{0}^2}\n\nsigma_e_0 &lt;- get_variance_residual(m_0)\nsigma_e_1 &lt;- get_variance_residual(m_1)\n\n(sigma_e_0 - sigma_e_1) / sigma_e_0\n\nvar.residual \n    2.37e-09 \n\n\nYou can see that the reduction is very close to 0. Why? Because treatment was cluster-randomized. As a level-2 variable, it would be hard to see how it could influence individual variability. That is, everyone in the same classroom has the same treatment. The level-1 residuals (eij) are created by subtracting from the classroom means. Thus, the treatment variable cannot distinguish between anyone in the same class and cannot explain any individual variability.\nLater we will see that treatment might interact with an individual difference variable such that the treatment works for some students better than for others. In such a model, a level-2 predictor could influence individual variability."
  },
  {
    "objectID": "tutorials/RandomIntercepts/random_intercepts.html#not-so-basic-plot",
    "href": "tutorials/RandomIntercepts/random_intercepts.html#not-so-basic-plot",
    "title": "Random Intercept Models in R",
    "section": "Not-So Basic Plot",
    "text": "Not-So Basic Plot\nFigure 9 makes it quite clear that the treatment is associated with higher overall scores.\n\n\nCode\nd_augmented_1 &lt;- augment(m_1, conf.int = TRUE)\n\n# Get unique values of the fitted responses\nd_classroom_1 &lt;- d_augmented_1 %&gt;% \n  select(classroom_id, .fitted, treatment) %&gt;%\n  unique() %&gt;% \n  ungroup()\n\n# Get the mean values (.fixed) for each treatment condition\nd_treatment_1 &lt;- d_augmented_1 %&gt;% \n  select(.fixed, treatment) %&gt;% \n  unique()\n\n# plot distributions and predicted values\naugment(m_1) %&gt;%\n  mutate(treatment = fct_rev(treatment)) %&gt;% \n  ggplot(aes(x = classroom_id,\n             y = reading_comprehension)) +\n  geom_violin(color = NA, \n              aes(fill = treatment), \n              alpha = 0.5) +\n  geom_point(data = d_classroom_1,\n             aes(y = .fitted)) +\n  geom_hline(data = d_treatment_1, \n             aes(yintercept = .fixed)) +\n  facet_grid(cols = vars(treatment), \n             scales = \"free\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 9: Treatment effects for Model 1"
  },
  {
    "objectID": "tutorials/Regression/regression.html",
    "href": "tutorials/Regression/regression.html",
    "title": "Regression in R",
    "section": "",
    "text": "Here we are going to use a small data set to predict people’s height using the height of their parents."
  },
  {
    "objectID": "tutorials/Regression/regression.html#install-tidyverse",
    "href": "tutorials/Regression/regression.html#install-tidyverse",
    "title": "Regression in R",
    "section": "Install tidyverse",
    "text": "Install tidyverse\nSome packages are designed to work with several other packages as a system. The tidyverse package is a “meta-package” that installs and loads a coherent set of packages designed to help you import, manipulate, visualize, and interpret data. If you do not have a recent version of tidyverse already installed, you can install it with this code:\n\ninstall.packages(\"tidyverse\")"
  },
  {
    "objectID": "tutorials/Regression/regression.html#install-easystats",
    "href": "tutorials/Regression/regression.html#install-easystats",
    "title": "Regression in R",
    "section": "Install easystats",
    "text": "Install easystats\nThe easystats package is another “meta-package” that installs a set of packages designed to work together to make data analysis easier.\nIf you do not have a recent version of easystats already installed, you can install it with this code:\n\ninstall.packages(\"easystats\")"
  },
  {
    "objectID": "tutorials/Regression/regression.html#save-the-plot",
    "href": "tutorials/Regression/regression.html#save-the-plot",
    "title": "Regression in R",
    "section": "Save the plot!",
    "text": "Save the plot!\nYou can save to your hard drive a high-quality plot with the ggsave function. It will save whatever the last plot you created with ggplot. It will guess the file format from the file extension.\nHere I save a .pdf file of the plot to the working directory:\n\nggsave(\"my_plot.pdf\")\n\nWhat is the working directory? Is the folder on your machine that R thinks it should look first if it needs to find files or save files. If you are ever curious about which directory is the working directory, you can see the current working directory with the getwd function:\n\ngetwd()\n\nIf you saved the plot above as \"my_plot.pdf\", you will find the file in the working directory returned by getwd.\nIf you need to set the working working directly to something different from what it is, use the setwd function. In the Session menu in RStudio, you can also set the working directory with a point-an-click dialog box."
  },
  {
    "objectID": "tutorials/Regression/regression.html#vector-based-images",
    "href": "tutorials/Regression/regression.html#vector-based-images",
    "title": "Regression in R",
    "section": "Vector-based images",
    "text": "Vector-based images\nThe .pdf format gives the best image quality but can only be viewed in a .pdf reader. The .svg format is almost as good and can be incorporated into webpages and Office documents. One downside of their near-perfect image quality is that .pdf and .svg image file sizes can become quite large."
  },
  {
    "objectID": "tutorials/Regression/regression.html#raster-images",
    "href": "tutorials/Regression/regression.html#raster-images",
    "title": "Regression in R",
    "section": "Raster images",
    "text": "Raster images\nThe .png format gives good image quality and renders small file sizes. I prefer using the ragg::agg_png device to render a .png because it allows me to use any system font with no extra fuss.\n\nggsave(\"my_plot.png\", device = ragg::agg_png)\n\nThe primary use of the .gif format is to create animated plots. Otherwise stick with .png.\nAlthough the .jpg format is good for photos, it is terrible for plots—it often renders text and sharp corners with pixelated smudges."
  },
  {
    "objectID": "tutorials/Regression/regression.html#posterier-predictions",
    "href": "tutorials/Regression/regression.html#posterier-predictions",
    "title": "Regression in R",
    "section": "Posterier Predictions",
    "text": "Posterier Predictions\n\ndiagnostic_plots &lt;- plot(check_model(m1, panel = FALSE))\ndiagnostic_plots[[1]]\n\n\n\n\n\n\n\nFigure 5: Posterior Predictive Check\n\n\n\n\n\nThe Figure 5 generates several sets of random “simulated” data based on the model and plots the distributions as the thin blue lines. Each simulated data set is of the same size as the original data. The thicker green line is based on the observed data. If the blue lines have roughly the same shape as the green line, then the model is likely of the right form.\nThe green line is not that far off from the blue lines, but the green line appears to have 2 peaks, and most of the blue lines have one peak. Because we know that males and females have different mean heights, there is a good chance that we need to model their heights separately. We will do so later in the tutorial.\nThe tutorial for the check_model function gives an example of a posterior prediction check that signals that something is awry."
  },
  {
    "objectID": "tutorials/Regression/regression.html#linearity-assumption",
    "href": "tutorials/Regression/regression.html#linearity-assumption",
    "title": "Regression in R",
    "section": "Linearity Assumption",
    "text": "Linearity Assumption\n\ndiagnostic_plots[[2]]\n\n\n\n\n\n\n\nFigure 6: Linearity assumption\n\n\n\n\n\nIn Figure 6, we see that the fitted values (i.e., \\hat{Y} or predicted values) plotted against the residuals is roughly flat and horizontal. If the green line were clearly not flat, we would consider non-linear models."
  },
  {
    "objectID": "tutorials/Regression/regression.html#homogeneity-of-variance-assumption",
    "href": "tutorials/Regression/regression.html#homogeneity-of-variance-assumption",
    "title": "Regression in R",
    "section": "Homogeneity of Variance Assumption",
    "text": "Homogeneity of Variance Assumption\n\ndiagnostic_plots[[3]]\n\n\n\n\n\n\n\nFigure 7: Homogeneity of variance assumption\n\n\n\n\n\nFigure 7 is similar to Figure 6 except that the fitted values are standardized (i.e., converted to z-scores) and then square root of their absolute values are plotted on the Y-axis. The homogeneity of variance assumption requires that variability of the residuals should be roughly the same across the entire distribution of fitted values. Thus, the line in Figure 7 should be roughly flat and horizontal, which it is. Indeed, we can draw a horizontal line entirely within the gray confidence region around the green line."
  },
  {
    "objectID": "tutorials/Regression/regression.html#influential-observations-and-outlier-detection",
    "href": "tutorials/Regression/regression.html#influential-observations-and-outlier-detection",
    "title": "Regression in R",
    "section": "Influential Observations and Outlier Detection",
    "text": "Influential Observations and Outlier Detection\n\ndiagnostic_plots[[4]]\n\n\n\n\n\n\n\nFigure 8: Influential observations\n\n\n\n\n\nSometimes a single outlier can radically alter a regression model. The plot in Figure 8 shows how influential each point is in creating the regression model. If any point is outside the green dotted lines, we might worry that the point has had undue influence on the model.\nMany additional outlier detection methods are available via the check_outliers function:\n\ncheck_outliers(m1, method = \"all\")\n\nOK: No outliers detected.\n- Based on the following methods and thresholds: zscore_robust (3.291), iqr (2), ci (1), cook (0.706), mahalanobis (13.816), mahalanobis_robust (13.816), mcd (13.816), ics (0.001), optics (4), lof (0.001).\n- For variable: (Whole model)"
  },
  {
    "objectID": "tutorials/Regression/regression.html#normality-assumption",
    "href": "tutorials/Regression/regression.html#normality-assumption",
    "title": "Regression in R",
    "section": "Normality Assumption",
    "text": "Normality Assumption\nThe normality assumption does not require that all variables be normal. It requires that the prediction residuals be approximately normal.\n\ndiagnostic_plots[[5]]\n\n\n\n\n\n\n\nFigure 9: Normality assumption\n\n\n\n\n\nIf the residuals were perfectly normal, the blue dots would fall exactly on the green line in Figure 9. The blue dots are not that far from the green line. Because ordinary least squares is robust to minor violations of the normality assumption, nothing in Figure 9 should make us worry about needing to model the data with some other kind of residuals.\nIf we suspect that the residuals are generated from a different distribution, the check_distribution function will estimate the probability that the residuals (and response/outcome variable) come from 12 major distribution families:\n\ncheck_distribution(m1) \n\n\n\n\n\nDistribution\np_Residuals\np_Response\n\n\n\n\nbernoulli\n0.00000\n0.00000\n\n\nbeta\n0.03125\n0.00000\n\n\nbeta-binomial\n0.00000\n0.40625\n\n\nbinomial\n0.00000\n0.40625\n\n\ncauchy\n0.09375\n0.03125\n\n\nchi\n0.03125\n0.03125\n\n\nexponential\n0.00000\n0.00000\n\n\nF\n0.00000\n0.00000\n\n\ngamma\n0.00000\n0.00000\n\n\nhalf-cauchy\n0.00000\n0.00000\n\n\ninverse-gamma\n0.00000\n0.00000\n\n\nlognormal\n0.00000\n0.00000\n\n\nneg. binomial (zero-infl.)\n0.00000\n0.03125\n\n\nnegative binomial\n0.06250\n0.00000\n\n\nnormal\n0.59375\n0.03125\n\n\npareto\n0.03125\n0.00000\n\n\npoisson\n0.00000\n0.00000\n\n\npoisson (zero-infl.)\n0.00000\n0.03125\n\n\ntweedie\n0.15625\n0.00000\n\n\nuniform\n0.00000\n0.00000\n\n\nweibull\n0.00000\n0.03125\n\n\n\n\n\nplot(check_distribution(m1))\n\n\n\n\n\n\n\n\nHere we see that of the 12 major distribution families tested, the residuals are more likely normal than any of the others. After we control for gender, this probability will increase. If the results had strongly suggested the residuals were generated from one of the other distributions, we might consider switching from a linear model to a generalized linear model."
  },
  {
    "objectID": "tutorials/Regression/regression.html#model-level-statistics",
    "href": "tutorials/Regression/regression.html#model-level-statistics",
    "title": "Regression in R",
    "section": "Model-level statistics",
    "text": "Model-level statistics\nSome statistics like the coefficient of determination (R2) or the standard error of the estimate (σe) describe the model as a whole.\n\nR2\nThe R2 statistic measures the percentage of variance in the outcome explained by the predictors.\n\nR^2=\\frac{\\sigma_{\\hat{Y}}^2}{\\sigma_{Y}^2}=1-\\frac{\\sigma_e^2}{\\sigma_Y^2}\n\nThe model-level statistics can be extracted with the performance package’s model_performance function.\n\nperformance(m1)\n\n\n\n\n\nAIC\nAICc\nBIC\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\n196\n197\n201\n0.641\n0.631\n2.61\n2.68\n\n\n\n\n\n\nR2 is the percentage of variance in the outcome variable (i.e., Height) explained by the predictor variables (i.e., Average Parent Height). In Figure 10, the black line toggles between the best fitting regression line and the horizontal line running through the outcome variable’s mean. The vertical line segments running from each point to the black line are, on average, smaller for the best fitting regression line. Their squared distances shrink, on average, by 64%. Thus, we say that the predictor variance “explains” 64% of the variance in the outcome variable.\n\n\nCode\nr &lt;- d %&gt;% select(avgphgt, height) %&gt;% cor() %&gt;% `[[`(1, 2)\nm &lt;- d %&gt;% select(avgphgt, height) %&gt;% colMeans()\ns &lt;- d %&gt;% select(avgphgt, height) %&gt;% sapply(sd)\nn &lt;- 2\nd_slope &lt;- tibble(id = c(\"0.00\", round(r^2, 2)),\n                  r = seq(0, r, length.out = n)) %&gt;%\n  mutate(i = -1 * r * m[1] * s[2] / s[1] + m[2], \n         b = r * s[2] / s[1])\nlibrary(gganimate)\nanim &lt;- d %&gt;% select(idnum, avgphgt, height) %&gt;%\n  crossing(d_slope) %&gt;%\n  mutate(yhat = b * avgphgt + i, \n         Error = height - yhat) %&gt;%\n  arrange(idnum, Error) %&gt;%\n  ggplot(aes(avgphgt, height)) +\n  geom_abline(aes(slope = b, intercept = i)) +\n  geom_segment(aes(xend = avgphgt, yend = yhat, color = Error)) +\n  geom_point() +\n  transition_states(id, state_length = 3, transition_length = 1) +\n  scale_color_gradient2(\n    low = \"royalblue4\",\n    mid = \"gray\",\n    high = \"firebrick4\",\n    breaks = seq(-6, 6, 2),\n    labels = signs::signs\n  ) +\n  scale_x_continuous(\"Average Parent Height (inches)\", limits = c(60, 75)) +\n  scale_y_continuous(\"Student Height (inches)\", limits = c(60, 75)) +\n  labs(title = \"Variance Explained = {closest_state}\") +\n  coord_equal() +\n  theme_minimal(base_family = \"Roboto Condensed\", base_size = 18) +\n  theme(\n    legend.text.position = \"left\",\n    legend.text = element_text(hjust = 1),\n    legend.key.height = unit(2, \"cm\")\n  )\ngganimate::animate(\n  anim,\n  nframes = 50 * 4,\n  device = \"ragg_png\",\n  width = 10,\n  height = 10,\n  fps = 50,\n  res = 144\n)\n\n\n\n\n\n\n\n\nFigure 10: Percentage of Variance Explained\n\n\n\n\n\nIf all you wanted was the R2, you could do this:\n\nperformance(m1)$R2\n\n[1] 0.641\n\n\nWhy would you want just one number instead of reading it from a table? In reproducible research, we intermingle text and code so that it is clear where every number came from. Thus, “hard-coding” your results like this is considered poor practice:\nThe model explains 64% of the variance.\nUsing rmarkdown, instead of typing the numeric results, we type pull the results using an inline code chunk:\nThe model explains `{r} round(100 * r2(m1)$R2, 0)`% of the variance.\nWhich, when rendered, produces the correct output:\n\nThe model explains 64% of the variance.\n\nThat seems like a lot of extra work, right? Yes, it is—unless there is a possibility that your underlying data might change or that you might copy your numbers incorrectly. If you are imperfect, the extra time and effort is worth it. It makes it easy for other scholars to see exactly where each number came from. Hard-coded results are harder to trust.\nRegression errors are the vertical distances of the outcome variable and regression line. That is, errors are the difference between the outcome and the predicted outcome:\n\ne=Y-\\hat{Y}\n\n\n\nStandard Error of the Estimate (σe)\nThe standard error of the estimate (i.e., Sigma in the performance output table) is the standard deviation of the regression errors \\sigma_{e}. It represents the typical size of the prediction error. That is, when we make a prediction, how far off is that prediction likely to be?\nThe standard error of the estimate can be extracted from a regression fit object with the sigma function:\n\nsigma(m1)\n\n[1] 2.68\n\n\nAlternately, it can be extracted from the performance function’s output:\n\nperformance(m1)$Sigma\n\n[1] 2.681034"
  },
  {
    "objectID": "tutorials/Regression/regression.html#coefficient-level-statistics",
    "href": "tutorials/Regression/regression.html#coefficient-level-statistics",
    "title": "Regression in R",
    "section": "Coefficient-level statistics",
    "text": "Coefficient-level statistics\nThe regression coefficients—the intercept (b0) and the slope (b1)—have a number of statistics associated with them, which we will discuss later in the course.\nIf you just wanted the intercept and the slope coefficients, use the coef function:\n\ncoef(m1)\n\n(Intercept)     avgphgt \n     -12.88        1.19 \n\n\nThus, the intercept is -12.88 and the slope is 1.19.\nTo get the model parameters (intercept and slope coefficients) along with their standard errors, confidence intervals, t statistics, and p-values:\n\nparameters(m1) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n-12.88\n9.779\n0.95\n-32.675\n6.92\n-1.32\n38\n0.196\n\n\navgphgt\n1.19\n0.145\n0.95\n0.899\n1.49\n8.23\n38\n0.000\n\n\n\n\n\n\nStandardized parameters are the regression coefficients if all variables in the analysis were standardized (i.e., convernted to z-scores). Standardized coefficients have a straightforward interpretation: They represent the predicted change in the outcome associated with a change of 1 standard deviation in the predictor variable.\n\nstandardize_parameters(m1)\n\n\n\n\n\nParameter\nStd_Coefficient\nCI\nCI_low\nCI_high\n\n\n\n\n(Intercept)\n0.0\n0.95\n-0.194\n0.194\n\n\navgphgt\n0.8\n0.95\n0.603\n0.997"
  },
  {
    "objectID": "tutorials/Regression/regression.html#checking-assumptions-1",
    "href": "tutorials/Regression/regression.html#checking-assumptions-1",
    "title": "Regression in R",
    "section": "Checking assumptions",
    "text": "Checking assumptions\n\ncheck_model(m2)\n\n\n\n\n\n\n\nFigure 11: Assumption check for model 2\n\n\n\n\n\nThe diagnostic plots in Figure 11 looks good. It has one additional plot checking the collinearity of the predictors. If any predictor is strongly predictable from the other predictors, it is difficult for ordinary least squares regression to locate the regression coefficients with precision. Why? Because the regression coefficients represent the independent effect of each variable controlling for all the other variables. If a predictor has little variability left after controlling for the other variables, estimating its independent effect is difficult, which is manifest in “inflated” standardized errors around the coefficient. The VIF (variance inflation factor) statistic estimates how much the standard errors are made large due to collinearity. If the VIF is larger than 10 or so, it is likely that the estimate of the coefficient is not precise."
  },
  {
    "objectID": "tutorials/Regression/regression.html#summarizing-results",
    "href": "tutorials/Regression/regression.html#summarizing-results",
    "title": "Regression in R",
    "section": "Summarizing results",
    "text": "Summarizing results\nTo summarize the results, use the summary function:\n\nsummary(m2)\n\n\nCall:\nlm(formula = height ~ avgphgt + gender, data = d)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.826 -1.094 -0.347  1.656  4.503 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   20.099     10.304    1.95  0.05870 .  \navgphgt        0.671      0.157    4.26  0.00013 ***\ngendermale     4.468      0.920    4.85  2.2e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.12 on 37 degrees of freedom\nMultiple R-squared:  0.78,  Adjusted R-squared:  0.769 \nF-statistic: 65.7 on 2 and 37 DF,  p-value: 6.6e-13\n\n\nModel-level statistics:\n\nperformance(m2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAIC\nAICc\nBIC\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\n178.6511\n179.7939\n185.4066\n0.780413\n0.7685434\n2.042551\n2.123744\n\n\n\n\n\n\nCoefficient-level statistics:\n\nparameters(m2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n20.0985997\n10.3035446\n0.95\n-0.7783648\n40.9755641\n1.950649\n37\n0.0587039\n\n\navgphgt\n0.6705107\n0.1573791\n0.95\n0.3516303\n0.9893911\n4.260480\n37\n0.0001345\n\n\ngendermale\n4.4679572\n0.9204998\n0.95\n2.6028475\n6.3330669\n4.853838\n37\n0.0000221\n\n\n\n\n\n\nStandardized coefficients:\n\nstandardize_parameters(m2)\n\n\n\n\n\nParameter\nStd_Coefficient\nCI\nCI_low\nCI_high\n\n\n\n\n(Intercept)\n-0.506\n0.95\n-0.768\n-0.245\n\n\navgphgt\n0.450\n0.95\n0.236\n0.664\n\n\ngendermale\n1.012\n0.95\n0.590\n1.435\n\n\n\n\n\n\nAn automated report:\n\nreport(m2)\n\nWe fitted a linear model (estimated using OLS) to predict height with avgphgt\nand gender (formula: height ~ avgphgt + gender). The model explains a\nstatistically significant and substantial proportion of variance (R2 = 0.78,\nF(2, 37) = 65.75, p &lt; .001, adj. R2 = 0.77). The model's intercept,\ncorresponding to avgphgt = 0 and gender = female, is at 20.10 (95% CI [-0.78,\n40.98], t(37) = 1.95, p = 0.059). Within this model:\n\n  - The effect of avgphgt is statistically significant and positive (beta = 0.67,\n95% CI [0.35, 0.99], t(37) = 4.26, p &lt; .001; Std. beta = 0.45, 95% CI [0.24,\n0.66])\n  - The effect of gender [male] is statistically significant and positive (beta =\n4.47, 95% CI [2.60, 6.33], t(37) = 4.85, p &lt; .001; Std. beta = 1.01, 95% CI\n[0.59, 1.43])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "tutorials/Regression/regression.html#comparing-models",
    "href": "tutorials/Regression/regression.html#comparing-models",
    "title": "Regression in R",
    "section": "Comparing models",
    "text": "Comparing models\nIn the first model, there was only one predictor. The second model had an additional predictor. To test whether the second predictor has incremental validity (predicts variance in the outcome beyond what is predicted by other predictors), we can compare the two models using the Wald test:\n\ntest_wald(m1, m2)\n\n\n\n\n\nName\nModel\ndf\ndf_diff\nF\np\n\n\n\n\nm1\nlm\n38\n\n\n\n\n\nm2\nlm\n37\n1\n23.6\n0\n\n\n\n\n\n\nThe Wald test can also be conducted in base R with the anova function:\n\nanova(m1, m2)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n38\n273.1419\nNA\nNA\nNA\nNA\n\n\n37\n166.8806\n1\n106.2612\n23.55975\n2.21e-05\n\n\n\n\n\n\nEither way, the p-value is significant, meaning that m2 explains more variance than m1.\nThe semi-partial correlation coefficient squared tells us how much incremental variance each predictor has over the other:\n\nr2_semipartial(m2)\n\n\n\n\n\nTerm\nr2_semipartial\nCI\nCI_low\nCI_high\n\n\n\n\navgphgt\n0.1077264\n0.95\n0.0192805\n1\n\n\ngender\n0.1398220\n0.95\n0.0371881\n1\n\n\n\n\n\n\nThus, gender explains 14% variance beyond avgphgt. Likewise, avgphgt explains 11% variance beyond gender.\n\nBayes Factors\nAn alternate method of comparing two models uses the Bayes factor. It tells us under which model the observed data are more probable.\n\ntest_bf(m1, m2)\n\n\n\n\n\nModel\nlog_BF\nBF\n\n\n\n\navgphgt\n\n\n\n\navgphgt + gender\n8.01\n3010\n\n\n\n\n\n\nA BF &gt; 1 means that m2 is more strongly supported than m1. A BF &lt; 1 means that m1 is more strongly supported than m2. If you are not sure how to interpret the output of test_bf, you can get an automated interpretation:\n\ntest_bf(m1, m2) %&gt;% \n  report()\n\nBayes factors were computed using the BIC approximation, by which BF10 =\nexp((BIC0 - BIC1)/2). Compared to the avgphgt model, we found extreme evidence\n(BF = 3.01e+03) in favour of the avgphgt + gender model (the least supported\nmodel).\n\n\nTo compare many performance statistics at once:\n\ncompare_performance(m1, m2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nAIC\nAIC_wt\nAICc\nAICc_wt\nBIC\nBIC_wt\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\nm1\nlm\n196\n0\n197\n0\n201\n0\n0.641\n0.631\n2.61\n2.68\n\n\nm2\nlm\n179\n1\n180\n1\n185\n1\n0.780\n0.769\n2.04\n2.12"
  },
  {
    "objectID": "tutorials/Regression/regression.html#question-1",
    "href": "tutorials/Regression/regression.html#question-1",
    "title": "Regression in R",
    "section": "Question 1",
    "text": "Question 1\nIs read_1 a significant predictor of read_2?\n\n\n\n\n\n\nNoteHint 1\n\n\n\n\n\nMake model fit object called m_read using the lm function.\n\nm_read &lt;- lm(read_2 ~ read_1, data = d_learn)\n\n\n\n\n\n\n\n\n\n\nNoteHint 2\n\n\n\n\n\nView coefficient-level statistics with the parameters function.\nYou could also use Base R’s summary function.\n\nparameters(m_read)\n\n# or\n\nsummary(m_read)\n\n\n\n\n\n\n\n\n\n\nNoteHint 3\n\n\n\n\n\nIn the read_1 row, is the p.value column less than 0.05?\n\nparameters(m_read)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n83.591\n10.922\n0.95\n61.44\n105.741\n7.65\n36\n0.000\n\n\nread_1\n0.251\n0.104\n0.95\n0.04\n0.463\n2.41\n36\n0.021"
  },
  {
    "objectID": "tutorials/Regression/regression.html#question-2",
    "href": "tutorials/Regression/regression.html#question-2",
    "title": "Regression in R",
    "section": "Question 2",
    "text": "Question 2\nWhat is the R2 for the m_read model?\n\n\n\n\n\n\nNoteHint\n\n\n\n\n\nView model-level statistics with the performance function.\nYou could also use Base R’s summary function.\n\nperformance(m_read)\n\n# or\n\nsummary(m_read)"
  },
  {
    "objectID": "tutorials/Regression/regression.html#question-3",
    "href": "tutorials/Regression/regression.html#question-3",
    "title": "Regression in R",
    "section": "Question 3",
    "text": "Question 3\nWhat does the scatter plot look like when read_1 is on the x-axis and read_2 is on the y-axis? Also plot the regression line. Save your plot using the ggsave function.\n\n\n\n\n\n\nNoteHint for Getting Started\n\n\n\n\n\nThis will get you started\n\nggplot(d_learn, aes(read_1, read_2))\n\n\n\n\n\n\n\n\n\n\nNoteHint for Adding Points\n\n\n\n\n\nHere is how you add points.\n\nggplot(d_learn, aes(read_1, read_2)) +\n  geom_point() \n\n\n\n\n\n\n\n\n\n\nNoteHint for Adding a Regression Line\n\n\n\n\n\nHere is how you add a regression line.\n\nggplot(d_learn, aes(read_1, read_2)) +\n  geom_point() + \n  geom_smooth(method = \"lm\") \n\n\n\n\n\n\n\n\n\n\nNotePlot Polishing\n\n\n\n\n\nThis is overkill for now. But someday you might want to be able to make your plots as polished as possible.\n\n# I want to put the equation at x = 130\nequation_x &lt;- 130\nequation_y &lt;- predict(m_read, newdata = tibble(read_1 = equation_x))\n\n# Extracting the coefficients\nb_read &lt;- round(coef(m_read),2)\n\n# The angle of the regression line is the inverse tangent of the slope (converted to degrees)\neq_angle &lt;- atan(b_read[2]) * 180 / pi\n\n# Equation\neq_read &lt;- paste0(\"italic(Y) == \", \n                  b_read[1], \n                  \" + \", \n                  b_read[2], \n                  \" *  italic(X) + italic(e)\")\n\nggplot(d_learn, aes(read_1, read_2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Reading at Time 1\",\n       y = \"Reading at Time 2\",\n       title = \"Using Reading at Time 1 to Predict Reading at Time 2\") +\n  coord_fixed() +\n  theme_minimal() +\n  annotate(\n    geom = \"text\",\n    x = equation_x,\n    y = equation_y,\n    angle = eq_angle,\n    label = eq_read,\n    parse = TRUE,\n    vjust = -0.5)"
  },
  {
    "objectID": "tutorials/Regression/regression.html#question-4",
    "href": "tutorials/Regression/regression.html#question-4",
    "title": "Regression in R",
    "section": "Question 4",
    "text": "Question 4\nCreate a regression model in which you predict math at time 2 (math_2) using reading at time 1 (math_1). Call the model fit object m_math1.\nDoes math_1 predict math_2?.\nDoes math_1 still predict math_2 after controlling for read_1?\n\n\n\n\n\n\nNoteHint\n\n\n\n\n\n\nm_math &lt;- lm(math_2 ~ math_1 + read_1, data = d_learn)\nsummary(m_math)"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Getting Ready to Use R and RStudio\n\n\n\n\n\nTips for Installation and Configuration\n\n\n\n\n\nJan 20, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling\n\n\n\n\n\nGetting Data into Useable Formats\n\n\n\n\n\nMar 1, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA in R\n\n\n\n\n\nPlot group means and test to see if their differences are statistically significant.\n\n\n\n\n\nMar 5, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRegression in R\n\n\n\n\n\nA step-by-step introduction to regresion in R\n\n\n\n\n\nMar 7, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Factor Analysis with R\n\n\n\n\n\nA step-by-step introduction to exploratory factor analysis in R\n\n\n\n\n\nMar 12, 2024\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Intercept Models in R\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nRegression in which each group has its own intercept\n\n\n\n\n\nJan 27, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Slopes\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nRegression in which each group has its own slope for some predictors\n\n\n\n\n\nFeb 6, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nMultilevel Models with Three Levels\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nRegression in which there are groups within groups\n\n\n\n\n\nFeb 11, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Linear Mixed Models\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nMultilevel models in which the residuals are not normally distributed\n\n\n\n\n\nFeb 18, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nData Simulation for Multilevel Models\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nSimulating data that conforms to a specified model. Useful for power analysis and other kinds of “what-if” thought experiments.\n\n\n\n\n\nFeb 25, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nMultilevel Modeling of Longitudinal Data in R\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nMultilevel models about how and why variables change over time\n\n\n\n\n\nMar 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLatent Growth Models\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nUsing structural equations to understand change over time\n\n\n\n\n\nMar 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBivariate Change Models\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nHow two variables influence each other over time\n\n\n\n\n\nMar 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPower Analysis with Multilevel Models\n\n\nAdvanced Data Analysis (EDUC 8825)\n\n\nWhat sample size is needed to reliably find effects in hypothesized models?\n\n\n\n\n\nApr 1, 2025\n\n\nW. Joel Schneider\n\n\n\n\n\n\n\n\n\n\n\n\nDistributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasurement Scales\n\n\nEDUC 5325 Introduction to Statistics and Research\n\n\n\n\n\n\n\n\nW. Joel Schneider\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html",
    "href": "tutorials/StatsIntro/MeasurementScales.html",
    "title": "Measurement Scales",
    "section": "",
    "text": "In science and mathematics, there are fundamental constants—things that always remain the same (e.g., c the speed of light in a vacuum, or \\pi, the ratio of a circle’s circumference to its diameter). However, most phenomena we study are capable of change. Indeed, must of scientific investigation consists of discovering how change in one process affects changes in other processes. Any process or quantity that can change is called a variable.\n\n\nA constant is a state or quantity that remains the same.\nA variable is an abstract category of information that can assume different values.\n\n\nAn algebraic variable usually takes on any value, but it can be constrained by equations and other kinds of relationships. Consider this equation:\n2x-6=0 \\tag{1}\nIn isolation, the variable x could be any real number. However, once inserted in an equation, x is constrained in some way. In Equation 1, x must equal 3.\n\n\n\nRandom variables are not like algebraic variables. The values that random variables can take on are determined by one or more random processes. For example, the outcome of a coin toss is a random variable. It can either be heads or tails. Think of a random variable as forever spitting out new values. In Figure 1, the random variable is an endless cascade of coins.\n\n\n\n\n\n\n\n\nFigure 1: An endless sequence of fair coin tosses is a random variable.\n\n\n\nIn this case, the set of possible values {heads,tails} is the sample space of the outcome of a coin toss. A random variable's sample space is the set of all possible values that the variable can assume. In the case of the roll of a six-sided die, the sample space is {1,2,3,4,5,6}. This sample space has a finite number of values. However, some sample spaces are infinite. For example, the number of siblings one can have is the set of all non-negative integers {0,1,2,3,...}. Of course, because there are limits as to how many children people can have, the probability of very high numbers is very low. A different kind of infinity is observed in the sample space of variables that are continuous. The length of one's foot in centimeters can be any positive real number. Between any two real numbers, (e.g., 20cm and 30cm) there is an infinite number of possibilities because real numbers can be sliced as thinly as needed (e.g., 28.465570098756642111cm)."
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#algebraic-variables",
    "href": "tutorials/StatsIntro/MeasurementScales.html#algebraic-variables",
    "title": "Measurement Scales",
    "section": "",
    "text": "An algebraic variable usually takes on any value, but it can be constrained by equations and other kinds of relationships. Consider this equation:\n2x-6=0 \\tag{1}\nIn isolation, the variable x could be any real number. However, once inserted in an equation, x is constrained in some way. In Equation 1, x must equal 3."
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#random-variables",
    "href": "tutorials/StatsIntro/MeasurementScales.html#random-variables",
    "title": "Measurement Scales",
    "section": "",
    "text": "Random variables are not like algebraic variables. The values that random variables can take on are determined by one or more random processes. For example, the outcome of a coin toss is a random variable. It can either be heads or tails. Think of a random variable as forever spitting out new values. In Figure 1, the random variable is an endless cascade of coins.\n\n\n\n\n\n\n\n\nFigure 1: An endless sequence of fair coin tosses is a random variable.\n\n\n\nIn this case, the set of possible values {heads,tails} is the sample space of the outcome of a coin toss. A random variable's sample space is the set of all possible values that the variable can assume. In the case of the roll of a six-sided die, the sample space is {1,2,3,4,5,6}. This sample space has a finite number of values. However, some sample spaces are infinite. For example, the number of siblings one can have is the set of all non-negative integers {0,1,2,3,...}. Of course, because there are limits as to how many children people can have, the probability of very high numbers is very low. A different kind of infinity is observed in the sample space of variables that are continuous. The length of one's foot in centimeters can be any positive real number. Between any two real numbers, (e.g., 20cm and 30cm) there is an infinite number of possibilities because real numbers can be sliced as thinly as needed (e.g., 28.465570098756642111cm)."
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#nominal-variables",
    "href": "tutorials/StatsIntro/MeasurementScales.html#nominal-variables",
    "title": "Measurement Scales",
    "section": "Nominal Variables",
    "text": "Nominal Variables\nNominal variables can take on any kind of value, including values that are not numbers. The values must constitute a set of mutually exclusive categories. For example, if I have a set of data about college students, I might record which major each person has. The variable college major consists of different labels (e.g., Accounting, Mathematics, and Psychology). Note that there is no true order to college majors, though we usually alphabetize them for convenience. There is no meaningful sense in which English majors are higher or lower than Biology majors. Nominal values are either the same or they are different. They are not less than or more than anything else.\n\n\nIn a set of mutually exclusive categories, nothing belongs to more than one of the categories in the set at the same time.\n\nExamples of nominal variables\n\nBiological sex {male, female}\nRace/Ethnicity {African-American, Asian-American,...}\nType of school {public, private}\nTreatment group {Untreated, Treated}\nDown Syndrome {present, not present}\nAttachment Style {dismissive-avoidant, anxious-preoccupied, secure}\nWhich emotion are you feeling right now? {Happiness, Sadness, Anger, Fear}"
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#ordinal-variables",
    "href": "tutorials/StatsIntro/MeasurementScales.html#ordinal-variables",
    "title": "Measurement Scales",
    "section": "Ordinal Variables",
    "text": "Ordinal Variables\nLike nominal variables, ordinal variables are categorical. Whereas the categories in nominal variables have no meaningful order, the categories in ordinal variables have a natural order. For example, questionnaires often ask multiple-choice questions like so:\nI like chatting with people I do not know.\n\nStrongly disagree\nDisagree\nNeutral\nAgree\nStrongly agree\n\nIt is clear that the response choices have an order that would generate no controversy. Note, however, that there is no meaningful distance between the categories. Is the distance between strongly disagree and disagree the same as the distance between disagree and neutral? It is not a meaningful question because no distance as been defined. All we can do is say is which category is higher than the other.\n\nExamples of ordinal variables\n\nDosage {placebo, low dose, high dose}\nOrder of finishing a race {1st place, 2nd place, 3rd place,…}\nISAT category {below standards, meets standards, exceeds standards}\nApgar score {0,1,…,10}"
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#interval-scales",
    "href": "tutorials/StatsIntro/MeasurementScales.html#interval-scales",
    "title": "Measurement Scales",
    "section": "Interval scales",
    "text": "Interval scales\nInterval scales are quantitative. The values that interval scales take on are almost always numbers. Furthermore, the distance between the numbers have a consistent meaning. The classic example of an interval scale is temperature on the Celsius or Fahrenheit scale. The distance between 25° and 35° is 10°. The distance between 90° and 100° is also 10°. In both cases, the difference involves the same amount of heat.\nUnlike with nominal and ordinal scales, we can add and subtract scores on an interval scale because there are meaningful distances between the numbers.\nInterestingly, the meaning of 0°C (or 0°F) is not what we are used to thinking about when we encounter the number zero. Usually, the number zero means the absence of something. Unfortunately, the number zero does not have this meaning in interval scales. When something has a temperature of 0°C, it does not mean that there is no heat. It just happens to be the temperature at which water freezes at sea level. It can get much, much colder. Thus, interval scales lack a true zero in which zero indicates a complete absence of something.\n\n\nA true zero indicates the absence of the quantity being measured.\nLacking a true zero, interval scales cannot be used to create meaningful ratios. For example, 20°C is not “twice as hot” as 10°C. Also, 110°F is not “10% hotter” than 100°F.\n\nNearly interval scales\nIn truth, there are very, very few examples of variables with a true interval scale. However, a large percentage of variables used in the social sciences are treated as if they are interval scales. It turns out that with a bit of fancy math, many ordinal variables can be transformed, weighted, and summed in such a way that the resulting score is reasonably close to having interval properties. The advantage of doing this is that, unlike with nominal and ordinal scales, you can calculate means, standard deviations, and a host of other statistics that depend on there being meaningful distances between numbers.\nPsychological and educational measures regularly make use of these procedures. For example, on tests like the ACT, we take information about which questions were answered correctly and then transform the scores into a scale that ranges from 1 to 36. As a group, people who score a higher on the ACT tend to perform better in college than people who score lower. Of course, many individuals perform much better than their ACT scores suggest. An equal number of individuals perform much worse than their ACT scores suggest. Among many other things, thirst for knowledge and hard work matter quite a bit. Even so, on average, individuals with a 10 on the ACT are likely to perform worse in college than people with a 20. Roughly by the same amount, people with a 30 on the ACT are likely to perform better in college than people with a 20. Again, we talking about averages, not individuals. Every day, some people beat expectations and some people fail to meet them, often by wide margins.\n\n\nExamples of interval scales\n\nTruly interval:\nTemperature on the Celsius and Fahrenheit scale (not on the Kelvin scale)\nCalendar year (e.g., 431BC, 1066AD)\nNotes on an even-tempered instrument such as a piano {A, A#, B, C, C#, D, D#, E, F, F#, G, G#}\nA ratio scale converted to a z-score metric (or any other kind of standard score metric)\nNearly interval:\nMost scores from well-constructed ability tests (e.g., IQ, ACT, GRE) and personality measures (e.g., self-esteem, extroversion).\n\n\n\nA z-score metric has a mean of 0 and a standard deviation of 1. Thus, the zero indicates the mean of the variable, not the absence of the quantity."
  },
  {
    "objectID": "tutorials/StatsIntro/MeasurementScales.html#ratio-scales",
    "href": "tutorials/StatsIntro/MeasurementScales.html#ratio-scales",
    "title": "Measurement Scales",
    "section": "Ratio Scales",
    "text": "Ratio Scales\nA ratio scale has all of the properties of an interval scale. In addition, it has a true zero. When a ratio scale has a value of zero, it indicates the absence of the quantity being measured. For example, if I say that I have 0 coins in my pocket, there are no coins in my pocket. The fact that ratio scales have true zeroes means that ratios are meaningful. For example, if you have 2 coins and I have one, you have twice as many coins as I do. If I have 100 coins and then you give me 10 more, the number of coins I have has increased by 10%.\n\nExamples of Ratio Scales\nRatio scales involve countable quantities, such as:\n\ncoins\nmarbles\ncomputers\nspeeding tickets\npregnancies\nsoldiers\nplanets\n\nCountable quantities are discrete variables like the scale in Figure 3. Countable integers are discrete because they have particular values but not values between those values.\n\n\nA discrete variable can only take on exact, isolated values from a specified list.\nMany physical properties are also ratio scales, such as:\n\ndistance\nmass\nforce\nheat (on the Kelvin scale)\npressure\nvoltage\nacceleration\nproportions\n\nThese dimensions are not discrete countable quantities like cars and bricks but are instead continuous quantities that can be measured with decimals and fractions.\n\n\n\n\n\n\nFigure 3: A continuous scale from 1 to 10 can have any real value between and including 1 and 10. A discrete scale from 1 to 10 can only take on the values shown.\n\n\n\nNotice that even though ratio variables have a true zero, on some of them it is possible to have negative numbers. For example, negative acceleration would indicate a slowing down. A negative value in a checking account means that you owe the bank money.\nIn the social sciences, there are many examples of ratio scales:\n\nIncome\nAge\nYears of education\nReaction time\nFamily size\nHours of study\nPercentage of household chores completed (compared to other members of the household)"
  },
  {
    "objectID": "tutorials/StatsIntro/Distributions.html",
    "href": "tutorials/StatsIntro/Distributions.html",
    "title": "Distributions",
    "section": "",
    "text": "The statistical program R is a free, open-source project that is used by millions of people all over the world. It is so good that I would prefer it over other programs even if it were not free. The fact that it will always be free is simply amazing.\nThe downside of R is that it can be intimidating for beginners. It can be finicky in ways that can frustrate even advanced users. Fortunately, R can be extended and adapted by anyone (including you). These extensions are called packages. The program we use, Jamovi, is built atop R, and has a number of R packages associated with it to make Jamovi a free and easy-to-use alternative to commercial statistical programs that can be quite costly. Think of Jamovi as an interface that makes R much easier way to interact with R. In fact, to use Jamovi, you do not need to know anything about R at all. However, if you need Jamovi to do something that it does not do out of the box, you can ask Jamovi to ask R to do it."
  },
  {
    "objectID": "tutorials/StatsIntro/Distributions.html#making-frequency-distribution-tables-with-jamovi",
    "href": "tutorials/StatsIntro/Distributions.html#making-frequency-distribution-tables-with-jamovi",
    "title": "Distributions",
    "section": "Making Frequency Distribution Tables with Jamovi",
    "text": "Making Frequency Distribution Tables with Jamovi\nIn the menu, select Analyses→Exploration→Descriptions. That is, select the Analyses tab, then the Exploration icon, then the Descriptives menu item.\n\nNow drag the Quiz1 variable into the Variables box. Alternately, you can select Quiz1 and then click the arrow next to the Variables box.\nCheck the Frequency tables checkbox.\n\nIn the Results pane, you should see Descriptives with some summary statistics for Quiz1 and a Frequencies table for Quiz1\n\nThe Levels are the values the variables has. The Counts are the frequencies of the each level. The % of Total column is the Counts divided by the sample size. The Cumulative % column tells the percentage of scores at that level or less."
  },
  {
    "objectID": "tutorials/StatsIntro/Distributions.html#bar-plots",
    "href": "tutorials/StatsIntro/Distributions.html#bar-plots",
    "title": "Distributions",
    "section": "Bar Plots",
    "text": "Bar Plots\nTo display the distribution of a categorical variable one should use a bar plot. These are used most often to display the distribution of subjects or cases in certain categories, such as the number of A, B, C, D, and F grades in a given class.\nLet’s start with looking at the distribution of ethnicity in our data file. So what our graph will show are the counts (or frequency) for each of ethnic category.\n\nFrom the menu click Analyses→Exploration→Descriptives.\nDrag the Ethnicity variable into the Variables box.\nClick Plots\nCheck Bar plot.\n\nYou should get a bar chart that looks something like this.\n\nMake a bar graph of the counts of the final grades (called FinalClassGrade in the file) in the class (i.e. A, B, C,…).\nMake a bar graph of the counts of the final grades in the class (i.e. A, B, C,…), further broken down by whether they attended the review session or not.\nDrag the Review variable into the SplitBy box."
  },
  {
    "objectID": "tutorials/StatsIntro/Distributions.html#histograms",
    "href": "tutorials/StatsIntro/Distributions.html#histograms",
    "title": "Distributions",
    "section": "Histograms",
    "text": "Histograms\nSuppose that we wish to know how the students did on quiz 1. We could try looking at all of the scores, but that’s a lot of numbers. Instead, it is better to try to look at the entire distribution, rather than all of the individual scores. We should use a histogram because our variable (score on Quiz1) is a continuous variable.\nHistogram: A histogram is a pictorial representation of the distribution of values for a particular variable. The bars represent the number of occurrences of each value. These look similar to bar graphs except they are used more often to indicate the number of subjects or cases in ranges of values for a continuous variable, such as the number of subjects or cases in ranges of values for a continuous variable."
  },
  {
    "objectID": "tutorials/StatsIntro/Distributions.html#using-jamovi-to-create-a-histogram",
    "href": "tutorials/StatsIntro/Distributions.html#using-jamovi-to-create-a-histogram",
    "title": "Distributions",
    "section": "Using Jamovi to create a histogram:",
    "text": "Using Jamovi to create a histogram:\n\nFrom the menu click Analyses→Exploration→Descriptives.\nDrag the TotalPercent variable into the Variables box.\nClick Plots\nCheck Histogram."
  }
]